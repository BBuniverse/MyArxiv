{"2025-10-24T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2510.21704v1","updated":"2025-10-24T17:59:02Z","published":"2025-10-24T17:59:02Z","title":"Automated Detection of Visual Attribute Reliance with a Self-Reflective\n  Agent","summary":"  When a vision model performs image recognition, which visual attributes drive\nits predictions? Detecting unintended reliance on specific visual features is\ncritical for ensuring model robustness, preventing overfitting, and avoiding\nspurious correlations. We introduce an automated framework for detecting such\ndependencies in trained vision models. At the core of our method is a\nself-reflective agent that systematically generates and tests hypotheses about\nvisual attributes that a model may rely on. This process is iterative: the\nagent refines its hypotheses based on experimental outcomes and uses a\nself-evaluation protocol to assess whether its findings accurately explain\nmodel behavior. When inconsistencies arise, the agent self-reflects over its\nfindings and triggers a new cycle of experimentation. We evaluate our approach\non a novel benchmark of 130 models designed to exhibit diverse visual attribute\ndependencies across 18 categories. Our results show that the agent's\nperformance consistently improves with self-reflection, with a significant\nperformance increase over non-reflective baselines. We further demonstrate that\nthe agent identifies real-world visual attribute dependencies in\nstate-of-the-art models, including CLIP's vision encoder and the YOLOv8 object\ndetector.\n","authors":["Christy Li","Josep Lopez Camuñas","Jake Thomas Touchet","Jacob Andreas","Agata Lapedriza","Antonio Torralba","Tamar Rott Shaham"],"pdf_url":"https://arxiv.org/pdf/2510.21704v1.pdf","comment":"32 pages, 10 figures, Neurips 2025"},{"id":"http://arxiv.org/abs/2510.21697v1","updated":"2025-10-24T17:57:31Z","published":"2025-10-24T17:57:31Z","title":"Visual Diffusion Models are Geometric Solvers","summary":"  In this paper we show that visual diffusion models can serve as effective\ngeometric solvers: they can directly reason about geometric problems by working\nin pixel space. We first demonstrate this on the Inscribed Square Problem, a\nlong-standing problem in geometry that asks whether every Jordan curve contains\nfour points forming a square. We then extend the approach to two other\nwell-known hard geometric problems: the Steiner Tree Problem and the Simple\nPolygon Problem.\n  Our method treats each problem instance as an image and trains a standard\nvisual diffusion model that transforms Gaussian noise into an image\nrepresenting a valid approximate solution that closely matches the exact one.\nThe model learns to transform noisy geometric structures into correct\nconfigurations, effectively recasting geometric reasoning as image generation.\n  Unlike prior work that necessitates specialized architectures and\ndomain-specific adaptations when applying diffusion to parametric geometric\nrepresentations, we employ a standard visual diffusion model that operates on\nthe visual representation of the problem. This simplicity highlights a\nsurprising bridge between generative modeling and geometric problem solving.\nBeyond the specific problems studied here, our results point toward a broader\nparadigm: operating in image space provides a general and practical framework\nfor approximating notoriously hard problems, and opens the door to tackling a\nfar wider class of challenging geometric tasks.\n","authors":["Nir Goren","Shai Yehezkel","Omer Dahary","Andrey Voynov","Or Patashnik","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2510.21697v1.pdf","comment":"Project page: https://kariander1.github.io/visual-geo-solver/"},{"id":"http://arxiv.org/abs/2510.21696v1","updated":"2025-10-24T17:56:37Z","published":"2025-10-24T17:56:37Z","title":"BachVid: Training-Free Video Generation with Consistent Background and\n  Character","summary":"  Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training.\n","authors":["Han Yan","Xibin Song","Yifu Wang","Hongdong Li","Pan Ji","Chao Ma"],"pdf_url":"https://arxiv.org/pdf/2510.21696v1.pdf","comment":"Project page: https://wolfball.github.io/bachvid"},{"id":"http://arxiv.org/abs/2510.21689v1","updated":"2025-10-24T17:46:24Z","published":"2025-10-24T17:46:24Z","title":"On Thin Ice: Towards Explainable Conservation Monitoring via Attribution\n  and Perturbations","summary":"  Computer vision can accelerate ecological research and conservation\nmonitoring, yet adoption in ecology lags in part because of a lack of trust in\nblack-box neural-network-based models. We seek to address this challenge by\napplying post-hoc explanations to provide evidence for predictions and document\nlimitations that are important to field deployment. Using aerial imagery from\nGlacier Bay National Park, we train a Faster R-CNN to detect pinnipeds (harbor\nseals) and generate explanations via gradient-based class activation mapping\n(HiResCAM, LayerCAM), local interpretable model-agnostic explanations (LIME),\nand perturbation-based explanations. We assess explanations along three axes\nrelevant to field use: (i) localization fidelity: whether high-attribution\nregions coincide with the animal rather than background context; (ii)\nfaithfulness: whether deletion/insertion tests produce changes in detector\nconfidence; and (iii) diagnostic utility: whether explanations reveal\nsystematic failure modes. Explanations concentrate on seal torsos and contours\nrather than surrounding ice/rock, and removal of the seals reduces detection\nconfidence, providing model-evidence for true positives. The analysis also\nuncovers recurrent error sources, including confusion between seals and black\nice and rocks. We translate these findings into actionable next steps for model\ndevelopment, including more targeted data curation and augmentation. By pairing\nobject detection with post-hoc explainability, we can move beyond \"black-box\"\npredictions toward auditable, decision-supporting tools for conservation\nmonitoring.\n","authors":["Jiayi Zhou","Günel Aghakishiyeva","Saagar Arya","Julian Dale","James David Poling","Holly R. Houliston","Jamie N. Womble","Gregory D. Larsen","David W. Johnston","Brinnae Bent"],"pdf_url":"https://arxiv.org/pdf/2510.21689v1.pdf","comment":"NeurIPS Imageomics Workshop 2025"},{"id":"http://arxiv.org/abs/2510.18813v2","updated":"2025-10-24T17:42:45Z","published":"2025-10-21T17:10:48Z","title":"A Geometric Approach to Steerable Convolutions","summary":"  In contrast to the somewhat abstract, group theoretical approach adopted by\nmany papers, our work provides a new and more intuitive derivation of steerable\nconvolutional neural networks in $d$ dimensions. This derivation is based on\ngeometric arguments and fundamental principles of pattern matching. We offer an\nintuitive explanation for the appearance of the Clebsch--Gordan decomposition\nand spherical harmonic basis functions. Furthermore, we suggest a novel way to\nconstruct steerable convolution layers using interpolation kernels that improve\nupon existing implementation, and offer greater robustness to noisy data.\n","authors":["Soumyabrata Kundu","Risi Kondor"],"pdf_url":"https://arxiv.org/pdf/2510.18813v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08053v3","updated":"2025-10-24T17:42:08Z","published":"2024-12-11T03:00:15Z","title":"DynamicPAE: Generating Scene-Aware Physical Adversarial Examples in\n  Real-Time","summary":"  Physical adversarial examples (PAEs) are regarded as whistle-blowers of\nreal-world risks in deep-learning applications, thus worth further\ninvestigation. However, current PAE generation studies show limited adaptive\nattacking ability to diverse and varying scenes, revealing the urgent\nrequirement of dynamic PAEs that are generated in real time and conditioned on\nthe observation from the attacker. The key challenge in generating dynamic PAEs\nis learning the sparse relation between PAEs and the observation of attackers\nunder the noisy feedback of attack training. To address the challenge, we\npresent DynamicPAE, the first generative framework that enables scene-aware\nreal-time physical attacks. Specifically, to address the noisy feedback problem\nthat obfuscates the exploration of scene-related PAEs, we introduce the\nresidual-guided adversarial pattern exploration technique. Residual-guided\ntraining, which relaxes the attack training with a reconstruction task, is\nproposed to enrich the feedback information, thereby achieving a more\ncomprehensive exploration of PAEs. To address the alignment problem between the\ntrained generator and the real-world scenario, we introduce the\ndistribution-matched attack scenario alignment, consisting of the\nconditional-uncertainty-aligned data module and the skewness-aligned objective\nre-weighting module. The former aligns the training environment with the\nincomplete observation of the real-world attacker. The latter facilitates\nconsistent stealth control across different attack targets with the skewness\ncontroller. Extensive digital and physical evaluations demonstrate the superior\nattack performance of DynamicPAE, attaining a 2.07 $\\times$ boost (58.8%\naverage AP drop under attack) on representative object detectors (e.g., DETR)\nover state-of-the-art static PAE generating methods. Overall, our work opens\nthe door to end-to-end modeling of dynamic PAEs.\n","authors":["Jin Hu","Xianglong Liu","Jiakai Wang","Junkai Zhang","Xianqi Yang","Haotong Qin","Yuqing Ma","Ke Xu"],"pdf_url":"https://arxiv.org/pdf/2412.08053v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21682v1","updated":"2025-10-24T17:39:52Z","published":"2025-10-24T17:39:52Z","title":"WorldGrow: Generating Infinite 3D World","summary":"  We tackle the challenge of generating the infinitely extendable 3D world --\nlarge, continuous environments with coherent geometry and realistic appearance.\nExisting methods face key challenges: 2D-lifting approaches suffer from\ngeometric and appearance inconsistencies across views, 3D implicit\nrepresentations are hard to scale up, and current 3D foundation models are\nmostly object-centric, limiting their applicability to scene-level generation.\nOur key insight is leveraging strong generation priors from pre-trained 3D\nmodels for structured scene block generation. To this end, we propose\nWorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our\nmethod features three core components: (1) a data curation pipeline that\nextracts high-quality scene blocks for training, making the 3D structured\nlatent representations suitable for scene generation; (2) a 3D block inpainting\nmechanism that enables context-aware scene extension; and (3) a coarse-to-fine\ngeneration strategy that ensures both global layout plausibility and local\ngeometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset,\nWorldGrow achieves SOTA performance in geometry reconstruction, while uniquely\nsupporting infinite scene generation with photorealistic and structurally\nconsistent outputs. These results highlight its capability for constructing\nlarge-scale virtual environments and potential for building future world\nmodels.\n","authors":["Sikuang Li","Chen Yang","Jiemin Fang","Taoran Yi","Jia Lu","Jiazhong Cen","Lingxi Xie","Wei Shen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2510.21682v1.pdf","comment":"Project page: https://world-grow.github.io/ Code:\n  https://github.com/world-grow/WorldGrow"},{"id":"http://arxiv.org/abs/2411.18322v2","updated":"2025-10-24T17:36:28Z","published":"2024-11-27T13:23:11Z","title":"Mixture of Experts in Image Classification: What's the Sweet Spot?","summary":"  Mixture-of-Experts (MoE) models have shown promising potential for\nparameter-efficient scaling across domains. However, their application to image\nclassification remains limited, often requiring billion-scale datasets to be\ncompetitive. In this work, we explore the integration of MoE layers into image\nclassification architectures using open datasets. We conduct a systematic\nanalysis across different MoE configurations and model scales. We find that\nmoderate parameter activation per sample provides the best trade-off between\nperformance and efficiency. However, as the number of activated parameters\nincreases, the benefits of MoE diminish. Our analysis yields several practical\ninsights for vision MoE design. First, MoE layers most effectively strengthen\ntiny and mid-sized models, while gains taper off for large-capacity networks\nand do not redefine state-of-the-art ImageNet performance. Second, a Last-2\nplacement heuristic offers the most robust cross-architecture choice, with\nEvery-2 slightly better for Vision Transform (ViT), and both remaining\neffective as data and model scale increase. Third, larger datasets (e.g.,\nImageNet-21k) allow more experts, up to 16, for ConvNeXt to be utilized\neffectively without changing placement, as increased data reduces overfitting\nand promotes broader expert specialization. Finally, a simple linear router\nperforms best, suggesting that additional routing complexity yields no\nconsistent benefit.\n","authors":["Mathurin Videau","Alessandro Leite","Marc Schoenauer","Olivier Teytaud"],"pdf_url":"https://arxiv.org/pdf/2411.18322v2.pdf","comment":"Published in Transactions on Machine Learning Research"},{"id":"http://arxiv.org/abs/2506.12945v2","updated":"2025-10-24T17:23:51Z","published":"2025-06-15T19:12:37Z","title":"Metropolis-Hastings Sampling for 3D Gaussian Reconstruction","summary":"  We propose an adaptive sampling framework for 3D Gaussian Splatting (3DGS)\nthat leverages comprehensive multi-view photometric error signals within a\nunified Metropolis-Hastings approach. Vanilla 3DGS heavily relies on\nheuristic-based density-control mechanisms (e.g., cloning, splitting, and\npruning), which can lead to redundant computations or premature removal of\nbeneficial Gaussians. Our framework overcomes these limitations by\nreformulating densification and pruning as a probabilistic sampling process,\ndynamically inserting and relocating Gaussians based on aggregated multi-view\nerrors and opacity scores. Guided by Bayesian acceptance tests derived from\nthese error-based importance scores, our method substantially reduces reliance\non heuristics, offers greater flexibility, and adaptively infers Gaussian\ndistributions without requiring predefined scene complexity. Experiments on\nbenchmark datasets, including Mip-NeRF360, Tanks and Temples and Deep Blending,\nshow that our approach reduces the number of Gaussians needed, achieving faster\nconvergence while matching or modestly surpassing the view-synthesis quality of\nstate-of-the-art models.\n","authors":["Hyunjin Kim","Haebeom Jung","Jaesik Park"],"pdf_url":"https://arxiv.org/pdf/2506.12945v2.pdf","comment":"NeurIPS 2025. Project Page: https://hjhyunjinkim.github.io/MH-3DGS"},{"id":"http://arxiv.org/abs/2510.21664v1","updated":"2025-10-24T17:21:43Z","published":"2025-10-24T17:21:43Z","title":"Foundation Models in Dermatopathology: Skin Tissue Classification","summary":"  The rapid generation of whole-slide images (WSIs) in dermatopathology\nnecessitates automated methods for efficient processing and accurate\nclassification. This study evaluates the performance of two foundation models,\nUNI and Virchow2, as feature extractors for classifying WSIs into three\ndiagnostic categories: melanocytic, basaloid, and squamous lesions. Patch-level\nembeddings were aggregated into slide-level features using a mean-aggregation\nstrategy and subsequently used to train multiple machine learning classifiers,\nincluding logistic regression, gradient-boosted trees, and random forest\nmodels. Performance was assessed using precision, recall, true positive rate,\nfalse positive rate, and the area under the receiver operating characteristic\ncurve (AUROC) on the test set. Results demonstrate that patch-level features\nextracted using Virchow2 outperformed those extracted via UNI across most\nslide-level classifiers, with logistic regression achieving the highest\naccuracy (90%) for Virchow2, though the difference was not statistically\nsignificant. The study also explored data augmentation techniques and image\nnormalization to enhance model robustness and generalizability. The\nmean-aggregation approach provided reliable slide-level feature\nrepresentations. All experimental results and metrics were tracked and\nvisualized using WandB.ai, facilitating reproducibility and interpretability.\nThis research highlights the potential of foundation models for automated WSI\nclassification, providing a scalable and effective approach for\ndermatopathological diagnosis while paving the way for future advancements in\nslide-level representation learning.\n","authors":["Riya Gupta","Yiwei Zong","Dennis H. Murphree"],"pdf_url":"https://arxiv.org/pdf/2510.21664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21663v1","updated":"2025-10-24T17:17:46Z","published":"2025-10-24T17:17:46Z","title":"Self-Supervised Learning of Synapse Types from EM Images","summary":"  Separating synapses into different classes based on their appearance in EM\nimages has many applications in biology. Examples may include assigning a\nneurotransmitter to a particular class, or separating synapses whose strength\ncan be modulated from those whose strength is fixed. Traditionally, this has\nbeen done in a supervised manner, giving the classification algorithm examples\nof the different classes. Here we instead separate synapses into classes based\nonly on the observation that nearby synapses in the same neuron are likely more\nsimilar than synapses chosen randomly from different cells. We apply our\nmethodology to data from {\\it Drosophila}. Our approach has the advantage that\nthe number of synapse types does not need to be known in advance. It may also\nprovide a principled way to select ground-truth that spans the range of synapse\nstructure.\n","authors":["Aarav Shetty","Gary B Huang"],"pdf_url":"https://arxiv.org/pdf/2510.21663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.11128v2","updated":"2025-10-24T17:14:46Z","published":"2025-10-13T08:19:56Z","title":"Lightweight Facial Landmark Detection in Thermal Images via Multi-Level\n  Cross-Modal Knowledge Transfer","summary":"  Facial Landmark Detection (FLD) in thermal imagery is critical for\napplications in challenging lighting conditions, but it is hampered by the lack\nof rich visual cues. Conventional cross-modal solutions, like feature fusion or\nimage translation from RGB data, are often computationally expensive or\nintroduce structural artifacts, limiting their practical deployment. To address\nthis, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a\nnovel framework that decouples high-fidelity RGB-to-thermal knowledge transfer\nfrom model compression to create both accurate and efficient thermal FLD\nmodels. A central challenge during knowledge transfer is the profound modality\ngap between RGB and thermal data, where traditional unidirectional distillation\nfails to enforce semantic consistency across disparate feature spaces. To\novercome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a\nbidirectional mechanism designed specifically for this task. DIKD establishes a\nconnection between modalities: it not only guides the thermal student with rich\nRGB features but also validates the student's learned representations by\nfeeding them back into the frozen teacher's prediction head. This closed-loop\nsupervision forces the student to learn modality-invariant features that are\nsemantically aligned with the teacher, ensuring a robust and profound knowledge\ntransfer. Experiments show that our approach sets a new state-of-the-art on\npublic thermal FLD benchmarks, notably outperforming previous methods while\ndrastically reducing computational overhead.\n","authors":["Qiyi Tong","Olivia Nocentini","Marta Lagomarsino","Kuanqi Cai","Marta Lorenzini","Arash Ajoudani"],"pdf_url":"https://arxiv.org/pdf/2510.11128v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21657v1","updated":"2025-10-24T17:13:37Z","published":"2025-10-24T17:13:37Z","title":"Long-tailed Species Recognition in the NACTI Wildlife Dataset","summary":"  As most ''in the wild'' data collections of the natural world, the North\nAmerica Camera Trap Images (NACTI) dataset shows severe long-tailed class\nimbalance, noting that the largest 'Head' class alone covers >50% of the 3.7M\nimages in the corpus. Building on the PyTorch Wildlife model, we present a\nsystematic study of Long-Tail Recognition methodologies for species recognition\non the NACTI dataset covering experiments on various LTR loss functions plus\nLTR-sensitive regularisation. Our best configuration achieves 99.40% Top-1\naccuracy on our NACTI test data split, substantially improving over a 95.51%\nbaseline using standard cross-entropy with Adam. This also improves on\npreviously reported top performance in MLWIC2 at 96.8% albeit using partly\nunpublished (potentially different) partitioning, optimiser, and evaluation\nprotocols. To evaluate domain shifts (e.g. night-time captures, occlusion,\nmotion-blur) towards other datasets we construct a Reduced-Bias Test set from\nthe ENA-Detection dataset where our experimentally optimised long-tail enhanced\nmodel achieves leading 52.55% accuracy (up from 51.20% with WCE loss),\ndemonstrating stronger generalisation capabilities under distribution shift. We\ndocument the consistent improvements of LTR-enhancing scheduler choices in this\nNACTI wildlife domain, particularly when in tandem with state-of-the-art LTR\nlosses. We finally discuss qualitative and quantitative shortcomings that LTR\nmethods cannot sufficiently address, including catastrophic breakdown for\n'Tail' classes under severe domain shift. For maximum reproducibility we\npublish all dataset splits, key code, and full network weights.\n","authors":["Zehua Liu","Tilo Burghardt"],"pdf_url":"https://arxiv.org/pdf/2510.21657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21654v1","updated":"2025-10-24T17:11:50Z","published":"2025-10-24T17:11:50Z","title":"Group Inertial Poser: Multi-Person Pose and Global Translation from\n  Sparse Inertial Sensors and Ultra-Wideband Ranging","summary":"  Tracking human full-body motion using sparse wearable inertial measurement\nunits (IMUs) overcomes the limitations of occlusion and instrumentation of the\nenvironment inherent in vision-based approaches. However, purely IMU-based\ntracking compromises translation estimates and accurate relative positioning\nbetween individuals, as inertial cues are inherently self-referential and\nprovide no direct spatial reference for others. In this paper, we present a\nnovel approach for robustly estimating body poses and global translation for\nmultiple individuals by leveraging the distances between sparse wearable\nsensors - both on each individual and across multiple individuals. Our method\nGroup Inertial Poser estimates these absolute distances between pairs of\nsensors from ultra-wideband ranging (UWB) and fuses them with inertial\nobservations as input into structured state-space models to integrate temporal\nmotion patterns for precise 3D pose estimation. Our novel two-step optimization\nfurther leverages the estimated distances for accurately tracking people's\nglobal trajectories through the world. We also introduce GIP-DB, the first\nIMU+UWB dataset for two-person tracking, which comprises 200 minutes of motion\nrecordings from 14 participants. In our evaluation, Group Inertial Poser\noutperforms previous state-of-the-art methods in accuracy and robustness across\nsynthetic and real-world data, showing the promise of IMU+UWB-based multi-human\nmotion capture in the wild. Code, models, dataset:\nhttps://github.com/eth-siplab/GroupInertialPoser\n","authors":["Ying Xue","Jiaxi Jiang","Rayan Armani","Dominik Hollidt","Yi-Chi Liao","Christian Holz"],"pdf_url":"https://arxiv.org/pdf/2510.21654v1.pdf","comment":"Accepted by ICCV 2025, Code:\n  https://github.com/eth-siplab/GroupInertialPoser"},{"id":"http://arxiv.org/abs/2510.21649v1","updated":"2025-10-24T17:07:27Z","published":"2025-10-24T17:07:27Z","title":"A Dynamic Knowledge Distillation Method Based on the Gompertz Curve","summary":"  This paper introduces a novel dynamic knowledge distillation framework,\nGompertz-CNN, which integrates the Gompertz growth model into the training\nprocess to address the limitations of traditional knowledge distillation.\nConventional methods often fail to capture the evolving cognitive capacity of\nstudent models, leading to suboptimal knowledge transfer. To overcome this, we\npropose a stage-aware distillation strategy that dynamically adjusts the weight\nof distillation loss based on the Gompertz curve, reflecting the student's\nlearning progression: slow initial growth, rapid mid-phase improvement, and\nlate-stage saturation. Our framework incorporates Wasserstein distance to\nmeasure feature-level discrepancies and gradient matching to align backward\npropagation behaviors between teacher and student models. These components are\nunified under a multi-loss objective, where the Gompertz curve modulates the\ninfluence of distillation losses over time. Extensive experiments on CIFAR-10\nand CIFAR-100 using various teacher-student architectures (e.g., ResNet50 and\nMobileNet_v2) demonstrate that Gompertz-CNN consistently outperforms\ntraditional distillation methods, achieving up to 8% and 4% accuracy gains on\nCIFAR-10 and CIFAR-100, respectively.\n","authors":["Han Yang","Guangjun Qin"],"pdf_url":"https://arxiv.org/pdf/2510.21649v1.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2510.21635v1","updated":"2025-10-24T16:44:40Z","published":"2025-10-24T16:44:40Z","title":"DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective\n  Cross-Domain Learning","summary":"  Compared to 2D data, the scale of point cloud data in different domains\navailable for training, is quite limited. Researchers have been trying to\ncombine these data of different domains for masked autoencoder (MAE)\npre-training to leverage such a data scarcity issue. However, the prior\nknowledge learned from mixed domains may not align well with the downstream 3D\npoint cloud analysis tasks, leading to degraded performance. To address such an\nissue, we propose the Domain-Adaptive Point Cloud Masked Autoencoder (DAP-MAE),\nan MAE pre-training method, to adaptively integrate the knowledge of\ncross-domain datasets for general point cloud analysis. In DAP-MAE, we design a\nheterogeneous domain adapter that utilizes an adaptation mode during\npre-training, enabling the model to comprehensively learn information from\npoint clouds across different domains, while employing a fusion mode in the\nfine-tuning to enhance point cloud features. Meanwhile, DAP-MAE incorporates a\ndomain feature generator to guide the adaptation of point cloud features to\nvarious downstream tasks. With only one pre-training, DAP-MAE achieves\nexcellent performance across four different point cloud analysis tasks,\nreaching 95.18% in object classification on ScanObjectNN and 88.45% in facial\nexpression recognition on Bosphorus.\n","authors":["Ziqi Gao","Qiufu Li","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2510.21635v1.pdf","comment":"14 pages, 7 figures, conference"},{"id":"http://arxiv.org/abs/2506.02964v2","updated":"2025-10-24T16:25:21Z","published":"2025-06-03T14:59:22Z","title":"FORLA: Federated Object-centric Representation Learning with Slot\n  Attention","summary":"  Learning efficient visual representations across heterogeneous unlabeled\ndatasets remains a central challenge in federated learning. Effective federated\nrepresentations require features that are jointly informative across clients\nwhile disentangling domain-specific factors without supervision. We introduce\nFORLA, a novel framework for federated object-centric representation learning\nand feature adaptation across clients using unsupervised slot attention. At the\ncore of our method is a shared feature adapter, trained collaboratively across\nclients to adapt features from foundation models, and a shared slot attention\nmodule that learns to reconstruct the adapted features. To optimize this\nadapter, we design a two-branch student-teacher architecture. In each client, a\nstudent decoder learns to reconstruct full features from foundation models,\nwhile a teacher decoder reconstructs their adapted, low-dimensional\ncounterpart. The shared slot attention module bridges cross-domain learning by\naligning object-level representations across clients. Experiments in multiple\nreal-world datasets show that our framework not only outperforms centralized\nbaselines on object discovery but also learns a compact, universal\nrepresentation that generalizes well across domains. This work highlights\nfederated slot attention as an effective tool for scalable, unsupervised visual\nrepresentation learning from cross-domain data with distributed concepts.\n","authors":["Guiqiu Liao","Matjaz Jogan","Eric Eaton","Daniel A. Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2506.02964v2.pdf","comment":"Accepted by Neurips2025"},{"id":"http://arxiv.org/abs/2510.05051v2","updated":"2025-10-24T16:24:47Z","published":"2025-10-06T17:31:32Z","title":"SegMASt3R: Geometry Grounded Segment Matching","summary":"  Segment matching is an important intermediate task in computer vision that\nestablishes correspondences between semantically or geometrically coherent\nregions across images. Unlike keypoint matching, which focuses on localized\nfeatures, segment matching captures structured regions, offering greater\nrobustness to occlusions, lighting variations, and viewpoint changes. In this\npaper, we leverage the spatial understanding of 3D foundation models to tackle\nwide-baseline segment matching, a challenging setting involving extreme\nviewpoint shifts. We propose an architecture that uses the inductive bias of\nthese 3D foundation models to match segments across image pairs with up to 180\ndegree view-point change rotation. Extensive experiments show that our approach\noutperforms state-of-the-art methods, including the SAM2 video propagator and\nlocal feature matching methods, by up to 30% on the AUPRC metric, on ScanNet++\nand Replica datasets. We further demonstrate benefits of the proposed model on\nrelevant downstream tasks, including 3D instance mapping and object-relative\nnavigation. Project Page: https://segmast3r.github.io/\n","authors":["Rohit Jayanti","Swayam Agrawal","Vansh Garg","Siddharth Tourani","Muhammad Haris Khan","Sourav Garg","Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2510.05051v2.pdf","comment":"Accepted to The 39th Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2025) as a Spotlight (top 3.5%)"},{"id":"http://arxiv.org/abs/2412.06646v3","updated":"2025-10-24T16:24:17Z","published":"2024-12-09T16:39:40Z","title":"The Narrow Gate: Localized Image-Text Communication in Native Multimodal\n  Models","summary":"  Recent advances in multimodal training have significantly improved the\nintegration of image understanding and generation within a unified model. This\nstudy investigates how vision-language models (VLMs) handle image-understanding\ntasks, focusing on how visual information is processed and transferred to the\ntextual domain. We compare native multimodal VLMs, models trained from scratch\non multimodal data to generate both text and images, and non-native multimodal\nVLMs, models adapted from pre-trained large language models or capable of\ngenerating only text, highlighting key differences in information flow. We find\nthat in native multimodal VLMs, image and text embeddings are more separated\nwithin the residual stream. Moreover, VLMs differ in how visual information\nreaches text: non-native multimodal VLMs exhibit a distributed communication\npattern, where information is exchanged through multiple image tokens, whereas\nmodels trained natively for joint image and text generation tend to rely on a\nsingle post-image token that acts as a narrow gate for visual information. We\nshow that ablating this single token significantly deteriorates\nimage-understanding performance, whereas targeted, token-level interventions\nreliably steer image semantics and downstream text with fine-grained control.\n","authors":["Alessandro Serra","Francesco Ortu","Emanuele Panizon","Lucrezia Valeriani","Lorenzo Basile","Alessio Ansuini","Diego Doimo","Alberto Cazzaniga"],"pdf_url":"https://arxiv.org/pdf/2412.06646v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21615v1","updated":"2025-10-24T16:21:37Z","published":"2025-10-24T16:21:37Z","title":"Epipolar Geometry Improves Video Generation Models","summary":"  Video generation models have progressed tremendously through large latent\ndiffusion transformers trained with rectified flow techniques. Yet these models\nstill struggle with geometric inconsistencies, unstable motion, and visual\nartifacts that break the illusion of realistic 3D scenes. 3D-consistent video\ngeneration could significantly impact numerous downstream applications in\ngeneration and reconstruction tasks. We explore how epipolar geometry\nconstraints improve modern video diffusion models. Despite massive training\ndata, these models fail to capture fundamental geometric principles underlying\nvisual content. We align diffusion models using pairwise epipolar geometry\nconstraints via preference-based optimization, directly addressing unstable\ncamera trajectories and geometric artifacts through mathematically principled\ngeometric enforcement. Our approach efficiently enforces geometric principles\nwithout requiring end-to-end differentiability. Evaluation demonstrates that\nclassical geometric constraints provide more stable optimization signals than\nmodern learned metrics, which produce noisy targets that compromise alignment\nquality. Training on static scenes with dynamic cameras ensures high-quality\nmeasurements while the model generalizes effectively to diverse dynamic\ncontent. By bridging data-driven deep learning with classical geometric\ncomputer vision, we present a practical method for generating spatially\nconsistent videos without compromising visual quality.\n","authors":["Orest Kupyn","Fabian Manhardt","Federico Tombari","Christian Rupprecht"],"pdf_url":"https://arxiv.org/pdf/2510.21615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14109v2","updated":"2025-10-24T16:20:41Z","published":"2025-03-18T10:25:28Z","title":"Operational Change Detection for Geographical Information: Overview and\n  Challenges","summary":"  Rapid evolution of territories due to climate change and human impact\nrequires prompt and effective updates to geospatial databases maintained by the\nNational Mapping Agency. This paper presents a comprehensive overview of change\ndetection methods tailored for the operational updating of large-scale\ngeographic databases. This review first outlines the fundamental definition of\nchange, emphasizing its multifaceted nature, from temporal to semantic\ncharacterization. It categorizes automatic change detection methods into four\nmain families: rule-based, statistical, machine learning, and simulation\nmethods. The strengths, limitations, and applicability of every family are\ndiscussed in the context of various input data. Then, key applications for\nNational Mapping Agencies are identified, particularly the optimization of\ngeospatial database updating, change-based phenomena, and dynamics monitoring.\nFinally, the paper highlights the current challenges for leveraging change\ndetection such as the variability of change definition, the missing of relevant\nlarge-scale datasets, the diversity of input data, the unstudied no-change\ndetection, the human in the loop integration and the operational constraints.\nThe discussion underscores the necessity for ongoing innovation in change\ndetection techniques to address the future needs of geographic information\nsystems for national mapping agencies.\n","authors":["Nicolas Gonthier"],"pdf_url":"https://arxiv.org/pdf/2503.14109v2.pdf","comment":"Preprint under review"},{"id":"http://arxiv.org/abs/2507.06485v2","updated":"2025-10-24T16:19:27Z","published":"2025-07-09T02:06:13Z","title":"Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for\n  Efficient and Enhanced Video Reasoning","summary":"  Despite advances in reinforcement learning (RL)-based video reasoning with\nlarge language models (LLMs), data collection and fine-tuning remain\nsignificant challenges. These methods often rely on large-scale supervised\nfine-tuning (SFT) with extensive video data and long Chain-of-Thought (CoT)\nannotations, making them costly and hard to scale. To address this, we present\nVideo-RTS, a new approach to improve video reasoning capability with\ndrastically improved data efficiency by combining data-efficient RL with a\nvideo-adaptive test-time scaling (TTS) strategy. Building on observations about\nthe data scaling, we skip the resource-intensive SFT step and employ efficient\npure-RL training with output-based rewards, requiring no additional annotations\nor extensive fine-tuning. Furthermore, to utilize computational resources more\nefficiently, we introduce a sparse-to-dense video TTS strategy that improves\ninference by iteratively adding frames based on output consistency. We validate\nour approach on multiple video reasoning benchmarks, showing that Video-RTS\nsurpasses existing video reasoning models by 2.4% in accuracy using only 3.6%\ntraining samples. Specifically, Video-RTS achieves a 4.2% improvement on\nVideo-Holmes, a recent and challenging video reasoning benchmark. Notably, our\npure RL training and adaptive video TTS offer complementary strengths, enabling\nVideo-RTS's strong reasoning performance.\n","authors":["Ziyang Wang","Jaehong Yoon","Shoubin Yu","Md Mohaiminul Islam","Gedas Bertasius","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2507.06485v2.pdf","comment":"EMNLP 2025. The first two authors contributed equally. Project page:\n  https://sites.google.com/cs.unc.edu/videorts2025/"},{"id":"http://arxiv.org/abs/2510.21606v1","updated":"2025-10-24T16:11:10Z","published":"2025-10-24T16:11:10Z","title":"Modest-Align: Data-Efficient Alignment for Vision-Language Models","summary":"  Cross-modal alignment aims to map heterogeneous modalities into a shared\nlatent space, as exemplified by models like CLIP, which benefit from\nlarge-scale image-text pretraining for strong recognition capabilities.\nHowever, when operating in resource-constrained settings with limited or\nlow-quality data, these models often suffer from overconfidence and degraded\nperformance due to the prevalence of ambiguous or weakly correlated image-text\npairs. Current contrastive learning approaches, which rely on single positive\npairs, further exacerbate this issue by reinforcing overconfidence on uncertain\nsamples. To address these challenges, we propose Modest-Align, a lightweight\nalignment framework designed for robustness and efficiency. Our approach\nleverages two complementary strategies -- Random Perturbation, which introduces\ncontrolled noise to simulate uncertainty, and Embedding Smoothing, which\ncalibrates similarity distributions in the embedding space. These mechanisms\ncollectively reduce overconfidence and improve performance on noisy or weakly\naligned samples. Extensive experiments across multiple benchmark datasets\ndemonstrate that Modest-Align outperforms state-of-the-art methods in retrieval\ntasks, achieving competitive results with over 100x less training data and 600x\nless GPU time than CLIP. Our method offers a practical and scalable solution\nfor cross-modal alignment in real-world, low-resource scenarios.\n","authors":["Jiaxiang Liu","Yuan Wang","Jiawei Du","Joey Tianyi Zhou","Mingkun Xu","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2510.21606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21605v1","updated":"2025-10-24T16:10:09Z","published":"2025-10-24T16:10:09Z","title":"S3OD: Towards Generalizable Salient Object Detection with Synthetic Data","summary":"  Salient object detection exemplifies data-bounded tasks where expensive\npixel-precise annotations force separate model training for related subtasks\nlike DIS and HR-SOD. We present a method that dramatically improves\ngeneralization through large-scale synthetic data generation and\nambiguity-aware architecture. We introduce S3OD, a dataset of over 139,000\nhigh-resolution images created through our multi-modal diffusion pipeline that\nextracts labels from diffusion and DINO-v3 features. The iterative generation\nframework prioritizes challenging categories based on model performance. We\npropose a streamlined multi-mask decoder that naturally handles the inherent\nambiguity in salient object detection by predicting multiple valid\ninterpretations. Models trained solely on synthetic data achieve 20-50% error\nreduction in cross-dataset generalization, while fine-tuned versions reach\nstate-of-the-art performance across DIS and HR-SOD benchmarks.\n","authors":["Orest Kupyn","Hirokatsu Kataoka","Christian Rupprecht"],"pdf_url":"https://arxiv.org/pdf/2510.21605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21596v1","updated":"2025-10-24T16:02:05Z","published":"2025-10-24T16:02:05Z","title":"Automated interictal epileptic spike detection from simple and noisy\n  annotations in MEG data","summary":"  In drug-resistant epilepsy, presurgical evaluation of epilepsy can be\nconsidered. Magnetoencephalography (MEG) has been shown to be an effective exam\nto inform the localization of the epileptogenic zone through the localization\nof interictal epileptic spikes. Manual detection of these pathological\nbiomarkers remains a fastidious and error-prone task due to the high\ndimensionality of MEG recordings, and interrater agreement has been reported to\nbe only moderate. Current automated methods are unsuitable for clinical\npractice, either requiring extensively annotated data or lacking robustness on\nnon-typical data. In this work, we demonstrate that deep learning models can be\nused for detecting interictal spikes in MEG recordings, even when only temporal\nand single-expert annotations are available, which represents real-world\nclinical practice. We propose two model architectures: a feature-based\nartificial neural network (ANN) and a convolutional neural network (CNN),\ntrained on a database of 59 patients, and evaluated against a state-of-the-art\nmodel to classify short time windows of signal. In addition, we employ an\ninteractive machine learning strategy to iteratively improve our data\nannotation quality using intermediary model outputs. Both proposed models\noutperform the state-of-the-art model (F1-scores: CNN=0.46, ANN=0.44) when\ntested on 10 holdout test patients. The interactive machine learning strategy\ndemonstrates that our models are robust to noisy annotations. Overall, results\nhighlight the robustness of models with simple architectures when analyzing\ncomplex and imperfectly annotated data. Our method of interactive machine\nlearning offers great potential for faster data annotation, while our models\nrepresent useful and efficient tools for automated interictal spikes detection.\n","authors":["Pauline Mouches","Julien Jung","Armand Demasson","Agnès Guinard","Romain Bouet","Rosalie Marchal","Romain Quentin"],"pdf_url":"https://arxiv.org/pdf/2510.21596v1.pdf","comment":"17 pages, 7 Figures"},{"id":"http://arxiv.org/abs/2308.01042v2","updated":"2025-10-24T16:01:58Z","published":"2023-08-02T09:35:21Z","title":"WCCNet: Wavelet-context Cooperative Network for Efficient Multispectral\n  Pedestrian Detection","summary":"  Multispectral pedestrian detection achieves better visibility in challenging\nconditions and thus is essential to autonomous driving, for which both the\naccuracy and computational cost are of paramount importance. Most existing\napproaches treat RGB and infrared modalities equally. They typically adopt two\nsymmetrical backbones for multimodal feature extraction, which ignore the\nsubstantial differences between modalities and bring great difficulty for the\nreduction of the computational cost as well as effective crossmodal fusion. In\nthis work, we propose a novel and efficient framework named Wavelet-context\nCooperative Network (WCCNet) that is able to differentially extract\ncomplementary features of different spectra with lower computational\ncomplexity, and further fuse these diverse features based on their spatially\nrelevant crossmodal semantics. In particular, WCCNet simultaneously explore\nwavelet context and RGB textures within a cooperative dual-stream backbone,\nwhich is composed of adaptive discrete wavelet transform (ADWT) layers and\nheavyweight neural layers. The ADWT layers extract frequency components for\ninfrared modality, while neural layers handle RGB modality features. Since ADWT\nlayers are lightweight and extract complementary features, this cooperative\nstructure not only significantly reduces the computational complexity, but also\nfacilitates the subsequent crossmodal fusion. To further fuse these infrared\nand RGB features with significant semantic differences, we elaborately design\nthe crossmodal rearranging fusion module (CMRF), which can mitigate spatial\nmisalignment and merge semantically complementary features in spatially-related\nlocal regions to amplify the crossmodal reciprocal information. Experimental\nresults on KAIST and FLIR benchmarks indicate that WCCNet outperforms\nstate-of-the-art methods with considerable efficiency and competitive accuracy.\n","authors":["Xingjian Wang","Li Chai","Jiming Chen","Zhiguo Shi"],"pdf_url":"https://arxiv.org/pdf/2308.01042v2.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.21590v1","updated":"2025-10-24T15:59:04Z","published":"2025-10-24T15:59:04Z","title":"Restore Text First, Enhance Image Later: Two-Stage Scene Text Image\n  Super-Resolution with Glyph Structure Guidance","summary":"  Current generative super-resolution methods show strong performance on\nnatural images but distort text, creating a fundamental trade-off between image\nquality and textual readability. To address this, we introduce \\textbf{TIGER}\n(\\textbf{T}ext-\\textbf{I}mage \\textbf{G}uided\nsup\\textbf{E}r-\\textbf{R}esolution), a novel two-stage framework that breaks\nthis trade-off through a \\textit{\"text-first, image-later\"} paradigm.\n\\textbf{TIGER} explicitly decouples glyph restoration from image enhancement:\nit first reconstructs precise text structures and then uses them to guide\nsubsequent full-image super-resolution. This glyph-to-image guidance ensures\nboth high fidelity and visual consistency. To support comprehensive training\nand evaluation, we also contribute the \\textbf{UltraZoom-ST} (UltraZoom-Scene\nText), the first scene text dataset with extreme zoom (\\textbf{$\\times$14.29}).\nExtensive experiments show that \\textbf{TIGER} achieves\n\\textbf{state-of-the-art} performance, enhancing readability while preserving\noverall image quality.\n","authors":["Minxing Luo","Linlong Fan","Wang Qiushi","Ge Wu","Yiyan Luo","Yuhang Yu","Jinwei Chen","Yaxing Wang","Qingnan Fan","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2510.21590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21586v1","updated":"2025-10-24T15:54:05Z","published":"2025-10-24T15:54:05Z","title":"MATrack: Efficient Multiscale Adaptive Tracker for Real-Time Nighttime\n  UAV Operations","summary":"  Nighttime UAV tracking faces significant challenges in real-world robotics\noperations. Low-light conditions not only limit visual perception capabilities,\nbut cluttered backgrounds and frequent viewpoint changes also cause existing\ntrackers to drift or fail during deployment. To address these difficulties,\nresearchers have proposed solutions based on low-light enhancement and domain\nadaptation. However, these methods still have notable shortcomings in actual\nUAV systems: low-light enhancement often introduces visual artifacts, domain\nadaptation methods are computationally expensive and existing lightweight\ndesigns struggle to fully leverage dynamic object information. Based on an\nin-depth analysis of these key issues, we propose MATrack-a multiscale adaptive\nsystem designed specifically for nighttime UAV tracking. MATrack tackles the\nmain technical challenges of nighttime tracking through the collaborative work\nof three core modules: Multiscale Hierarchy Blende (MHB) enhances feature\nconsistency between static and dynamic templates. Adaptive Key Token Gate\naccurately identifies object information within complex backgrounds. Nighttime\nTemplate Calibrator (NTC) ensures stable tracking performance over long\nsequences. Extensive experiments show that MATrack achieves a significant\nperformance improvement. On the UAVDark135 benchmark, its precision, normalized\nprecision and AUC surpass state-of-the-art (SOTA) methods by 5.9%, 5.4% and\n4.2% respectively, while maintaining a real-time processing speed of 81 FPS.\nFurther tests on a real-world UAV platform validate the system's reliability,\ndemonstrating that MATrack can provide stable and effective nighttime UAV\ntracking support for critical robotics applications such as nighttime search\nand rescue and border patrol.\n","authors":["Xuzhao Li","Xuchen Li","Shiyu Hu"],"pdf_url":"https://arxiv.org/pdf/2510.21586v1.pdf","comment":"Preprint, Under Review"},{"id":"http://arxiv.org/abs/2510.21583v1","updated":"2025-10-24T15:50:36Z","published":"2025-10-24T15:50:36Z","title":"Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image\n  Generation","summary":"  Group Relative Policy Optimization (GRPO) has shown strong potential for\nflow-matching-based text-to-image (T2I) generation, but it faces two key\nlimitations: inaccurate advantage attribution, and the neglect of temporal\ndynamics of generation. In this work, we argue that shifting the optimization\nparadigm from the step level to the chunk level can effectively alleviate these\nissues. Building on this idea, we propose Chunk-GRPO, the first chunk-level\nGRPO-based approach for T2I generation. The insight is to group consecutive\nsteps into coherent 'chunk's that capture the intrinsic temporal dynamics of\nflow matching, and to optimize policies at the chunk level. In addition, we\nintroduce an optional weighted sampling strategy to further enhance\nperformance. Extensive experiments show that ChunkGRPO achieves superior\nresults in both preference alignment and image quality, highlighting the\npromise of chunk-level optimization for GRPO-based methods.\n","authors":["Yifu Luo","Penghui Du","Bo Li","Sinan Du","Tiantian Zhang","Yongzhe Chang","Kai Wu","Kun Gai","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2510.21583v1.pdf","comment":"11 pages, preprint"},{"id":"http://arxiv.org/abs/2510.21581v1","updated":"2025-10-24T15:49:54Z","published":"2025-10-24T15:49:54Z","title":"Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video","summary":"  Foley Control is a lightweight approach to video-guided Foley that keeps\npretrained single-modality models frozen and learns only a small\ncross-attention bridge between them. We connect V-JEPA2 video embeddings to a\nfrozen Stable Audio Open DiT text-to-audio (T2A) model by inserting compact\nvideo cross-attention after the model's existing text cross-attention, so\nprompts set global semantics while video refines timing and local dynamics. The\nfrozen backbones retain strong marginals (video; audio given text) and the\nbridge learns the audio-video dependency needed for synchronization -- without\nretraining the audio prior. To cut memory and stabilize training, we pool video\ntokens before conditioning. On curated video-audio benchmarks, Foley Control\ndelivers competitive temporal and semantic alignment with far fewer trainable\nparameters than recent multi-modal systems, while preserving prompt-driven\ncontrollability and production-friendly modularity (swap/upgrade encoders or\nthe T2A backbone without end-to-end retraining). Although we focus on\nVideo-to-Foley, the same bridge design can potentially extend to other audio\nmodalities (e.g., speech).\n","authors":["Ciara Rowles","Varun Jampani","Simon Donné","Shimon Vainer","Julian Parker","Zach Evans"],"pdf_url":"https://arxiv.org/pdf/2510.21581v1.pdf","comment":"Project Page: https://stability-ai.github.io/foleycontrol.github.io/"},{"id":"http://arxiv.org/abs/2510.21571v1","updated":"2025-10-24T15:39:31Z","published":"2025-10-24T15:39:31Z","title":"Scalable Vision-Language-Action Model Pretraining for Robotic\n  Manipulation with Real-Life Human Activity Videos","summary":"  This paper presents a novel approach for pretraining robotic manipulation\nVision-Language-Action (VLA) models using a large corpus of unscripted\nreal-life video recordings of human hand activities. Treating human hand as\ndexterous robot end-effector, we show that \"in-the-wild\" egocentric human\nvideos without any annotations can be transformed into data formats fully\naligned with existing robotic V-L-A training data in terms of task granularity\nand labels. This is achieved by the development of a fully-automated holistic\nhuman activity analysis approach for arbitrary human hand videos. This approach\ncan generate atomic-level hand activity segments and their language\ndescriptions, each accompanied with framewise 3D hand motion and camera motion.\nWe process a large volume of egocentric videos and create a hand-VLA training\ndataset containing 1M episodes and 26M frames. This training data covers a wide\nrange of objects and concepts, dexterous manipulation tasks, and environment\nvariations in real life, vastly exceeding the coverage of existing robot data.\nWe design a dexterous hand VLA model architecture and pretrain the model on\nthis dataset. The model exhibits strong zero-shot capabilities on completely\nunseen real-world observations. Additionally, fine-tuning it on a small amount\nof real robot action data significantly improves task success rates and\ngeneralization to novel objects in real robotic experiments. We also\ndemonstrate the appealing scaling behavior of the model's task performance with\nrespect to pretraining data scale. We believe this work lays a solid foundation\nfor scalable VLA pretraining, advancing robots toward truly generalizable\nembodied intelligence.\n","authors":["Qixiu Li","Yu Deng","Yaobo Liang","Lin Luo","Lei Zhou","Chengtang Yao","Lingqi Zeng","Zhiyuan Feng","Huizhi Liang","Sicheng Xu","Yizhong Zhang","Xi Chen","Hao Chen","Lily Sun","Dong Chen","Jiaolong Yang","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2510.21571v1.pdf","comment":"Project page: https://microsoft.github.io/VITRA/"},{"id":"http://arxiv.org/abs/2506.09237v2","updated":"2025-10-24T15:38:14Z","published":"2025-06-10T20:45:54Z","title":"PatchGuard: Adversarially Robust Anomaly Detection and Localization\n  through Vision Transformers and Pseudo Anomalies","summary":"  Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields\nthat demand high reliability, such as medical imaging and industrial\nmonitoring. However, current AD and AL approaches are often susceptible to\nadversarial attacks due to limitations in training data, which typically\ninclude only normal, unlabeled samples. This study introduces PatchGuard, an\nadversarially robust AD and AL method that incorporates pseudo anomalies with\nlocalization masks within a Vision Transformer (ViT)-based architecture to\naddress these vulnerabilities. We begin by examining the essential properties\nof pseudo anomalies, and follow it by providing theoretical insights into the\nattention mechanisms required to enhance the adversarial robustness of AD and\nAL systems. We then present our approach, which leverages Foreground-Aware\nPseudo-Anomalies to overcome the deficiencies of previous anomaly-aware\nmethods. Our method incorporates these crafted pseudo-anomaly samples into a\nViT-based framework, with adversarial training guided by a novel loss function\ndesigned to improve model robustness, as supported by our theoretical analysis.\nExperimental results on well-established industrial and medical datasets\ndemonstrate that PatchGuard significantly outperforms previous methods in\nadversarial settings, achieving performance gains of $53.2\\%$ in AD and\n$68.5\\%$ in AL, while also maintaining competitive accuracy in non-adversarial\nsettings. The code repository is available at\nhttps://github.com/rohban-lab/PatchGuard .\n","authors":["Mojtaba Nafez","Amirhossein Koochakian","Arad Maleki","Jafar Habibi","Mohammad Hossein Rohban"],"pdf_url":"https://arxiv.org/pdf/2506.09237v2.pdf","comment":"Accepted to the Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025"},{"id":"http://arxiv.org/abs/2502.09564v5","updated":"2025-10-24T15:32:07Z","published":"2025-02-13T18:17:03Z","title":"Diffusing DeBias: Synthetic Bias Amplification for Model Debiasing","summary":"  Deep learning model effectiveness in classification tasks is often challenged\nby the quality and quantity of training data whenever they are affected by\nstrong spurious correlations between specific attributes and target labels.\nThis results in a form of bias affecting training data, which typically leads\nto unrecoverable weak generalization in prediction. This paper aims at facing\nthis problem by leveraging bias amplification with generated synthetic data: we\nintroduce Diffusing DeBias (DDB), a novel approach acting as a plug-in for\ncommon methods of unsupervised model debiasing exploiting the inherent\nbias-learning tendency of diffusion models in data generation. Specifically,\nour approach adopts conditional diffusion models to generate synthetic\nbias-aligned images, which replace the original training set for learning an\neffective bias amplifier model that we subsequently incorporate into an\nend-to-end and a two-step unsupervised debiasing approach. By tackling the\nfundamental issue of bias-conflicting training samples memorization in learning\nauxiliary models, typical of this type of techniques, our proposed method beats\ncurrent state-of-the-art in multiple benchmark datasets, demonstrating its\npotential as a versatile and effective tool for tackling bias in deep learning\nmodels. Code is available at https://github.com/Malga-Vision/DiffusingDeBias\n","authors":["Massimiliano Ciranni","Vito Paolo Pastore","Roberto Di Via","Enzo Tartaglione","Francesca Odone","Vittorio Murino"],"pdf_url":"https://arxiv.org/pdf/2502.09564v5.pdf","comment":"18 Pages, 9 Figures; Accepted at NeurIPS2025 (Poster)"},{"id":"http://arxiv.org/abs/2506.03525v2","updated":"2025-10-24T15:17:29Z","published":"2025-06-04T03:18:01Z","title":"Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video\n  Reasoning","summary":"  Recent advances in Chain-of-Thought (CoT) reasoning have improved complex\nvideo understanding, but existing methods often struggle to adapt to\ndomain-specific skills (e.g., event detection, spatial relation understanding,\nemotion understanding) over various video content. To address this, we propose\nVideo-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs\nand leverages skill-aware CoT supervisions for domain-adaptive video reasoning.\nFirst, we construct skill-based CoT annotations: we extract domain-relevant\nreasoning skills from training questions, cluster them into a shared skill\ntaxonomy, and create detailed multi-step CoT rationale tailored to each\nvideo-question pair for training. Second, we introduce a skill-specific expert\nlearning framework. Each expert module specializes in a subset of reasoning\nskills and is trained with lightweight adapters using the collected CoT\nsupervision. We demonstrate the effectiveness of the proposed approach on three\nvideo understanding benchmarks, where Video-SKoT consistently outperforms\nstrong baselines. We also provide in-depth analyses on comparing different CoT\nannotation pipelines and learned skills over multiple video domains.\n","authors":["Daeun Lee","Jaehong Yoon","Jaemin Cho","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2506.03525v2.pdf","comment":"Project website: https://video-skill-cot.github.io/"},{"id":"http://arxiv.org/abs/2410.14878v2","updated":"2025-10-24T15:04:29Z","published":"2024-10-18T21:52:02Z","title":"On the Influence of Shape, Texture and Color for Learning Semantic\n  Segmentation","summary":"  Recent research has investigated the shape and texture biases of pre-trained\ndeep neural networks (DNNs) in image classification. Those works test how much\na trained DNN relies on specific image cues like texture. The present study\nshifts the focus to understanding the cue influence during training, analyzing\nwhat DNNs can learn from shape, texture, and color cues in absence of the\nothers; investigating their individual and combined influence on the learning\nsuccess. We analyze these cue influences at multiple levels by decomposing\ndatasets into cue-specific versions. Addressing semantic segmentation, we learn\nthe given task from these reduced cue datasets, creating cue experts. Early\nfusion of cues is performed by constructing appropriate datasets. This is\ncomplemented by a late fusion of experts which allows us to study cue influence\nlocation-dependent on pixel level. Experiments on Cityscapes, PASCAL Context,\nand a synthetic CARLA dataset show that while no single cue dominates, the\nshape + color expert predominantly improves the prediction of small objects and\nborder pixels. The cue performance order is consistent for the tested\nconvolutional and transformer architecture, indicating similar cue extraction\ncapabilities, although pre-trained transformers are said to be more biased\ntowards shape than convolutional neural networks.\n","authors":["Annika Mütze","Natalie Grabowsky","Edgar Heinert","Matthias Rottmann","Hanno Gottschalk"],"pdf_url":"https://arxiv.org/pdf/2410.14878v2.pdf","comment":"Accepted at the 28th European Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2510.21536v1","updated":"2025-10-24T15:01:18Z","published":"2025-10-24T15:01:18Z","title":"AURASeg: Attention Guided Upsampling with Residual Boundary-Assistive\n  Refinement for Drivable-Area Segmentation","summary":"  Free space ground segmentation is essential to navigate robots and autonomous\nvehicles, recognize drivable zones, and traverse efficiently. Fine-grained\nfeatures remain challenging for existing segmentation models, particularly for\nrobots in indoor and structured environments. These difficulties arise from\nineffective multi-scale processing, suboptimal boundary refinement, and limited\nfeature representation. In order to overcome these limitations, we propose\nAttention-Guided Upsampling with Residual Boundary-Assistive Refinement\n(AURASeg), a ground-plane semantic segmentation model that maintains high\nsegmentation accuracy while improving border precision. Our method uses\nCSP-Darknet backbone by adding a Residual Border Refinement Module (RBRM) for\naccurate edge delineation and an Attention Progressive Upsampling Decoder\n(APUD) for strong feature integration. We also incorporate a lightweight Atrous\nSpatial Pyramid Pooling (ASPP-Lite) module to ensure multi-scale context\nextraction without compromising real-time performance. The proposed model beats\nbenchmark segmentation architectures in mIoU and F1 metrics when tested on the\nGround Mobile Robot Perception (GMRP) Dataset and a custom Gazebo indoor\ndataset. Our approach achieves an improvement in mean Intersection-over-Union\n(mIoU) of +1.26% and segmentation precision of +1.65% compared to\nstate-of-the-art models. These results show that our technique is feasible for\nautonomous perception in both indoor and outdoor environments, enabling precise\nborder refinement with minimal effect on inference speed.\n","authors":["Narendhiran Vijayakumar","Sridevi. M"],"pdf_url":"https://arxiv.org/pdf/2510.21536v1.pdf","comment":"10 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2507.14798v2","updated":"2025-10-24T14:57:40Z","published":"2025-07-20T03:09:04Z","title":"An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric\n  Aerial Blocks","summary":"  State-of-the-art 3D computer vision algorithms continue to advance in\nhandling sparse, unordered image sets. Recently developed foundational models\nfor 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction\n(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry\nGrounded Transformer (VGGT), have attracted attention due to their ability to\nhandle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical\naerial images matters, as these models may handle extremely low image overlaps,\nstereo occlusions, and textureless regions. For redundant collections, they can\naccelerate 3D reconstruction by using extremely sparsified image sets. Despite\ntests on various computer vision benchmarks, their potential on photogrammetric\naerial blocks remains unexplored. This paper conducts a comprehensive\nevaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of\nthe UseGeo dataset for pose estimation and dense 3D reconstruction. Results\nshow these methods can accurately reconstruct dense point clouds from very\nsparse image sets (fewer than 10 images, up to 518 pixels resolution), with\ncompleteness gains up to +50% over COLMAP. VGGT also demonstrates higher\ncomputational efficiency, scalability, and more reliable camera pose\nestimation. However, all exhibit limitations with high-resolution images and\nlarge sets, as pose reliability declines with more images and geometric\ncomplexity. These findings suggest transformer-based methods cannot fully\nreplace traditional SfM and MVS, but offer promise as complementary approaches,\nespecially in challenging, low-resolution, and sparse scenarios.\n","authors":["Xinyi Wu","Steven Landgraf","Markus Ulrich","Rongjun Qin"],"pdf_url":"https://arxiv.org/pdf/2507.14798v2.pdf","comment":"23 pages, 7 figures, this manuscript has been submitted to\n  Geo-spatial Information Science for consideration"},{"id":"http://arxiv.org/abs/2505.22535v3","updated":"2025-10-24T14:56:29Z","published":"2025-05-28T16:21:58Z","title":"RiverMamba: A State Space Model for Global River Discharge and Flood\n  Forecasting","summary":"  Recent deep learning approaches for river discharge forecasting have improved\nthe accuracy and efficiency in flood forecasting, enabling more reliable early\nwarning systems for risk management. Nevertheless, existing deep learning\napproaches in hydrology remain largely confined to local-scale applications and\ndo not leverage the inherent spatial connections of bodies of water. Thus,\nthere is a strong need for new deep learning methodologies that are capable of\nmodeling spatio-temporal relations to improve river discharge and flood\nforecasting for scientific and operational applications. To address this, we\npresent RiverMamba, a novel deep learning model that is pretrained with\nlong-term reanalysis data and that can forecast global river discharge and\nfloods on a $0.05^\\circ$ grid up to $7$ days lead time, which is of high\nrelevance in early warning. To achieve this, RiverMamba leverages efficient\nMamba blocks that enable the model to capture spatio-temporal relations in very\nlarge river networks and enhance its forecast capability for longer lead times.\nThe forecast blocks integrate ECMWF HRES meteorological forecasts, while\naccounting for their inaccuracies through spatio-temporal modeling. Our\nanalysis demonstrates that RiverMamba provides reliable predictions of river\ndischarge across various flood return periods, including extreme floods, and\nlead times, surpassing both AI- and physics-based models. The source code and\ndatasets are publicly available at the project page\nhttps://hakamshams.github.io/RiverMamba.\n","authors":["Mohamad Hakam Shams Eddin","Yikui Zhang","Stefan Kollet","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2505.22535v3.pdf","comment":"Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025). Main paper 10 pages, Appendix 54 pages"},{"id":"http://arxiv.org/abs/2510.21518v1","updated":"2025-10-24T14:41:47Z","published":"2025-10-24T14:41:47Z","title":"Head Pursuit: Probing Attention Specialization in Multimodal\n  Transformers","summary":"  Language and vision-language models have shown impressive performance across\na wide range of tasks, but their internal mechanisms remain only partly\nunderstood. In this work, we study how individual attention heads in\ntext-generative models specialize in specific semantic or visual attributes.\nBuilding on an established interpretability method, we reinterpret the practice\nof probing intermediate activations with the final decoding layer through the\nlens of signal processing. This lets us analyze multiple samples in a\nprincipled way and rank attention heads based on their relevance to target\nconcepts. Our results show consistent patterns of specialization at the head\nlevel across both unimodal and multimodal transformers. Remarkably, we find\nthat editing as few as 1% of the heads, selected using our method, can reliably\nsuppress or enhance targeted concepts in the model output. We validate our\napproach on language tasks such as question answering and toxicity mitigation,\nas well as vision-language tasks including image classification and captioning.\nOur findings highlight an interpretable and controllable structure within\nattention layers, offering simple tools for understanding and editing\nlarge-scale generative models.\n","authors":["Lorenzo Basile","Valentino Maiorca","Diego Doimo","Francesco Locatello","Alberto Cazzaniga"],"pdf_url":"https://arxiv.org/pdf/2510.21518v1.pdf","comment":"Accepted at NeurIPS 2025 (spotlight)"},{"id":"http://arxiv.org/abs/2510.21512v1","updated":"2025-10-24T14:39:07Z","published":"2025-10-24T14:39:07Z","title":"Towards a Golden Classifier-Free Guidance Path via Foresight Fixed Point\n  Iterations","summary":"  Classifier-Free Guidance (CFG) is an essential component of text-to-image\ndiffusion models, and understanding and advancing its operational mechanisms\nremains a central focus of research. Existing approaches stem from divergent\ntheoretical interpretations, thereby limiting the design space and obscuring\nkey design choices. To address this, we propose a unified perspective that\nreframes conditional guidance as fixed point iterations, seeking to identify a\ngolden path where latents produce consistent outputs under both conditional and\nunconditional generation. We demonstrate that CFG and its variants constitute a\nspecial case of single-step short-interval iteration, which is theoretically\nproven to exhibit inefficiency. To this end, we introduce Foresight Guidance\n(FSG), which prioritizes solving longer-interval subproblems in early diffusion\nstages with increased iterations. Extensive experiments across diverse datasets\nand model architectures validate the superiority of FSG over state-of-the-art\nmethods in both image quality and computational efficiency. Our work offers\nnovel perspectives for conditional guidance and unlocks the potential of\nadaptive design.\n","authors":["Kaibo Wang","Jianda Mao","Tong Wu","Yang Xiang"],"pdf_url":"https://arxiv.org/pdf/2510.21512v1.pdf","comment":"Accepted at NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2411.14269v2","updated":"2025-10-24T14:29:54Z","published":"2024-11-21T16:25:56Z","title":"Guided MRI Reconstruction via Schrödinger Bridge","summary":"  Magnetic Resonance Imaging (MRI) is an inherently multi-contrast modality,\nwhere cross-contrast priors can be exploited to improve image reconstruction\nfrom undersampled data. Recently, diffusion models have shown remarkable\nperformance in MRI reconstruction. However, they still struggle to effectively\nutilize such priors, mainly because existing methods rely on feature-level\nfusion in image or latent spaces, which lacks explicit structural\ncorrespondence and thus leads to suboptimal performance. To address this issue,\nwe propose $\\mathbf{I}^2$SB-Inversion, a multi-contrast guided reconstruction\nframework based on the Schr\\\"odinger Bridge (SB). The proposed method performs\npixel-wise translation between paired contrasts, providing explicit structural\nconstraints between the guidance and target images. Furthermore, an Inversion\nstrategy is introduced to correct inter-modality misalignment, which often\noccurs in guided reconstruction, thereby mitigating artifacts and improving\nreconstruction accuracy. Experiments on paired T1- and T2-weighted datasets\ndemonstrate that $\\mathbf{I}^2$SB-Inversion achieves a high acceleration factor\nof up to 14.4 and consistently outperforms existing methods in both\nquantitative and qualitative evaluations.\n","authors":["Yue Wang","Yuanbiao Yang","Zhuo-xu Cui","Tian Zhou","Bingsheng Huang","Hairong Zheng","Dong Liang","Yanjie Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.14269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11217v2","updated":"2025-10-24T14:21:57Z","published":"2025-05-16T13:13:25Z","title":"Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI\n  models in Sound Localization","summary":"  Imagine hearing a dog bark and turning toward the sound only to see a parked\ncar, while the real, silent dog sits elsewhere. Such sensory conflicts test\nperception, yet humans reliably resolve them by prioritizing sound over\nmisleading visuals. Despite advances in multimodal AI integrating vision and\naudio, little is known about how these systems handle cross-modal conflicts or\nwhether they favor one modality. In this study, we systematically examine\nmodality bias and conflict resolution in AI sound localization. We assess\nleading multimodal models and benchmark them against human performance in\npsychophysics experiments across six audiovisual conditions, including\ncongruent, conflicting, and absent cues. Humans consistently outperform AI,\ndemonstrating superior resilience to conflicting or missing visuals by relying\non auditory information. In contrast, AI models often default to visual input,\ndegrading performance to near chance levels. To address this, we propose a\nneuroscience-inspired model, EchoPin, which uses a stereo audio-image dataset\ngenerated via 3D simulations. Even with limited training data, EchoPin\nsurpasses existing benchmarks. Notably, it also mirrors human-like horizontal\nlocalization bias favoring left-right precision-likely due to the stereo audio\nstructure reflecting human ear placement. These findings underscore how sensory\ninput quality and system architecture shape multimodal representation accuracy.\n","authors":["Yanhao Jia","Ji Xie","S Jivaganesh","Hao Li","Xu Wu","Mengmi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.11217v2.pdf","comment":"NeurIPS 2025, Spotlight"},{"id":"http://arxiv.org/abs/2510.21495v1","updated":"2025-10-24T14:20:34Z","published":"2025-10-24T14:20:34Z","title":"An Automatic Detection Method for Hematoma Features in Placental\n  Abruption Ultrasound Images Based on Few-Shot Learning","summary":"  Placental abruption is a severe complication during pregnancy, and its early\naccurate diagnosis is crucial for ensuring maternal and fetal safety.\nTraditional ultrasound diagnostic methods heavily rely on physician experience,\nleading to issues such as subjective bias and diagnostic inconsistencies. This\npaper proposes an improved model, EH-YOLOv11n (Enhanced Hemorrhage-YOLOv11n),\nbased on small-sample learning, aiming to achieve automatic detection of\nhematoma features in placental ultrasound images. The model enhances\nperformance through multidimensional optimization: it integrates wavelet\nconvolution and coordinate convolution to strengthen frequency and spatial\nfeature extraction; incorporates a cascaded group attention mechanism to\nsuppress ultrasound artifacts and occlusion interference, thereby improving\nbounding box localization accuracy. Experimental results demonstrate a\ndetection accuracy of 78%, representing a 2.5% improvement over YOLOv11n and a\n13.7% increase over YOLOv8. The model exhibits significant superiority in\nprecision-recall curves, confidence scores, and occlusion scenarios. Combining\nhigh accuracy with real-time processing, this model provides a reliable\nsolution for computer-aided diagnosis of placental abruption, holding\nsignificant clinical application value.\n","authors":["Xiaoqing Liu","Jitai Han","Hua Yan","Peng Li","Sida Tang","Ying Li","Kaiwen Zhang","Min Yu"],"pdf_url":"https://arxiv.org/pdf/2510.21495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21482v1","updated":"2025-10-24T14:07:11Z","published":"2025-10-24T14:07:11Z","title":"GRAP-MOT: Unsupervised Graph-based Position Weighted Person Multi-camera\n  Multi-object Tracking in a Highly Congested Space","summary":"  GRAP-MOT is a new approach for solving the person MOT problem dedicated to\nvideos of closed areas with overlapping multi-camera views, where person\nocclusion frequently occurs. Our novel graph-weighted solution updates a\nperson's identification label online based on tracks and the person's\ncharacteristic features. To find the best solution, we deeply investigated all\nelements of the MOT process, including feature extraction, tracking, and\ncommunity search. Furthermore, GRAP-MOT is equipped with a person's position\nestimation module, which gives additional key information to the MOT method,\nensuring better results than methods without position data. We tested GRAP-MOT\non recordings acquired in a closed-area model and on publicly available real\ndatasets that fulfil the requirement of a highly congested space, showing the\nsuperiority of our proposition. Finally, we analyzed existing metrics used to\ncompare MOT algorithms and concluded that IDF1 is more adequate than MOTA in\nsuch comparisons. We made our code, along with the acquired dataset, publicly\navailable.\n","authors":["Marek Socha","Michał Marczyk","Aleksander Kempski","Michał Cogiel","Paweł Foszner","Radosław Zawiski","Michał Staniszewski"],"pdf_url":"https://arxiv.org/pdf/2510.21482v1.pdf","comment":"13 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2510.21479v1","updated":"2025-10-24T14:03:52Z","published":"2025-10-24T14:03:52Z","title":"ITC-RWKV: Interactive Tissue-Cell Modeling with Recurrent Key-Value\n  Aggregation for Histopathological Subtyping","summary":"  Accurate interpretation of histopathological images demands integration of\ninformation across spatial and semantic scales, from nuclear morphology and\ncellular textures to global tissue organization and disease-specific patterns.\nAlthough recent foundation models in pathology have shown strong capabilities\nin capturing global tissue context, their omission of cell-level feature\nmodeling remains a key limitation for fine-grained tasks such as cancer subtype\nclassification. To address this, we propose a dual-stream architecture that\nmodels the interplay between macroscale tissue features and aggregated cellular\nrepresentations. To efficiently aggregate information from large cell sets, we\npropose a receptance-weighted key-value aggregation model, a recurrent\ntransformer that captures inter-cell dependencies with linear complexity.\nFurthermore, we introduce a bidirectional tissue-cell interaction module to\nenable mutual attention between localized cellular cues and their surrounding\ntissue environment. Experiments on four histopathological subtype\nclassification benchmarks show that the proposed method outperforms existing\nmodels, demonstrating the critical role of cell-level aggregation and\ntissue-cell interaction in fine-grained computational pathology.\n","authors":["Yating Huang","Qijun Yang","Lintao Xiang","Hujun Yin"],"pdf_url":"https://arxiv.org/pdf/2510.21479v1.pdf","comment":"Accept by BMVC 2025"},{"id":"http://arxiv.org/abs/2510.13626v2","updated":"2025-10-24T13:50:04Z","published":"2025-10-15T14:51:36Z","title":"LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action\n  Models","summary":"  Visual-Language-Action (VLA) models report impressive success rates on\nrobotic manipulation benchmarks, yet these results may mask fundamental\nweaknesses in robustness. We perform a systematic vulnerability analysis by\nintroducing controlled perturbations across seven dimensions: objects layout,\ncamera viewpoints, robot initial states, language instructions, light\nconditions, background textures and sensor noise. We comprehensively analyzed\nmultiple state-of-the-art models and revealed consistent brittleness beneath\napparent competence. Our analysis exposes critical weaknesses: models exhibit\nextreme sensitivity to perturbation factors, including camera viewpoints and\nrobot initial states, with performance dropping from 95% to below 30% under\nmodest perturbations. Surprisingly, models are largely insensitive to language\nvariations, with further experiments revealing that models tend to ignore\nlanguage instructions completely. Our findings challenge the assumption that\nhigh benchmark scores equate to true competency and highlight the need for\nevaluation practices that assess reliability under realistic variation.\n","authors":["Senyu Fei","Siyin Wang","Junhao Shi","Zihao Dai","Jikun Cai","Pengfang Qian","Li Ji","Xinzhe He","Shiduo Zhang","Zhaoye Fei","Jinlan Fu","Jingjing Gong","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2510.13626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11544v3","updated":"2025-10-24T13:47:21Z","published":"2025-03-14T16:10:21Z","title":"AugGen: Synthetic Augmentation using Diffusion Models Can Improve\n  Recognition","summary":"  The increasing reliance on large-scale datasets in machine learning poses\nsignificant privacy and ethical challenges, particularly in sensitive domains\nsuch as face recognition. Synthetic data generation offers a promising\nalternative; however, most existing methods depend heavily on external datasets\nor pre-trained models, increasing complexity and resource demands. In this\npaper, we introduce AugGen, a self-contained synthetic augmentation technique.\nAugGen strategically samples from a class-conditional generative model trained\nexclusively on the target FR dataset, eliminating the need for external\nresources. Evaluated across 8 FR benchmarks, including IJB-C and IJB-B, our\nmethod achieves 1-12% performance improvements, outperforming models trained\nsolely on real data and surpassing state-of-the-art synthetic data generation\napproaches, while using less real data. Notably, these gains often exceed those\nfrom architectural enhancements, underscoring the value of synthetic\naugmentation in data-limited scenarios. Our findings demonstrate that carefully\nintegrated synthetic data can both mitigate privacy constraints and\nsubstantially enhance recognition performance. Paper website:\nhttps://parsa-ra.github.io/auggen/.\n","authors":["Parsa Rahimi","Damien Teney","Sebastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2503.11544v3.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.21464v1","updated":"2025-10-24T13:46:18Z","published":"2025-10-24T13:46:18Z","title":"CXR-LanIC: Language-Grounded Interpretable Classifier for Chest X-Ray\n  Diagnosis","summary":"  Deep learning models have achieved remarkable accuracy in chest X-ray\ndiagnosis, yet their widespread clinical adoption remains limited by the\nblack-box nature of their predictions. Clinicians require transparent,\nverifiable explanations to trust automated diagnoses and identify potential\nfailure modes. We introduce CXR-LanIC (Language-Grounded Interpretable\nClassifier for Chest X-rays), a novel framework that addresses this\ninterpretability challenge through task-aligned pattern discovery. Our approach\ntrains transcoder-based sparse autoencoders on a BiomedCLIP diagnostic\nclassifier to decompose medical image representations into interpretable visual\npatterns. By training an ensemble of 100 transcoders on multimodal embeddings\nfrom the MIMIC-CXR dataset, we discover approximately 5,000 monosemantic\npatterns spanning cardiac, pulmonary, pleural, structural, device, and artifact\ncategories. Each pattern exhibits consistent activation behavior across images\nsharing specific radiological features, enabling transparent attribution where\npredictions decompose into 20-50 interpretable patterns with verifiable\nactivation galleries. CXR-LanIC achieves competitive diagnostic accuracy on\nfive key findings while providing the foundation for natural language\nexplanations through planned large multimodal model annotation. Our key\ninnovation lies in extracting interpretable features from a classifier trained\non specific diagnostic objectives rather than general-purpose embeddings,\nensuring discovered patterns are directly relevant to clinical decision-making,\ndemonstrating that medical AI systems can be both accurate and interpretable,\nsupporting safer clinical deployment through transparent, clinically grounded\nexplanations.\n","authors":["Yiming Tang","Wenjia Zhong","Rushi Shah","Dianbo Liu"],"pdf_url":"https://arxiv.org/pdf/2510.21464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21461v1","updated":"2025-10-24T13:44:09Z","published":"2025-10-24T13:44:09Z","title":"VidSplice: Towards Coherent Video Inpainting via Explicit Spaced Frame\n  Guidance","summary":"  Recent video inpainting methods often employ image-to-video (I2V) priors to\nmodel temporal consistency across masked frames. While effective in moderate\ncases, these methods struggle under severe content degradation and tend to\noverlook spatiotemporal stability, resulting in insufficient control over the\nlatter parts of the video. To address these limitations, we decouple video\ninpainting into two sub-tasks: multi-frame consistent image inpainting and\nmasked area motion propagation. We propose VidSplice, a novel framework that\nintroduces spaced-frame priors to guide the inpainting process with\nspatiotemporal cues. To enhance spatial coherence, we design a CoSpliced Module\nto perform first-frame propagation strategy that diffuses the initial frame\ncontent into subsequent reference frames through a splicing mechanism.\nAdditionally, we introduce a delicate context controller module that encodes\ncoherent priors after frame duplication and injects the spliced video into the\nI2V generative backbone, effectively constraining content distortion during\ngeneration. Extensive evaluations demonstrate that VidSplice achieves\ncompetitive performance across diverse video inpainting scenarios. Moreover,\nits design significantly improves both foreground alignment and motion\nstability, outperforming existing approaches.\n","authors":["Ming Xie","Junqiu Yu","Qiaole Dong","Xiangyang Xue","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2510.21461v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2507.06363v2","updated":"2025-10-24T13:30:27Z","published":"2025-07-08T19:57:27Z","title":"Mamba Goes HoME: Hierarchical Soft Mixture-of-Experts for 3D Medical\n  Image Segmentation","summary":"  In recent years, artificial intelligence has significantly advanced medical\nimage segmentation. Nonetheless, challenges remain, including efficient 3D\nmedical image processing across diverse modalities and handling data\nvariability. In this work, we introduce Hierarchical Soft Mixture-of-Experts\n(HoME), a two-level token-routing layer for efficient long-context modeling,\nspecifically designed for 3D medical image segmentation. Built on the Mamba\nSelective State Space Model (SSM) backbone, HoME enhances sequential modeling\nthrough adaptive expert routing. In the first level, a Soft Mixture-of-Experts\n(SMoE) layer partitions input sequences into local groups, routing tokens to\nspecialized per-group experts for localized feature extraction. The second\nlevel aggregates these outputs through a global SMoE layer, enabling\ncross-group information fusion and global context refinement. This hierarchical\ndesign, combining local expert routing with global expert refinement, enhances\ngeneralizability and segmentation performance, surpassing state-of-the-art\nresults across datasets from the three most widely used 3D medical imaging\nmodalities and varying data qualities. The code is publicly available at\nhttps://github.com/gmum/MambaHoME.\n","authors":["Szymon Płotka","Gizem Mert","Maciej Chrabaszcz","Ewa Szczurek","Arkadiusz Sitek"],"pdf_url":"https://arxiv.org/pdf/2507.06363v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.21449v1","updated":"2025-10-24T13:28:29Z","published":"2025-10-24T13:28:29Z","title":"MoniTor: Exploiting Large Language Models with Instruction for Online\n  Video Anomaly Detection","summary":"  Video Anomaly Detection (VAD) aims to locate unusual activities or behaviors\nwithin videos. Recently, offline VAD has garnered substantial research\nattention, which has been invigorated by the progress in large language models\n(LLMs) and vision-language models (VLMs), offering the potential for a more\nnuanced understanding of anomalies. However, online VAD has seldom received\nattention due to real-time constraints and computational intensity. In this\npaper, we introduce a novel Memory-based online scoring queue scheme for\nTraining-free VAD (MoniTor), to address the inherent complexities in online\nVAD. Specifically, MoniTor applies a streaming input to VLMs, leveraging the\ncapabilities of pre-trained large-scale models. To capture temporal\ndependencies more effectively, we incorporate a novel prediction mechanism\ninspired by Long Short-Term Memory (LSTM) networks. This ensures the model can\neffectively model past states and leverage previous predictions to identify\nanomalous behaviors. Thereby, it better understands the current frame.\nMoreover, we design a scoring queue and an anomaly prior to dynamically store\nrecent scores and cover all anomalies in the monitoring scenario, providing\nguidance for LLMs to distinguish between normal and abnormal behaviors over\ntime. We evaluate MoniTor on two large datasets (i.e., UCF-Crime and\nXD-Violence) containing various surveillance and real-world scenarios. The\nresults demonstrate that MoniTor outperforms state-of-the-art methods and is\ncompetitive with weakly supervised methods without training. Code is available\nat https://github.com/YsTvT/MoniTor.\n","authors":["Shengtian Yang","Yue Feng","Yingshi Liu","Jingrou Zhang","Jie Qin"],"pdf_url":"https://arxiv.org/pdf/2510.21449v1.pdf","comment":"Accepted to NeurIPS 2025. The first two authors hold equal\n  contributions"},{"id":"http://arxiv.org/abs/2510.21447v1","updated":"2025-10-24T13:25:39Z","published":"2025-10-24T13:25:39Z","title":"PhysWorld: From Real Videos to World Models of Deformable Objects via\n  Physics-Aware Demonstration Synthesis","summary":"  Interactive world models that simulate object dynamics are crucial for\nrobotics, VR, and AR. However, it remains a significant challenge to learn\nphysics-consistent dynamics models from limited real-world video data,\nespecially for deformable objects with spatially-varying physical properties.\nTo overcome the challenge of data scarcity, we propose PhysWorld, a novel\nframework that utilizes a simulator to synthesize physically plausible and\ndiverse demonstrations to learn efficient world models. Specifically, we first\nconstruct a physics-consistent digital twin within MPM simulator via\nconstitutive model selection and global-to-local optimization of physical\nproperties. Subsequently, we apply part-aware perturbations to the physical\nproperties and generate various motion patterns for the digital twin,\nsynthesizing extensive and diverse demonstrations. Finally, using these\ndemonstrations, we train a lightweight GNN-based world model that is embedded\nwith physical properties. The real video can be used to further refine the\nphysical properties. PhysWorld achieves accurate and fast future predictions\nfor various deformable objects, and also generalizes well to novel\ninteractions. Experiments show that PhysWorld has competitive performance while\nenabling inference speeds 47 times faster than the recent state-of-the-art\nmethod, i.e., PhysTwin.\n","authors":["Yu Yang","Zhilu Zhang","Xiang Zhang","Yihan Zeng","Hui Li","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2510.21447v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.21445v1","updated":"2025-10-24T13:23:38Z","published":"2025-10-24T13:23:38Z","title":"REMONI: An Autonomous System Integrating Wearables and Multimodal Large\n  Language Models for Enhanced Remote Health Monitoring","summary":"  With the widespread adoption of wearable devices in our daily lives, the\ndemand and appeal for remote patient monitoring have significantly increased.\nMost research in this field has concentrated on collecting sensor data,\nvisualizing it, and analyzing it to detect anomalies in specific diseases such\nas diabetes, heart disease and depression. However, this domain has a notable\ngap in the aspect of human-machine interaction. This paper proposes REMONI, an\nautonomous REmote health MONItoring system that integrates multimodal large\nlanguage models (MLLMs), the Internet of Things (IoT), and wearable devices.\nThe system automatically and continuously collects vital signs, accelerometer\ndata from a special wearable (such as a smartwatch), and visual data in patient\nvideo clips collected from cameras. This data is processed by an anomaly\ndetection module, which includes a fall detection model and algorithms to\nidentify and alert caregivers of the patient's emergency conditions. A\ndistinctive feature of our proposed system is the natural language processing\ncomponent, developed with MLLMs capable of detecting and recognizing a\npatient's activity and emotion while responding to healthcare worker's\ninquiries. Additionally, prompt engineering is employed to integrate all\npatient information seamlessly. As a result, doctors and nurses can access\nreal-time vital signs and the patient's current state and mood by interacting\nwith an intelligent agent through a user-friendly web application. Our\nexperiments demonstrate that our system is implementable and scalable for\nreal-life scenarios, potentially reducing the workload of medical professionals\nand healthcare costs. A full-fledged prototype illustrating the functionalities\nof the system has been developed and being tested to demonstrate the robustness\nof its various capabilities.\n","authors":["Thanh Cong Ho","Farah Kharrat","Abderrazek Abid","Fakhri Karray"],"pdf_url":"https://arxiv.org/pdf/2510.21445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.12753v2","updated":"2025-10-24T13:21:48Z","published":"2025-10-14T17:33:44Z","title":"E-MoFlow: Learning Egomotion and Optical Flow from Event Data via\n  Implicit Regularization","summary":"  The estimation of optical flow and 6-DoF ego-motion, two fundamental tasks in\n3D vision, has typically been addressed independently. For neuromorphic vision\n(e.g., event cameras), however, the lack of robust data association makes\nsolving the two problems separately an ill-posed challenge, especially in the\nabsence of supervision via ground truth. Existing works mitigate this\nill-posedness by either enforcing the smoothness of the flow field via an\nexplicit variational regularizer or leveraging explicit structure-and-motion\npriors in the parametrization to improve event alignment. The former notably\nintroduces bias in results and computational overhead, while the latter, which\nparametrizes the optical flow in terms of the scene depth and the camera\nmotion, often converges to suboptimal local minima. To address these issues, we\npropose an unsupervised framework that jointly optimizes egomotion and optical\nflow via implicit spatial-temporal and geometric regularization. First, by\nmodeling camera's egomotion as a continuous spline and optical flow as an\nimplicit neural representation, our method inherently embeds spatial-temporal\ncoherence through inductive biases. Second, we incorporate structure-and-motion\npriors through differential geometric constraints, bypassing explicit depth\nestimation while maintaining rigorous geometric consistency. As a result, our\nframework (called E-MoFlow) unifies egomotion and optical flow estimation via\nimplicit regularization under a fully unsupervised paradigm. Experiments\ndemonstrate its versatility to general 6-DoF motion scenarios, achieving\nstate-of-the-art performance among unsupervised methods and competitive even\nwith supervised approaches.\n","authors":["Wenpu Li","Bangyan Liao","Yi Zhou","Qi Xu","Pian Wan","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.12753v2.pdf","comment":"The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems(NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.21441v1","updated":"2025-10-24T13:17:56Z","published":"2025-10-24T13:17:56Z","title":"OpenHype: Hyperbolic Embeddings for Hierarchical Open-Vocabulary\n  Radiance Fields","summary":"  Modeling the inherent hierarchical structure of 3D objects and 3D scenes is\nhighly desirable, as it enables a more holistic understanding of environments\nfor autonomous agents. Accomplishing this with implicit representations, such\nas Neural Radiance Fields, remains an unexplored challenge. Existing methods\nthat explicitly model hierarchical structures often face significant\nlimitations: they either require multiple rendering passes to capture\nembeddings at different levels of granularity, significantly increasing\ninference time, or rely on predefined, closed-set discrete hierarchies that\ngeneralize poorly to the diverse and nuanced structures encountered by agents\nin the real world. To address these challenges, we propose OpenHype, a novel\napproach that represents scene hierarchies using a continuous hyperbolic latent\nspace. By leveraging the properties of hyperbolic geometry, OpenHype naturally\nencodes multi-scale relationships and enables smooth traversal of hierarchies\nthrough geodesic paths in latent space. Our method outperforms state-of-the-art\napproaches on standard benchmarks, demonstrating superior efficiency and\nadaptability in 3D scene understanding.\n","authors":["Lisa Weijler","Sebastian Koch","Fabio Poiesi","Timo Ropinski","Pedro Hermosilla"],"pdf_url":"https://arxiv.org/pdf/2510.21441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21437v1","updated":"2025-10-24T13:14:57Z","published":"2025-10-24T13:14:57Z","title":"Anisotropic Pooling for LUT-realizable CNN Image Restoration","summary":"  Table look-up realization of image restoration CNNs has the potential of\nachieving competitive image quality while being much faster and resource frugal\nthan the straightforward CNN implementation. The main technical challenge\nfacing the LUT-based CNN algorithm designers is to manage the table size\nwithout overly restricting the receptive field. The prevailing strategy is to\nreuse the table for small pixel patches of different orientations (apparently\nassuming a degree of isotropy) and then fuse the look-up results. The fusion is\ncurrently done by average pooling, which we find being ill suited to\nanisotropic signal structures. To alleviate the problem, we investigate and\ndiscuss anisotropic pooling methods to replace naive averaging for improving\nthe performance of the current LUT-realizable CNN restoration methods. First,\nwe introduce the method of generalized median pooling which leads to measurable\ngains over average pooling. We then extend this idea by learning data-dependent\npooling coefficients for each orientation, so that they can adaptively weigh\nthe contributions of differently oriented pixel patches. Experimental results\non various restoration benchmarks show that our anisotropic pooling strategy\nyields both perceptually and numerically superior results compared to existing\nLUT-realizable CNN methods.\n","authors":["Xi Zhang","Xiaolin Wu"],"pdf_url":"https://arxiv.org/pdf/2510.21437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22342v3","updated":"2025-10-24T13:10:31Z","published":"2025-05-28T13:26:52Z","title":"Progressive Data Dropout: An Embarrassingly Simple Approach to Faster\n  Training","summary":"  The success of the machine learning field has reliably depended on training\non large datasets. While effective, this trend comes at an extraordinary cost.\nThis is due to two deeply intertwined factors: the size of models and the size\nof datasets. While promising research efforts focus on reducing the size of\nmodels, the other half of the equation remains fairly mysterious. Indeed, it is\nsurprising that the standard approach to training remains to iterate over and\nover, uniformly sampling the training dataset. In this paper we explore a\nseries of alternative training paradigms that leverage insights from\nhard-data-mining and dropout, simple enough to implement and use that can\nbecome the new training standard. The proposed Progressive Data Dropout reduces\nthe number of effective epochs to as little as 12.4% of the baseline. This\nsavings actually do not come at any cost for accuracy. Surprisingly, the\nproposed method improves accuracy by up to 4.82%. Our approach requires no\nchanges to model architecture or optimizer, and can be applied across standard\ntraining pipelines, thus posing an excellent opportunity for wide adoption.\nCode can be found here: https://github.com/bazyagami/LearningWithRevision\n","authors":["Shriram M Sathiyanarayanan","Xinyue Hao","Shihao Hou","Yang Lu","Laura Sevilla-Lara","Anurag Arnab","Shreyank N Gowda"],"pdf_url":"https://arxiv.org/pdf/2505.22342v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21432v1","updated":"2025-10-24T13:08:15Z","published":"2025-10-24T13:08:15Z","title":"ArtiLatent: Realistic Articulated 3D Object Generation via Structured\n  Latents","summary":"  We propose ArtiLatent, a generative framework that synthesizes human-made 3D\nobjects with fine-grained geometry, accurate articulation, and realistic\nappearance. Our approach jointly models part geometry and articulation dynamics\nby embedding sparse voxel representations and associated articulation\nproperties, including joint type, axis, origin, range, and part category, into\na unified latent space via a variational autoencoder. A latent diffusion model\nis then trained over this space to enable diverse yet physically plausible\nsampling. To reconstruct photorealistic 3D shapes, we introduce an\narticulation-aware Gaussian decoder that accounts for articulation-dependent\nvisibility changes (e.g., revealing the interior of a drawer when opened). By\nconditioning appearance decoding on articulation state, our method assigns\nplausible texture features to regions that are typically occluded in static\nposes, significantly improving visual realism across articulation\nconfigurations. Extensive experiments on furniture-like objects from\nPartNet-Mobility and ACD datasets demonstrate that ArtiLatent outperforms\nexisting approaches in geometric consistency and appearance fidelity. Our\nframework provides a scalable solution for articulated 3D object synthesis and\nmanipulation.\n","authors":["Honghua Chen","Yushi Lan","Yongwei Chen","Xingang Pan"],"pdf_url":"https://arxiv.org/pdf/2510.21432v1.pdf","comment":"accepted to SIGGRAPH Asia; Project page:\n  https://chenhonghua.github.io/MyProjects/ArtiLatent/"},{"id":"http://arxiv.org/abs/2510.21424v1","updated":"2025-10-24T13:04:13Z","published":"2025-10-24T13:04:13Z","title":"Vision Language Models for Dynamic Human Activity Recognition in\n  Healthcare Settings","summary":"  As generative AI continues to evolve, Vision Language Models (VLMs) have\nemerged as promising tools in various healthcare applications. One area that\nremains relatively underexplored is their use in human activity recognition\n(HAR) for remote health monitoring. VLMs offer notable strengths, including\ngreater flexibility and the ability to overcome some of the constraints of\ntraditional deep learning models. However, a key challenge in applying VLMs to\nHAR lies in the difficulty of evaluating their dynamic and often\nnon-deterministic outputs. To address this gap, we introduce a descriptive\ncaption data set and propose comprehensive evaluation methods to evaluate VLMs\nin HAR. Through comparative experiments with state-of-the-art deep learning\nmodels, our findings demonstrate that VLMs achieve comparable performance and,\nin some cases, even surpass conventional approaches in terms of accuracy. This\nwork contributes a strong benchmark and opens new possibilities for the\nintegration of VLMs into intelligent healthcare systems.\n","authors":["Abderrazek Abid","Thanh-Cong Ho","Fakhri Karray"],"pdf_url":"https://arxiv.org/pdf/2510.21424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10226v2","updated":"2025-10-24T13:03:54Z","published":"2025-06-11T23:06:44Z","title":"ScoreMix: Synthetic Data Generation by Score Composition in Diffusion\n  Models Improves Recognition","summary":"  Synthetic data generation is increasingly used in machine learning for\ntraining and data augmentation. Yet, current strategies often rely on external\nfoundation models or datasets, whose usage is restricted in many scenarios due\nto policy or legal constraints. We propose ScoreMix, a self-contained synthetic\ngeneration method to produce hard synthetic samples for recognition tasks by\nleveraging the score compositionality of diffusion models. The approach mixes\nclass-conditioned scores along reverse diffusion trajectories, yielding\ndomain-specific data augmentation without external resources. We systematically\nstudy class-selection strategies and find that mixing classes distant in the\ndiscriminator's embedding space yields larger gains, providing up to 3%\nadditional average improvement, compared to selection based on proximity.\nInterestingly, we observe that condition and embedding spaces are largely\nuncorrelated under standard alignment metrics, and the generator's condition\nspace has a negligible effect on downstream performance. Across 8 public face\nrecognition benchmarks, ScoreMix improves accuracy by up to 7 percentage\npoints, without hyperparameter search, highlighting both robustness and\npracticality. Our method provides a simple yet effective way to maximize\ndiscriminator performance using only the available dataset, without reliance on\nthird-party resources. Paper website: https://parsa-ra.github.io/scoremix/.\n","authors":["Parsa Rahimi","Sebastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2506.10226v2.pdf","comment":"Extended version of ICMLw25 Oral"},{"id":"http://arxiv.org/abs/2505.22854v2","updated":"2025-10-24T13:03:13Z","published":"2025-05-28T20:41:24Z","title":"CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian\n  Splatting","summary":"  Gaussian Splatting (GS) has recently emerged as an efficient representation\nfor rendering 3D scenes from 2D images and has been extended to images, videos,\nand dynamic 4D content. However, applying style transfer to GS-based\nrepresentations, especially beyond simple color changes, remains challenging.\nIn this work, we introduce CLIPGaussian, the first unified style transfer\nframework that supports text- and image-guided stylization across multiple\nmodalities: 2D images, videos, 3D objects, and 4D scenes. Our method operates\ndirectly on Gaussian primitives and integrates into existing GS pipelines as a\nplug-in module, without requiring large generative models or retraining from\nscratch. The CLIPGaussian approach enables joint optimization of color and\ngeometry in 3D and 4D settings, and achieves temporal coherence in videos,\nwhile preserving the model size. We demonstrate superior style fidelity and\nconsistency across all tasks, validating CLIPGaussian as a universal and\nefficient solution for multimodal style transfer.\n","authors":["Kornel Howil","Joanna Waczyńska","Piotr Borycki","Tadeusz Dziarmaga","Marcin Mazur","Przemysław Spurek"],"pdf_url":"https://arxiv.org/pdf/2505.22854v2.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.14674v2","updated":"2025-10-24T13:02:36Z","published":"2025-06-17T16:07:58Z","title":"Recognition through Reasoning: Reinforcing Image Geo-localization with\n  Large Vision-Language Models","summary":"  Previous methods for image geo-localization have typically treated the task\nas either classification or retrieval, often relying on black-box decisions\nthat lack interpretability. The rise of large vision-language models (LVLMs)\nhas enabled a rethinking of geo-localization as a reasoning-driven task\ngrounded in visual cues. However, two major challenges persist. On the data\nside, existing reasoning-focused datasets are primarily based on street-view\nimagery, offering limited scene diversity and constrained viewpoints. On the\nmodeling side, current approaches predominantly rely on supervised fine-tuning,\nwhich yields only marginal improvements in reasoning capabilities. To address\nthese challenges, we propose a novel pipeline that constructs a\nreasoning-oriented geo-localization dataset, MP16-Reason, using diverse social\nmedia images. We introduce GLOBE, Group-relative policy optimization for\nLocalizability assessment and Optimized visual-cue reasoning, yielding\nBi-objective geo-Enhancement for the VLM in recognition and reasoning. GLOBE\nincorporates task-specific rewards that jointly enhance localizability\nassessment, visual-cue reasoning, and geolocation accuracy. Both qualitative\nand quantitative results demonstrate that GLOBE outperforms state-of-the-art\nopen-source LVLMs on geo-localization tasks, particularly in diverse visual\nscenes, while also generating more insightful and interpretable reasoning\ntrajectories. The data and code are available at\nhttps://github.com/lingli1996/GLOBE.\n","authors":["Ling Li","Yao Zhou","Yuxuan Liang","Fugee Tsung","Jiaheng Wei"],"pdf_url":"https://arxiv.org/pdf/2506.14674v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.21412v1","updated":"2025-10-24T12:54:13Z","published":"2025-10-24T12:54:13Z","title":"Bridging the gap to real-world language-grounded visual concept learning","summary":"  Human intelligence effortlessly interprets visual scenes along a rich\nspectrum of semantic dimensions. However, existing approaches to\nlanguage-grounded visual concept learning are limited to a few predefined\nprimitive axes, such as color and shape, and are typically explored in\nsynthetic datasets. In this work, we propose a scalable framework that\nadaptively identifies image-related concept axes and grounds visual concepts\nalong these axes in real-world scenes. Leveraging a pretrained vision-language\nmodel and our universal prompting strategy, our framework identifies a diverse\nimage-related axes without any prior knowledge. Our universal concept encoder\nadaptively binds visual features to the discovered axes without introducing\nadditional model parameters for each concept. To ground visual concepts along\nthe discovered axes, we optimize a compositional anchoring objective, which\nensures that each axis can be independently manipulated without affecting\nothers. We demonstrate the effectiveness of our framework on subsets of\nImageNet, CelebA-HQ, and AFHQ, showcasing superior editing capabilities across\ndiverse real-world concepts that are too varied to be manually predefined. Our\nmethod also exhibits strong compositional generalization, outperforming\nexisting visual concept learning and text-based editing methods. The code is\navailable at https://github.com/whieya/Language-grounded-VCL.\n","authors":["Whie Jung","Semin Kim","Junee Kim","Seunghoon Hong"],"pdf_url":"https://arxiv.org/pdf/2510.21412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.20671v2","updated":"2025-10-24T12:53:30Z","published":"2025-06-25T17:59:45Z","title":"IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive\n  Instance Proposals","summary":"  Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly\nlearning scene geometry and semantics, enabling downstream applications such as\nnavigation in mobile robotics. The recent generalization to Panoptic Scene\nCompletion (PSC) advances the SSC domain by integrating instance-level\ninformation, thereby enhancing object-level sensitivity in scene understanding.\nWhile PSC was introduced using LiDAR modality, methods based on camera images\nremain largely unexplored. Moreover, recent Transformer-based approaches\nutilize a fixed set of learned queries to reconstruct objects within the scene\nvolume. Although these queries are typically updated with image context during\ntraining, they remain static at test time, limiting their ability to\ndynamically adapt specifically to the observed scene. To overcome these\nlimitations, we propose IPFormer, the first method that leverages\ncontext-adaptive instance proposals at train and test time to address\nvision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively\ninitializes these queries as panoptic instance proposals derived from image\ncontext and further refines them through attention-based encoding and decoding\nto reason about semantic instance-voxel relationships. Extensive experimental\nresults show that our approach achieves state-of-the-art in-domain performance,\nexhibits superior zero-shot generalization on out-of-domain data, and achieves\na runtime reduction exceeding 14x. These results highlight our introduction of\ncontext-adaptive instance proposals as a pioneering effort in addressing\nvision-based 3D Panoptic Scene Completion.\n","authors":["Markus Gross","Aya Fahmy","Danit Niwattananan","Dominik Muhle","Rui Song","Daniel Cremers","Henri Meeß"],"pdf_url":"https://arxiv.org/pdf/2506.20671v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21406v1","updated":"2025-10-24T12:50:02Z","published":"2025-10-24T12:50:02Z","title":"MUVR: A Multi-Modal Untrimmed Video Retrieval Benchmark with Multi-Level\n  Visual Correspondence","summary":"  We propose the Multi-modal Untrimmed Video Retrieval task, along with a new\nbenchmark (MUVR) to advance video retrieval for long-video platforms. MUVR aims\nto retrieve untrimmed videos containing relevant segments using multi-modal\nqueries. It has the following features: 1) Practical retrieval paradigm: MUVR\nsupports video-centric multi-modal queries, expressing fine-grained retrieval\nneeds through long text descriptions, video tag prompts, and mask prompts. It\nadopts a one-to-many retrieval paradigm and focuses on untrimmed videos,\ntailored for long-video platform applications. 2) Multi-level visual\ncorrespondence: To cover common video categories (e.g., news, travel, dance)\nand precisely define retrieval matching criteria, we construct multi-level\nvisual correspondence based on core video content (e.g., news events, travel\nlocations, dance moves) which users are interested in and want to retrieve. It\ncovers six levels: copy, event, scene, instance, action, and others. 3)\nComprehensive evaluation criteria: We develop 3 versions of MUVR (i.e., Base,\nFilter, QA). MUVR-Base/Filter evaluates retrieval models, while MUVR-QA\nassesses MLLMs in a question-answering format. We also propose a Reranking\nScore to evaluate the reranking ability of MLLMs. MUVR consists of 53K\nuntrimmed videos from the video platform Bilibili, with 1,050 multi-modal\nqueries and 84K matches. Extensive evaluations of 3 state-of-the-art video\nretrieval models, 6 image-based VLMs, and 10 MLLMs are conducted. MUVR reveals\nthe limitations of retrieval methods in processing untrimmed videos and\nmulti-modal queries, as well as MLLMs in multi-video understanding and\nreranking. Our code and benchmark is available at\nhttps://github.com/debby-0527/MUVR.\n","authors":["Yue Feng","Jinwei Hu","Qijia Lu","Jiawei Niu","Li Tan","Shuo Yuan","Ziyi Yan","Yizhen Jia","Qingzhi He","Shiping Ge","Ethan Q. Chen","Wentong Li","Limin Wang","Jie Qin"],"pdf_url":"https://arxiv.org/pdf/2510.21406v1.pdf","comment":"Accepted to NeurIPS 2025 D&B Track"},{"id":"http://arxiv.org/abs/2510.21402v1","updated":"2025-10-24T12:46:19Z","published":"2025-10-24T12:46:19Z","title":"Disentangled Representation Learning via Modular Compositional Bias","summary":"  Recent disentangled representation learning (DRL) methods heavily rely on\nfactor specific strategies-either learning objectives for attributes or model\narchitectures for objects-to embed inductive biases. Such divergent approaches\nresult in significant overhead when novel factors of variation do not align\nwith prior assumptions, such as statistical independence or spatial\nexclusivity, or when multiple factors coexist, as practitioners must redesign\narchitectures or objectives. To address this, we propose a compositional bias,\na modular inductive bias decoupled from both objectives and architectures. Our\nkey insight is that different factors obey distinct recombination rules in the\ndata distribution: global attributes are mutually exclusive, e.g., a face has\none nose, while objects share a common support (any subset of objects can\nco-exist). We therefore randomly remix latents according to factor-specific\nrules, i.e., a mixing strategy, and force the encoder to discover whichever\nfactor structure the mixing strategy reflects through two complementary\nobjectives: (i) a prior loss that ensures every remix decodes into a realistic\nimage, and (ii) the compositional consistency loss introduced by Wiedemer et\nal. (arXiv:2310.05327), which aligns each composite image with its\ncorresponding composite latent. Under this general framework, simply adjusting\nthe mixing strategy enables disentanglement of attributes, objects, and even\nboth, without modifying the objectives or architectures. Extensive experiments\ndemonstrate that our method shows competitive performance in both attribute and\nobject disentanglement, and uniquely achieves joint disentanglement of global\nstyle and objects. Code is available at\nhttps://github.com/whieya/Compositional-DRL.\n","authors":["Whie Jung","Dong Hoon Lee","Seunghoon Hong"],"pdf_url":"https://arxiv.org/pdf/2510.21402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21396v1","updated":"2025-10-24T12:36:08Z","published":"2025-10-24T12:36:08Z","title":"Depth-Supervised Fusion Network for Seamless-Free Image Stitching","summary":"  Image stitching synthesizes images captured from multiple perspectives into a\nsingle image with a broader field of view. The significant variations in object\ndepth often lead to large parallax, resulting in ghosting and misalignment in\nthe stitched results. To address this, we propose a\ndepth-consistency-constrained seamless-free image stitching method. First, to\ntackle the multi-view alignment difficulties caused by parallax, a multi-stage\nmechanism combined with global depth regularization constraints is developed to\nenhance the alignment accuracy of the same apparent target across different\ndepth ranges. Second, during the multi-view image fusion process, an optimal\nstitching seam is determined through graph-based low-cost computation, and a\nsoft-seam region is diffused to precisely locate transition areas, thereby\neffectively mitigating alignment errors induced by parallax and achieving\nnatural and seamless stitching results. Furthermore, considering the\ncomputational overhead in the shift regression process, a reparameterization\nstrategy is incorporated to optimize the structural design, significantly\nimproving algorithm efficiency while maintaining optimal performance. Extensive\nexperiments demonstrate the superior performance of the proposed method against\nthe existing methods. Code is available at https://github.com/DLUT-YRH/DSFN.\n","authors":["Zhiying Jiang","Ruhao Yan","Zengxi Zhang","Bowei Zhang","Jinyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2510.21396v1.pdf","comment":"Accepted to Neurips 2025"},{"id":"http://arxiv.org/abs/2510.21391v1","updated":"2025-10-24T12:29:12Z","published":"2025-10-24T12:29:12Z","title":"TerraGen: A Unified Multi-Task Layout Generation Framework for Remote\n  Sensing Data Augmentation","summary":"  Remote sensing vision tasks require extensive labeled data across multiple,\ninterconnected domains. However, current generative data augmentation\nframeworks are task-isolated, i.e., each vision task requires training an\nindependent generative model, and ignores the modeling of geographical\ninformation and spatial constraints. To address these issues, we propose\n\\textbf{TerraGen}, a unified layout-to-image generation framework that enables\nflexible, spatially controllable synthesis of remote sensing imagery for\nvarious high-level vision tasks, e.g., detection, segmentation, and extraction.\nSpecifically, TerraGen introduces a geographic-spatial layout encoder that\nunifies bounding box and segmentation mask inputs, combined with a multi-scale\ninjection scheme and mask-weighted loss to explicitly encode spatial\nconstraints, from global structures to fine details. Also, we construct the\nfirst large-scale multi-task remote sensing layout generation dataset\ncontaining 45k images and establish a standardized evaluation protocol for this\ntask. Experimental results show that our TerraGen can achieve the best\ngeneration image quality across diverse tasks. Additionally, TerraGen can be\nused as a universal data-augmentation generator, enhancing downstream task\nperformance significantly and demonstrating robust cross-task generalisation in\nboth full-data and few-shot scenarios.\n","authors":["Datao Tang","Hao Wang","Yudeng Xin","Hui Qiao","Dongsheng Jiang","Yin Li","Zhiheng Yu","Xiangyong Cao"],"pdf_url":"https://arxiv.org/pdf/2510.21391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17982v4","updated":"2025-10-24T11:54:03Z","published":"2025-05-23T14:48:32Z","title":"Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language\n  Alignment and Modeling","summary":"  Vision-language models (VLMs) have recently been integrated into multiple\ninstance learning (MIL) frameworks to address the challenge of few-shot, weakly\nsupervised classification of whole slide images (WSIs). A key trend involves\nleveraging multi-scale information to better represent hierarchical tissue\nstructures. However, existing methods often face two key limitations: (1)\ninsufficient modeling of interactions within the same modalities across scales\n(e.g., 5x and 20x) and (2) inadequate alignment between visual and textual\nmodalities on the same scale. To address these gaps, we propose HiVE-MIL, a\nhierarchical vision-language framework that constructs a unified graph\nconsisting of (1) parent-child links between coarse (5x) and fine (20x)\nvisual/textual nodes to capture hierarchical relationships, and (2)\nheterogeneous intra-scale edges linking visual and textual nodes on the same\nscale. To further enhance semantic consistency, HiVE-MIL incorporates a\ntwo-stage, text-guided dynamic filtering mechanism that removes weakly\ncorrelated patch-text pairs, and introduces a hierarchical contrastive loss to\nalign textual semantics across scales. Extensive experiments on TCGA breast,\nlung, and kidney cancer datasets demonstrate that HiVE-MIL consistently\noutperforms both traditional MIL and recent VLM-based MIL approaches, achieving\ngains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate\nthe value of jointly modeling hierarchical structure and multimodal alignment\nfor efficient and scalable learning from limited pathology data. The code is\navailable at https://github.com/bryanwong17/HiVE-MIL.\n","authors":["Bryan Wong","Jong Woo Kim","Huazhu Fu","Mun Yong Yi"],"pdf_url":"https://arxiv.org/pdf/2505.17982v4.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.21366v1","updated":"2025-10-24T11:50:03Z","published":"2025-10-24T11:50:03Z","title":"BADiff: Bandwidth Adaptive Diffusion Model","summary":"  In this work, we propose a novel framework to enable diffusion models to\nadapt their generation quality based on real-time network bandwidth\nconstraints. Traditional diffusion models produce high-fidelity images by\nperforming a fixed number of denoising steps, regardless of downstream\ntransmission limitations. However, in practical cloud-to-device scenarios,\nlimited bandwidth often necessitates heavy compression, leading to loss of fine\ntextures and wasted computation. To address this, we introduce a joint\nend-to-end training strategy where the diffusion model is conditioned on a\ntarget quality level derived from the available bandwidth. During training, the\nmodel learns to adaptively modulate the denoising process, enabling early-stop\nsampling that maintains perceptual quality appropriate to the target\ntransmission condition. Our method requires minimal architectural changes and\nleverages a lightweight quality embedding to guide the denoising trajectory.\nExperimental results demonstrate that our approach significantly improves the\nvisual fidelity of bandwidth-adapted generations compared to naive\nearly-stopping, offering a promising solution for efficient image delivery in\nbandwidth-constrained environments. Code is available at:\nhttps://github.com/xzhang9308/BADiff.\n","authors":["Xi Zhang","Hanwei Zhu","Yan Zhong","Jiamang Wang","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2510.21366v1.pdf","comment":"NeurIPS 2025 Poster"},{"id":"http://arxiv.org/abs/2510.21363v1","updated":"2025-10-24T11:47:15Z","published":"2025-10-24T11:47:15Z","title":"FairImagen: Post-Processing for Bias Mitigation in Text-to-Image Models","summary":"  Text-to-image diffusion models, such as Stable Diffusion, have demonstrated\nremarkable capabilities in generating high-quality and diverse images from\nnatural language prompts. However, recent studies reveal that these models\noften replicate and amplify societal biases, particularly along demographic\nattributes like gender and race. In this paper, we introduce FairImagen\n(https://github.com/fuzihaofzh/FairImagen), a post-hoc debiasing framework that\noperates on prompt embeddings to mitigate such biases without retraining or\nmodifying the underlying diffusion model. Our method integrates Fair Principal\nComponent Analysis to project CLIP-based input embeddings into a subspace that\nminimizes group-specific information while preserving semantic content. We\nfurther enhance debiasing effectiveness through empirical noise injection and\npropose a unified cross-demographic projection method that enables simultaneous\ndebiasing across multiple demographic attributes. Extensive experiments across\ngender, race, and intersectional settings demonstrate that FairImagen\nsignificantly improves fairness with a moderate trade-off in image quality and\nprompt fidelity. Our framework outperforms existing post-hoc methods and offers\na simple, scalable, and model-agnostic solution for equitable text-to-image\ngeneration.\n","authors":["Zihao Fu","Ryan Brown","Shun Shao","Kai Rawal","Eoin Delaney","Chris Russell"],"pdf_url":"https://arxiv.org/pdf/2510.21363v1.pdf","comment":"Neurips 2025"},{"id":"http://arxiv.org/abs/2510.21358v1","updated":"2025-10-24T11:40:21Z","published":"2025-10-24T11:40:21Z","title":"Why Registration Quality Matters: Enhancing sCT Synthesis with\n  IMPACT-Based Registration","summary":"  We participated in the SynthRAD2025 challenge (Tasks 1 and 2) with a unified\npipeline for synthetic CT (sCT) generation from MRI and CBCT, implemented using\nthe KonfAI framework. Our model is a 2.5D U-Net++ with a ResNet-34 encoder,\ntrained jointly across anatomical regions and fine-tuned per region. The loss\nfunction combined pixel-wise L1 loss with IMPACT-Synth, a perceptual loss\nderived from SAM and TotalSegmentator to enhance structural fidelity. Training\nwas performed using AdamW (initial learning rate = 0.001, halved every 25k\nsteps) on patch-based, normalized, body-masked inputs (320x320 for MRI, 256x256\nfor CBCT), with random flipping as the only augmentation. No post-processing\nwas applied. Final predictions leveraged test-time augmentation and five-fold\nensembling. The best model was selected based on validation MAE. Two\nregistration strategies were evaluated: (i) Elastix with mutual information,\nconsistent with the challenge pipeline, and (ii) IMPACT, a feature-based\nsimilarity metric leveraging pretrained segmentation networks. On the local\ntest sets, IMPACT-based registration achieved more accurate and anatomically\nconsistent alignments than mutual-information-based registration, resulting in\nimproved sCT synthesis with lower MAE and more realistic anatomical structures.\nOn the public validation set, however, models trained with Elastix-aligned data\nachieved higher scores, reflecting a registration bias favoring alignment\nstrategies consistent with the evaluation pipeline. This highlights how\nregistration errors can propagate into supervised learning, influencing both\ntraining and evaluation, and potentially inflating performance metrics at the\nexpense of anatomical fidelity. By promoting anatomically consistent alignment,\nIMPACT helps mitigate this bias and supports the development of more robust and\ngeneralizable sCT synthesis models.\n","authors":["Valentin Boussot","Cédric Hémon","Jean-Claude Nunes","Jean-Louis Dillenseger"],"pdf_url":"https://arxiv.org/pdf/2510.21358v1.pdf","comment":"Paper for the SynthRAD2025 challenge, Team BreizhCT"},{"id":"http://arxiv.org/abs/2510.21356v1","updated":"2025-10-24T11:33:03Z","published":"2025-10-24T11:33:03Z","title":"Gaze-VLM:Bridging Gaze and VLMs through Attention Regularization for\n  Egocentric Understanding","summary":"  Eye gaze offers valuable cues about attention, short-term intent, and future\nactions, making it a powerful signal for modeling egocentric behavior. In this\nwork, we propose a gaze-regularized framework that enhances VLMs for two key\negocentric understanding tasks: fine-grained future event prediction and\ncurrent activity understanding. Unlike prior approaches that rely solely on\nvisual inputs or use gaze as an auxiliary input signal , our method uses gaze\nonly during training. We introduce a gaze-regularized attention mechanism that\naligns model focus with human visual gaze. This design is flexible and modular,\nallowing it to generalize across multiple VLM architectures that utilize\nattention. Experimental results show that our approach improves semantic\nprediction scores by up to 11 for future event prediction and around 7 for\ncurrent activity understanding, compared to the corresponding baseline models\ntrained without gaze regularization. These results highlight the value of\ngaze-guided training in improving the accuracy and robustness of egocentric\nVLMs. Overall, this work establishes a foundation for using human gaze to\nenhance the predictive capabilities of VLMs in real-world scenarios like\nassistive robots and human-machine collaboration. Code and additional\ninformation is available at: https://github.com/anupampani/Gaze-VLM\n","authors":["Anupam Pani","Yanchao Yang"],"pdf_url":"https://arxiv.org/pdf/2510.21356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05892v2","updated":"2025-10-24T11:31:50Z","published":"2025-05-09T09:00:17Z","title":"Register and [CLS] tokens yield a decoupling of local and global\n  features in large ViTs","summary":"  Recent work has shown that the attention maps of the widely popular DINOv2\nmodel exhibit artifacts, which hurt both model interpretability and performance\non dense image tasks. These artifacts emerge due to the model repurposing patch\ntokens with redundant local information for the storage of global image\ninformation. To address this problem, additional register tokens have been\nincorporated in which the model can store such information instead. We\ncarefully examine the influence of these register tokens on the relationship\nbetween global and local image features, showing that while register tokens\nyield cleaner attention maps, these maps do not accurately reflect the\nintegration of local image information in large models. Instead, global\ninformation is dominated by information extracted from register tokens, leading\nto a disconnect between local and global features. Inspired by these findings,\nwe show that the [CLS] token itself leads to a very similar phenomenon in\nmodels without explicit register tokens. Our work shows that care must be taken\nwhen interpreting attention maps of large ViTs. Further, by clearly attributing\nthe faulty behavior to register and [CLS] tokens, we show a path towards more\ninterpretable vision models.\n","authors":["Alexander Lappe","Martin A. Giese"],"pdf_url":"https://arxiv.org/pdf/2505.05892v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21351v1","updated":"2025-10-24T11:28:06Z","published":"2025-10-24T11:28:06Z","title":"Dynamic Semantic-Aware Correlation Modeling for UAV Tracking","summary":"  UAV tracking can be widely applied in scenarios such as disaster rescue,\nenvironmental monitoring, and logistics transportation. However, existing UAV\ntracking methods predominantly emphasize speed and lack exploration in semantic\nawareness, which hinders the search region from extracting accurate\nlocalization information from the template. The limitation results in\nsuboptimal performance under typical UAV tracking challenges such as camera\nmotion, fast motion, and low resolution, etc. To address this issue, we propose\na dynamic semantic aware correlation modeling tracking framework. The core of\nour framework is a Dynamic Semantic Relevance Generator, which, in combination\nwith the correlation map from the Transformer, explore semantic relevance. The\napproach enhances the search region's ability to extract important information\nfrom the template, improving accuracy and robustness under the aforementioned\nchallenges. Additionally, to enhance the tracking speed, we design a pruning\nmethod for the proposed framework. Therefore, we present multiple model\nvariants that achieve trade-offs between speed and accuracy, enabling flexible\ndeployment according to the available computational resources. Experimental\nresults validate the effectiveness of our method, achieving competitive\nperformance on multiple UAV tracking datasets. The code is available at\nhttps://github.com/zxyyxzz/DSATrack.\n","authors":["Xinyu Zhou","Tongxin Pan","Lingyi Hong","Pinxue Guo","Haijing Guo","Zhaoyu Chen","Kaixun Jiang","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.21351v1.pdf","comment":"Accepted by NeurIPS2025"},{"id":"http://arxiv.org/abs/2510.03317v2","updated":"2025-10-24T11:24:57Z","published":"2025-10-01T01:18:27Z","title":"Photorealistic Inpainting for Perturbation-based Explanations in\n  Ecological Monitoring","summary":"  Ecological monitoring is increasingly automated by vision models, yet opaque\npredictions limit trust and field adoption. We present an inpainting-guided,\nperturbation-based explanation technique that produces photorealistic,\nmask-localized edits that preserve scene context. Unlike masking or blurring,\nthese edits stay in-distribution and reveal which fine-grained morphological\ncues drive predictions in tasks such as species recognition and trait\nattribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for\nharbor seal detection in Glacier Bay drone imagery, using\nSegment-Anything-Model-refined masks to support two interventions: (i) object\nremoval/replacement (e.g., replacing seals with plausible ice/water or boats)\nand (ii) background replacement with original animals composited onto new\nscenes. Explanations are assessed by re-scoring perturbed images (flip rate,\nconfidence drop) and by expert review for ecological plausibility and\ninterpretability. The resulting explanations localize diagnostic structures,\navoid deletion artifacts common to traditional perturbations, and yield\ndomain-relevant insights that support expert validation and more trustworthy\ndeployment of AI in ecology.\n","authors":["Günel Aghakishiyeva","Jiayi Zhou","Saagar Arya","Julian Dale","James David Poling","Holly R. Houliston","Jamie N. Womble","Gregory D. Larsen","David W. Johnston","Brinnae Bent"],"pdf_url":"https://arxiv.org/pdf/2510.03317v2.pdf","comment":"NeurIPS 2025 Imageomics Workshop"},{"id":"http://arxiv.org/abs/2510.21346v1","updated":"2025-10-24T11:23:47Z","published":"2025-10-24T11:23:47Z","title":"CT-CLIP: A Multi-modal Fusion Framework for Robust Apple Leaf Disease\n  Recognition in Complex Environments","summary":"  In complex orchard environments, the phenotypic heterogeneity of different\napple leaf diseases, characterized by significant variation among lesions,\nposes a challenge to traditional multi-scale feature fusion methods. These\nmethods only integrate multi-layer features extracted by convolutional neural\nnetworks (CNNs) and fail to adequately account for the relationships between\nlocal and global features. Therefore, this study proposes a multi-branch\nrecognition framework named CNN-Transformer-CLIP (CT-CLIP). The framework\nsynergistically employs a CNN to extract local lesion detail features and a\nVision Transformer to capture global structural relationships. An Adaptive\nFeature Fusion Module (AFFM) then dynamically fuses these features, achieving\noptimal coupling of local and global information and effectively addressing the\ndiversity in lesion morphology and distribution. Additionally, to mitigate\ninterference from complex backgrounds and significantly enhance recognition\naccuracy under few-shot conditions, this study proposes a multimodal image-text\nlearning approach. By leveraging pre-trained CLIP weights, it achieves deep\nalignment between visual features and disease semantic descriptions.\nExperimental results show that CT-CLIP achieves accuracies of 97.38% and 96.12%\non a publicly available apple disease and a self-built dataset, outperforming\nseveral baseline methods. The proposed CT-CLIP demonstrates strong capabilities\nin recognizing agricultural diseases, significantly enhances identification\naccuracy under complex environmental conditions, provides an innovative and\npractical solution for automated disease recognition in agricultural\napplications.\n","authors":["Lemin Liu","Fangchao Hu","Honghua Jiang","Yaru Chen","Limin Liu","Yongliang Qiao"],"pdf_url":"https://arxiv.org/pdf/2510.21346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03674v2","updated":"2025-10-24T11:09:01Z","published":"2025-01-07T10:20:16Z","title":"Action Quality Assessment via Hierarchical Pose-guided Multi-stage\n  Contrastive Regression","summary":"  Action Quality Assessment (AQA), which aims at automatic and fair evaluation\nof athletic performance, has gained increasing attention in recent years.\nHowever, athletes are often in rapid movement and the corresponding visual\nappearance variances are subtle, making it challenging to capture fine-grained\npose differences and leading to poor estimation performance. Furthermore, most\ncommon AQA tasks, such as diving in sports, are usually divided into multiple\nsub-actions, each of which contains different durations. However, existing\nmethods focus on segmenting the video into fixed frames, which disrupts the\ntemporal continuity of sub-actions resulting in unavoidable prediction errors.\nTo address these challenges, we propose a novel action quality assessment\nmethod through hierarchically pose-guided multi-stage contrastive regression.\nFirstly, we introduce a multi-scale dynamic visual-skeleton encoder to capture\nfine-grained spatio-temporal visual and skeletal features. Then, a procedure\nsegmentation network is introduced to separate different sub-actions and obtain\nsegmented features. Afterwards, the segmented visual and skeletal features are\nboth fed into a multi-modal fusion module as physics structural priors, to\nguide the model in learning refined activity similarities and variances.\nFinally, a multi-stage contrastive learning regression approach is employed to\nlearn discriminative representations and output prediction results. In\naddition, we introduce a newly-annotated FineDiving-Pose Dataset to improve the\ncurrent low-quality human pose labels. In experiments, the results on\nFineDiving and MTL-AQA datasets demonstrate the effectiveness and superiority\nof our proposed approach. Our source code and dataset are available at\nhttps://github.com/Lumos0507/HP-MCoRe.\n","authors":["Mengshi Qi","Hao Ye","Jiaxuan Peng","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2501.03674v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21337v1","updated":"2025-10-24T11:03:20Z","published":"2025-10-24T11:03:20Z","title":"Morphologically Intelligent Perturbation Prediction with FORM","summary":"  Understanding how cells respond to external stimuli is a central challenge in\nbiomedical research and drug development. Current computational frameworks for\nmodelling cellular responses remain restricted to two-dimensional\nrepresentations, limiting their capacity to capture the complexity of cell\nmorphology under perturbation. This dimensional constraint poses a critical\nbottleneck for the development of accurate virtual cell models. Here, we\npresent FORM, a machine learning framework for predicting perturbation-induced\nchanges in three-dimensional cellular structure. FORM consists of two\ncomponents: a morphology encoder, trained end-to-end via a novel multi-channel\nVQGAN to learn compact 3D representations of cell shape, and a diffusion-based\nperturbation trajectory module that captures how morphology evolves across\nperturbation conditions. Trained on a large-scale dataset of over 65,000\nmulti-fluorescence 3D cell volumes spanning diverse chemical and genetic\nperturbations, FORM supports both unconditional morphology synthesis and\nconditional simulation of perturbed cell states. Beyond generation, FORM can\npredict downstream signalling activity, simulate combinatorial perturbation\neffects, and model morphodynamic transitions between states of unseen\nperturbations. To evaluate performance, we introduce MorphoEval, a benchmarking\nsuite that quantifies perturbation-induced morphological changes in structural,\nstatistical, and biological dimensions. Together, FORM and MorphoEval work\ntoward the realisation of the 3D virtual cell by linking morphology,\nperturbation, and function through high-resolution predictive simulation.\n","authors":["Reed Naidoo","Matt De Vries","Olga Fourkioti","Vicky Bousgouni","Mar Arias-Garcia","Maria Portillo-Malumbres","Chris Bakal"],"pdf_url":"https://arxiv.org/pdf/2510.21337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.22690v2","updated":"2025-10-24T11:02:36Z","published":"2025-09-19T13:47:57Z","title":"A review of Recent Techniques for Person Re-Identification","summary":"  Person re-identification (ReId), a crucial task in surveillance, involves\nmatching individuals across different camera views. The advent of Deep\nLearning, especially supervised techniques like Convolutional Neural Networks\nand Attention Mechanisms, has significantly enhanced person Re-ID. However, the\nsuccess of supervised approaches hinges on vast amounts of annotated data,\nposing scalability challenges in data labeling and computational costs. To\naddress these limitations, recent research has shifted towards unsupervised\nperson re-identification. Leveraging abundant unlabeled data, unsupervised\nmethods aim to overcome the need for pairwise labelled data. Although\ntraditionally trailing behind supervised approaches, unsupervised techniques\nhave shown promising developments in recent years, signalling a narrowing\nperformance gap. Motivated by this evolving landscape, our survey pursues two\nprimary objectives. First, we review and categorize significant publications in\nsupervised person re-identification, providing an in-depth overview of the\ncurrent state-of-the-art and emphasizing little room for further improvement in\nthis domain. Second, we explore the latest advancements in unsupervised person\nre-identification over the past three years, offering insights into emerging\ntrends and shedding light on the potential convergence of performance between\nsupervised and unsupervised paradigms. This dual-focus survey aims to\ncontribute to the evolving narrative of person re-identification, capturing\nboth the mature landscape of supervised techniques and the promising outcomes\nin the realm of unsupervised learning.\n","authors":["Andrea Asperti","Salvatore Fiorilla","Simone Nardi","Lorenzo Orsini"],"pdf_url":"https://arxiv.org/pdf/2509.22690v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.08974v3","updated":"2025-10-24T10:53:51Z","published":"2025-08-12T14:37:53Z","title":"Text-conditioned State Space Model For Domain-generalized Change\n  Detection Visual Question Answering","summary":"  The Earth's surface is constantly changing, and detecting these changes\nprovides valuable insights that benefit various aspects of human society. While\ntraditional change detection methods have been employed to detect changes from\nbi-temporal images, these approaches typically require expert knowledge for\naccurate interpretation. To enable broader and more flexible access to change\ninformation by non-expert users, the task of Change Detection Visual Question\nAnswering (CDVQA) has been introduced. However, existing CDVQA methods have\nbeen developed under the assumption that training and testing datasets share\nsimilar distributions. This assumption does not hold in real-world\napplications, where domain shifts often occur. In this paper, the CDVQA task is\nrevisited with a focus on addressing domain shift. To this end, a new\nmulti-modal and multi-domain dataset, BrightVQA, is introduced to facilitate\ndomain generalization research in CDVQA. Furthermore, a novel state space\nmodel, termed Text-Conditioned State Space Model (TCSSM), is proposed. The\nTCSSM framework is designed to leverage both bi-temporal imagery and\ngeo-disaster-related textual information in an unified manner to extract\ndomain-invariant features across domains. Input-dependent parameters existing\nin TCSSM are dynamically predicted by using both bi-temporal images and\ngeo-disaster-related description, thereby facilitating the alignment between\nbi-temporal visual data and the associated textual descriptions. Extensive\nexperiments are conducted to evaluate the proposed method against\nstate-of-the-art models, and superior performance is consistently demonstrated.\nThe code and dataset will be made publicly available upon acceptance at\nhttps://github.com/Elman295/TCSSM.\n","authors":["Elman Ghazaei","Erchan Aptoula"],"pdf_url":"https://arxiv.org/pdf/2508.08974v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20189v2","updated":"2025-10-24T10:52:12Z","published":"2025-10-23T04:20:07Z","title":"SPAN: Continuous Modeling of Suspicion Progression for Temporal\n  Intention Localization","summary":"  Temporal Intention Localization (TIL) is crucial for video surveillance,\nfocusing on identifying varying levels of suspicious intentions to improve\nsecurity monitoring. However, existing discrete classification methods fail to\ncapture the continuous nature of suspicious intentions, limiting early\nintervention and explainability. In this paper, we propose the Suspicion\nProgression Analysis Network (SPAN), which shifts from discrete classification\nto continuous regression, enabling the capture of fluctuating and evolving\nsuspicious intentions. We reveal that suspicion exhibits long-term dependencies\nand cumulative effects, similar to Temporal Point Process (TPP) theory. Based\non these insights, we define a suspicion score formula that models continuous\nchanges while accounting for temporal characteristics. We also introduce\nSuspicion Coefficient Modulation, which adjusts suspicion coefficients using\nmultimodal information to reflect the varying impacts of suspicious actions.\nAdditionally, the Concept-Anchored Mapping method is proposed to link\nsuspicious actions to predefined intention concepts, offering insights into\nboth the actions and their potential underlying intentions. Extensive\nexperiments on the HAI dataset show that SPAN significantly outperforms\nexisting methods, reducing MSE by 19.8% and improving average mAP by 1.78%.\nNotably, SPAN achieves a 2.74% mAP gain in low-frequency cases, demonstrating\nits superior ability to capture subtle behavioral changes. Compared to discrete\nclassification systems, our continuous suspicion modeling approach enables\nearlier detection and proactive intervention, greatly enhancing system\nexplainability and practical utility in security applications.\n","authors":["Xinyi Hu","Yuran Wang","Ruixu Zhang","Yue Li","Wenxuan Liu","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2510.20189v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09538v2","updated":"2025-10-24T10:39:36Z","published":"2025-06-11T09:14:50Z","title":"AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant\n  T2I Adversarial Patches","summary":"  Cutting-edge works have demonstrated that text-to-image (T2I) diffusion\nmodels can generate adversarial patches that mislead state-of-the-art object\ndetectors in the physical world, revealing detectors' vulnerabilities and\nrisks. However, these methods neglect the T2I patches' attack effectiveness\nwhen observed from different views in the physical world (i.e., angle\nrobustness of the T2I adversarial patches). In this paper, we study the angle\nrobustness of T2I adversarial patches comprehensively, revealing their\nangle-robust issues, demonstrating that texts affect the angle robustness of\ngenerated patches significantly, and task-specific linguistic instructions fail\nto enhance the angle robustness. Motivated by the studies, we introduce\nAngle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that\nlearns a generalizable concept (i.e., text embeddings in implementation)\nrepresenting the capability of generating angle-robust patches. The learned\nconcept can be incorporated into textual prompts and guides T2I models to\ngenerate patches with their attack effectiveness inherently resistant to\nviewpoint variations. Through extensive simulation and physical-world\nexperiments on five SOTA detectors across multiple views, we demonstrate that\nAngleRoCL significantly enhances the angle robustness of T2I adversarial\npatches compared to baseline methods. Our patches maintain high attack success\nrates even under challenging viewing conditions, with over 50% average relative\nimprovement in attack effectiveness across multiple angles. This research\nadvances the understanding of physically angle-robust patches and provides\ninsights into the relationship between textual concepts and physical properties\nin T2I-generated contents. We released our code at\nhttps://github.com/tsingqguo/anglerocl.\n","authors":["Wenjun Ji","Yuxiang Fu","Luyang Ying","Deng-Ping Fan","Yuyi Wang","Ming-Ming Cheng","Ivor Tsang","Qing Guo"],"pdf_url":"https://arxiv.org/pdf/2506.09538v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18987v4","updated":"2025-10-24T10:30:00Z","published":"2024-10-09T17:19:22Z","title":"Point Cloud Synthesis Using Inner Product Transforms","summary":"  Point cloud synthesis, i.e. the generation of novel point clouds from an\ninput distribution, remains a challenging task, for which numerous complex\nmachine learning models have been devised. We develop a novel method that\nencodes geometrical-topological characteristics of point clouds using inner\nproducts, leading to a highly-efficient point cloud representation with\nprovable expressivity properties. Integrated into deep learning models, our\nencoding exhibits high quality in typical tasks like reconstruction,\ngeneration, and interpolation, with inference times orders of magnitude faster\nthan existing methods.\n","authors":["Ernst Röell","Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2410.18987v4.pdf","comment":"Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS) 2025. Our code is available at\n  https://github.com/aidos-lab/inner-product-transforms"},{"id":"http://arxiv.org/abs/2510.21323v1","updated":"2025-10-24T10:29:31Z","published":"2025-10-24T10:29:31Z","title":"VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a\n  Unified Concept Set","summary":"  The alignment of vision-language representations endows current\nVision-Language Models (VLMs) with strong multi-modal reasoning capabilities.\nHowever, the interpretability of the alignment component remains uninvestigated\ndue to the difficulty in mapping the semantics of multi-modal representations\ninto a unified concept set. To address this problem, we propose VL-SAE, a\nsparse autoencoder that encodes vision-language representations into its hidden\nactivations. Each neuron in its hidden layer correlates to a concept\nrepresented by semantically similar images and texts, thereby interpreting\nthese representations with a unified concept set. To establish the\nneuron-concept correlation, we encourage semantically similar representations\nto exhibit consistent neuron activations during self-supervised training.\nFirst, to measure the semantic similarity of multi-modal representations, we\nperform their alignment in an explicit form based on cosine similarity. Second,\nwe construct the VL-SAE with a distance-based encoder and two modality-specific\ndecoders to ensure the activation consistency of semantically similar\nrepresentations. Experiments across multiple VLMs (e.g., CLIP, LLaVA)\ndemonstrate the superior capability of VL-SAE in interpreting and enhancing the\nvision-language alignment. For interpretation, the alignment between vision and\nlanguage representations can be understood by comparing their semantics with\nconcepts. For enhancement, the alignment can be strengthened by aligning\nvision-language representations at the concept level, contributing to\nperformance improvements in downstream tasks, including zero-shot image\nclassification and hallucination elimination. Codes are available at\nhttps://github.com/ssfgunner/VL-SAE.\n","authors":["Shufan Shen","Junshu Sun","Qingming Huang","Shuhui Wang"],"pdf_url":"https://arxiv.org/pdf/2510.21323v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2508.01139v3","updated":"2025-10-24T10:28:13Z","published":"2025-08-02T01:44:23Z","title":"Dataset Condensation with Color Compensation","summary":"  Dataset condensation always faces a constitutive trade-off: balancing\nperformance and fidelity under extreme compression. Existing methods struggle\nwith two bottlenecks: image-level selection methods (Coreset Selection, Dataset\nQuantization) suffer from inefficiency condensation, while pixel-level\noptimization (Dataset Distillation) introduces semantic distortion due to\nover-parameterization. With empirical observations, we find that a critical\nproblem in dataset condensation is the oversight of color's dual role as an\ninformation carrier and a basic semantic representation unit. We argue that\nimproving the colorfulness of condensed images is beneficial for representation\nlearning. Motivated by this, we propose DC3: a Dataset Condensation framework\nwith Color Compensation. After a calibrated selection strategy, DC3 utilizes\nthe latent diffusion model to enhance the color diversity of an image rather\nthan creating a brand-new one. Extensive experiments demonstrate the superior\nperformance and generalization of DC3 that outperforms SOTA methods across\nmultiple benchmarks. To the best of our knowledge, besides focusing on\ndownstream tasks, DC3 is the first research to fine-tune pre-trained diffusion\nmodels with condensed datasets. The Frechet Inception Distance (FID) and\nInception Score (IS) results prove that training networks with our high-quality\ndatasets is feasible without model collapse or other degradation issues. Code\nand generated data are available at\nhttps://github.com/528why/Dataset-Condensation-with-Color-Compensation.\n","authors":["Huyu Wu","Duo Su","Junjie Hou","Guang Li"],"pdf_url":"https://arxiv.org/pdf/2508.01139v3.pdf","comment":"Accepted in TMLR"},{"id":"http://arxiv.org/abs/2510.21311v1","updated":"2025-10-24T10:14:17Z","published":"2025-10-24T10:14:17Z","title":"FineRS: Fine-grained Reasoning and Segmentation of Small Objects with\n  Reinforcement Learning","summary":"  Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities\nacross a wide range of vision-language tasks. However, due to the restricted\ninput resolutions, MLLMs face significant challenges in precisely understanding\nand localizing visual details in high-resolution images -- particularly when\ndealing with extra-small objects embedded in cluttered contexts. To address\nthis issue, we propose \\textsc{FineRS}, a two-stage MLLM-based reinforcement\nlearning framework for jointly reasoning and segmenting extremely small objects\nwithin high-resolution scenes. \\textsc{FineRS} adopts a coarse-to-fine pipeline\ncomprising Global Semantic Exploration (GSE) and Localized Perceptual\nRefinement (LPR). Specifically, GSE performs instruction-guided reasoning to\ngenerate a textural response and a coarse target region, while LPR refines this\nregion to produce an accurate bounding box and segmentation mask. To couple the\ntwo stages, we introduce a locate-informed retrospective reward, where LPR's\noutputs are used to optimize GSE for more robust coarse region exploration. %\nAdditionally, we present \\textsc{FineRS}-4k, a new dataset for evaluating MLLMs\non attribute-level reasoning and pixel-level segmentation on subtle,\nsmall-scale targets in complex high-resolution scenes. Experimental results on\n\\textsc{FineRS}-4k and public datasets demonstrate that our method consistently\noutperforms state-of-the-art MLLM-based approaches on both instruction-guided\nsegmentation and visual reasoning tasks.\n","authors":["Lu Zhang","Jiazuo Yu","Haomiao Xiong","Ping Hu","Yunzhi Zhuge","Huchuan Lu","You He"],"pdf_url":"https://arxiv.org/pdf/2510.21311v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.19195v2","updated":"2025-10-24T10:10:43Z","published":"2025-10-22T03:02:38Z","title":"Rethinking Driving World Model as Synthetic Data Generator for\n  Perception Tasks","summary":"  Recent advancements in driving world models enable controllable generation of\nhigh-quality RGB videos or multimodal videos. Existing methods primarily focus\non metrics related to generation quality and controllability. However, they\noften overlook the evaluation of downstream perception tasks, which are\n$\\mathbf{really\\ crucial}$ for the performance of autonomous driving. Existing\nmethods usually leverage a training strategy that first pretrains on synthetic\ndata and finetunes on real data, resulting in twice the epochs compared to the\nbaseline (real data only). When we double the epochs in the baseline, the\nbenefit of synthetic data becomes negligible. To thoroughly demonstrate the\nbenefit of synthetic data, we introduce Dream4Drive, a novel synthetic data\ngeneration framework designed for enhancing the downstream perception tasks.\nDream4Drive first decomposes the input video into several 3D-aware guidance\nmaps and subsequently renders the 3D assets onto these guidance maps. Finally,\nthe driving world model is fine-tuned to produce the edited, multi-view\nphotorealistic videos, which can be used to train the downstream perception\nmodels. Dream4Drive enables unprecedented flexibility in generating multi-view\ncorner cases at scale, significantly boosting corner case perception in\nautonomous driving. To facilitate future research, we also contribute a\nlarge-scale 3D asset dataset named DriveObj3D, covering the typical categories\nin driving scenarios and enabling diverse 3D-aware video editing. We conduct\ncomprehensive experiments to show that Dream4Drive can effectively boost the\nperformance of downstream perception models under various training epochs.\nPage: https://wm-research.github.io/Dream4Drive/ GitHub Link:\nhttps://github.com/wm-research/Dream4Drive\n","authors":["Kai Zeng","Zhanqian Wu","Kaixin Xiong","Xiaobao Wei","Xiangyu Guo","Zhenxin Zhu","Kalok Ho","Lijun Zhou","Bohan Zeng","Ming Lu","Haiyang Sun","Bing Wang","Guang Chen","Hangjun Ye","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.19195v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21307v1","updated":"2025-10-24T10:05:00Z","published":"2025-10-24T10:05:00Z","title":"Towards Physically Executable 3D Gaussian for Embodied Navigation","summary":"  3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic\nreal-time rendering capabilities, is regarded as an effective tool for\nnarrowing the sim-to-real gap. However, it lacks fine-grained semantics and\nphysical executability for Visual-Language Navigation (VLN). To address this,\nwe propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments\nfor 3D Navigation), a new paradigm that upgrades 3DGS into an executable,\nsemantically and physically aligned environment. It comprises two components:\n(1) Object-Centric Semantic Grounding, which adds object-level fine-grained\nannotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds\ncollision objects into 3DGS and constructs rich physical interfaces. We release\nInteriorGS, containing 1K object-annotated 3DGS indoor scene data, and\nintroduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data.\nExperiments show that 3DGS scene data is more difficult to converge, while\nexhibiting strong generalizability, improving baseline performance by 31% on\nthe VLN-CE Unseen task. The data and code will be available soon.\n","authors":["Bingchen Miao","Rong Wei","Zhiqi Ge","Xiaoquan sun","Shiqi Gao","Jingzhe Zhu","Renhan Wang","Siliang Tang","Jun Xiao","Rui Tang","Juncheng Li"],"pdf_url":"https://arxiv.org/pdf/2510.21307v1.pdf","comment":"Download link of InteriorGS:\n  https://huggingface.co/datasets/spatialverse/InteriorGS"},{"id":"http://arxiv.org/abs/2508.17054v2","updated":"2025-10-24T09:47:47Z","published":"2025-08-23T15:06:59Z","title":"DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method","summary":"  Previous dominant methods for scene flow estimation focus mainly on input\nfrom two consecutive frames, neglecting valuable information in the temporal\ndomain. While recent trends shift towards multi-frame reasoning, they suffer\nfrom rapidly escalating computational costs as the number of frames grows. To\nleverage temporal information more efficiently, we propose DeltaFlow\n($\\Delta$Flow), a lightweight 3D framework that captures motion cues via a\n$\\Delta$ scheme, extracting temporal features with minimal computational cost,\nregardless of the number of frames. Additionally, scene flow estimation faces\nchallenges such as imbalanced object class distributions and motion\ninconsistency. To tackle these issues, we introduce a Category-Balanced Loss to\nenhance learning across underrepresented classes and an Instance Consistency\nLoss to enforce coherent object motion, improving flow accuracy. Extensive\nevaluations on the Argoverse 2, Waymo and nuScenes datasets show that\n$\\Delta$Flow achieves state-of-the-art performance with up to 22% lower error\nand $2\\times$ faster inference compared to the next-best multi-frame supervised\nmethod, while also demonstrating a strong cross-domain generalization ability.\nThe code is open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with\ntrained model weights.\n","authors":["Qingwen Zhang","Xiaomeng Zhu","Yushan Zhang","Yixi Cai","Olov Andersson","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2508.17054v2.pdf","comment":"NeurIPS 2025 Spotlight, 18 pages (10 main pages + 8 supp materail),\n  11 figures, code at https://github.com/Kin-Zhang/DeltaFlow"},{"id":"http://arxiv.org/abs/2503.14012v3","updated":"2025-10-24T09:47:22Z","published":"2025-03-18T08:20:24Z","title":"LEGNet: A Lightweight Edge-Gaussian Network for Low-Quality Remote\n  Sensing Image Object Detection","summary":"  Remote sensing object detection (RSOD) often suffers from degradations such\nas low spatial resolution, sensor noise, motion blur, and adverse illumination.\nThese factors diminish feature distinctiveness, leading to ambiguous object\nrepresentations and inadequate foreground-background separation. Existing RSOD\nmethods exhibit limitations in robust detection of low-quality objects. To\naddress these pressing challenges, we introduce LEGNet, a lightweight backbone\nnetwork featuring a novel Edge-Gaussian Aggregation (EGA) module specifically\nengineered to enhance feature representation derived from low-quality remote\nsensing images. EGA module integrates: (a) orientation-aware Scharr filters to\nsharpen crucial edge details often lost in low-contrast or blurred objects, and\n(b) Gaussian-prior-based feature refinement to suppress noise and regularize\nambiguous feature responses, enhancing foreground saliency under challenging\nconditions. EGA module alleviates prevalent problems in reduced contrast,\nstructural discontinuities, and ambiguous feature responses prevalent in\ndegraded images, effectively improving model robustness while maintaining\ncomputational efficiency. Comprehensive evaluations across five benchmarks\n(DOTA-v1.0, v1.5, DIOR-R, FAIR1M-v1.0, and VisDrone2019) demonstrate that\nLEGNet achieves state-of-the-art performance, particularly in detecting\nlow-quality objects.The code is available at\nhttps://github.com/AeroVILab-AHU/LEGNet.\n","authors":["Wei Lu","Si-Bao Chen","Hui-Dong Li","Qing-Ling Shu","Chris H. Q. Ding","Jin Tang","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2503.14012v3.pdf","comment":"19 pages, 9 figures. Accepted by ICCV 2025 Workshop"},{"id":"http://arxiv.org/abs/2505.19853v2","updated":"2025-10-24T09:46:52Z","published":"2025-05-26T11:37:34Z","title":"Two Causally Related Needles in a Video Haystack","summary":"  Properly evaluating the ability of Video-Language Models (VLMs) to understand\nlong videos remains a challenge. We propose a long-context video understanding\nbenchmark, Causal2Needles, that assesses two crucial abilities insufficiently\naddressed by existing benchmarks: (1) extracting information from two separate\nlocations (two needles) in a long video and understanding them jointly, and (2)\nmodeling the world in terms of cause and effect in human behaviors.\nCausal2Needles evaluates these abilities using noncausal one-needle, causal\none-needle, and causal two-needle questions. The most complex question type,\ncausal two-needle questions, require extracting information from both the cause\nand effect events from a long video and the associated narration text. To\nprevent textual bias, we introduce two complementary question formats: locating\nthe video clip containing the answer, and verbal description of a visual detail\nfrom that video clip. Our experiments reveal that models excelling on existing\nbenchmarks struggle with causal 2-needle questions, and the model performance\nis negatively correlated with the distance between the two needles. These\nfindings highlight critical limitations in current VLMs. The dataset is\navailable at: https://huggingface.co/datasets/causal2needles/Causal2Needles\n","authors":["Miaoyu Li","Qin Chao","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2505.19853v2.pdf","comment":"Accepted to NeurIPS 2025 D&B Track"},{"id":"http://arxiv.org/abs/2505.15966v3","updated":"2025-10-24T09:35:22Z","published":"2025-05-21T19:35:08Z","title":"Pixel Reasoner: Incentivizing Pixel-Space Reasoning with\n  Curiosity-Driven Reinforcement Learning","summary":"  Chain-of-thought reasoning has significantly improved the performance of\nLarge Language Models (LLMs) across various domains. However, this reasoning\nprocess has been confined exclusively to textual space, limiting its\neffectiveness in visually intensive tasks. To address this limitation, we\nintroduce the concept of reasoning in the pixel-space. Within this novel\nframework, Vision-Language Models (VLMs) are equipped with a suite of visual\nreasoning operations, such as zoom-in and select-frame. These operations enable\nVLMs to directly inspect, interrogate, and infer from visual evidences, thereby\nenhancing reasoning fidelity for visual tasks. Cultivating such pixel-space\nreasoning capabilities in VLMs presents notable challenges, including the\nmodel's initially imbalanced competence and its reluctance to adopt the newly\nintroduced pixel-space operations. We address these challenges through a\ntwo-phase training approach. The first phase employs instruction tuning on\nsynthesized reasoning traces to familiarize the model with the novel visual\noperations. Following this, a reinforcement learning (RL) phase leverages a\ncuriosity-driven reward scheme to balance exploration between pixel-space\nreasoning and textual reasoning. With these visual operations, VLMs can\ninteract with complex visual inputs, such as information-rich images or videos\nto proactively gather necessary information. We demonstrate that this approach\nsignificantly improves VLM performance across diverse visual reasoning\nbenchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on\nTallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy\nachieved by any open-source model to date. These results highlight the\nimportance of pixel-space reasoning and the effectiveness of our framework.\n","authors":["Haozhe Wang","Alex Su","Weiming Ren","Fangzhen Lin","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2505.15966v3.pdf","comment":"Project Page: https://tiger-ai-lab.github.io/Pixel-Reasoner/,\n  Hands-on Demo: https://huggingface.co/spaces/TIGER-Lab/Pixel-Reasoner"},{"id":"http://arxiv.org/abs/2510.18313v3","updated":"2025-10-24T09:35:18Z","published":"2025-10-21T05:49:01Z","title":"OmniNWM: Omniscient Driving Navigation World Models","summary":"  Autonomous driving world models are expected to work effectively across three\ncore dimensions: state, action, and reward. Existing models, however, are\ntypically restricted to limited state modalities, short video sequences,\nimprecise action control, and a lack of reward awareness. In this paper, we\nintroduce OmniNWM, an omniscient panoramic navigation world model that\naddresses all three dimensions within a unified framework. For state, OmniNWM\njointly generates panoramic videos of RGB, semantics, metric depth, and 3D\noccupancy. A flexible forcing strategy enables high-quality long-horizon\nauto-regressive generation. For action, we introduce a normalized panoramic\nPlucker ray-map representation that encodes input trajectories into pixel-level\nsignals, enabling highly precise and generalizable control over panoramic video\ngeneration. Regarding reward, we move beyond learning reward functions with\nexternal image-based models: instead, we leverage the generated 3D occupancy to\ndirectly define rule-based dense rewards for driving compliance and safety.\nExtensive experiments demonstrate that OmniNWM achieves state-of-the-art\nperformance in video generation, control accuracy, and long-horizon stability,\nwhile providing a reliable closed-loop evaluation framework through\noccupancy-grounded rewards. Project page is available at\nhttps://arlo0o.github.io/OmniNWM/.\n","authors":["Bohan Li","Zhuang Ma","Dalong Du","Baorui Peng","Zhujin Liang","Zhenqiang Liu","Chao Ma","Yueming Jin","Hao Zhao","Wenjun Zeng","Xin Jin"],"pdf_url":"https://arxiv.org/pdf/2510.18313v3.pdf","comment":"https://arlo0o.github.io/OmniNWM/"},{"id":"http://arxiv.org/abs/2503.22330v3","updated":"2025-10-24T09:32:10Z","published":"2025-03-28T11:11:19Z","title":"WMCopier: Forging Invisible Image Watermarks on Arbitrary Images","summary":"  Invisible Image Watermarking is crucial for ensuring content provenance and\naccountability in generative AI. While Gen-AI providers are increasingly\nintegrating invisible watermarking systems, the robustness of these schemes\nagainst forgery attacks remains poorly characterized. This is critical, as\nforging traceable watermarks onto illicit content leads to false attribution,\npotentially harming the reputation and legal standing of Gen-AI service\nproviders who are not responsible for the content. In this work, we propose\nWMCopier, an effective watermark forgery attack that operates without requiring\nany prior knowledge of or access to the target watermarking algorithm. Our\napproach first models the target watermark distribution using an unconditional\ndiffusion model, and then seamlessly embeds the target watermark into a\nnon-watermarked image via a shallow inversion process. We also incorporate an\niterative optimization procedure that refines the reconstructed image to\nfurther trade off the fidelity and forgery efficiency. Experimental results\ndemonstrate that WMCopier effectively deceives both open-source and\nclosed-source watermark systems (e.g., Amazon's system), achieving a\nsignificantly higher success rate than existing methods. Additionally, we\nevaluate the robustness of forged samples and discuss the potential defenses\nagainst our attack.\n","authors":["Ziping Dong","Chao Shuai","Zhongjie Ba","Peng Cheng","Zhan Qin","Qinglong Wang","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2503.22330v3.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.21281v1","updated":"2025-10-24T09:26:07Z","published":"2025-10-24T09:26:07Z","title":"Physics-Informed Deep Learning for Improved Input Function Estimation in\n  Motion-Blurred Dynamic [${}^{18}$F]FDG PET Images","summary":"  Kinetic modeling enables \\textit{in vivo} quantification of tracer uptake and\nglucose metabolism in [${}^{18}$F]Fluorodeoxyglucose ([${}^{18}$F]FDG) dynamic\npositron emission tomography (dPET) imaging of mice. However, kinetic modeling\nrequires the accurate determination of the arterial input function (AIF) during\nimaging, which is time-consuming and invasive. Recent studies have shown the\nefficacy of using deep learning to directly predict the input function,\nsurpassing established methods such as the image-derived input function (IDIF).\nIn this work, we trained a physics-informed deep learning-based input function\nprediction model (PIDLIF) to estimate the AIF directly from the PET images,\nincorporating a kinetic modeling loss during training. The proposed method uses\na two-tissue compartment model over two regions, the myocardium and brain of\nthe mice, and is trained on a dataset of 70 [${}^{18}$F]FDG dPET images of mice\naccompanied by the measured AIF during imaging. The proposed method had\ncomparable performance to the network without a physics-informed loss, and when\nsudden movement causing blurring in the images was simulated, the PIDLIF model\nmaintained high performance in severe cases of image degradation. The proposed\nphysics-informed method exhibits an improved robustness that is promoted by\nphysically constraining the problem, enforcing consistency for\nout-of-distribution samples. In conclusion, the PIDLIF model offers insight\ninto the effects of leveraging physiological distribution mechanics in mice to\nguide a deep learning-based AIF prediction network in images with severe\ndegradation as a result of blurring due to movement during imaging.\n","authors":["Christian Salomonsen","Kristoffer K. Wickstrøm","Samuel Kuttner","Elisabeth Wetzer"],"pdf_url":"https://arxiv.org/pdf/2510.21281v1.pdf","comment":"12 pages, 4 figures, 1 table. Preprint: Accepted to PRIME @ MICCAI\n  2025. This is the submitted (pre-review) version (url:\n  https://openreview.net/forum?id=twg1nba5ep)"},{"id":"http://arxiv.org/abs/2508.15860v2","updated":"2025-10-24T09:26:03Z","published":"2025-08-20T15:18:59Z","title":"Robust Residual Finite Scalar Quantization for Neural Compression","summary":"  Finite Scalar Quantization (FSQ) offers simplified training but suffers from\nresidual magnitude decay in multi-stage settings, where subsequent stages\nreceive exponentially weaker signals. We propose Robust Residual Finite Scalar\nQuantization (RFSQ), addressing this fundamental limitation through two novel\nconditioning strategies: learnable scaling factors and invertible layer\nnormalization. Our experiments across audio and image modalities demonstrate\nRFSQ's effectiveness and generalizability. In audio reconstruction at 24\nbits/frame, RFSQ-LayerNorm achieves 3.646 DNSMOS, a 3.6% improvement over\nstate-of-the-art RVQ (3.518). On ImageNet, RFSQ achieves 0.102 L1 loss and\n0.100 perceptual loss, with LayerNorm providing 9.7% L1 improvement and 17.4%\nperceptual improvement over unconditioned variants. The LayerNorm strategy\nconsistently outperforms alternatives by maintaining normalized input\nstatistics across stages, effectively preventing exponential magnitude decay\nthat limits naive residual approaches. RFSQ combines FSQ's simplicity with\nmulti-stage quantization's representational power, establishing a new standard\nfor neural compression across diverse modalities.\n","authors":["Xiaoxu Zhu","Jiakui Li","Ken Zheng","Guiping Zhong","Huimeng Wang","Shiyin Kang","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2508.15860v2.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2510.21271v1","updated":"2025-10-24T09:12:59Z","published":"2025-10-24T09:12:59Z","title":"Buffer layers for Test-Time Adaptation","summary":"  In recent advancements in Test Time Adaptation (TTA), most existing\nmethodologies focus on updating normalization layers to adapt to the test\ndomain. However, the reliance on normalization-based adaptation presents key\nchallenges. First, normalization layers such as Batch Normalization (BN) are\nhighly sensitive to small batch sizes, leading to unstable and inaccurate\nstatistics. Moreover, normalization-based adaptation is inherently constrained\nby the structure of the pre-trained model, as it relies on training-time\nstatistics that may not generalize well to unseen domains. These issues limit\nthe effectiveness of normalization-based TTA approaches, especially under\nsignificant domain shift. In this paper, we introduce a novel paradigm based on\nthe concept of a Buffer layer, which addresses the fundamental limitations of\nnormalization layer updates. Unlike existing methods that modify the core\nparameters of the model, our approach preserves the integrity of the\npre-trained backbone, inherently mitigating the risk of catastrophic forgetting\nduring online adaptation. Through comprehensive experimentation, we demonstrate\nthat our approach not only outperforms traditional methods in mitigating domain\nshift and enhancing model robustness, but also exhibits strong resilience to\nforgetting. Furthermore, our Buffer layer is modular and can be seamlessly\nintegrated into nearly all existing TTA frameworks, resulting in consistent\nperformance improvements across various architectures. These findings validate\nthe effectiveness and versatility of the proposed solution in real-world domain\nadaptation scenarios. The code is available at\nhttps://github.com/hyeongyu-kim/Buffer_TTA.\n","authors":["Hyeongyu Kim","Geonhui Han","Dosik Hwang"],"pdf_url":"https://arxiv.org/pdf/2510.21271v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.21270v1","updated":"2025-10-24T09:11:50Z","published":"2025-10-24T09:11:50Z","title":"Sparser Block-Sparse Attention via Token Permutation","summary":"  Scaling the context length of large language models (LLMs) offers significant\nbenefits but is computationally expensive. This expense stems primarily from\nthe self-attention mechanism, whose $O(N^2)$ complexity with respect to\nsequence length presents a major bottleneck for both memory and latency.\nFortunately, the attention matrix is often sparse, particularly for long\nsequences, suggesting an opportunity for optimization. Block-sparse attention\nhas emerged as a promising solution that partitions sequences into blocks and\nskips computation for a subset of these blocks. However, the effectiveness of\nthis method is highly dependent on the underlying attention patterns, which can\nlead to sub-optimal block-level sparsity. For instance, important key tokens\nfor queries within a single block may be scattered across numerous other\nblocks, leading to computational redundancy. In this work, we propose Permuted\nBlock-Sparse Attention (\\textbf{PBS-Attn}), a plug-and-play method that\nleverages the permutation properties of attention to increase block-level\nsparsity and enhance the computational efficiency of LLM prefilling. We conduct\ncomprehensive experiments on challenging real-world long-context datasets,\ndemonstrating that PBS-Attn consistently outperforms existing block-sparse\nattention methods in model accuracy and closely matches the full attention\nbaseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn\nachieves an end-to-end speedup of up to $2.75\\times$ in long-context\nprefilling, confirming its practical viability. Code available at\nhttps://github.com/xinghaow99/pbs-attn\n","authors":["Xinghao Wang","Pengyu Wang","Dong Zhang","Chenkun Tan","Shaojun Zhou","Zhaoxiang Liu","Shiguo Lian","Fangxu Liu","Kai Song","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2510.21270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.25016v2","updated":"2025-10-24T09:11:35Z","published":"2025-09-29T16:41:30Z","title":"CLASP: Adaptive Spectral Clustering for Unsupervised Per-Image\n  Segmentation","summary":"  We introduce CLASP (Clustering via Adaptive Spectral Processing), a\nlightweight framework for unsupervised image segmentation that operates without\nany labeled data or finetuning. CLASP first extracts per patch features using a\nself supervised ViT encoder (DINO); then, it builds an affinity matrix and\napplies spectral clustering. To avoid manual tuning, we select the segment\ncount automatically with a eigengap silhouette search, and we sharpen the\nboundaries with a fully connected DenseCRF. Despite its simplicity and training\nfree nature, CLASP attains competitive mIoU and pixel accuracy on COCO Stuff\nand ADE20K, matching recent unsupervised baselines. The zero training design\nmakes CLASP a strong, easily reproducible baseline for large unannotated\ncorpora especially common in digital advertising and marketing workflows such\nas brand safety screening, creative asset curation, and social media content\nmoderation\n","authors":["Max Curie","Paulo da Costa"],"pdf_url":"https://arxiv.org/pdf/2509.25016v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.12712v3","updated":"2025-10-24T23:29:20Z","published":"2025-10-14T16:50:49Z","title":"Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image\n  Perception, Transformation, and Reasoning","summary":"  Multimodal Large Language Models (MLLMs) are increasingly applied in\nreal-world scenarios where user-provided images are often imperfect, requiring\nactive image manipulations such as cropping, editing, or enhancement to uncover\nsalient visual cues. Beyond static visual perception, MLLMs must also think\nwith images: dynamically transforming visual content and integrating it with\nother tools to solve complex tasks. However, this shift from treating vision as\npassive context to a manipulable cognitive workspace remains underexplored.\nMost existing benchmarks still follow a think about images paradigm, where\nimages are regarded as static inputs. To address this gap, we introduce\nVisualToolBench, a visual tool-use reasoning benchmark that rigorously\nevaluates MLLMs' ability to perceive, transform, and reason across complex\nvisual-textual tasks under the think-with-images paradigm. VisualToolBench\ncomprises 1,204 challenging, open-ended vision tasks (603 single-turn, 601\nmulti-turn) spanning across five diverse domains, each paired with detailed\nrubrics to enable systematic evaluation. Our evaluation shows that current\nMLLMs struggle with tasks requiring effective integration of vision and\ngeneral-purpose tools. Even the strongest model (GPT-5-think) reaches only\n18.68% pass rate. We further observe divergent tool-use behaviors, with OpenAI\nmodels benefiting from diverse image manipulations while Gemini-2.5-pro shows\nno improvement. By introducing the first benchmark centered on think with\nimages, VisualToolBench offers critical insights for advancing visual\nintelligence in MLLMs.\n","authors":["Xingang Guo","Utkarsh Tyagi","Advait Gosai","Paula Vergara","Jayeon Park","Ernesto Gabriel Hernández Montoya","Chen Bo Calvin Zhang","Bin Hu","Yunzhong He","Bing Liu","Rakshith Sharma Srinivasa"],"pdf_url":"https://arxiv.org/pdf/2510.12712v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22073v1","updated":"2025-10-24T23:19:02Z","published":"2025-10-24T23:19:02Z","title":"Scanner-Agnostic MRI Harmonization via SSIM-Guided Disentanglement","summary":"  The variability introduced by differences in MRI scanner models, acquisition\nprotocols, and imaging sites hinders consistent analysis and generalizability\nacross multicenter studies. We present a novel image-based harmonization\nframework for 3D T1-weighted brain MRI, which disentangles anatomical content\nfrom scanner- and site-specific variations. The model incorporates a\ndifferentiable loss based on the Structural Similarity Index (SSIM) to preserve\nbiologically meaningful features while reducing inter-site variability. This\nloss enables separate evaluation of image luminance, contrast, and structural\ncomponents. Training and validation were performed on multiple publicly\navailable datasets spanning diverse scanners and sites, with testing on both\nhealthy and clinical populations. Harmonization using multiple style targets,\nincluding style-agnostic references, produced consistent and high-quality\noutputs. Visual comparisons, voxel intensity distributions, and SSIM-based\nmetrics demonstrated that harmonized images achieved strong alignment across\nacquisition settings while maintaining anatomical fidelity. Following\nharmonization, structural SSIM reached 0.97, luminance SSIM ranged from 0.98 to\n0.99, and Wasserstein distances between mean voxel intensity distributions\ndecreased substantially. Downstream tasks showed substantial improvements: mean\nabsolute error for brain age prediction decreased from 5.36 to 3.30 years, and\nAlzheimer's disease classification AUC increased from 0.78 to 0.85. Overall,\nour framework enhances cross-site image consistency, preserves anatomical\nfidelity, and improves downstream model performance, providing a robust and\ngeneralizable solution for large-scale multicenter neuroimaging studies.\n","authors":["Luca Caldera","Lara Cavinato","Francesca Ieva"],"pdf_url":"https://arxiv.org/pdf/2510.22073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22070v1","updated":"2025-10-24T23:11:25Z","published":"2025-10-24T23:11:25Z","title":"MAGIC-Flow: Multiscale Adaptive Conditional Flows for Generation and\n  Interpretable Classification","summary":"  Generative modeling has emerged as a powerful paradigm for representation\nlearning, but its direct applicability to challenging fields like medical\nimaging remains limited: mere generation, without task alignment, fails to\nprovide a robust foundation for clinical use. We propose MAGIC-Flow, a\nconditional multiscale normalizing flow architecture that performs generation\nand classification within a single modular framework. The model is built as a\nhierarchy of invertible and differentiable bijections, where the Jacobian\ndeterminant factorizes across sub-transformations. We show how this ensures\nexact likelihood computation and stable optimization, while invertibility\nenables explicit visualization of sample likelihoods, providing an\ninterpretable lens into the model's reasoning. By conditioning on class labels,\nMAGIC-Flow supports controllable sample synthesis and principled\nclass-probability estimation, effectively aiding both generative and\ndiscriminative objectives. We evaluate MAGIC-Flow against top baselines using\nmetrics for similarity, fidelity, and diversity. Across multiple datasets, it\naddresses generation and classification under scanner noise, and\nmodality-specific synthesis and identification. Results show MAGIC-Flow creates\nrealistic, diverse samples and improves classification. MAGIC-Flow is an\neffective strategy for generation and classification in data-limited domains,\nwith direct benefits for privacy-preserving augmentation, robust\ngeneralization, and trustworthy medical AI.\n","authors":["Luca Caldera","Giacomo Bottacini","Lara Cavinato"],"pdf_url":"https://arxiv.org/pdf/2510.22070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22067v1","updated":"2025-10-24T23:04:26Z","published":"2025-10-24T23:04:26Z","title":"Capturing Gaze Shifts for Guidance: Cross-Modal Fusion Enhancement for\n  VLM Hallucination Mitigation","summary":"  Vision language models (VLMs) often generate hallucination, i.e., content\nthat cannot be substantiated by either textual or visual inputs. Prior work\nprimarily attributes this to over-reliance on linguistic prior knowledge rather\nthan visual inputs. Some methods attempt to mitigate hallucination by\namplifying visual token attention proportionally to their attention scores.\nHowever, these methods overlook the visual attention sink problem, where\nattention is frequently misallocated to task-irrelevant visual regions, and\nneglect cross-modal fusion balance by enhancing only visual attention without\nadjusting attention to the user query. This can result in amplifying incorrect\nareas while failing to properly interpret the user query. To address these\nchallenges, we propose a simple yet effective method called Gaze Shift-Guided\nCross-modal Fusion Enhancement (GIFT). GIFT pre-computes a holistic visual\nsaliency map by tracking positive changes in visual attention, or \"gaze\nshifts\", during user query comprehension, and leverages this map to amplify\nattention to both salient visual information and the user query at each\ndecoding step. This reduces the impact of visual attention sink, as irrelevant\ntokens exhibit minimal shifts, while ensuring balanced cross-modal fusion for\nwell-integrated representation. Extensive experiments show that GIFT\neffectively mitigates hallucination in VLMs across both generative and\nclassification tasks, achieving up to 20.7% improvement over greedy decoding,\nwhile maintaining general vision-language performance with low computational\noverhead.\n","authors":["Zheng Qi","Chao Shang","Evangelia Spiliopoulou","Nikolaos Pappas"],"pdf_url":"https://arxiv.org/pdf/2510.22067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.13493v2","updated":"2025-10-24T23:01:05Z","published":"2025-10-15T12:42:49Z","title":"ExpressNet-MoE: A Hybrid Deep Neural Network for Emotion Recognition","summary":"  In many domains, including online education, healthcare, security, and\nhuman-computer interaction, facial emotion recognition (FER) is essential.\nReal-world FER is still difficult despite its significance because of some\nfactors such as variable head positions, occlusions, illumination shifts, and\ndemographic diversity. Engagement detection, which is essential for\napplications like virtual learning and customer services, is frequently\nchallenging due to FER limitations by many current models. In this article, we\npropose ExpressNet-MoE, a novel hybrid deep learning model that blends both\nConvolution Neural Networks (CNNs) and Mixture of Experts (MoE) framework, to\novercome the difficulties. Our model dynamically chooses the most pertinent\nexpert networks, thus it aids in the generalization and providing flexibility\nto model across a wide variety of datasets. Our model improves on the accuracy\nof emotion recognition by utilizing multi-scale feature extraction to collect\nboth global and local facial features. ExpressNet-MoE includes numerous\nCNN-based feature extractors, a MoE module for adaptive feature selection, and\nfinally a residual network backbone for deep feature learning. To demonstrate\nefficacy of our proposed model we evaluated on several datasets, and compared\nwith current state-of-the-art methods. Our model achieves accuracies of 74.77%\non AffectNet (v7), 72.55% on AffectNet (v8), 84.29% on RAF-DB, and 64.66% on\nFER-2013. The results show how adaptive our model is and how it may be used to\ndevelop end-to-end emotion recognition systems in practical settings.\nReproducible codes and results are made publicly accessible at\nhttps://github.com/DeeptimaanB/ExpressNet-MoE.\n","authors":["Deeptimaan Banerjee","Prateek Gothwal","Ashis Kumer Biswas"],"pdf_url":"https://arxiv.org/pdf/2510.13493v2.pdf","comment":"* Current version of the manuscript contains 17 pages including text,\n  13 figures, and 4 tables. The manuscript is currently under review at a\n  journal"},{"id":"http://arxiv.org/abs/2507.09052v2","updated":"2025-10-24T22:58:47Z","published":"2025-07-11T21:58:03Z","title":"Contrastive Conditional-Unconditional Alignment for Long-tailed\n  Diffusion Model","summary":"  Training data for class-conditional image synthesis often exhibit a\nlong-tailed distribution with limited images for tail classes. Such an\nimbalance causes mode collapse and reduces the diversity of synthesized images\nfor tail classes. For class-conditional diffusion models trained on imbalanced\ndata, we aim to improve the diversity and fidelity of tail class images without\ncompromising the quality of head class images. We achieve this by introducing\ntwo simple but highly effective loss functions. Firstly, we employ an\nUnsupervised Contrastive Loss (UCL) utilizing negative samples to increase the\ndistance/dissimilarity among synthetic images. Such regularization is coupled\nwith a standard trick of batch resampling to further diversify tail-class\nimages. Our second loss is an Alignment Loss (AL) that aligns class-conditional\ngeneration with unconditional generation at large timesteps. This second loss\nmakes the denoising process insensitive to class conditions for the initial\nsteps, which enriches tail classes through knowledge sharing from head classes.\nWe successfully leverage contrastive learning and conditional-unconditional\nalignment for class-imbalanced diffusion models. Our framework is easy to\nimplement as demonstrated on both U-Net based architecture and Diffusion\nTransformer. Our method outperforms vanilla denoising diffusion probabilistic\nmodels, score-based diffusion model, and alternative methods for\nclass-imbalanced image generation across various datasets, in particular\nImageNet-LT with 256x256 resolution.\n","authors":["Fang Chen","Alex Villa","Gongbo Liang","Xiaoyi Lu","Meng Tang"],"pdf_url":"https://arxiv.org/pdf/2507.09052v2.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2510.18016v2","updated":"2025-10-24T22:42:11Z","published":"2025-10-20T18:48:25Z","title":"ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and\n  Scene-Aware Spatiotemporal Cues","summary":"  Engagement detection in online learning environments is vital for improving\nstudent outcomes and personalizing instruction. We present ViBED-Net\n(Video-Based Engagement Detection Network), a novel deep learning framework\ndesigned to assess student engagement from video data using a dual-stream\narchitecture. ViBED-Net captures both facial expressions and full-scene context\nby processing facial crops and entire video frames through EfficientNetV2 for\nspatial feature extraction. These features are then analyzed over time using\ntwo temporal modeling strategies: Long Short-Term Memory (LSTM) networks and\nTransformer encoders. Our model is evaluated on the DAiSEE dataset, a\nlarge-scale benchmark for affective state recognition in e-learning. To enhance\nperformance on underrepresented engagement classes, we apply targeted data\naugmentation techniques. Among the tested variants, ViBED-Net with LSTM\nachieves 73.43\\% accuracy, outperforming existing state-of-the-art approaches.\nViBED-Net demonstrates that combining face-aware and scene-aware spatiotemporal\ncues significantly improves engagement detection accuracy. Its modular design\nallows flexibility for application across education, user experience research,\nand content personalization. This work advances video-based affective computing\nby offering a scalable, high-performing solution for real-world engagement\nanalysis. The source code for this project is available on\nhttps://github.com/prateek-gothwal/ViBED-Net .\n","authors":["Prateek Gothwal","Deeptimaan Banerjee","Ashis Kumer Biswas"],"pdf_url":"https://arxiv.org/pdf/2510.18016v2.pdf","comment":"10 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2510.22056v1","updated":"2025-10-24T22:38:17Z","published":"2025-10-24T22:38:17Z","title":"Human-Centric Anomaly Detection in Surveillance Videos Using YOLO-World\n  and Spatio-Temporal Deep Learning","summary":"  Anomaly detection in surveillance videos remains a challenging task due to\nthe diversity of abnormal events, class imbalance, and scene-dependent visual\nclutter. To address these issues, we propose a robust deep learning framework\nthat integrates human-centric preprocessing with spatio-temporal modeling for\nmulti-class anomaly classification. Our pipeline begins by applying YOLO-World\n- an open-vocabulary vision-language detector - to identify human instances in\nraw video clips, followed by ByteTrack for consistent identity-aware tracking.\nBackground regions outside detected bounding boxes are suppressed via Gaussian\nblurring, effectively reducing scene-specific distractions and focusing the\nmodel on behaviorally relevant foreground content. The refined frames are then\nprocessed by an ImageNet-pretrained InceptionV3 network for spatial feature\nextraction, and temporal dynamics are captured using a bidirectional LSTM\n(BiLSTM) for sequence-level classification. Evaluated on a five-class subset of\nthe UCF-Crime dataset (Normal, Burglary, Fighting, Arson, Explosion), our\nmethod achieves a mean test accuracy of 92.41% across three independent trials,\nwith per-class F1-scores consistently exceeding 0.85. Comprehensive evaluation\nmetrics - including confusion matrices, ROC curves, and macro/weighted averages\n- demonstrate strong generalization and resilience to class imbalance. The\nresults confirm that foreground-focused preprocessing significantly enhances\nanomaly discrimination in real-world surveillance scenarios.\n","authors":["Mohammad Ali Etemadi Naeen","Hoda Mohammadzade","Saeed Bagheri Shouraki"],"pdf_url":"https://arxiv.org/pdf/2510.22056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22045v1","updated":"2025-10-24T22:06:56Z","published":"2025-10-24T22:06:56Z","title":"VLM-SlideEval: Evaluating VLMs on Structured Comprehension and\n  Perturbation Sensitivity in PPT","summary":"  Vision-language models (VLMs) are increasingly used to evaluate multimodal\ncontent, including presentation slides, yet their slide-specific understanding\nremains underexplored {despite their growing role as critics in agentic,\nmodel-forward pipelines}. We introduce VLM-SlideEval, an evaluation framework\nthat probes VLMs along three axes: (1) element-level extraction from slide\nimages aligned to ground truth; (2) robustness to controlled perturbations in\ngeometry, style, and text; and (3) higher-level comprehension, such as\nrecovering a deck's narrative order from shuffled slides. Using publicly\navailable decks from Zenodo\n(https://huggingface.co/datasets/Forceless/Zenodo10K/viewer/default/pptx), we\nstandardize ground-truth element metadata from PowerPoint XML and live\nrenderings into a unified, verifiable schema. Empirically, VLMs underperform on\npixel-accurate extraction and show non-trivial agreement, fidelity, and\nconsistency under controlled perturbations, while performing better on\nsingle-slide content understanding; however, they do not reliably capture\nnarrative structure across slides. These results highlight the limits of\ncurrent VLMs for slide evaluation and motivate calibrated, critic-in-the-loop\nevaluators that drive iterative refinement and selection in agentic pipelines.\n","authors":["Hyeonsu Kang","Emily Bao","Anjan Goswami"],"pdf_url":"https://arxiv.org/pdf/2510.22045v1.pdf","comment":"39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: Evaluating the Evolving LLM Lifecycle - Benchmarks, Emergent\n  Abilities, and Scaling"},{"id":"http://arxiv.org/abs/2506.23467v2","updated":"2025-10-24T22:01:37Z","published":"2025-06-30T02:19:22Z","title":"AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training\n  for Chest X-rays","summary":"  Contrastive Language-Image Pre-training (CLIP) models have demonstrated\nsuperior performance across various visual tasks including medical image\nclassification. However, fairness concerns, including demographic biases, have\nreceived limited attention for CLIP models. This oversight leads to critical\nissues, particularly those related to race and gender, resulting in disparities\nin diagnostic outcomes and reduced reliability for underrepresented groups. To\naddress these challenges, we introduce AdFair-CLIP, a novel framework employing\nadversarial feature intervention to suppress sensitive attributes, thereby\nmitigating spurious correlations and improving prediction fairness. We conduct\ncomprehensive experiments on chest X-ray (CXR) datasets, and show that\nAdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while\nmaintaining robust generalization in zero-shot and few-shot scenarios. These\nresults establish new benchmarks for fairness-aware learning in CLIP-based\nmedical diagnostic models, particularly for CXR analysis.\n","authors":["Chenlang Yi","Zizhan Xiong","Qi Qi","Xiyuan Wei","Girish Bathla","Ching-Long Lin","Bobak Jack Mortazavi","Tianbao Yang"],"pdf_url":"https://arxiv.org/pdf/2506.23467v2.pdf","comment":"This preprint has been accepted by MICCAI 2025"},{"id":"http://arxiv.org/abs/2510.16751v2","updated":"2025-10-24T20:55:17Z","published":"2025-10-19T08:28:06Z","title":"Visual Autoregressive Models Beat Diffusion Models on Inference Time\n  Scaling","summary":"  While inference-time scaling through search has revolutionized Large Language\nModels, translating these gains to image generation has proven difficult.\nRecent attempts to apply search strategies to continuous diffusion models show\nlimited benefits, with simple random sampling often performing best. We\ndemonstrate that the discrete, sequential nature of visual autoregressive\nmodels enables effective search for image generation. We show that beam search\nsubstantially improves text-to-image generation, enabling a 2B parameter\nautoregressive model to outperform a 12B parameter diffusion model across\nbenchmarks. Systematic ablations show that this advantage comes from the\ndiscrete token space, which allows early pruning and computational reuse, and\nour verifier analysis highlights trade-offs between speed and reasoning\ncapability. These findings suggest that model architecture, not just scale, is\ncritical for inference-time optimization in visual generation.\n","authors":["Erik Riise","Mehmet Onurcan Kaya","Dim P. Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2510.16751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12477v3","updated":"2025-10-24T20:31:18Z","published":"2025-01-21T19:59:22Z","title":"Slot-BERT: Self-supervised Object Discovery in Surgical Video","summary":"  Object-centric slot attention is a powerful framework for unsupervised\nlearning of structured and explainable representations that can support\nreasoning about objects and actions, including in surgical videos. While\nconventional object-centric methods for videos leverage recurrent processing to\nachieve efficiency, they often struggle with maintaining long-range temporal\ncoherence required for long videos in surgical applications. On the other hand,\nfully parallel processing of entire videos enhances temporal consistency but\nintroduces significant computational overhead, making it impractical for\nimplementation on hardware in medical facilities. We present Slot-BERT, a\nbidirectional long-range model that learns object-centric representations in a\nlatent space while ensuring robust temporal coherence. Slot-BERT scales object\ndiscovery seamlessly to long videos of unconstrained lengths. A novel slot\ncontrastive loss further reduces redundancy and improves the representation\ndisentanglement by enhancing slot orthogonality. We evaluate Slot-BERT on\nreal-world surgical video datasets from abdominal, cholecystectomy, and\nthoracic procedures. Our method surpasses state-of-the-art object-centric\napproaches under unsupervised training achieving superior performance across\ndiverse domains. We also demonstrate efficient zero-shot domain adaptation to\ndata from diverse surgical specialties and databases.\n","authors":["Guiqiu Liao","Matjaz Jogan","Marcel Hussing","Kenta Nakahashi","Kazuhiro Yasufuku","Amin Madani","Eric Eaton","Daniel A. Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2501.12477v3.pdf","comment":"28 pages, 10 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2510.21706v1","updated":"2025-10-24T17:59:46Z","published":"2025-10-24T17:59:46Z","title":"Equivariance by Contrast: Identifiable Equivariant Embeddings from\n  Unlabeled Finite Group Actions","summary":"  We propose Equivariance by Contrast (EbC) to learn equivariant embeddings\nfrom observation pairs $(\\mathbf{y}, g \\cdot \\mathbf{y})$, where $g$ is drawn\nfrom a finite group acting on the data. Our method jointly learns a latent\nspace and a group representation in which group actions correspond to\ninvertible linear maps -- without relying on group-specific inductive biases.\nWe validate our approach on the infinite dSprites dataset with structured\ntransformations defined by the finite group $G:= (R_m \\times \\mathbb{Z}_n\n\\times \\mathbb{Z}_n)$, combining discrete rotations and periodic translations.\nThe resulting embeddings exhibit high-fidelity equivariance, with group\noperations faithfully reproduced in latent space. On synthetic data, we further\nvalidate the approach on the non-abelian orthogonal group $O(n)$ and the\ngeneral linear group $GL(n)$. We also provide a theoretical proof for\nidentifiability. While broad evaluation across diverse group types on\nreal-world data remains future work, our results constitute the first\nsuccessful demonstration of general-purpose encoder-only equivariant learning\nfrom group action observations alone, including non-trivial non-abelian groups\nand a product group motivated by modeling affine equivariances in computer\nvision.\n","authors":["Tobias Schmidt","Steffen Schneider","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2510.21706v1.pdf","comment":"Accepted at NeurIPS 2025. The last two authors contributed equally.\n  Code is available at https://github.com/dynamical-inference/ebc"},{"id":"http://arxiv.org/abs/2510.21697v1","updated":"2025-10-24T17:57:31Z","published":"2025-10-24T17:57:31Z","title":"Visual Diffusion Models are Geometric Solvers","summary":"  In this paper we show that visual diffusion models can serve as effective\ngeometric solvers: they can directly reason about geometric problems by working\nin pixel space. We first demonstrate this on the Inscribed Square Problem, a\nlong-standing problem in geometry that asks whether every Jordan curve contains\nfour points forming a square. We then extend the approach to two other\nwell-known hard geometric problems: the Steiner Tree Problem and the Simple\nPolygon Problem.\n  Our method treats each problem instance as an image and trains a standard\nvisual diffusion model that transforms Gaussian noise into an image\nrepresenting a valid approximate solution that closely matches the exact one.\nThe model learns to transform noisy geometric structures into correct\nconfigurations, effectively recasting geometric reasoning as image generation.\n  Unlike prior work that necessitates specialized architectures and\ndomain-specific adaptations when applying diffusion to parametric geometric\nrepresentations, we employ a standard visual diffusion model that operates on\nthe visual representation of the problem. This simplicity highlights a\nsurprising bridge between generative modeling and geometric problem solving.\nBeyond the specific problems studied here, our results point toward a broader\nparadigm: operating in image space provides a general and practical framework\nfor approximating notoriously hard problems, and opens the door to tackling a\nfar wider class of challenging geometric tasks.\n","authors":["Nir Goren","Shai Yehezkel","Omer Dahary","Andrey Voynov","Or Patashnik","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2510.21697v1.pdf","comment":"Project page: https://kariander1.github.io/visual-geo-solver/"},{"id":"http://arxiv.org/abs/2506.09891v2","updated":"2025-10-24T17:57:09Z","published":"2025-06-11T16:00:55Z","title":"Causal Climate Emulation with Bayesian Filtering","summary":"  Traditional models of climate change use complex systems of coupled equations\nto simulate physical processes across the Earth system. These simulations are\nhighly computationally expensive, limiting our predictions of climate change\nand analyses of its causes and effects. Machine learning has the potential to\nquickly emulate data from climate models, but current approaches are not able\nto incorporate physically-based causal relationships. Here, we develop an\ninterpretable climate model emulator based on causal representation learning.\nWe derive a novel approach including a Bayesian filter for stable long-term\nautoregressive emulation. We demonstrate that our emulator learns accurate\nclimate dynamics, and we show the importance of each one of its components on a\nrealistic synthetic dataset and data from two widely deployed climate models.\n","authors":["Sebastian Hickman","Ilija Trajkovic","Julia Kaltenborn","Francis Pelletier","Alex Archibald","Yaniv Gurwicz","Peer Nowack","David Rolnick","Julien Boussard"],"pdf_url":"https://arxiv.org/pdf/2506.09891v2.pdf","comment":"37 pages, 26 figures"},{"id":"http://arxiv.org/abs/2510.21693v1","updated":"2025-10-24T17:54:19Z","published":"2025-10-24T17:54:19Z","title":"Mechanistic Interpretability for Neural TSP Solvers","summary":"  Neural networks have advanced combinatorial optimization, with\nTransformer-based solvers achieving near-optimal solutions on the Traveling\nSalesman Problem (TSP) in milliseconds. However, these models operate as black\nboxes, providing no insight into the geometric patterns they learn or the\nheuristics they employ during tour construction. We address this opacity by\napplying sparse autoencoders (SAEs), a mechanistic interpretability technique,\nto a Transformer-based TSP solver, representing the first application of\nactivation-based interpretability methods to operations research models. We\ntrain a pointer network with reinforcement learning on 100-node instances, then\nfit an SAE to the encoder's residual stream to discover an overcomplete\ndictionary of interpretable features. Our analysis reveals that the solver\nnaturally develops features mirroring fundamental TSP concepts: boundary\ndetectors that activate on convex-hull nodes, cluster-sensitive features\nresponding to locally dense regions, and separator features encoding geometric\npartitions. These findings provide the first model-internal account of what\nneural TSP solvers compute before node selection, demonstrate that geometric\nstructure emerges without explicit supervision, and suggest pathways toward\ntransparent hybrid systems that combine neural efficiency with algorithmic\ninterpretability. Interactive feature explorer:\nhttps://reubennarad.github.io/TSP_interp\n","authors":["Reuben Narad","Leonard Boussioux","Michael Wagner"],"pdf_url":"https://arxiv.org/pdf/2510.21693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00138v2","updated":"2025-10-24T17:52:29Z","published":"2025-05-30T18:21:40Z","title":"Intrinsic Goals for Autonomous Agents: Model-Based Exploration in\n  Virtual Zebrafish Predicts Ethological Behavior and Whole-Brain Dynamics","summary":"  Autonomy is a hallmark of animal intelligence, enabling adaptive and\nintelligent behavior in complex environments without relying on external reward\nor task structure. Existing reinforcement learning approaches to exploration in\nreward-free environments, including a class of methods known as model-based\nintrinsic motivation, exhibit inconsistent exploration patterns and do not\nconverge to an exploratory policy, thus failing to capture robust autonomous\nbehaviors observed in animals. Moreover, systems neuroscience has largely\noverlooked the neural basis of autonomy, focusing instead on experimental\nparadigms where animals are motivated by external reward rather than engaging\nin ethological, naturalistic and task-independent behavior. To bridge these\ngaps, we introduce a novel model-based intrinsic drive explicitly designed\nafter the principles of autonomous exploration in animals. Our method\n(3M-Progress) achieves animal-like exploration by tracking divergence between\nan online world model and a fixed prior learned from an ecological niche. To\nthe best of our knowledge, we introduce the first autonomous embodied agent\nthat predicts brain data entirely from self-supervised optimization of an\nintrinsic goal -- without any behavioral or neural training data --\ndemonstrating that 3M-Progress agents capture the explainable variance in\nbehavioral patterns and whole-brain neural-glial dynamics recorded from\nautonomously behaving larval zebrafish, thereby providing the first\ngoal-driven, population-level model of neural-glial computation. Our findings\nestablish a computational framework connecting model-based intrinsic motivation\nto naturalistic behavior, providing a foundation for building artificial agents\nwith animal-like autonomy.\n","authors":["Reece Keller","Alyn Kirsch","Felix Pei","Xaq Pitkow","Leo Kozachkov","Aran Nayebi"],"pdf_url":"https://arxiv.org/pdf/2506.00138v2.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2510.21691v1","updated":"2025-10-24T17:50:41Z","published":"2025-10-24T17:50:41Z","title":"On Uncertainty Calibration for Equivariant Functions","summary":"  Data-sparse settings such as robotic manipulation, molecular physics, and\ngalaxy morphology classification are some of the hardest domains for deep\nlearning. For these problems, equivariant networks can help improve modeling\nacross undersampled parts of the input space, and uncertainty estimation can\nguard against overconfidence. However, until now, the relationships between\nequivariance and model confidence, and more generally equivariance and model\ncalibration, has yet to be studied. Since traditional classification and\nregression error terms show up in the definitions of calibration error, it is\nnatural to suspect that previous work can be used to help understand the\nrelationship between equivariance and calibration error. In this work, we\npresent a theory relating equivariance to uncertainty estimation. By proving\nlower and upper bounds on uncertainty calibration errors (ECE and ENCE) under\nvarious equivariance conditions, we elucidate the generalization limits of\nequivariant models and illustrate how symmetry mismatch can result in\nmiscalibration in both classification and regression. We complement our\ntheoretical framework with numerical experiments that clarify the relationship\nbetween equivariance and uncertainty using a variety of real and simulated\ndatasets, and we comment on trends with symmetry mismatch, group size, and\naleatoric and epistemic uncertainties.\n","authors":["Edward Berman","Jacob Ginesin","Marco Pacini","Robin Walters"],"pdf_url":"https://arxiv.org/pdf/2510.21691v1.pdf","comment":"Under review at Transactions on Machine Learning Research (TMLR).\n  Code is available at https://github.com/EdwardBerman/EquiUQ . Excited to\n  share this paper, comments welcome :D"},{"id":"http://arxiv.org/abs/2506.23726v2","updated":"2025-10-24T17:47:21Z","published":"2025-06-30T10:58:49Z","title":"System-Embedded Diffusion Bridge Models","summary":"  Solving inverse problems -- recovering signals from incomplete or noisy\nmeasurements -- is fundamental in science and engineering. Score-based\ngenerative models (SGMs) have recently emerged as a powerful framework for this\ntask. Two main paradigms have formed: unsupervised approaches that adapt\npretrained generative models to inverse problems, and supervised bridge methods\nthat train stochastic processes conditioned on paired clean and corrupted data.\nWhile the former typically assume knowledge of the measurement model, the\nlatter have largely overlooked this structural information. We introduce System\nembedded Diffusion Bridge Models (SDBs), a new class of supervised bridge\nmethods that explicitly embed the known linear measurement system into the\ncoefficients of a matrix-valued SDE. This principled integration yields\nconsistent improvements across diverse linear inverse problems and demonstrates\nrobust generalization under system misspecification between training and\ndeployment, offering a promising solution to real-world applications.\n","authors":["Bartlomiej Sobieski","Matthew Tivnan","Yuang Wang","Siyeop Yoon","Pengfei Jin","Dufan Wu","Quanzheng Li","Przemyslaw Biecek"],"pdf_url":"https://arxiv.org/pdf/2506.23726v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2507.23773v2","updated":"2025-10-24T17:44:52Z","published":"2025-07-31T17:57:20Z","title":"SimuRA: A World-Model-Driven Simulative Reasoning Architecture for\n  General Goal-Oriented Agents","summary":"  AI agents built on foundation models hold enormous promise. Current practice,\nhowever, focuses on a one-task-one-agent approach, which not only falls short\nof scalability and generality, but also faces practical limitations from\nblack-box autoregressive reasoning, where decisions unfold token by token\nwithout explicit simulation or counterfactual evaluation of outcomes. Humans,\non the other hand, reason and plan by mentally simulating the consequences of\nactions within an internal model of the world -- a capability that supports\nflexible, goal-directed behavior across diverse contexts. Moving towards a more\ngeneral and powerful AI agent, we introduce SimuRA, a goal-oriented\narchitecture for generalized agentic reasoning. Based on a principled\nformulation of an optimal agent in any general environment, SimuRA addresses\nthe limitations of black-box autoregressive reasoning by incorporating the\nworld model for planning via simulation. Our prototype world model is\nimplemented using LLMs as a substrate, leveraging the natural language as a\ndiscrete, hierarchical representation grounded in concepts for planning, while\nremaining model-agnostic. On complex web-browsing tasks such as flight search,\nSimuRA improves the success rate from 0% to 32.2% compared to a representative\nopen-web agent baseline. Across tasks, world-model-based planning achieves up\nto 124% higher task completion rates than a matched black-box autoregressive\nbaseline, demonstrating the advantages of simulative reasoning. We release\nReasonerAgent-Web, a web-browsing agent built on SimuRA, as an open-source\nresearch demo.\n","authors":["Mingkai Deng","Jinyu Hou","Zhiting Hu","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2507.23773v2.pdf","comment":"This submission has been updated to adjust the scope and presentation\n  of the work"},{"id":"http://arxiv.org/abs/2510.21686v1","updated":"2025-10-24T17:44:40Z","published":"2025-10-24T17:44:40Z","title":"Multimodal Datasets with Controllable Mutual Information","summary":"  We introduce a framework for generating highly multimodal datasets with\nexplicitly calculable mutual information between modalities. This enables the\nconstruction of benchmark datasets that provide a novel testbed for systematic\nstudies of mutual information estimators and multimodal self-supervised\nlearning techniques. Our framework constructs realistic datasets with known\nmutual information using a flow-based generative model and a structured causal\nframework for generating correlated latent variables.\n","authors":["Raheem Karim Hashmani","Garrett W. Merz","Helen Qu","Mariel Pettee","Kyle Cranmer"],"pdf_url":"https://arxiv.org/pdf/2510.21686v1.pdf","comment":"15 pages, 4 figures, 1 table. Our code is publicly available at\n  https://github.com/RKHashmani/MmMi-Datasets"},{"id":"http://arxiv.org/abs/2504.05822v2","updated":"2025-10-24T17:44:21Z","published":"2025-04-08T09:05:33Z","title":"Federated Unlearning Made Practical: Seamless Integration via Negated\n  Pseudo-Gradients","summary":"  The right to be forgotten is a fundamental principle of privacy-preserving\nregulations and extends to Machine Learning (ML) paradigms such as Federated\nLearning (FL). While FL enhances privacy by enabling collaborative model\ntraining without sharing private data, trained models still retain the\ninfluence of training data. Federated Unlearning (FU) methods recently proposed\noften rely on impractical assumptions for real-world FL deployments, such as\nstoring client update histories or requiring access to a publicly available\ndataset. To address these constraints, this paper introduces a novel method\nthat leverages negated Pseudo-gradients Updates for Federated Unlearning (PUF).\nOur approach only uses standard client model updates, which are employed during\nregular FL rounds, and interprets them as pseudo-gradients. When a client needs\nto be forgotten, we apply the negation of their pseudo-gradients, appropriately\nscaled, to the global model. Unlike state-of-the-art mechanisms, PUF seamlessly\nintegrates with FL workflows, incurs no additional computational and\ncommunication overhead beyond standard FL rounds, and supports concurrent\nunlearning requests. We extensively evaluated the proposed method on two\nwell-known benchmark image classification datasets (CIFAR-10 and CIFAR-100) and\na real-world medical imaging dataset for segmentation (ProstateMRI), using\nthree different neural architectures: two residual networks and a vision\ntransformer. The experimental results across various settings demonstrate that\nPUF achieves state-of-the-art forgetting effectiveness and recovery time,\nwithout relying on any additional assumptions.\n","authors":["Alessio Mora","Carlo Mazzocca","Rebecca Montanari","Paolo Bellavista"],"pdf_url":"https://arxiv.org/pdf/2504.05822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07969v3","updated":"2025-10-24T17:37:23Z","published":"2025-07-10T17:48:03Z","title":"Reinforcement Learning with Action Chunking","summary":"  We present Q-chunking, a simple yet effective recipe for improving\nreinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.\nOur recipe is designed for the offline-to-online RL setting, where the goal is\nto leverage an offline prior dataset to maximize the sample-efficiency of\nonline learning. Effective exploration and sample-efficient learning remain\ncentral challenges in this setting, as it is not obvious how the offline data\nshould be utilized to acquire a good exploratory policy. Our key insight is\nthat action chunking, a technique popularized in imitation learning where\nsequences of future actions are predicted rather than a single action at each\ntimestep, can be applied to temporal difference (TD)-based RL methods to\nmitigate the exploration challenge. Q-chunking adopts action chunking by\ndirectly running RL in a 'chunked' action space, enabling the agent to (1)\nleverage temporally consistent behaviors from offline data for more effective\nonline exploration and (2) use unbiased $n$-step backups for more stable and\nefficient TD learning. Our experimental results demonstrate that Q-chunking\nexhibits strong offline performance and online sample efficiency, outperforming\nprior best offline-to-online methods on a range of long-horizon, sparse-reward\nmanipulation tasks.\n","authors":["Qiyang Li","Zhiyuan Zhou","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2507.07969v3.pdf","comment":"The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems (NeurIPS 2025); 36 pages, 17 figures"},{"id":"http://arxiv.org/abs/2508.04048v2","updated":"2025-10-24T17:37:07Z","published":"2025-08-06T03:21:20Z","title":"Quantum Temporal Fusion Transformer","summary":"  The \\textit{Temporal Fusion Transformer} (TFT), proposed by Lim \\textit{et\nal.}, published in \\textit{International Journal of Forecasting} (2021), is a\nstate-of-the-art attention-based deep neural network architecture specifically\ndesigned for multi-horizon time series forecasting. It has demonstrated\nsignificant performance improvements over existing benchmarks. In this work, we\nintroduce the Quantum Temporal Fusion Transformer (QTFT), a quantum-enhanced\nhybrid quantum-classical architecture that extends the capabilities of the\nclassical TFT framework. The core idea of this work is inspired by the\nfoundation studies, \\textit{The Power of Quantum Neural Networks} by Amira\nAbbas \\textit{et al.} and \\textit{Quantum Vision Transformers} by El Amine\nCherrat \\textit{et al.}, published in \\textit{ Nature Computational Science}\n(2021) and \\textit{Quantum} (2024), respectively. A key advantage of our\napproach lies in its foundation on a variational quantum algorithm, enabling\nimplementation on current noisy intermediate-scale quantum (NISQ) devices\nwithout strict requirements on the number of qubits or circuit depth. Our\nresults demonstrate that QTFT is successfully trained on the forecasting\ndatasets and is capable of accurately predicting future values. In particular,\nour experimental results on two different datasets display that the model\noutperforms its classical counterpart in terms of both training and test loss.\nThese results indicate the prospect of using quantum computing to boost deep\nlearning architectures in complex machine learning tasks.\n","authors":["Krishnakanta Barik","Goutam Paul"],"pdf_url":"https://arxiv.org/pdf/2508.04048v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13678v4","updated":"2025-10-24T17:36:52Z","published":"2025-06-16T16:32:51Z","title":"A Gravity-informed Spatiotemporal Transformer for Human Activity\n  Intensity Prediction","summary":"  Human activity intensity prediction is crucial to many location-based\nservices. Despite tremendous progress in modeling dynamics of human activity,\nmost existing methods overlook physical constraints of spatial interaction,\nleading to uninterpretable spatial correlations and over-smoothing phenomenon.\nTo address these limitations, this work proposes a physics-informed deep\nlearning framework, namely Gravity-informed Spatiotemporal Transformer\n(Gravityformer) by integrating the universal law of gravitation to refine\ntransformer attention. Specifically, it (1) estimates two spatially explicit\nmass parameters based on spatiotemporal embedding feature, (2) models the\nspatial interaction in end-to-end neural network using proposed adaptive\ngravity model to learn the physical constraint, and (3) utilizes the learned\nspatial interaction to guide and mitigate the over-smoothing phenomenon in\ntransformer attention. Moreover, a parallel spatiotemporal graph convolution\ntransformer is proposed for achieving a balance between coupled spatial and\ntemporal learning. Systematic experiments on six real-world large-scale\nactivity datasets demonstrate the quantitative and qualitative superiority of\nour model over state-of-the-art benchmarks. Additionally, the learned gravity\nattention matrix can be not only disentangled and interpreted based on\ngeographical laws, but also improved the generalization in zero-shot\ncross-region inference. This work provides a novel insight into integrating\nphysical laws with deep learning for spatiotemporal prediction.\n","authors":["Yi Wang","Zhenghong Wang","Fan Zhang","Chaogui Kang","Sijie Ruan","Di Zhu","Chengling Tang","Zhongfu Ma","Weiyu Zhang","Yu Zheng","Philip S. Yu","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13678v4.pdf","comment":"IEEE TPAMI 2025. 18 pages, 14 figures"},{"id":"http://arxiv.org/abs/2411.18322v2","updated":"2025-10-24T17:36:28Z","published":"2024-11-27T13:23:11Z","title":"Mixture of Experts in Image Classification: What's the Sweet Spot?","summary":"  Mixture-of-Experts (MoE) models have shown promising potential for\nparameter-efficient scaling across domains. However, their application to image\nclassification remains limited, often requiring billion-scale datasets to be\ncompetitive. In this work, we explore the integration of MoE layers into image\nclassification architectures using open datasets. We conduct a systematic\nanalysis across different MoE configurations and model scales. We find that\nmoderate parameter activation per sample provides the best trade-off between\nperformance and efficiency. However, as the number of activated parameters\nincreases, the benefits of MoE diminish. Our analysis yields several practical\ninsights for vision MoE design. First, MoE layers most effectively strengthen\ntiny and mid-sized models, while gains taper off for large-capacity networks\nand do not redefine state-of-the-art ImageNet performance. Second, a Last-2\nplacement heuristic offers the most robust cross-architecture choice, with\nEvery-2 slightly better for Vision Transform (ViT), and both remaining\neffective as data and model scale increase. Third, larger datasets (e.g.,\nImageNet-21k) allow more experts, up to 16, for ConvNeXt to be utilized\neffectively without changing placement, as increased data reduces overfitting\nand promotes broader expert specialization. Finally, a simple linear router\nperforms best, suggesting that additional routing complexity yields no\nconsistent benefit.\n","authors":["Mathurin Videau","Alessandro Leite","Marc Schoenauer","Olivier Teytaud"],"pdf_url":"https://arxiv.org/pdf/2411.18322v2.pdf","comment":"Published in Transactions on Machine Learning Research"},{"id":"http://arxiv.org/abs/2510.18878v2","updated":"2025-10-24T17:31:44Z","published":"2025-09-13T18:16:29Z","title":"CityAQVis: Integrated ML-Visualization Sandbox Tool for Pollutant\n  Estimation in Urban Regions Using Multi-Source Data (Software Article)","summary":"  Urban air pollution poses significant risks to public health, environmental\nsustainability, and policy planning. Effective air quality management requires\npredictive tools that can integrate diverse datasets and communicate complex\nspatial and temporal pollution patterns. There is a gap in interactive tools\nwith seamless integration of forecasting and visualization of spatial\ndistributions of air pollutant concentrations. We present CityAQVis, an\ninteractive machine learning ML sandbox tool designed to predict and visualize\npollutant concentrations at the ground level using multi-source data, which\nincludes satellite observations, meteorological parameters, population density,\nelevation, and nighttime lights. While traditional air quality visualization\ntools often lack forecasting capabilities, CityAQVis enables users to build and\ncompare predictive models, visualizing the model outputs and offering insights\ninto pollution dynamics at the ground level. The pilot implementation of the\ntool is tested through case studies predicting nitrogen dioxide (NO2)\nconcentrations in metropolitan regions, highlighting its adaptability to\nvarious pollutants. Through an intuitive graphical user interface (GUI), the\nuser can perform comparative visualizations of the spatial distribution of\nsurface-level pollutant concentration in two different urban scenarios. Our\nresults highlight the potential of ML-driven visual analytics to improve\nsituational awareness and support data-driven decision-making in air quality\nmanagement.\n","authors":["Brij Bidhin Desai","Yukta Arvind Rajapur","Aswathi Mundayatt","Jaya Sreevalsan-Nair"],"pdf_url":"https://arxiv.org/pdf/2510.18878v2.pdf","comment":"19 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2503.10799v3","updated":"2025-10-24T17:27:32Z","published":"2025-03-13T18:50:22Z","title":"Fixed-Point RNNs: Interpolating from Diagonal to Dense","summary":"  Linear recurrent neural networks (RNNs) and state-space models (SSMs) such as\nMamba have become promising alternatives to softmax-attention as sequence\nmixing layers in Transformer architectures. Current models, however, do not\nexhibit the full state-tracking expressivity of RNNs because they rely on\nchannel-wise (i.e. diagonal) sequence mixing. In this paper, we investigate\nparameterizations of a large class of dense linear RNNs as fixed-points of\nparallelizable diagonal linear RNNs. The resulting models can naturally trade\nexpressivity for efficiency at a fixed number of parameters and achieve\nstate-of-the-art results on the state-tracking benchmarks $A_5$ and $S_5$,\nwhile matching performance on copying and other tasks.\n","authors":["Sajad Movahedi","Felix Sarnthein","Nicola Muca Cirone","Antonio Orvieto"],"pdf_url":"https://arxiv.org/pdf/2503.10799v3.pdf","comment":"NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.21669v1","updated":"2025-10-24T17:24:26Z","published":"2025-10-24T17:24:26Z","title":"Optimal Graph Clustering without Edge Density Signals","summary":"  This paper establishes the theoretical limits of graph clustering under the\nPopularity-Adjusted Block Model (PABM), addressing limitations of existing\nmodels. In contrast to the Stochastic Block Model (SBM), which assumes uniform\nvertex degrees, and to the Degree-Corrected Block Model (DCBM), which applies\nuniform degree corrections across clusters, PABM introduces separate popularity\nparameters for intra- and inter-cluster connections. Our main contribution is\nthe characterization of the optimal error rate for clustering under PABM, which\nprovides novel insights on clustering hardness: we demonstrate that unlike SBM\nand DCBM, cluster recovery remains possible in PABM even when traditional\nedge-density signals vanish, provided intra- and inter-cluster popularity\ncoefficients differ. This highlights a dimension of degree heterogeneity\ncaptured by PABM but overlooked by DCBM: local differences in connectivity\npatterns can enhance cluster separability independently of global edge\ndensities. Finally, because PABM exhibits a richer structure, its expected\nadjacency matrix has rank between $k$ and $k^2$, where $k$ is the number of\nclusters. As a result, spectral embeddings based on the top $k$ eigenvectors\nmay fail to capture important structural information. Our numerical experiments\non both synthetic and real datasets confirm that spectral clustering algorithms\nincorporating $k^2$ eigenvectors outperform traditional spectral approaches.\n","authors":["Maximilien Dreveton","Elaine Siyu Liu","Matthias Grossglauser","Patrick Thiran"],"pdf_url":"https://arxiv.org/pdf/2510.21669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.17412v3","updated":"2025-10-24T17:22:51Z","published":"2025-08-24T15:34:17Z","title":"Convergence and Generalization of Anti-Regularization for Parametric\n  Models","summary":"  Anti-regularization introduces a reward term with a reversed sign into the\nloss function, deliberately amplifying model expressivity in small-sample\nregimes while ensuring that the intervention gradually vanishes as the sample\nsize grows through a power-law decay schedule. We formalize spectral safety\nconditions and trust-region constraints, and we design a lightweight safeguard\nthat combines a projection operator with gradient clipping to guarantee stable\nintervention. Theoretical analysis extends to linear smoothers and the Neural\nTangent Kernel regime, providing practical guidance on the choice of decay\nexponents through the balance between empirical risk and variance. Empirical\nresults show that Anti-regularization mitigates underfitting in both regression\nand classification while preserving generalization and improving calibration.\nAblation studies confirm that the decay schedule and safeguards are essential\nto avoiding overfitting and instability. As an alternative, we also propose a\ndegrees-of-freedom targeting schedule that maintains constant per-sample\ncomplexity. Anti-regularization constitutes a simple and reproducible procedure\nthat integrates seamlessly into standard empirical risk minimization pipelines,\nenabling robust learning under limited data and resource constraints by\nintervening only when necessary and vanishing otherwise.\n","authors":["Dongseok Kim","Wonjun Jeong","Gisung Oh"],"pdf_url":"https://arxiv.org/pdf/2508.17412v3.pdf","comment":"v3: Revised the paragraph under Theoretical Analysis (English\n  translation and typo corrections)"},{"id":"http://arxiv.org/abs/2505.23579v2","updated":"2025-10-24T17:16:49Z","published":"2025-05-29T15:49:27Z","title":"BioReason: Incentivizing Multimodal Biological Reasoning within a\n  DNA-LLM Model","summary":"  Unlocking deep and interpretable biological reasoning from complex genomic\ndata remains a major AI challenge limiting scientific progress. While current\nDNA foundation models excel at representing sequences, they struggle with\nmulti-step reasoning and lack transparent, biologically meaningful\nexplanations. BioReason addresses this by tightly integrating a DNA foundation\nmodel with a large language model (LLM), enabling the LLM to directly interpret\nand reason over genomic information. Through supervised fine-tuning and\nreinforcement learning, BioReason learns to produce logical, biologically\ncoherent deductions. It achieves major performance gains, boosting KEGG-based\ndisease pathway prediction accuracy from 86% to 98% and improving variant\neffect prediction by an average of 15% over strong baselines. BioReason can\nreason over unseen biological entities and explain its decisions step by step,\noffering a transformative framework for interpretable, mechanistic AI in\nbiology. All data, code, and checkpoints are available at\nhttps://github.com/bowang-lab/BioReason\n","authors":["Adibvafa Fallahpour","Andrew Magnuson","Purav Gupta","Shihao Ma","Jack Naimer","Arnav Shah","Haonan Duan","Omar Ibrahim","Hani Goodarzi","Chris J. Maddison","Bo Wang"],"pdf_url":"https://arxiv.org/pdf/2505.23579v2.pdf","comment":"28 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2510.11128v2","updated":"2025-10-24T17:14:46Z","published":"2025-10-13T08:19:56Z","title":"Lightweight Facial Landmark Detection in Thermal Images via Multi-Level\n  Cross-Modal Knowledge Transfer","summary":"  Facial Landmark Detection (FLD) in thermal imagery is critical for\napplications in challenging lighting conditions, but it is hampered by the lack\nof rich visual cues. Conventional cross-modal solutions, like feature fusion or\nimage translation from RGB data, are often computationally expensive or\nintroduce structural artifacts, limiting their practical deployment. To address\nthis, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a\nnovel framework that decouples high-fidelity RGB-to-thermal knowledge transfer\nfrom model compression to create both accurate and efficient thermal FLD\nmodels. A central challenge during knowledge transfer is the profound modality\ngap between RGB and thermal data, where traditional unidirectional distillation\nfails to enforce semantic consistency across disparate feature spaces. To\novercome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a\nbidirectional mechanism designed specifically for this task. DIKD establishes a\nconnection between modalities: it not only guides the thermal student with rich\nRGB features but also validates the student's learned representations by\nfeeding them back into the frozen teacher's prediction head. This closed-loop\nsupervision forces the student to learn modality-invariant features that are\nsemantically aligned with the teacher, ensuring a robust and profound knowledge\ntransfer. Experiments show that our approach sets a new state-of-the-art on\npublic thermal FLD benchmarks, notably outperforming previous methods while\ndrastically reducing computational overhead.\n","authors":["Qiyi Tong","Olivia Nocentini","Marta Lagomarsino","Kuanqi Cai","Marta Lorenzini","Arash Ajoudani"],"pdf_url":"https://arxiv.org/pdf/2510.11128v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.18119v2","updated":"2025-10-24T17:13:05Z","published":"2025-09-10T13:09:27Z","title":"MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents","summary":"  Building general-purpose graphical user interface (GUI) agents has become\nincreasingly promising with the progress in vision language models. However,\ndeveloping effective mobile GUI agents with reinforcement learning (RL) remains\nchallenging due to the heavy-tailed distribution of task difficulty and the\ninefficiency of large-scale environment sampling. We present an online agentic\nreinforcement learning framework MobileRL to enhance GUI agents in mobile\nenvironments. Its core component is the Difficulty-ADAptive GRPO (ADAGRPO)\nalgorithm. In ADAGRPO, we design difficulty-adaptive positive replay and\nfailure curriculum filtering to adapt the model to different task difficulties.\nWe introduce the shortest-path reward adjustment strategy to reshape rewards\nconcerning the task length in multi-turn agentic tasks. Those strategies\njointly stabilize RL training, improve sample efficiency, and generate strong\nperformance across diverse mobile apps and tasks. We apply MOBILERL to two open\nmodels (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B\nmodel achieves state-of-the-art results in terms of success rates on both\nAndroidWorld (80.2%) and AndroidLab (53.6%). The MOBILERL framework is\nopen-sourced at: https://github.com/THUDM/MobileRL.\n","authors":["Yifan Xu","Xiao Liu","Xinghan Liu","Jiaqi Fu","Hanchen Zhang","Bohao Jing","Shudan Zhang","Yuting Wang","Wenyi Zhao","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2509.18119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17470v2","updated":"2025-10-24T17:04:17Z","published":"2025-09-22T08:05:44Z","title":"Transformer-Gather, Fuzzy-Reconsider: A Scalable Hybrid Framework for\n  Entity Resolution","summary":"  Entity resolution plays a significant role in enterprise systems where data\nintegrity must be rigorously maintained. Traditional methods often struggle\nwith handling noisy data or semantic understanding, while modern methods suffer\nfrom computational costs or the excessive need for parallel computation. In\nthis study, we introduce a scalable hybrid framework, which is designed to\naddress several important problems, including scalability, noise robustness,\nand reliable results. We utilized a pre-trained language model to encode each\nstructured data into corresponding semantic embedding vectors. Subsequently,\nafter retrieving a semantically relevant subset of candidates, we apply a\nsyntactic verification stage using fuzzy string matching techniques to refine\nclassification on the unlabeled data. This approach was applied to a real-world\nentity resolution task, which exposed a linkage between a central user\nmanagement database and numerous shared hosting server records. Compared to\nother methods, this approach exhibits an outstanding performance in terms of\nboth processing time and robustness, making it a reliable solution for a\nserver-side product. Crucially, this efficiency does not compromise results, as\nthe system maintains a high retrieval recall of approximately 0.97. The\nscalability of the framework makes it deployable on standard CPU-based\ninfrastructure, offering a practical and effective solution for\nenterprise-level data integrity auditing.\n","authors":["Mohammadreza Sharifi","Danial Ahmadzadeh"],"pdf_url":"https://arxiv.org/pdf/2509.17470v2.pdf","comment":"Accepted at ICCKE 2025 Conference. 6 tables, 7 figures"},{"id":"http://arxiv.org/abs/2510.20787v2","updated":"2025-10-24T16:56:22Z","published":"2025-10-23T17:53:03Z","title":"Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction","summary":"  Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches.\n","authors":["Mutian He","Philip N. Garner"],"pdf_url":"https://arxiv.org/pdf/2510.20787v2.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.21638v1","updated":"2025-10-24T16:51:17Z","published":"2025-10-24T16:51:17Z","title":"DEEDEE: Fast and Scalable Out-of-Distribution Dynamics Detection","summary":"  Deploying reinforcement learning (RL) in safety-critical settings is\nconstrained by brittleness under distribution shift. We study\nout-of-distribution (OOD) detection for RL time series and introduce DEEDEE, a\ntwo-statistic detector that revisits representation-heavy pipelines with a\nminimal alternative. DEEDEE uses only an episodewise mean and an RBF kernel\nsimilarity to a training summary, capturing complementary global and local\ndeviations. Despite its simplicity, DEEDEE matches or surpasses contemporary\ndetectors across standard RL OOD suites, delivering a 600-fold reduction in\ncompute (FLOPs / wall-time) and an average 5% absolute accuracy gain over\nstrong baselines. Conceptually, our results indicate that diverse anomaly types\noften imprint on RL trajectories through a small set of low-order statistics,\nsuggesting a compact foundation for OOD detection in complex environments.\n","authors":["Tala Aljaafari","Varun Kanade","Philip Torr","Christian Schroeder de Witt"],"pdf_url":"https://arxiv.org/pdf/2510.21638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21631v1","updated":"2025-10-24T16:36:34Z","published":"2025-10-24T16:36:34Z","title":"Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations","summary":"  Knowledge distillation is a promising approach to transfer capabilities from\ncomplex teacher models to smaller, resource-efficient student models that can\nbe deployed easily, particularly in task-aware scenarios. However, existing\nmethods of task-aware distillation typically require substantial quantities of\ndata which may be unavailable or expensive to obtain in many practical\nscenarios. In this paper, we address this challenge by introducing a novel\nstrategy called Counterfactual-explanation-infused Distillation CoD for\nfew-shot task-aware knowledge distillation by systematically infusing\ncounterfactual explanations. Counterfactual explanations (CFEs) refer to inputs\nthat can flip the output prediction of the teacher model with minimum\nperturbation. Our strategy CoD leverages these CFEs to precisely map the\nteacher's decision boundary with significantly fewer samples. We provide\ntheoretical guarantees for motivating the role of CFEs in distillation, from\nboth statistical and geometric perspectives. We mathematically show that CFEs\ncan improve parameter estimation by providing more informative examples near\nthe teacher's decision boundary. We also derive geometric insights on how CFEs\neffectively act as knowledge probes, helping the students mimic the teacher's\ndecision boundaries more effectively than standard data. We perform experiments\nacross various datasets and LLMs to show that CoD outperforms standard\ndistillation approaches in few-shot regimes (as low as 8-512 samples). Notably,\nCoD only uses half of the original samples used by the baselines, paired with\ntheir corresponding CFEs and still improves performance.\n","authors":["Faisal Hamman","Pasan Dissanayake","Yanjun Fu","Sanghamitra Dutta"],"pdf_url":"https://arxiv.org/pdf/2510.21631v1.pdf","comment":"NeurIPS 2025"}]},"2025-10-27T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2510.23607v1","updated":"2025-10-27T17:59:59Z","published":"2025-10-27T17:59:59Z","title":"Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial\n  Representations","summary":"  Humans learn abstract concepts through multisensory synergy, and once formed,\nsuch representations can often be recalled from a single modality. Inspired by\nthis principle, we introduce Concerto, a minimalist simulation of human concept\nlearning for spatial cognition, combining 3D intra-modal self-distillation with\n2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more\ncoherent and informative spatial features, as demonstrated by zero-shot\nvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervised\nmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,\nin linear probing for 3D scene perception. With full fine-tuning, Concerto sets\nnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%\nmIoU on ScanNet). We further present a variant of Concerto tailored for\nvideo-lifted point cloud spatial understanding, and a translator that linearly\nprojects Concerto representations into CLIP's language space, enabling\nopen-world perception. These results highlight that Concerto emerges spatial\nrepresentations with superior fine-grained geometric and semantic consistency.\n","authors":["Yujia Zhang","Xiaoyang Wu","Yixing Lao","Chengyao Wang","Zhuotao Tian","Naiyan Wang","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.23607v1.pdf","comment":"NeurIPS 2025, produced by Pointcept, project page:\n  https://pointcept.github.io/Concerto"},{"id":"http://arxiv.org/abs/2510.23605v1","updated":"2025-10-27T17:59:51Z","published":"2025-10-27T17:59:51Z","title":"Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with\n  Progressive Texture Infilling","summary":"  Current 3D/4D generation methods are usually optimized for photorealism,\nefficiency, and aesthetics. However, they often fail to preserve the semantic\nidentity of the subject across different viewpoints. Adapting generation\nmethods with one or few images of a specific subject (also known as\nPersonalization or Subject-driven generation) allows generating visual content\nthat align with the identity of the subject. However, personalized 3D/4D\ngeneration is still largely underexplored. In this work, we introduce TIRE\n(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.\nIt takes an initial 3D asset produced by an existing 3D generative model as\ninput and uses video tracking to identify the regions that need to be modified.\nThen, we adopt a subject-driven 2D inpainting model for progressively infilling\nthe identified regions. Finally, we resplat the modified 2D multi-view\nobservations back to 3D while still maintaining consistency. Extensive\nexperiments demonstrate that our approach significantly improves identity\npreservation in 3D/4D generation compared to state-of-the-art methods. Our\nproject website is available at\nhttps://zsh2000.github.io/track-inpaint-resplat.github.io/.\n","authors":["Shuhong Zheng","Ashkan Mirzaei","Igor Gilitschenski"],"pdf_url":"https://arxiv.org/pdf/2510.23605v1.pdf","comment":"NeurIPS 2025, 38 pages, 22 figures"},{"id":"http://arxiv.org/abs/2510.23603v1","updated":"2025-10-27T17:59:32Z","published":"2025-10-27T17:59:32Z","title":"PixelRefer: A Unified Framework for Spatio-Temporal Object Referring\n  with Arbitrary Granularity","summary":"  Multimodal large language models (MLLMs) have demonstrated strong\ngeneral-purpose capabilities in open-world visual comprehension. However, most\nexisting MLLMs primarily focus on holistic, scene-level understanding, often\noverlooking the need for fine-grained, object-centric reasoning. In this paper,\nwe present PixelRefer, a unified region-level MLLM framework that enables\nadvanced fine-grained understanding over user-specified regions across both\nimages and videos. Motivated by the observation that LLM attention\npredominantly focuses on object-level tokens, we propose a Scale-Adaptive\nObject Tokenizer (SAOT) to generate compact and semantically rich object\nrepresentations from free-form regions. Our analysis reveals that global visual\ntokens contribute mainly in early LLM layers, inspiring the design of\nPixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion\nmodule to pre-fuse global context into object tokens. This yields a lightweight\nObject-Only Framework that substantially reduces computational cost while\nmaintaining high semantic fidelity. To facilitate fine-grained instruction\ntuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction\ndataset. Extensive experiments across a range of benchmarks validate that\nPixelRefer achieves leading performance with fewer training samples, while\nPixelRefer-Lite offers competitive accuracy with notable gains in efficiency.\n","authors":["Yuqian Yuan","Wenqiao Zhang","Xin Li","Shihao Wang","Kehan Li","Wentong Li","Jun Xiao","Lei Zhang","Beng Chin Ooi"],"pdf_url":"https://arxiv.org/pdf/2510.23603v1.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2510.23594v1","updated":"2025-10-27T17:57:52Z","published":"2025-10-27T17:57:52Z","title":"PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error\n  Detection","summary":"  We introduce \\textbf{PRISM-Bench}, a benchmark of puzzle-based visual\nchallenges designed to evaluate not only whether models can solve problems, but\nhow their reasoning unfolds. Unlike prior evaluations that measure only\nfinal-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual\npuzzle and a step-by-step chain-of-thought (CoT) containing exactly one error,\nmodels must identify the first incorrect step. This setting enables\nfine-grained assessment of logical consistency, error detection, and visual\nreasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric,\nand analogical reasoning, resisting shortcuts based on superficial pattern\nmatching. Evaluations across state-of-the-art MLLMs reveal a persistent gap\nbetween fluent generation and faithful reasoning: models that produce plausible\nCoTs often fail to locate simple logical faults. By disentangling answer\ngeneration from reasoning verification, PRISM-Bench offers a sharper lens on\nmultimodal reasoning competence and underscores the need for diagnostic\nevaluation protocols in the development of trustworthy MLLMs.\n","authors":["Yusu Qian","Cheng Wan","Chao Jia","Yinfei Yang","Qingyu Zhao","Zhe Gan"],"pdf_url":"https://arxiv.org/pdf/2510.23594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23589v1","updated":"2025-10-27T17:54:57Z","published":"2025-10-27T17:54:57Z","title":"InFlux: A Benchmark for Self-Calibration of Dynamic Intrinsics of Video\n  Cameras","summary":"  Accurately tracking camera intrinsics is crucial for achieving 3D\nunderstanding from 2D video. However, most 3D algorithms assume that camera\nintrinsics stay constant throughout a video, which is often not true for many\nreal-world in-the-wild videos. A major obstacle in this field is a lack of\ndynamic camera intrinsics benchmarks--existing benchmarks typically offer\nlimited diversity in scene content and intrinsics variation, and none provide\nper-frame intrinsic changes for consecutive video frames. In this paper, we\npresent Intrinsics in Flux (InFlux), a real-world benchmark that provides\nper-frame ground truth intrinsics annotations for videos with dynamic\nintrinsics. Compared to prior benchmarks, InFlux captures a wider range of\nintrinsic variations and scene diversity, featuring 143K+ annotated frames from\n386 high-resolution indoor and outdoor videos with dynamic camera intrinsics.\nTo ensure accurate per-frame intrinsics, we build a comprehensive lookup table\nof calibration experiments and extend the Kalibr toolbox to improve its\naccuracy and robustness. Using our benchmark, we evaluate existing baseline\nmethods for predicting camera intrinsics and find that most struggle to achieve\naccurate predictions on videos with dynamic intrinsics. For the dataset, code,\nvideos, and submission, please visit https://influx.cs.princeton.edu/.\n","authors":["Erich Liang","Roma Bhattacharjee","Sreemanti Dey","Rafael Moschopoulos","Caitlin Wang","Michel Liao","Grace Tan","Andrew Wang","Karhan Kayan","Stamatis Alexandropoulos","Jia Deng"],"pdf_url":"https://arxiv.org/pdf/2510.23589v1.pdf","comment":"Accepted at NeurIPS 2025 DB Track, Camera Ready Version.\n  Supplementary material included"},{"id":"http://arxiv.org/abs/2510.23588v1","updated":"2025-10-27T17:54:08Z","published":"2025-10-27T17:54:08Z","title":"FARMER: Flow AutoRegressive Transformer over Pixels","summary":"  Directly modeling the explicit likelihood of the raw data distribution is key\ntopic in the machine learning area, which achieves the scaling successes in\nLarge Language Models by autoregressive modeling. However, continuous AR\nmodeling over visual pixel data suffer from extremely long sequences and\nhigh-dimensional spaces. In this paper, we present FARMER, a novel end-to-end\ngenerative framework that unifies Normalizing Flows (NF) and Autoregressive\n(AR) models for tractable likelihood estimation and high-quality image\nsynthesis directly from raw pixels. FARMER employs an invertible autoregressive\nflow to transform images into latent sequences, whose distribution is modeled\nimplicitly by an autoregressive model. To address the redundancy and complexity\nin pixel-level modeling, we propose a self-supervised dimension reduction\nscheme that partitions NF latent channels into informative and redundant\ngroups, enabling more effective and efficient AR modeling. Furthermore, we\ndesign a one-step distillation scheme to significantly accelerate inference\nspeed and introduce a resampling-based classifier-free guidance algorithm to\nboost image generation quality. Extensive experiments demonstrate that FARMER\nachieves competitive performance compared to existing pixel-based generative\nmodels while providing exact likelihoods and scalable training.\n","authors":["Guangting Zheng","Qinyu Zhao","Tao Yang","Fei Xiao","Zhijie Lin","Jie Wu","Jiajun Deng","Yanyong Zhang","Rui Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.23588v1.pdf","comment":"Bytedance Seed Technical Report"},{"id":"http://arxiv.org/abs/2510.20820v2","updated":"2025-10-27T17:53:30Z","published":"2025-10-23T17:59:55Z","title":"LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered\n  Canvas","summary":"  Despite their impressive visual fidelity, existing personalized generative\nmodels lack interactive control over spatial composition and scale poorly to\nmultiple subjects. To address these limitations, we present LayerComposer, an\ninteractive framework for personalized, multi-subject text-to-image generation.\nOur approach introduces two main contributions: (1) a layered canvas, a novel\nrepresentation in which each subject is placed on a distinct layer, enabling\nocclusion-free composition; and (2) a locking mechanism that preserves selected\nlayers with high fidelity while allowing the remaining layers to adapt flexibly\nto the surrounding context. Similar to professional image-editing software, the\nproposed layered canvas allows users to place, resize, or lock input subjects\nthrough intuitive layer manipulation. Our versatile locking mechanism requires\nno architectural changes, relying instead on inherent positional embeddings\ncombined with a new complementary data sampling strategy. Extensive experiments\ndemonstrate that LayerComposer achieves superior spatial control and identity\npreservation compared to the state-of-the-art methods in multi-subject\npersonalized image generation.\n","authors":["Guocheng Gordon Qian","Ruihang Zhang","Tsai-Shien Chen","Yusuf Dalva","Anujraaj Argo Goyal","Willi Menapace","Ivan Skorokhodov","Meng Dong","Arpit Sahni","Daniil Ostashev","Ju Hu","Sergey Tulyakov","Kuan-Chieh Jackson Wang"],"pdf_url":"https://arxiv.org/pdf/2510.20820v2.pdf","comment":"9 pages, preprint. Project page:\n  https://snap-research.github.io/layercomposer/"},{"id":"http://arxiv.org/abs/2507.22030v2","updated":"2025-10-27T17:51:47Z","published":"2025-07-29T17:27:15Z","title":"ReXGroundingCT: A 3D Chest CT Dataset for Segmentation of Findings from\n  Free-Text Reports","summary":"  We introduce ReXGroundingCT, the first publicly available dataset linking\nfree-text findings to pixel-level 3D segmentations in chest CT scans. The\ndataset includes 3,142 non-contrast chest CT scans paired with standardized\nradiology reports from CT-RATE. Construction followed a structured three-stage\npipeline. First, GPT-4 was used to extract and standardize findings,\ndescriptors, and metadata from reports originally written in Turkish and\nmachine-translated into English. Second, GPT-4o-mini categorized each finding\ninto a hierarchical ontology of lung and pleural abnormalities. Third, 3D\nannotations were produced for all CT volumes: the training set was\nquality-assured by board-certified radiologists, and the validation and test\nsets were fully annotated by board-certified radiologists. Additionally, a\ncomplementary chain-of-thought dataset was created to provide step-by-step\nhierarchical anatomical reasoning for localizing findings within the CT volume,\nusing GPT-4o and localization coordinates derived from organ segmentation\nmodels. ReXGroundingCT contains 16,301 annotated entities across 8,028\ntext-to-3D-segmentation pairs, covering diverse radiological patterns from\n3,142 non-contrast CT scans. About 79% of findings are focal abnormalities and\n21% are non-focal. The dataset includes a public validation set of 50 cases and\na private test set of 100 cases, both annotated by board-certified\nradiologists. The dataset establishes a foundation for enabling free-text\nfinding segmentation and grounded radiology report generation in CT imaging.\nModel performance on the private test set is hosted on a public leaderboard at\nhttps://rexrank.ai/ReXGroundingCT. The dataset is available at\nhttps://huggingface.co/datasets/rajpurkarlab/ReXGroundingCT.\n","authors":["Mohammed Baharoon","Luyang Luo","Michael Moritz","Abhinav Kumar","Sung Eun Kim","Xiaoman Zhang","Miao Zhu","Mahmoud Hussain Alabbad","Maha Sbayel Alhazmi","Neel P. Mistry","Lucas Bijnens","Kent Ryan Kleinschmidt","Brady Chrisler","Sathvik Suryadevara","Sri Sai Dinesh Jaliparthi","Noah Michael Prudlo","Mark David Marino","Jeremy Palacio","Rithvik Akula","Di Zhou","Hong-Yu Zhou","Ibrahim Ethem Hamamci","Scott J. Adams","Hassan Rayhan AlOmaish","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2507.22030v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.15963v2","updated":"2025-10-27T17:51:21Z","published":"2025-10-11T20:13:59Z","title":"ESCA: Contextualizing Embodied Agents via Scene-Graph Generation","summary":"  Multi-modal large language models (MLLMs) are making rapid progress toward\ngeneral-purpose embodied agents. However, existing MLLMs do not reliably\ncapture fine-grained links between low-level visual features and high-level\ntextual semantics, leading to weak grounding and inaccurate perception. To\novercome this challenge, we propose ESCA, a framework that contextualizes\nembodied agents by grounding their perception in spatial-temporal scene graphs.\nAt its core is SGCLIP, a novel, open-domain, promptable foundation model for\ngenerating scene graphs that is based on CLIP. SGCLIP is trained on 87K+\nopen-domain videos using a neurosymbolic pipeline that aligns automatically\ngenerated captions with scene graphs produced by the model itself, eliminating\nthe need for human-labeled annotations. We demonstrate that SGCLIP excels in\nboth prompt-based inference and task-specific fine-tuning, achieving\nstate-of-the-art results on scene graph generation and action localization\nbenchmarks. ESCA with SGCLIP improves perception for embodied agents based on\nboth open-source and commercial MLLMs, achieving state of-the-art performance\nacross two embodied environments. Notably, ESCA significantly reduces agent\nperception errors and enables open-source models to surpass proprietary\nbaselines. We release the source code for SGCLIP model training at\nhttps://github.com/video-fm/LASER and for the embodied agent at\nhttps://github.com/video-fm/ESCA.\n","authors":["Jiani Huang","Amish Sethi","Matthew Kuo","Mayank Keoliya","Neelay Velingker","JungHo Jung","Ser-Nam Lim","Ziyang Li","Mayur Naik"],"pdf_url":"https://arxiv.org/pdf/2510.15963v2.pdf","comment":"Accepted as a Spotlight Paper at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23581v1","updated":"2025-10-27T17:50:19Z","published":"2025-10-27T17:50:19Z","title":"Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human\n  Animation","summary":"  Audio-driven human animation models often suffer from identity drift during\ntemporal autoregressive generation, where characters gradually lose their\nidentity over time. One solution is to generate keyframes as intermediate\ntemporal anchors that prevent degradation, but this requires an additional\nkeyframe generation stage and can restrict natural motion dynamics. To address\nthis, we propose Lookahead Anchoring, which leverages keyframes from future\ntimesteps ahead of the current generation window, rather than within it. This\ntransforms keyframes from fixed boundaries into directional beacons: the model\ncontinuously pursues these future anchors while responding to immediate audio\ncues, maintaining consistent identity through persistent guidance. This also\nenables self-keyframing, where the reference image serves as the lookahead\ntarget, eliminating the need for keyframe generation entirely. We find that the\ntemporal lookahead distance naturally controls the balance between expressivity\nand consistency: larger distances allow for greater motion freedom, while\nsmaller ones strengthen identity adherence. When applied to three recent human\nanimation models, Lookahead Anchoring achieves superior lip synchronization,\nidentity preservation, and visual quality, demonstrating improved temporal\nconditioning across several different architectures. Video results are\navailable at the following link: https://lookahead-anchoring.github.io.\n","authors":["Junyoung Seo","Rodrigo Mira","Alexandros Haliassos","Stella Bounareli","Honglie Chen","Linh Tran","Seungryong Kim","Zoe Landgraf","Jie Shen"],"pdf_url":"https://arxiv.org/pdf/2510.23581v1.pdf","comment":"Project page: https://lookahead-anchoring.github.io"},{"id":"http://arxiv.org/abs/2510.23576v1","updated":"2025-10-27T17:46:43Z","published":"2025-10-27T17:46:43Z","title":"UrbanVLA: A Vision-Language-Action Model for Urban Micromobility","summary":"  Urban micromobility applications, such as delivery robots, demand reliable\nnavigation across large-scale urban environments while following long-horizon\nroute instructions. This task is particularly challenging due to the dynamic\nand unstructured nature of real-world city areas, yet most existing navigation\nmethods remain tailored to short-scale and controllable scenarios. Effective\nurban micromobility requires two complementary levels of navigation skills:\nlow-level capabilities such as point-goal reaching and obstacle avoidance, and\nhigh-level capabilities, such as route-visual alignment. To this end, we\npropose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework\ndesigned for scalable urban navigation. Our method explicitly aligns noisy\nroute waypoints with visual observations during execution, and subsequently\nplans trajectories to drive the robot. To enable UrbanVLA to master both levels\nof navigation, we employ a two-stage training pipeline. The process begins with\nSupervised Fine-Tuning (SFT) using simulated environments and trajectories\nparsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on\na mixture of simulation and real-world data, which enhances the model's safety\nand adaptability in real-world settings. Experiments demonstrate that UrbanVLA\nsurpasses strong baselines by more than 55% in the SocialNav task on MetaUrban.\nFurthermore, UrbanVLA achieves reliable real-world navigation, showcasing both\nscalability to large-scale urban environments and robustness against real-world\nuncertainties.\n","authors":["Anqi Li","Zhiyong Wang","Jiazhao Zhang","Minghan Li","Yunpeng Qi","Zhibo Chen","Zhizheng Zhang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07346v2","updated":"2025-10-27T17:45:30Z","published":"2025-03-10T13:59:57Z","title":"Now you see me! Attribution Distributions Reveal What is Truly Important\n  for a Prediction","summary":"  Neural networks are regularly employed in high-stakes decision-making, where\nunderstanding and transparency is key. Attribution methods have been developed\nto gain understanding into which input features neural networks use for a\nspecific prediction. Although widely used in computer vision, these methods\noften result in unspecific saliency maps that fail to identify the relevant\ninformation that led to a decision, supported by different benchmarks results.\nHere, we revisit the common attribution pipeline and identify one cause for the\nlack of specificity in attributions as the computation of attribution of\nisolated logits. Instead, we suggest to combine attributions of multiple class\nlogits in analogy to how the softmax combines the information across logits. By\ncomputing probability distributions of attributions over classes for each\nspatial location in the image, we unleash the true capabilities of existing\nattribution methods, revealing better object- and instance-specificity and\nuncovering discriminative as well as shared features between classes. On common\nbenchmarks, including the grid-pointing game and randomization-based sanity\nchecks, we show that this reconsideration of how and where we compute\nattributions across the network improves established attribution methods while\nstaying agnostic to model architectures. We make the code publicly available:\nhttps://github.com/nilspwalter/var.\n","authors":["Nils Philipp Walter","Jilles Vreeken","Jonas Fischer"],"pdf_url":"https://arxiv.org/pdf/2503.07346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23574v1","updated":"2025-10-27T17:44:56Z","published":"2025-10-27T17:44:56Z","title":"More Than Generation: Unifying Generation and Depth Estimation via\n  Text-to-Image Diffusion Models","summary":"  Generative depth estimation methods leverage the rich visual priors stored in\npre-trained text-to-image diffusion models, demonstrating astonishing zero-shot\ncapability. However, parameter updates during training lead to catastrophic\ndegra- dation in the image generation capability of the pre-trained model. We\nintroduce MERGE, a unified model for image generation and depth estimation,\nstarting from a fixed pre-trained text-to-image model. MERGE demonstrates that\nthe pre-trained text-to-image model can do more than image generation, but also\nexpand to depth estimation effortlessly. Specifically, MERGE introduces a play-\nand-plug framework that enables seamless switching between image generation and\ndepth estimation modes through simple and pluggable converters. Meanwhile, we\npropose a Group Reuse Mechanism to encourage parameter reuse and im- prove the\nutilization of the additional learnable parameters. MERGE unleashes the\npowerful depth estimation capability of the pre-trained text-to-image model\nwhile preserving its original image generation ability. Compared to other\nunified models for image generation and depth estimation, MERGE achieves\nstate-of- the-art performance across multiple depth estimation benchmarks. The\ncode will be made available at https://github.com/H-EmbodVis/MERGE\n","authors":["Hongkai Lin","Dingkang Liang","Mingyang Du","Xin Zhou","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2510.23574v1.pdf","comment":"Accepted by NeurIPS 2025. The code will be made available at\n  https://github.com/H-EmbodVis/MERGE"},{"id":"http://arxiv.org/abs/2510.23571v1","updated":"2025-10-27T17:41:38Z","published":"2025-10-27T17:41:38Z","title":"RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim\n  Translation","summary":"  The pursuit of robot generalists - instructable agents capable of performing\ndiverse tasks across diverse environments - demands rigorous and scalable\nevaluation. Yet real-world testing of robot policies remains fundamentally\nconstrained: it is labor-intensive, slow, unsafe at scale, and difficult to\nreproduce. Existing simulation benchmarks are similarly limited, as they train\nand test policies within the same synthetic domains and cannot assess models\ntrained from real-world demonstrations or alternative simulation environments.\nAs policies expand in scope and complexity, these barriers only intensify,\nsince defining \"success\" in robotics often hinges on nuanced human judgments of\nexecution quality. In this paper, we introduce a new benchmarking framework\nthat overcomes these challenges by shifting VLA evaluation into large-scale\nsimulated environments augmented with online human feedback. Leveraging\nadvances in vision-language models, 2D-to-3D generative modeling, and\ndifferentiable rendering, our approach automatically converts video\ndemonstrations from widely used robot datasets into simulated counterparts.\nWithin these digital twins, we assess VLA policies using both automated\nVLM-guided scoring and scalable human preference judgments collected from\ncrowdworkers, transforming human involvement from tedious scene setup,\nresetting, and safety supervision into lightweight preference comparisons. To\nmeasure robustness, we systematically perturb simulated environments along\nmultiple axes, such as textures and object placements, stress-testing policy\ngeneralization under controlled variation. The result is a continuously\nevolving, reproducible, and scalable benchmark for real-world trained robot\nmanipulation policies, addressing a critical missing capability in today's\nrobotics landscape.\n","authors":["Yash Jangir","Yidi Zhang","Kashu Yamazaki","Chenyu Zhang","Kuan-Hsun Tu","Tsung-Wei Ke","Lei Ke","Yonatan Bisk","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2510.23571v1.pdf","comment":"Website: https://robotarenainf.github.io"},{"id":"http://arxiv.org/abs/2510.23569v1","updated":"2025-10-27T17:38:17Z","published":"2025-10-27T17:38:17Z","title":"EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT","summary":"  Egocentric video reasoning centers on an unobservable agent behind the camera\nwho dynamically shapes the environment, requiring inference of hidden\nintentions and recognition of fine-grained interactions. This core challenge\nlimits current multimodal large language models MLLMs, which excel at visible\nevent reasoning but lack embodied, first-person understanding. To bridge this\ngap, we introduce EgoThinker, a novel framework that endows MLLMs with robust\negocentric reasoning capabilities through spatio-temporal chain-of-thought\nsupervision and a two-stage learning curriculum. First, we introduce EgoRe-5M,\na large-scale egocentric QA dataset constructed from 13M diverse egocentric\nvideo clips. This dataset features multi-minute segments annotated with\ndetailed CoT rationales and dense hand-object grounding. Second, we employ SFT\non EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning\nRFT to further enhance spatio-temporal localization. Experimental results show\nthat EgoThinker outperforms existing methods across multiple egocentric\nbenchmarks, while achieving substantial improvements in fine-grained\nspatio-temporal localization tasks. Full code and data are released at\nhttps://github.com/InternRobotics/EgoThinker.\n","authors":["Baoqi Pei","Yifei Huang","Jilan Xu","Yuping He","Guo Chen","Fei Wu","Yu Qiao","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2510.23569v1.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23561v1","updated":"2025-10-27T17:32:08Z","published":"2025-10-27T17:32:08Z","title":"Revising Second Order Terms in Deep Animation Video Coding","summary":"  First Order Motion Model is a generative model that animates human heads\nbased on very little motion information derived from keypoints. It is a\npromising solution for video communication because first it operates at very\nlow bitrate and second its computational complexity is moderate compared to\nother learning based video codecs. However, it has strong limitations by\ndesign. Since it generates facial animations by warping source-images, it fails\nto recreate videos with strong head movements. This works concentrates on one\nspecific kind of head movements, namely head rotations. We show that replacing\nthe Jacobian transformations in FOMM by a global rotation helps the system to\nperform better on items with head-rotations while saving 40% to 80% of bitrate\non P-frames. Moreover, we apply state-of-the-art normalization techniques to\nthe discriminator to stabilize the adversarial training which is essential for\ngenerating visually appealing videos. We evaluate the performance by the\nlearned metics LPIPS and DISTS to show the success our optimizations.\n","authors":["Konstantin Schmidt","Thomas Richter"],"pdf_url":"https://arxiv.org/pdf/2510.23561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16503v2","updated":"2025-10-27T17:31:22Z","published":"2024-11-25T15:40:47Z","title":"Noise Diffusion for Enhancing Semantic Faithfulness in Text-to-Image\n  Synthesis","summary":"  Diffusion models have achieved impressive success in generating\nphotorealistic images, but challenges remain in ensuring precise semantic\nalignment with input prompts. Optimizing the initial noisy latent offers a more\nefficient alternative to modifying model architectures or prompt engineering\nfor improving semantic alignment. A latest approach, InitNo, refines the\ninitial noisy latent by leveraging attention maps; however, these maps capture\nonly limited information, and the effectiveness of InitNo is highly dependent\non the initial starting point, as it tends to converge on a local optimum near\nthis point. To this end, this paper proposes leveraging the language\ncomprehension capabilities of large vision-language models (LVLMs) to guide the\noptimization of the initial noisy latent, and introduces the Noise Diffusion\nprocess, which updates the noisy latent to generate semantically faithful\nimages while preserving distribution consistency. Furthermore, we provide a\ntheoretical analysis of the condition under which the update improves semantic\nfaithfulness. Experimental results demonstrate the effectiveness and\nadaptability of our framework, consistently enhancing semantic alignment across\nvarious diffusion models. The code is available at\nhttps://github.com/Bomingmiao/NoiseDiffusion.\n","authors":["Boming Miao","Chunxiao Li","Xiaoxiao Wang","Andi Zhang","Rui Sun","Zizhe Wang","Yao Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.16503v2.pdf","comment":"Updated author formatting; no substantive changes"},{"id":"http://arxiv.org/abs/2510.23554v1","updated":"2025-10-27T17:28:55Z","published":"2025-10-27T17:28:55Z","title":"A U-Net and Transformer Pipeline for Multilingual Image Translation","summary":"  This paper presents an end-to-end multilingual translation pipeline that\nintegrates a custom U-Net for text detection, the Tesseract engine for text\nrecognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for\nNeural Machine Translation (NMT). Our approach first utilizes a U-Net model,\ntrained on a synthetic dataset , to accurately segment and detect text regions\nfrom an image. These detected regions are then processed by Tesseract to\nextract the source text. This extracted text is fed into a custom Transformer\nmodel trained from scratch on a multilingual parallel corpus spanning 5\nlanguages. Unlike systems reliant on monolithic pre-trained models, our\narchitecture emphasizes full customization and adaptability. The system is\nevaluated on its text detection accuracy, text recognition quality, and\ntranslation performance via BLEU scores. The complete pipeline demonstrates\npromising results, validating the viability of a custom-built system for\ntranslating text directly from images.\n","authors":["Siddharth Sahay","Radhika Agarwal"],"pdf_url":"https://arxiv.org/pdf/2510.23554v1.pdf","comment":"6 pages, 3 figures, 5 tables, and 2 algorithms. Prepared in IEEE\n  double-column format"},{"id":"http://arxiv.org/abs/2510.23538v1","updated":"2025-10-27T17:13:49Z","published":"2025-10-27T17:13:49Z","title":"JanusCoder: Towards a Foundational Visual-Programmatic Interface for\n  Code Intelligence","summary":"  The scope of neural code intelligence is rapidly expanding beyond text-based\nsource code to encompass the rich visual outputs that programs generate. This\nvisual dimension is critical for advanced applications like flexible content\ngeneration and precise, program-driven editing of visualizations. However,\nprogress has been impeded by the scarcity of high-quality multimodal code data,\na bottleneck stemming from challenges in synthesis and quality assessment. To\naddress these challenges, we make contributions from both a data and modeling\nperspective. We first introduce a complete synthesis toolkit that leverages\nreciprocal synergies between data modalities to efficiently produce a\nlarge-scale, high-quality corpus spanning from standard charts to complex\ninteractive web UIs and code-driven animations. Leveraging this toolkit, we\nconstruct JanusCode-800K, the largest multimodal code corpus to date. This\npowers the training of our models, JanusCoder and JanusCoderV, which establish\na visual-programmatic interface for generating code from textual instructions,\nvisual inputs, or a combination of both. Our unified model is a departure from\nexisting approaches that build specialized models for isolated tasks. Extensive\nexperiments on both text-centric and vision-centric coding tasks demonstrate\nthe superior performance of the JanusCoder series, with our 7B to 14B scale\nmodels approaching or even exceeding the performance of commercial models.\nFurthermore, extensive analysis provides key insights into harmonizing\nprogrammatic logic with its visual expression. Our code and checkpoints will\nare available at https://github.com/InternLM/JanusCoder.\n","authors":["Qiushi Sun","Jingyang Gong","Yang Liu","Qiaosheng Chen","Lei Li","Kai Chen","Qipeng Guo","Ben Kao","Fei Yuan"],"pdf_url":"https://arxiv.org/pdf/2510.23538v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2510.23525v1","updated":"2025-10-27T17:05:59Z","published":"2025-10-27T17:05:59Z","title":"DPGLA: Bridging the Gap between Synthetic and Real Data for Unsupervised\n  Domain Adaptation in 3D LiDAR Semantic Segmentation","summary":"  Annotating real-world LiDAR point clouds for use in intelligent autonomous\nsystems is costly. To overcome this limitation, self-training-based\nUnsupervised Domain Adaptation (UDA) has been widely used to improve point\ncloud semantic segmentation by leveraging synthetic point cloud data. However,\nwe argue that existing methods do not effectively utilize unlabeled data, as\nthey either rely on predefined or fixed confidence thresholds, resulting in\nsuboptimal performance. In this paper, we propose a Dynamic Pseudo-Label\nFiltering (DPLF) scheme to enhance real data utilization in point cloud UDA\nsemantic segmentation. Additionally, we design a simple and efficient\nPrior-Guided Data Augmentation Pipeline (PG-DAP) to mitigate domain shift\nbetween synthetic and real-world point clouds. Finally, we utilize data mixing\nconsistency loss to push the model to learn context-free representations. We\nimplement and thoroughly evaluate our approach through extensive comparisons\nwith state-of-the-art methods. Experiments on two challenging synthetic-to-real\npoint cloud semantic segmentation tasks demonstrate that our approach achieves\nsuperior performance. Ablation studies confirm the effectiveness of the DPLF\nand PG-DAP modules. We release the code of our method in this paper.\n","authors":["Wanmeng Li","Simone Mosco","Daniel Fusaro","Alberto Pretto"],"pdf_url":"https://arxiv.org/pdf/2510.23525v1.pdf","comment":"This paper has been accepted for publication at the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2501.08458v3","updated":"2025-10-27T16:55:19Z","published":"2025-01-14T22:03:00Z","title":"RWKV-UNet: Improving UNet with Long-Range Cooperation for Effective\n  Medical Image Segmentation","summary":"  In recent years, significant advancements have been made in deep learning for\nmedical image segmentation, particularly with convolutional neural networks\n(CNNs) and transformer models. However, CNNs face limitations in capturing\nlong-range dependencies, while transformers suffer from high computational\ncomplexity. To address this, we propose RWKV-UNet, a novel model that\nintegrates the RWKV (Receptance Weighted Key Value) structure into the U-Net\narchitecture. This integration enhances the model's ability to capture\nlong-range dependencies and to improve contextual understanding, which is\ncrucial for accurate medical image segmentation. We build a strong encoder with\ndeveloped Global-Local Spatial Perception (GLSP) blocks combining CNNs and\nRWKVs. We also propose a Cross-Channel Mix (CCM) module to improve skip\nconnections with multi-scale feature fusion, achieving global channel\ninformation integration. Experiments on 11 benchmark datasets show that the\nRWKV-UNet achieves state-of-the-art performance on various types of medical\nimage segmentation tasks. Additionally, smaller variants, RWKV-UNet-S and\nRWKV-UNet-T, balance accuracy and computational efficiency, making them\nsuitable for broader clinical applications.\n","authors":["Juntao Jiang","Jiangning Zhang","Weixuan Liu","Muxuan Gao","Xiaobin Hu","Zhucun Xue","Yong Liu","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2501.08458v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23515v1","updated":"2025-10-27T16:54:08Z","published":"2025-10-27T16:54:08Z","title":"FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time","summary":"  This paper proposes FreeFuse, a novel training-free approach for\nmulti-subject text-to-image generation through automatic fusion of multiple\nsubject LoRAs. In contrast to existing methods that either focus on\npre-inference LoRA weight merging or rely on segmentation models and complex\ntechniques like noise blending to isolate LoRA outputs, our key insight is that\ncontext-aware dynamic subject masks can be automatically derived from\ncross-attention layer weights. Mathematical analysis shows that directly\napplying these masks to LoRA outputs during inference well approximates the\ncase where the subject LoRA is integrated into the diffusion model and used\nindividually for the masked region. FreeFuse demonstrates superior practicality\nand efficiency as it requires no additional training, no modification to LoRAs,\nno auxiliary models, and no user-defined prompt templates or region\nspecifications. Alternatively, it only requires users to provide the LoRA\nactivation words for seamless integration into standard workflows. Extensive\nexperiments validate that FreeFuse outperforms existing approaches in both\ngeneration quality and usability under the multi-subject generation tasks. The\nproject page is at https://future-item.github.io/FreeFuse/\n","authors":["Yaoli Liu","Yao-Xiang Ding","Kun Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.23515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23512v1","updated":"2025-10-27T16:50:12Z","published":"2025-10-27T16:50:12Z","title":"Localising under the drape: proprioception in the era of distributed\n  surgical robotic system","summary":"  Despite their mechanical sophistication, surgical robots remain blind to\ntheir surroundings. This lack of spatial awareness causes collisions, system\nrecoveries, and workflow disruptions, issues that will intensify with the\nintroduction of distributed robots with independent interacting arms. Existing\ntracking systems rely on bulky infrared cameras and reflective markers,\nproviding only limited views of the surgical scene and adding hardware burden\nin crowded operating rooms. We present a marker-free proprioception method that\nenables precise localisation of surgical robots under their sterile draping\ndespite associated obstruction of visual cues. Our method solely relies on\nlightweight stereo-RGB cameras and novel transformer-based deep learning\nmodels. It builds on the largest multi-centre spatial robotic surgery dataset\nto date (1.4M self-annotated images from human cadaveric and preclinical in\nvivo studies). By tracking the entire robot and surgical scene, rather than\nindividual markers, our approach provides a holistic view robust to occlusions,\nsupporting surgical scene understanding and context-aware control. We\ndemonstrate an example of potential clinical benefits during in vivo breathing\ncompensation with access to tissue dynamics, unobservable under state of the\nart tracking, and accurately locate in multi-robot systems for future\nintelligent interaction. In addition, and compared with existing systems, our\nmethod eliminates markers and improves tracking visibility by 25%. To our\nknowledge, this is the first demonstration of marker-free proprioception for\nfully draped surgical robots, reducing setup complexity, enhancing safety, and\npaving the way toward modular and autonomous robotic surgery.\n","authors":["Martin Huber","Nicola A. Cavalcanti","Ayoob Davoodi","Ruixuan Li","Christopher E. Mower","Fabio Carrillo","Christoph J. Laux","Francois Teyssere","Thibault Chandanson","Antoine Harlé","Elie Saghbiny","Mazda Farshad","Guillaume Morel","Emmanuel Vander Poorten","Philipp Fürnstahl","Sébastien Ourselin","Christos Bergeles","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2510.23512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.06170v2","updated":"2025-10-27T16:44:15Z","published":"2025-10-07T17:33:41Z","title":"Smartphone-based iris recognition through high-quality visible-spectrum\n  iris image capture.V2","summary":"  Smartphone-based iris recognition in the visible spectrum (VIS) remains\ndifficult due to illumination variability, pigmentation differences, and the\nabsence of standardized capture controls. This work presents a compact\nend-to-end pipeline that enforces ISO/IEC 29794-6 quality compliance at\nacquisition and demonstrates that accurate VIS iris recognition is feasible on\ncommodity devices. Using a custom Android application performing real-time\nframing, sharpness evaluation, and feedback, we introduce the CUVIRIS dataset\nof 752 compliant images from 47 subjects. A lightweight MobileNetV3-based\nmulti-task segmentation network (LightIrisNet) is developed for efficient\non-device processing, and a transformer matcher (IrisFormer) is adapted to the\nVIS domain. Under a standardized protocol and comparative benchmarking against\nprior CNN baselines, OSIRIS attains a TAR of 97.9% at FAR=0.01 (EER=0.76%),\nwhile IrisFormer, trained only on UBIRIS.v2, achieves an EER of 0.057% on\nCUVIRIS. The acquisition app, trained models, and a public subset of the\ndataset are released to support reproducibility. These results confirm that\nstandardized capture and VIS-adapted lightweight models enable accurate and\npractical iris recognition on smartphones.\n","authors":["Naveenkumar G Venkataswamy","Yu Liu","Soumyabrata Dey","Stephanie Schuckers","Masudul H Imtiaz"],"pdf_url":"https://arxiv.org/pdf/2510.06170v2.pdf","comment":"This submission has been withdrawn because it duplicates significant\n  content from another version of the paper already available on arXiv as\n  arXiv:2412.13063"},{"id":"http://arxiv.org/abs/2506.07555v3","updated":"2025-10-27T16:44:01Z","published":"2025-06-09T08:48:06Z","title":"Synthesize Privacy-Preserving High-Resolution Images via Private Textual\n  Intermediaries","summary":"  Generating high fidelity, differentially private (DP) synthetic images offers\na promising route to share and analyze sensitive visual data without\ncompromising individual privacy. However, existing DP image synthesis methods\nstruggle to produce high resolution outputs that faithfully capture the\nstructure of the original data. In this paper, we introduce a novel method,\nreferred to as Synthesis via Private Textual Intermediaries (SPTI), that can\ngenerate high resolution DP images with easy adoption. The key idea is to shift\nthe challenge of DP image synthesis from the image domain to the text domain by\nleveraging state of the art DP text generation methods. SPTI first summarizes\neach private image into a concise textual description using image to text\nmodels, then applies a modified Private Evolution algorithm to generate DP\ntext, and finally reconstructs images using text to image models. Notably, SPTI\nrequires no model training, only inference with off the shelf models. Given a\nprivate dataset, SPTI produces synthetic images of substantially higher quality\nthan prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID\nequal to 26.71 under epsilon equal to 1.0, improving over Private Evolution FID\nof 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID equal to 33.27 at\nepsilon equal to 1.0, compared to 57.01 from DP fine tuning baselines. Overall,\nour results demonstrate that Synthesis via Private Textual Intermediaries\nprovides a resource efficient and proprietary model compatible framework for\ngenerating high resolution DP synthetic images, greatly expanding access to\nprivate visual datasets.\n","authors":["Haoxiang Wang","Zinan Lin","Da Yu","Huishuai Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.07555v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.05913v2","updated":"2025-10-27T16:42:00Z","published":"2025-09-07T04:09:06Z","title":"A Fine-Grained Attention and Geometric Correspondence Model for\n  Musculoskeletal Risk Classification in Athletes Using Multimodal Visual and\n  Skeletal Features","summary":"  Musculoskeletal disorders pose significant risks to athletes, and assessing\nrisk early is important for prevention. However, most existing methods are\ndesigned for controlled settings and fail to reliably assess risk in complex\nenvironments due to their reliance on a single type of data. This research\nintroduces ViSK-GAT (Visual-Skeletal Geometric Attention Transformer), a novel\nmultimodal deep learning framework that classifies musculoskeletal risk using\nboth visual and skeletal coordinate-based features. A custom multimodal dataset\n(MusDis-Sports) was created by combining images and skeletal coordinates, with\neach sample labeled into eight risk categories based on the Rapid Entire Body\nAssessment (REBA) system. ViSK-GAT integrates two innovative modules: the\nFine-Grained Attention Module (FGAM), which refines inter-modal features via\ncross-attention between visual and skeletal inputs, and the Multimodal\nGeometric Correspondence Module (MGCM), which enhances cross-modal alignment\nbetween image features and coordinates. The model achieved robust performance,\nwith all key metrics exceeding 93%. Regression results also indicated a low\nRMSE of 0.1205 and MAE of 0.0156. ViSK-GAT consistently outperformed nine\npopular transfer learning backbones and showed its potential to advance\nAI-driven musculoskeletal risk assessment and enable early, impactful\ninterventions in sports.\n","authors":["Md. Abdur Rahman","Mohaimenul Azam Khan Raiaan","Tamanna Shermin","Md Rafiqul Islam","Mukhtar Hussain","Sami Azam"],"pdf_url":"https://arxiv.org/pdf/2509.05913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21264v2","updated":"2025-10-27T16:38:35Z","published":"2025-10-24T08:51:48Z","title":"Topology Sculptor, Shape Refiner: Discrete Diffusion Model for\n  High-Fidelity 3D Meshes Generation","summary":"  In this paper, we introduce Topology Sculptor, Shape Refiner (TSSR), a novel\nmethod for generating high-quality, artist-style 3D meshes based on Discrete\nDiffusion Models (DDMs). Our primary motivation for TSSR is to achieve highly\naccurate token prediction while enabling parallel generation, a significant\nadvantage over sequential autoregressive methods. By allowing TSSR to \"see\" all\nmesh tokens concurrently, we unlock a new level of efficiency and control. We\nleverage this parallel generation capability through three key innovations: 1)\nDecoupled Training and Hybrid Inference, which distinctly separates the\nDDM-based generation into a topology sculpting stage and a subsequent shape\nrefinement stage. This strategic decoupling enables TSSR to effectively capture\nboth intricate local topology and overarching global shape. 2) An Improved\nHourglass Architecture, featuring bidirectional attention enriched by\nface-vertex-sequence level Rotational Positional Embeddings (RoPE), thereby\ncapturing richer contextual information across the mesh structure. 3) A novel\nConnection Loss, which acts as a topological constraint to further enhance the\nrealism and fidelity of the generated meshes. Extensive experiments on complex\ndatasets demonstrate that TSSR generates high-quality 3D artist-style meshes,\ncapable of achieving up to 10,000 faces at a remarkable spatial resolution of\n$1024^3$. The code will be released at:\nhttps://github.com/psky1111/Tencent-TSSR.\n","authors":["Kaiyu Song","Hanjiang Lai","Yaqing Zhang","Chuangjian Cai","Yan Pan Kun Yue","Jian Yin"],"pdf_url":"https://arxiv.org/pdf/2510.21264v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23504v1","updated":"2025-10-27T16:37:16Z","published":"2025-10-27T16:37:16Z","title":"iPac: Incorporating Intra-image Patch Context into Graph Neural Networks\n  for Medical Image Classification","summary":"  Graph neural networks have emerged as a promising paradigm for image\nprocessing, yet their performance in image classification tasks is hindered by\na limited consideration of the underlying structure and relationships among\nvisual entities. This work presents iPac, a novel approach to introduce a new\ngraph representation of images to enhance graph neural network image\nclassification by recognizing the importance of underlying structure and\nrelationships in medical image classification. iPac integrates various stages,\nincluding patch partitioning, feature extraction, clustering, graph\nconstruction, and graph-based learning, into a unified network to advance graph\nneural network image classification. By capturing relevant features and\norganising them into clusters, we construct a meaningful graph representation\nthat effectively encapsulates the semantics of the image. Experimental\nevaluation on diverse medical image datasets demonstrates the efficacy of iPac,\nexhibiting an average accuracy improvement of up to 5% over baseline methods.\nOur approach offers a versatile and generic solution for image classification,\nparticularly in the realm of medical images, by leveraging the graph\nrepresentation and accounting for the inherent structure and relationships\namong visual entities.\n","authors":["Usama Zidan","Mohamed Gaber","Mohammed M. Abdelsamea"],"pdf_url":"https://arxiv.org/pdf/2510.23504v1.pdf","comment":"Accepted for publication in the proceedings of ICONIP 2025"},{"id":"http://arxiv.org/abs/2510.23497v1","updated":"2025-10-27T16:32:12Z","published":"2025-10-27T16:32:12Z","title":"VOLD: Reasoning Transfer from LLMs to Vision-Language Models via\n  On-Policy Distillation","summary":"  Training vision-language models (VLMs) for complex reasoning remains a\nchallenging task, i.a. due to the scarcity of high-quality image-text reasoning\ndata. Conversely, text-based reasoning resources are abundant and scalable, but\nit is still an open question how to leveraging them for VLM reasoning. To\naddress this problem, we propose VOLD, a framework to transfer reasoning\ncapabilities from text-only teacher models to VLM student models. To this end,\nVOLD combines reinforcement learning via Group Relative Policy Optimization\n(GRPO) with on-policy distillation, which allows the student reasoning traces\nto be guided by the teacher model, resulting in a significant gain over using\nGRPO alone. We further show that a cold-start alignment is essential for an\neffective transfer during the online training phase in this scenario and that\nwithout sufficient distributional alignment between teacher and student,\non-policy distillation fails to provide meaningful guidance. We evaluate VOLD\nacross diverse benchmarks including MMMU-Pro, MathVision, MathVista, and\nLogicVista, showing that VOLD outperforms the baseline model significantly and\nimproves over the state of the art by a margin. Our ablation shows the\nimportance of a cold-start alignment via SFT for on-policy distillation with a\ntext-only teacher.\n","authors":["Walid Bousselham","Hilde Kuehne","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2510.23497v1.pdf","comment":"www.walidbousselham.com/VOLD/"},{"id":"http://arxiv.org/abs/2510.23494v1","updated":"2025-10-27T16:28:55Z","published":"2025-10-27T16:28:55Z","title":"Yesnt: Are Diffusion Relighting Models Ready for Capture Stage\n  Compositing? A Hybrid Alternative to Bridge the Gap","summary":"  Volumetric video relighting is essential for bringing captured performances\ninto virtual worlds, but current approaches struggle to deliver temporally\nstable, production-ready results. Diffusion-based intrinsic decomposition\nmethods show promise for single frames, yet suffer from stochastic noise and\ninstability when extended to sequences, while video diffusion models remain\nconstrained by memory and scale. We propose a hybrid relighting framework that\ncombines diffusion-derived material priors with temporal regularization and\nphysically motivated rendering. Our method aggregates multiple stochastic\nestimates of per-frame material properties into temporally consistent shading\ncomponents, using optical-flow-guided regularization. For indirect effects such\nas shadows and reflections, we extract a mesh proxy from Gaussian Opacity\nFields and render it within a standard graphics pipeline. Experiments on real\nand synthetic captures show that this hybrid strategy achieves substantially\nmore stable relighting across sequences than diffusion-only baselines, while\nscaling beyond the clip lengths feasible for video diffusion. These results\nindicate that hybrid approaches, which balance learned priors with physically\ngrounded constraints, are a practical step toward production-ready volumetric\nvideo relighting.\n","authors":["Elisabeth Jüttner","Leona Krath","Stefan Korfhage","Hannah Dröge","Matthias B. Hullin","Markus Plack"],"pdf_url":"https://arxiv.org/pdf/2510.23494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23484v1","updated":"2025-10-27T16:16:40Z","published":"2025-10-27T16:16:40Z","title":"T-REGS: Minimum Spanning Tree Regularization for Self-Supervised\n  Learning","summary":"  Self-supervised learning (SSL) has emerged as a powerful paradigm for\nlearning representations without labeled data, often by enforcing invariance to\ninput transformations such as rotations or blurring. Recent studies have\nhighlighted two pivotal properties for effective representations: (i) avoiding\ndimensional collapse-where the learned features occupy only a low-dimensional\nsubspace, and (ii) enhancing uniformity of the induced distribution. In this\nwork, we introduce T-REGS, a simple regularization framework for SSL based on\nthe length of the Minimum Spanning Tree (MST) over the learned representation.\nWe provide theoretical analysis demonstrating that T-REGS simultaneously\nmitigates dimensional collapse and promotes distribution uniformity on\narbitrary compact Riemannian manifolds. Several experiments on synthetic data\nand on classical SSL benchmarks validate the effectiveness of our approach at\nenhancing representation quality.\n","authors":["Julie Mordacq","David Loiseaux","Vicky Kalogeiton","Steve Oudot"],"pdf_url":"https://arxiv.org/pdf/2510.23484v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23482v1","updated":"2025-10-27T16:15:54Z","published":"2025-10-27T16:15:54Z","title":"On the Faithfulness of Visual Thinking: Measurement and Enhancement","summary":"  Recent large vision-language models (LVLMs) can generate vision-text\nmultimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning\n(RFT). However, we observe that the visual information incorporated in MCoT is\noften inaccurate, though still yield correct answers, indicating a lack of\nfaithfulness in the MCoT reasoning process. We attribute this unfaithfulness to\nthe RL reward in RFT, which solely incentivizes the format of interleaved\nvision-text cues, ie, it encourages the model to incorporate visual information\ninto its text reasoning steps without considering the correctness of the visual\ninformation. In this paper, we first probe the faithfulness of MCoT by\nmeasuring how much the prediction changes when its visual and textual thoughts\nare intervened. Surprisingly, the model's predictions remain nearly unchanged\nunder visual intervention but change significantly under textual intervention,\nindicating that the visual evidence is largely ignored. To further analyze\nvisual information, we introduce an automated LVLM-based evaluation metric that\nquantifies the faithfulness of visual cues from two perspectives: reliability\nand sufficiency. Our evaluation reveals that the visual information in current\nMCoT traces is simultaneously unreliable and insufficient. To address this\nissue, we propose a novel MCoT learning strategy termed Sufficient-Component\nCause Model (SCCM) learning. This approach encourages the MCoT to generate\nsufficient yet minimal visual components that are independently capable of\nleading to correct answers. We note that the proposed SCCM is annotation-free\nand compatible with various RFT for MCoT in a plug-and-play manner. Empirical\nresults demonstrate that SCCM consistently improves the visual faithfulness\nacross a suite of fine-grained perception and reasoning benchmarks. Code is\navailable at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image.\n","authors":["Zujing Liu","Junwen Pan","Qi She","Yuan Gao","Guisong Xia"],"pdf_url":"https://arxiv.org/pdf/2510.23482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23479v1","updated":"2025-10-27T16:12:40Z","published":"2025-10-27T16:12:40Z","title":"MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal\n  Understanding","summary":"  Vision-language alignment in multi-modal large language models (MLLMs)\ntypically relies on supervised fine-tuning (SFT) or reinforcement learning\n(RL). SFT is stable and efficient but requires large-scale human annotations\nand cannot capture subtle preferences, while RL brings in a reward signal for\ntraining, but suffers from overhead and instability. These limitations\nhighlight a trade-off between scalability, robustness, and alignment quality.\nTo address this, we propose MergeMix, a training-time augmentation paradigm\nthat bridges SFT and RL. It first applies an attention-aware image mixing via\ntoken merge with more cluster representation and spatial context, and then\npresents a preference-driven training paradigm for MLLMs by building preference\npairs with mixed images and raw images, and optimizing via SimPO loss. As a\nmixup augmentation, MergeMix enhances attention consistency and efficiency,\nsurpassing other heuristic-based methods in classification. Extensive\nexperiments demonstrate that MergeMix achieves competitive accuracy with\nimproved efficiency, providing a scalable approach to preference alignment in\nclassification and MLLMs.\n","authors":["Xin Jin","Siyuan Li","Siyong Jian","Kai Yu","Huan Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23479v1.pdf","comment":"Code Link: https://github.com/JinXins/MergeMix"},{"id":"http://arxiv.org/abs/2510.23478v1","updated":"2025-10-27T16:12:12Z","published":"2025-10-27T16:12:12Z","title":"UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset\n  Across Multiple Intersections for Cooperative Perception","summary":"  Recent cooperative perception datasets have played a crucial role in\nadvancing smart mobility applications by enabling information exchange between\nintelligent agents, helping to overcome challenges such as occlusions and\nimproving overall scene understanding. While some existing real-world datasets\nincorporate both vehicle-to-vehicle and vehicle-to-infrastructure interactions,\nthey are typically limited to a single intersection or a single vehicle. A\ncomprehensive perception dataset featuring multiple connected vehicles and\ninfrastructure sensors across several intersections remains unavailable,\nlimiting the benchmarking of algorithms in diverse traffic environments.\nConsequently, overfitting can occur, and models may demonstrate misleadingly\nhigh performance due to similar intersection layouts and traffic participant\nbehavior. To address this gap, we introduce UrbanIng-V2X, the first\nlarge-scale, multi-modal dataset supporting cooperative perception involving\nvehicles and infrastructure sensors deployed across three urban intersections\nin Ingolstadt, Germany. UrbanIng-V2X consists of 34 temporally aligned and\nspatially calibrated sensor sequences, each lasting 20 seconds. All sequences\ncontain recordings from one of three intersections, involving two vehicles and\nup to three infrastructure-mounted sensor poles operating in coordinated\nscenarios. In total, UrbanIng-V2X provides data from 12 vehicle-mounted RGB\ncameras, 2 vehicle LiDARs, 17 infrastructure thermal cameras, and 12\ninfrastructure LiDARs. All sequences are annotated at a frequency of 10 Hz with\n3D bounding boxes spanning 13 object classes, resulting in approximately 712k\nannotated instances across the dataset. We provide comprehensive evaluations\nusing state-of-the-art cooperative perception methods and publicly release the\ncodebase, dataset, HD map, and a digital twin of the complete data collection\nenvironment.\n","authors":["Karthikeyan Chandra Sekaran","Markus Geisler","Dominik Rößle","Adithya Mohan","Daniel Cremers","Wolfgang Utschick","Michael Botsch","Werner Huber","Torsten Schön"],"pdf_url":"https://arxiv.org/pdf/2510.23478v1.pdf","comment":"Accepted to NeurIPS 2025. Including supplemental material. For code\n  and dataset, see https://github.com/thi-ad/UrbanIng-V2X"},{"id":"http://arxiv.org/abs/2510.23473v1","updated":"2025-10-27T16:10:45Z","published":"2025-10-27T16:10:45Z","title":"Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement\n  Learning","summary":"  Recent advances in image reasoning methods, particularly \"Thinking with\nImages\", have demonstrated remarkable success in Multimodal Large Language\nModels (MLLMs); however, this dynamic reasoning paradigm has not yet been\nextended to video reasoning tasks. In this paper, we propose Video-Thinker,\nwhich empowers MLLMs to think with videos by autonomously leveraging their\nintrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues\nthroughout the inference process. To spark this capability, we construct\nVideo-Thinker-10K, a curated dataset featuring autonomous tool usage within\nchain-of-thought reasoning sequences. Our training strategy begins with\nSupervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group\nRelative Policy Optimization (GRPO) to strengthen this reasoning capability.\nThrough this approach, Video-Thinker enables MLLMs to autonomously navigate\ngrounding and captioning tasks for video reasoning, eliminating the need for\nconstructing and calling external tools. Extensive experiments demonstrate that\nVideo-Thinker achieves significant performance gains on both in-domain tasks\nand challenging out-of-domain video reasoning benchmarks, including\nVideo-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B\nsubstantially outperforms existing baselines such as Video-R1 and establishes\nstate-of-the-art performance among 7B-sized MLLMs.\n","authors":["Shijian Wang","Jiarui Jin","Xingjian Wang","Linxin Song","Runhao Fu","Hecheng Wang","Zongyuan Ge","Yuan Lu","Xuelian Cheng"],"pdf_url":"https://arxiv.org/pdf/2510.23473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23451v1","updated":"2025-10-27T15:53:20Z","published":"2025-10-27T15:53:20Z","title":"Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with\n  Free-Form Preferences","summary":"  Reward models (RMs) play a critical role in aligning AI behaviors with human\npreferences, yet they face two fundamental challenges: (1) Modality Imbalance,\nwhere most RMs are mainly focused on text and image modalities, offering\nlimited support for video, audio, and other modalities; and (2) Preference\nRigidity, where training on fixed binary preference pairs fails to capture the\ncomplexity and diversity of personalized preferences. To address the above\nchallenges, we propose Omni-Reward, a step toward generalist omni-modal reward\nmodeling with support for free-form preferences, consisting of: (1) Evaluation:\nWe introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form\npreferences, covering nine tasks across five modalities including text, image,\nvideo, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal\npreference dataset comprising 248K general preference pairs and 69K\ninstruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We\npropose Omni-RewardModel, which includes both discriminative and generative\nRMs, and achieves strong performance on Omni-RewardBench as well as other\nwidely used reward modeling benchmarks.\n","authors":["Zhuoran Jin","Hongbang Yuan","Kejian Zhu","Jiachun Li","Pengfei Cao","Yubo Chen","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.23451v1.pdf","comment":"48 pages, 17 figures"},{"id":"http://arxiv.org/abs/2510.23444v1","updated":"2025-10-27T15:46:07Z","published":"2025-10-27T15:46:07Z","title":"FRBNet: Revisiting Low-Light Vision through Frequency-Domain Radial\n  Basis Network","summary":"  Low-light vision remains a fundamental challenge in computer vision due to\nsevere illumination degradation, which significantly affects the performance of\ndownstream tasks such as detection and segmentation. While recent\nstate-of-the-art methods have improved performance through invariant feature\nlearning modules, they still fall short due to incomplete modeling of low-light\nconditions. Therefore, we revisit low-light image formation and extend the\nclassical Lambertian model to better characterize low-light conditions. By\nshifting our analysis to the frequency domain, we theoretically prove that the\nfrequency-domain channel ratio can be leveraged to extract\nillumination-invariant features via a structured filtering process. We then\npropose a novel and end-to-end trainable module named \\textbf{F}requency-domain\n\\textbf{R}adial \\textbf{B}asis \\textbf{Net}work (\\textbf{FRBNet}), which\nintegrates the frequency-domain channel ratio operation with a learnable\nfrequency domain filter for the overall illumination-invariant feature\nenhancement. As a plug-and-play module, FRBNet can be integrated into existing\nnetworks for low-light downstream tasks without modifying loss functions.\nExtensive experiments across various downstream tasks demonstrate that FRBNet\nachieves superior performance, including +2.2 mAP for dark object detection and\n+2.9 mIoU for nighttime segmentation. Code is available at:\nhttps://github.com/Sing-Forevet/FRBNet.\n","authors":["Fangtong Sun","Congyu Li","Ke Yang","Yuchen Pan","Hanwen Yu","Xichuan Zhang","Yiying Li"],"pdf_url":"https://arxiv.org/pdf/2510.23444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23442v1","updated":"2025-10-27T15:46:02Z","published":"2025-10-27T15:46:02Z","title":"CURVETE: Curriculum Learning and Progressive Self-supervised Training\n  for Medical Image Classification","summary":"  Identifying high-quality and easily accessible annotated samples poses a\nnotable challenge in medical image analysis. Transfer learning techniques,\nleveraging pre-training data, offer a flexible solution to this issue. However,\nthe impact of fine-tuning diminishes when the dataset exhibits an irregular\ndistribution between classes. This paper introduces a novel deep convolutional\nneural network, named Curriculum Learning and Progressive Self-supervised\nTraining (CURVETE). CURVETE addresses challenges related to limited samples,\nenhances model generalisability, and improves overall classification\nperformance. It achieves this by employing a curriculum learning strategy based\non the granularity of sample decomposition during the training of generic\nunlabelled samples. Moreover, CURVETE address the challenge of irregular class\ndistribution by incorporating a class decomposition approach in the downstream\ntask. The proposed method undergoes evaluation on three distinct medical image\ndatasets: brain tumour, digital knee x-ray, and Mini-DDSM datasets. We\ninvestigate the classification performance using a generic self-supervised\nsample decomposition approach with and without the curriculum learning\ncomponent in training the pretext task. Experimental results demonstrate that\nthe CURVETE model achieves superior performance on test sets with an accuracy\nof 96.60% on the brain tumour dataset, 75.60% on the digital knee x-ray\ndataset, and 93.35% on the Mini-DDSM dataset using the baseline ResNet-50.\nFurthermore, with the baseline DenseNet-121, it achieved accuracies of 95.77%,\n80.36%, and 93.22% on the brain tumour, digital knee x-ray, and Mini-DDSM\ndatasets, respectively, outperforming other training strategies.\n","authors":["Asmaa Abbas","Mohamed Gaber","Mohammed M. Abdelsamea"],"pdf_url":"https://arxiv.org/pdf/2510.23442v1.pdf","comment":"Accepted for publication in the proceedings of ICONIP 2025"},{"id":"http://arxiv.org/abs/2508.03201v3","updated":"2025-10-27T15:43:20Z","published":"2025-08-05T08:16:35Z","title":"AlignCAT: Visual-Linguistic Alignment of Category and Attribute for\n  Weakly Supervised Visual Grounding","summary":"  Weakly supervised visual grounding (VG) aims to locate objects in images\nbased on text descriptions. Despite significant progress, existing methods lack\nstrong cross-modal reasoning to distinguish subtle semantic differences in text\nexpressions due to category-based and attribute-based ambiguity. To address\nthese challenges, we introduce AlignCAT, a novel query-based semantic matching\nframework for weakly supervised VG. To enhance visual-linguistic alignment, we\npropose a coarse-grained alignment module that utilizes category information\nand global context, effectively mitigating interference from\ncategory-inconsistent objects. Subsequently, a fine-grained alignment module\nleverages descriptive information and captures word-level text features to\nachieve attribute consistency. By exploiting linguistic cues to their fullest\nextent, our proposed AlignCAT progressively filters out misaligned visual\nqueries and enhances contrastive learning efficiency. Extensive experiments on\nthree VG benchmarks, namely RefCOCO, RefCOCO+, and RefCOCOg, verify the\nsuperiority of AlignCAT against existing weakly supervised methods on two VG\ntasks. Our code is available at: https://github.com/I2-Multimedia-Lab/AlignCAT.\n","authors":["Yidan Wang","Chenyi Zhuang","Wutao Liu","Pan Gao","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2508.03201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13939v5","updated":"2025-10-27T15:33:53Z","published":"2025-03-18T06:12:38Z","title":"Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in\n  Vision-Language Models","summary":"  Vision-language models (VLMs) have achieved impressive progress in natural\nimage reasoning, yet their potential in medical imaging remains underexplored.\nMedical vision-language tasks demand precise understanding and clinically\ncoherent answers, which are difficult to achieve due to the complexity of\nmedical data and the scarcity of high-quality expert annotations. These\nchallenges limit the effectiveness of conventional supervised fine-tuning (SFT)\nand Chain-of-Thought (CoT) strategies that work well in general domains. To\naddress these challenges, we propose Med-R1, a reinforcement learning\n(RL)-enhanced vision-language model designed to improve generalization and\nreliability in medical reasoning. Built on the DeepSeek strategy, Med-R1 adopts\nGroup Relative Policy Optimization (GRPO) to encourage reward-guided learning\nbeyond static annotations. We comprehensively evaluate Med-R1 across eight\ndistinct medical imaging modalities. Med-R1 achieves a 29.94% improvement in\naverage accuracy over its base model Qwen2-VL-2B, and even outperforms\nQwen2-VL-72B-a model with 36x more parameters. To assess cross-task\ngeneralization, we further evaluate Med-R1 on five question types. Med-R1\noutperforms Qwen2-VL-2B by 32.06% in question-type generalization, also\nsurpassing Qwen2-VL-72B. We further explore the thinking process in Med-R1, a\ncrucial component for the success of Deepseek-R1. Our results show that\nomitting intermediate rationales (No-Thinking-Med-R1) not only improves\nin-domain and cross-domain generalization with less training, but also\nchallenges the assumption that more reasoning always helps. These findings\nsuggest that in medical VQA, it is not reasoning itself, but its quality and\ndomain alignment, that determine effectiveness. Together, these results\nhighlight that RL improves medical reasoning and generalization, enabling\nefficient and reliable VLMs for real-world deployment.\n","authors":["Yuxiang Lai","Jike Zhong","Ming Li","Shitian Zhao","Yuheng Li","Konstantinos Psounis","Xiaofeng Yang"],"pdf_url":"https://arxiv.org/pdf/2503.13939v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23429v1","updated":"2025-10-27T15:33:51Z","published":"2025-10-27T15:33:51Z","title":"MiCADangelo: Fine-Grained Reconstruction of Constrained CAD Models from\n  3D Scans","summary":"  Computer-Aided Design (CAD) plays a foundational role in modern manufacturing\nand product development, often requiring designers to modify or build upon\nexisting models. Converting 3D scans into parametric CAD representations--a\nprocess known as CAD reverse engineering--remains a significant challenge due\nto the high precision and structural complexity of CAD models. Existing deep\nlearning-based approaches typically fall into two categories: bottom-up,\ngeometry-driven methods, which often fail to produce fully parametric outputs,\nand top-down strategies, which tend to overlook fine-grained geometric details.\nMoreover, current methods neglect an essential aspect of CAD modeling:\nsketch-level constraints. In this work, we introduce a novel approach to CAD\nreverse engineering inspired by how human designers manually perform the task.\nOur method leverages multi-plane cross-sections to extract 2D patterns and\ncapture fine parametric details more effectively. It enables the reconstruction\nof detailed and editable CAD models, outperforming state-of-the-art methods\nand, for the first time, incorporating sketch constraints directly into the\nreconstruction process.\n","authors":["Ahmet Serdar Karadeniz","Dimitrios Mallis","Danila Rukhovich","Kseniya Cherenkova","Anis Kacem","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2510.23429v1.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.01320v3","updated":"2025-10-27T15:22:41Z","published":"2025-06-02T05:02:33Z","title":"Psi-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models","summary":"  We introduce $\\Psi$-Sampler, an SMC-based framework incorporating pCNL-based\ninitial particle sampling for effective inference-time reward alignment with a\nscore-based generative model. Inference-time reward alignment with score-based\ngenerative models has recently gained significant traction, following a broader\nparadigm shift from pre-training to post-training optimization. At the core of\nthis trend is the application of Sequential Monte Carlo (SMC) to the denoising\nprocess. However, existing methods typically initialize particles from the\nGaussian prior, which inadequately captures reward-relevant regions and results\nin reduced sampling efficiency. We demonstrate that initializing from the\nreward-aware posterior significantly improves alignment performance. To enable\nposterior sampling in high-dimensional latent spaces, we introduce the\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines\ndimension-robust proposals with gradient-informed dynamics. This approach\nenables efficient and scalable posterior sampling and consistently improves\nperformance across various reward alignment tasks, including layout-to-image\ngeneration, quantity-aware generation, and aesthetic-preference generation, as\ndemonstrated in our experiments. Project Webpage:\nhttps://psi-sampler.github.io/\n","authors":["Taehoon Yoon","Yunhong Min","Kyeongmin Yeo","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2506.01320v3.pdf","comment":"NeurIPS 2025, Spotlight Presentation"},{"id":"http://arxiv.org/abs/2510.23416v1","updated":"2025-10-27T15:21:39Z","published":"2025-10-27T15:21:39Z","title":"Quality-controlled registration of urban MLS point clouds reducing drift\n  effects by adaptive fragmentation","summary":"  This study presents a novel workflow designed to efficiently and accurately\nregister large-scale mobile laser scanning (MLS) point clouds to a target model\npoint cloud in urban street scenarios. This workflow specifically targets the\ncomplexities inherent in urban environments and adeptly addresses the\nchallenges of integrating point clouds that vary in density, noise\ncharacteristics, and occlusion scenarios, which are common in bustling city\ncenters. Two methodological advancements are introduced. First, the proposed\nSemi-sphere Check (SSC) preprocessing technique optimally fragments MLS\ntrajectory data by identifying mutually orthogonal planar surfaces. This step\nreduces the impact of MLS drift on the accuracy of the entire point cloud\nregistration, while ensuring sufficient geometric features within each fragment\nto avoid local minima. Second, we propose Planar Voxel-based Generalized\nIterative Closest Point (PV-GICP), a fine registration method that selectively\nutilizes planar surfaces within voxel partitions. This pre-process strategy not\nonly improves registration accuracy but also reduces computation time by more\nthan 50% compared to conventional point-to-plane ICP methods. Experiments on\nreal-world datasets from Munich's inner city demonstrate that our workflow\nachieves sub-0.01 m average registration accuracy while significantly\nshortening processing times. The results underscore the potential of the\nproposed methods to advance automated 3D urban modeling and updating, with\ndirect applications in urban planning, infrastructure management, and dynamic\ncity monitoring.\n","authors":["Marco Antonio Ortiz Rincon","Yihui Yang","Christoph Holst"],"pdf_url":"https://arxiv.org/pdf/2510.23416v1.pdf","comment":"10 pages, 7 figures. This manuscript is currently under review at the\n  International Journal of Applied Earth Observation and Geoinformation\n  (Elsevier). A preprint version will also be available on SSRN (Elsevier\n  Preprints) with a DOI once processed. This is the original preprint version\n  submitted for peer review"},{"id":"http://arxiv.org/abs/2510.23415v1","updated":"2025-10-27T15:19:46Z","published":"2025-10-27T15:19:46Z","title":"Towards Generalisable Foundation Models for 3D Brain MRI","summary":"  Foundation models in artificial intelligence (AI) are transforming medical\nimaging by enabling general-purpose feature learning from large-scale,\nunlabeled datasets. In this work, we introduce BrainFound, a self-supervised\nfoundation model for brain MRI, built by extending DINO-v2, a vision\ntransformer originally designed for 2D natural images. BrainFound adapts\nDINO-v2 to model full 3D brain anatomy by incorporating volumetric information\nfrom sequential MRI slices, moving beyond conventional single-slice paradigms.\nIt supports both single- and multimodal inputs, enabling a broad range of\ndownstream tasks, including disease detection and image segmentation, while\ngeneralising across varied imaging protocols and clinical scenarios. We show\nthat BrainFound consistently outperforms existing self-supervised pretraining\nstrategies and supervised baselines, particularly in label-scarce and\nmulti-contrast settings. By integrating information from diverse 3D MRI\nmodalities (e.g., T1, T2, FLAIR), it enhances diagnostic accuracy and reduces\ndependency on extensive expert annotations. This flexibility makes BrainFound a\nscalable and practical solution for 3D neuroimaging pipelines, with significant\npotential for clinical deployment and research innovation.\n","authors":["Moona Mazher","Geoff J. M. Parker","Daniel C. Alexander"],"pdf_url":"https://arxiv.org/pdf/2510.23415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23414v1","updated":"2025-10-27T15:18:26Z","published":"2025-10-27T15:18:26Z","title":"Symmetria: A Synthetic Dataset for Learning in Point Clouds","summary":"  Unlike image or text domains that benefit from an abundance of large-scale\ndatasets, point cloud learning techniques frequently encounter limitations due\nto the scarcity of extensive datasets. To overcome this limitation, we present\nSymmetria, a formula-driven dataset that can be generated at any arbitrary\nscale. By construction, it ensures the absolute availability of precise ground\ntruth, promotes data-efficient experimentation by requiring fewer samples,\nenables broad generalization across diverse geometric settings, and offers easy\nextensibility to new tasks and modalities. Using the concept of symmetry, we\ncreate shapes with known structure and high variability, enabling neural\nnetworks to learn point cloud features effectively. Our results demonstrate\nthat this dataset is highly effective for point cloud self-supervised\npre-training, yielding models with strong performance in downstream tasks such\nas classification and segmentation, which also show good few-shot learning\ncapabilities. Additionally, our dataset can support fine-tuning models to\nclassify real-world objects, highlighting our approach's practical utility and\napplication. We also introduce a challenging task for symmetry detection and\nprovide a benchmark for baseline comparisons. A significant advantage of our\napproach is the public availability of the dataset, the accompanying code, and\nthe ability to generate very large collections, promoting further research and\ninnovation in point cloud learning.\n","authors":["Ivan Sipiran","Gustavo Santelices","Lucas Oyarzún","Andrea Ranieri","Chiara Romanengo","Silvia Biasotti","Bianca Falcidieno"],"pdf_url":"https://arxiv.org/pdf/2510.23414v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2506.14265v2","updated":"2025-10-27T15:07:02Z","published":"2025-06-17T07:25:57Z","title":"Self-supervised Representation Learning with Local Aggregation for\n  Image-based Profiling","summary":"  Image-based cell profiling aims to create informative representations of cell\nimages. This technique is critical in drug discovery and has greatly advanced\nwith recent improvements in computer vision. Inspired by recent developments in\nnon-contrastive Self-Supervised Learning (SSL), this paper provides an initial\nexploration into training a generalizable feature extractor for cell images\nusing such methods. However, there are two major challenges: 1) Unlike typical\nscenarios where each representation is based on a single image, cell profiling\noften involves multiple input images, making it difficult to effectively fuse\nall available information; and 2) There is a large difference between the\ndistributions of cell images and natural images, causing the view-generation\nprocess in existing SSL methods to fail. To address these issues, we propose a\nself-supervised framework with local aggregation to improve cross-site\nconsistency of cell representations. We introduce specialized data augmentation\nand representation post-processing methods tailored to cell images, which\neffectively address the issues mentioned above and result in a robust feature\nextractor. With these improvements, the proposed framework won the Cell Line\nTransferability challenge at CVPR 2025.\n","authors":["Siran Dai","Qianqian Xu","Peisong Wen","Yang Liu","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2506.14265v2.pdf","comment":"CVPR 2025 Computer Vision for Drug Discovery"},{"id":"http://arxiv.org/abs/2510.23399v1","updated":"2025-10-27T14:57:14Z","published":"2025-10-27T14:57:14Z","title":"Color and Frequency Correction for Image Colorization","summary":"  The project has carried out the re-optimization of image coloring in\naccordance with the existing Autocolorization direction model DDColor. For the\nexperiments on the existing weights of DDColor, we found that it has\nlimitations in some frequency bands and the color cast problem caused by\ninsufficient input dimension. We construct two optimization schemes and combine\nthem, which achieves the performance improvement of indicators such as PSNR and\nSSIM of the images after DDColor.\n","authors":["Yun Kai Zhuang"],"pdf_url":"https://arxiv.org/pdf/2510.23399v1.pdf","comment":"7 pages, 5 tables"},{"id":"http://arxiv.org/abs/2510.23397v1","updated":"2025-10-27T14:55:38Z","published":"2025-10-27T14:55:38Z","title":"VideoTG-R1: Boosting Video Temporal Grounding via Curriculum\n  Reinforcement Learning on Reflected Boundary Annotations","summary":"  Video temporal grounding (VTG) aims to locate precise segments in videos\nbased on language queries, which is a fundamental challenge in video\nunderstanding. While recent Multimodal Large Language Models (MLLMs) have shown\npromise in tackling VTG through reinforcement learning (RL), they overlook the\nchallenges arising from both the quality and difficulty of training samples.\n(1) Partially annotated samples. Many samples contain relevant segments beyond\nthe annotated interval, introducing ambiguous supervision. (2) Hard-to-ground\nsamples. Samples with poor zero-shot performance produce consistently low and\nindistinguishable rewards during RL training, exhibiting no clear preference\namong multiple outputs and thus hindering learning efficiency. To address these\nchallenges, we propose VideoTG-R1, a novel curriculum RL framework with\nreflected boundary annotations, enabling data-efficient training. Specifically,\nwe propose a Boundary Reflection Agent that utilizes MLLMs to predict\nquery-relevant timestamps outside the annotated intervals, allowing us to\nidentify and filter out partially annotated samples, thereby reducing\nambiguity. Furthermore, we introduce a Difficulty Estimation Agent to assess\nthe training difficulty of each sample and design a curriculum RL strategy that\ndynamically masks the videos of hard-to-ground samples according to the\ntraining steps, easing the training difficulty and providing clearer\npreference. Experiments on the VTG and grounded VideoQA tasks demonstrate the\neffectiveness of our method. Remarkably, with only 10% of the training samples\nand 21% of the computational budget, VideoTG-R1 outperforms full-data\ncounterparts under both group relative policy optimization (GRPO) and\nsupervised fine-tuning (SFT). The code is available at\nhttps://github.com/ldong1111/VideoTG-R1.\n","authors":["Lu Dong","Haiyu Zhang","Han Lin","Ziang Yan","Xiangyu Zeng","Hongjie Zhang","Yifei Huang","Yi Wang","Zhen-Hua Ling","Limin Wang","Yali Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23382v1","updated":"2025-10-27T14:34:52Z","published":"2025-10-27T14:34:52Z","title":"An Efficient Remote Sensing Super Resolution Method Exploring Diffusion\n  Priors and Multi-Modal Constraints for Crop Type Mapping","summary":"  Super resolution offers a way to harness medium even lowresolution but\nhistorically valuable remote sensing image archives. Generative models,\nespecially diffusion models, have recently been applied to remote sensing super\nresolution (RSSR), yet several challenges exist. First, diffusion models are\neffective but require expensive training from scratch resources and have slow\ninference speeds. Second, current methods have limited utilization of auxiliary\ninformation as real-world constraints to reconstruct scientifically realistic\nimages. Finally, most current methods lack evaluation on downstream tasks. In\nthis study, we present a efficient LSSR framework for RSSR, supported by a new\nmultimodal dataset of paired 30 m Landsat 8 and 10 m Sentinel 2 imagery. Built\non frozen pretrained Stable Diffusion, LSSR integrates crossmodal attention\nwith auxiliary knowledge (Digital Elevation Model, land cover, month) and\nSynthetic Aperture Radar guidance, enhanced by adapters and a tailored Fourier\nNDVI loss to balance spatial details and spectral fidelity. Extensive\nexperiments demonstrate that LSSR significantly improves crop boundary\ndelineation and recovery, achieving state-of-the-art performance with Peak\nSignal-to-Noise Ratio/Structural Similarity Index Measure of 32.63/0.84 (RGB)\nand 23.99/0.78 (IR), and the lowest NDVI Mean Squared Error (0.042), while\nmaintaining efficient inference (0.39 sec/image). Moreover, LSSR transfers\neffectively to NASA Harmonized Landsat and Sentinel (HLS) super resolution,\nyielding more reliable crop classification (F1: 0.86) than Sentinel-2 (F1:\n0.85). These results highlight the potential of RSSR to advance precision\nagriculture.\n","authors":["Songxi Yang","Tang Sui","Qunying Huang"],"pdf_url":"https://arxiv.org/pdf/2510.23382v1.pdf","comment":"41 pages"},{"id":"http://arxiv.org/abs/2509.04086v2","updated":"2025-10-27T14:28:49Z","published":"2025-09-04T10:32:40Z","title":"TEn-CATG:Text-Enriched Audio-Visual Video Parsing with Multi-Scale\n  Category-Aware Temporal Graph","summary":"  Audio-visual video parsing (AVVP) aims to detect event categories and their\ntemporal boundaries in videos, typically under weak supervision. Existing\nmethods mainly focus on (i) improving temporal modeling using attention-based\narchitectures or (ii) generating richer pseudo-labels to address the absence of\nframe-level annotations. However, attention-based models often overfit noisy\npseudo-labels, leading to cumulative training errors, while pseudo-label\ngeneration approaches distribute attention uniformly across frames, weakening\ntemporal localization accuracy. To address these challenges, we propose\nTEn-CATG, a text-enriched AVVP framework that combines semantic calibration\nwith category-aware temporal reasoning. More specifically, we design a\nbi-directional text fusion (BiT) module by leveraging audio-visual features as\nsemantic anchors to refine text embeddings, which departs from conventional\ntext-to-feature alignment, thereby mitigating noise and enhancing cross-modal\nconsistency. Furthermore, we introduce the category-aware temporal graph (CATG)\nmodule to model temporal relationships by selecting multi-scale temporal\nneighbors and learning category-specific temporal decay factors, enabling\neffective event-dependent temporal reasoning. Extensive experiments demonstrate\nthat TEn-CATG achieves state-of-the-art results across multiple evaluation\nmetrics on benchmark datasets LLP and UnAV-100, highlighting its robustness and\nsuperior ability to capture complex temporal and semantic dependencies in\nweakly supervised AVVP tasks.\n","authors":["Yaru Chen","Faegheh Sardari","Peiliang Zhang","Ruohao Guo","Yang Xiang","Zhenbo Li","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2509.04086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.16359v2","updated":"2025-10-27T14:23:31Z","published":"2025-08-22T13:05:55Z","title":"RotaTouille: Rotation Equivariant Deep Learning for Contours","summary":"  Contours or closed planar curves are common in many domains. For example,\nthey appear as object boundaries in computer vision, isolines in meteorology,\nand the orbits of rotating machinery. In many cases when learning from contour\ndata, planar rotations of the input will result in correspondingly rotated\noutputs. It is therefore desirable that deep learning models be rotationally\nequivariant. In addition, contours are typically represented as an ordered\nsequence of edge points, where the choice of starting point is arbitrary. It is\ntherefore also desirable for deep learning methods to be equivariant under\ncyclic shifts. We present RotaTouille, a deep learning framework for learning\nfrom contour data that achieves both rotation and cyclic shift equivariance\nthrough complex-valued circular convolution. We further introduce and\ncharacterize equivariant non-linearities, coarsening layers, and global pooling\nlayers to obtain invariant representations for downstream tasks. Finally, we\ndemonstrate the effectiveness of RotaTouille through experiments in shape\nclassification, reconstruction, and contour regression.\n","authors":["Odin Hoff Gardaa","Nello Blaser"],"pdf_url":"https://arxiv.org/pdf/2508.16359v2.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.05039v2","updated":"2025-10-27T14:22:30Z","published":"2024-06-07T16:02:10Z","title":"Bootstrapping Referring Multi-Object Tracking","summary":"  Referring understanding is a fundamental task that bridges natural language\nand visual content by localizing objects described in free-form expressions.\nHowever, existing works are constrained by limited language expressiveness,\nlacking the capacity to model object dynamics in spatial numbers and temporal\nstates. To address these limitations, we introduce a new and general referring\nunderstanding task, termed referring multi-object tracking (RMOT). Its core\nidea is to employ a language expression as a semantic cue to guide the\nprediction of multi-object tracking, comprehensively accounting for variations\nin object quantity and temporal semantics. Along with RMOT, we introduce a RMOT\nbenchmark named Refer-KITTI-V2, featuring scalable and diverse language\nexpressions. To efficiently generate high-quality annotations covering object\ndynamics with minimal manual effort, we propose a semi-automatic labeling\npipeline that formulates a total of 9,758 language prompts. In addition, we\npropose TempRMOT, an elegant end-to-end Transformer-based framework for RMOT.\nAt its core is a query-driven Temporal Enhancement Module that represents each\nobject as a Transformer query, enabling long-term spatial-temporal interactions\nwith other objects and past frames to efficiently refine these queries.\nTempRMOT achieves state-of-the-art performance on both Refer-KITTI and\nRefer-KITTI-V2, demonstrating the effectiveness of our approach. The source\ncode and dataset is available at https://github.com/zyn213/TempRMOT.\n","authors":["Yani Zhang","Dongming Wu","Wencheng Han","Xingping Dong"],"pdf_url":"https://arxiv.org/pdf/2406.05039v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23368v1","updated":"2025-10-27T14:18:13Z","published":"2025-10-27T14:18:13Z","title":"PlanarTrack: A high-quality and challenging benchmark for large-scale\n  planar object tracking","summary":"  Planar tracking has drawn increasing interest owing to its key roles in\nrobotics and augmented reality. Despite recent great advancement, further\ndevelopment of planar tracking, particularly in the deep learning era, is\nlargely limited compared to generic tracking due to the lack of large-scale\nplatforms. To mitigate this, we propose PlanarTrack, a large-scale high-quality\nand challenging benchmark for planar tracking. Specifically, PlanarTrack\nconsists of 1,150 sequences with over 733K frames, including 1,000 short-term\nand 150 new long-term videos, which enables comprehensive evaluation of short-\nand long-term tracking performance. All videos in PlanarTrack are recorded in\nunconstrained conditions from the wild, which makes PlanarTrack challenging but\nmore realistic for real-world applications. To ensure high-quality annotations,\neach video frame is manually annotated by four corner points with multi-round\nmeticulous inspection and refinement. To enhance target diversity of\nPlanarTrack, we only capture a unique target in one sequence, which is\ndifferent from existing benchmarks. To our best knowledge, PlanarTrack is by\nfar the largest and most diverse and challenging dataset dedicated to planar\ntracking. To understand performance of existing methods on PlanarTrack and to\nprovide a comparison for future research, we evaluate 10 representative planar\ntrackers with extensive comparison and in-depth analysis. Our evaluation\nreveals that, unsurprisingly, the top planar trackers heavily degrade on the\nchallenging PlanarTrack, which indicates more efforts are required for\nimproving planar tracking. Our data and results will be released at\nhttps://github.com/HengLan/PlanarTrack\n","authors":["Yifan Jiao","Xinran Liu","Xiaoqiong Liu","Xiaohui Yuan","Heng Fan","Libo Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.23368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23363v1","updated":"2025-10-27T14:13:51Z","published":"2025-10-27T14:13:51Z","title":"Interpretable Tile-Based Classification of Paclitaxel Exposure","summary":"  Medical image analysis is central to drug discovery and preclinical\nevaluation, where scalable, objective readouts can accelerate decision-making.\nWe address classification of paclitaxel (Taxol) exposure from phase-contrast\nmicroscopy of C6 glioma cells -- a task with subtle dose differences that\nchallenges full-image models. We propose a simple tiling-and-aggregation\npipeline that operates on local patches and combines tile outputs into an image\nlabel, achieving state-of-the-art accuracy on the benchmark dataset and\nimproving over the published baseline by around 20 percentage points, with\ntrends confirmed by cross-validation. To understand why tiling is effective, we\nfurther apply Grad-CAM and Score-CAM and attention analyses, which enhance\nmodel interpretability and point toward robustness-oriented directions for\nfuture medical image research. Code is released to facilitate reproduction and\nextension.\n","authors":["Sean Fletcher","Gabby Scott","Douglas Currie","Xin Zhang","Yuqi Song","Bruce MacLeod"],"pdf_url":"https://arxiv.org/pdf/2510.23363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23325v1","updated":"2025-10-27T13:42:16Z","published":"2025-10-27T13:42:16Z","title":"Multitask Multimodal Self-Supervised Learning for Medical Images","summary":"  This thesis works to address a pivotal challenge in medical image analysis:\nthe reliance on extensive labeled datasets, which are often limited due to the\nneed for expert annotation and constrained by privacy and legal issues. By\nfocusing on the development of self-supervised learning techniques and domain\nadaptation methods, this research aims to circumvent these limitations,\npresenting a novel approach to enhance the utility and efficacy of deep\nlearning in medical imaging.\n  Central to this thesis is the development of the Medformer, an innovative\nneural network architecture designed for multitask learning and deep domain\nadaptation. This model is adept at pre-training on diverse medical image\ndatasets, handling varying sizes and modalities, and is equipped with a dynamic\ninput-output adaptation mechanism. This enables efficient processing and\nintegration of a wide range of medical image types, from 2D X-rays to complex\n3D MRIs, thus mitigating the dependency on large labeled datasets.\n  Further, the thesis explores the current state of self-supervised learning in\nmedical imaging. It introduces novel pretext tasks that are capable of\nextracting meaningful information from unlabeled data, significantly advancing\nthe model's interpretative abilities. This approach is validated through\nrigorous experimentation, including the use of the MedMNIST dataset,\ndemonstrating the model's proficiency in learning generalized features\napplicable to various downstream tasks.\n  In summary, this thesis contributes to the advancement of medical image\nanalysis by offering a scalable, adaptable framework that reduces reliance on\nlabeled data. It paves the way for more accurate, efficient diagnostic tools in\nhealthcare, signifying a major step forward in the application of deep learning\nin medical imaging.\n","authors":["Cristian Simionescu"],"pdf_url":"https://arxiv.org/pdf/2510.23325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.14081v3","updated":"2025-10-27T13:30:00Z","published":"2025-10-15T20:36:28Z","title":"Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from\n  Unstructured Phone Images","summary":"  We present a novel, zero-shot pipeline for creating hyperrealistic,\nidentity-preserving 3D avatars from a few unstructured phone images. Existing\nmethods face several challenges: single-view approaches suffer from geometric\ninconsistencies and hallucinations, degrading identity preservation, while\nmodels trained on synthetic data fail to capture high-frequency details like\nskin wrinkles and fine hair, limiting realism. Our method introduces two key\ncontributions: (1) a generative canonicalization module that processes multiple\nunstructured views into a standardized, consistent representation, and (2) a\ntransformer-based model trained on a new, large-scale dataset of high-fidelity\nGaussian splatting avatars derived from dome captures of real people. This\n\"Capture, Canonicalize, Splat\" pipeline produces static quarter-body avatars\nwith compelling realism and robust identity preservation from unstructured\nphotos.\n","authors":["Emanuel Garbin","Guy Adam","Oded Krams","Zohar Barzelay","Eran Guendelman","Michael Schwarz","Matteo Presutto","Moran Vatelmacher","Yigal Shenkman","Eli Peker","Itai Druker","Uri Patish","Yoav Blum","Max Bluvstein","Junxuan Li","Rawal Khirodkar","Shunsuke Saito"],"pdf_url":"https://arxiv.org/pdf/2510.14081v3.pdf","comment":"This work received the Best Paper Honorable Mention at the AMFG\n  Workshop, ICCV 2025"},{"id":"http://arxiv.org/abs/2510.23306v1","updated":"2025-10-27T13:15:06Z","published":"2025-10-27T13:15:06Z","title":"ReconViaGen: Towards Accurate Multi-view 3D Object Reconstruction via\n  Generation","summary":"  Existing multi-view 3D object reconstruction methods heavily rely on\nsufficient overlap between input views, where occlusions and sparse coverage in\npractice frequently yield severe reconstruction incompleteness. Recent\nadvancements in diffusion-based 3D generative techniques offer the potential to\naddress these limitations by leveraging learned generative priors to\nhallucinate invisible parts of objects, thereby generating plausible 3D\nstructures. However, the stochastic nature of the inference process limits the\naccuracy and reliability of generation results, preventing existing\nreconstruction frameworks from integrating such 3D generative priors. In this\nwork, we comprehensively analyze the reasons why diffusion-based 3D generative\nmethods fail to achieve high consistency, including (a) the insufficiency in\nconstructing and leveraging cross-view connections when extracting multi-view\nimage features as conditions, and (b) the poor controllability of iterative\ndenoising during local detail generation, which easily leads to plausible but\ninconsistent fine geometric and texture details with inputs. Accordingly, we\npropose ReconViaGen to innovatively integrate reconstruction priors into the\ngenerative framework and devise several strategies that effectively address\nthese issues. Extensive experiments demonstrate that our ReconViaGen can\nreconstruct complete and accurate 3D models consistent with input views in both\nglobal structure and local details.Project page:\nhttps://jiahao620.github.io/reconviagen.\n","authors":["Jiahao Chang","Chongjie Ye","Yushuang Wu","Yuantao Chen","Yidan Zhang","Zhongjin Luo","Chenghong Li","Yihao Zhi","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2510.23306v1.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2510.23301v1","updated":"2025-10-27T13:08:46Z","published":"2025-10-27T13:08:46Z","title":"MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object\n  Re-Identification","summary":"  Real-world object re-identification (ReID) systems often face modality\ninconsistencies, where query and gallery images come from different sensors\n(e.g., RGB, NIR, TIR). However, most existing methods assume modality-matched\nconditions, which limits their robustness and scalability in practical\napplications. To address this challenge, we propose MDReID, a flexible\nany-to-any image-level ReID framework designed to operate under both\nmodality-matched and modality-mismatched scenarios. MDReID builds on the\ninsight that modality information can be decomposed into two components:\nmodality-shared features that are predictable and transferable, and\nmodality-specific features that capture unique, modality-dependent\ncharacteristics. To effectively leverage this, MDReID introduces two key\ncomponents: the Modality Decoupling Learning (MDL) and Modality-aware Metric\nLearning (MML). Specifically, MDL explicitly decomposes modality features into\nmodality-shared and modality-specific representations, enabling effective\nretrieval in both modality-aligned and mismatched scenarios. MML, a tailored\nmetric learning strategy, further enforces orthogonality and complementarity\nbetween the two components to enhance discriminative power across modalities.\nExtensive experiments conducted on three challenging multi-modality ReID\nbenchmarks (RGBNT201, RGBNT100, MSVR310) consistently demonstrate the\nsuperiority of MDReID. Notably, MDReID achieves significant mAP improvements of\n9.8\\%, 3.0\\%, and 11.5\\% in general modality-matched scenarios, and average\ngains of 3.4\\%, 11.8\\%, and 10.9\\% in modality-mismatched scenarios,\nrespectively. The code is available at:\n\\textcolor{magenta}{https://github.com/stone96123/MDReID}.\n","authors":["Yingying Feng","Jie Li","Jie Hu","Yukang Zhang","Lei Tan","Jiayi Ji"],"pdf_url":"https://arxiv.org/pdf/2510.23301v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23299v1","updated":"2025-10-27T13:05:27Z","published":"2025-10-27T13:05:27Z","title":"MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm\n  Detection","summary":"  Despite progress in multimodal sarcasm detection, existing datasets and\nmethods predominantly focus on single-image scenarios, overlooking potential\nsemantic and affective relations across multiple images. This leaves a gap in\nmodeling cases where sarcasm is triggered by multi-image cues in real-world\nsettings. To bridge this gap, we introduce MMSD3.0, a new benchmark composed\nentirely of multi-image samples curated from tweets and Amazon reviews. We\nfurther propose the Cross-Image Reasoning Model (CIRM), which performs targeted\ncross-image sequence modeling to capture latent inter-image connections. In\naddition, we introduce a relevance-guided, fine-grained cross-modal fusion\nmechanism based on text-image correspondence to reduce information loss during\nintegration. We establish a comprehensive suite of strong and representative\nbaselines and conduct extensive experiments, showing that MMSD3.0 is an\neffective and reliable benchmark that better reflects real-world conditions.\nMoreover, CIRM demonstrates state-of-the-art performance across MMSD, MMSD2.0\nand MMSD3.0, validating its effectiveness in both single-image and multi-image\nscenarios.\n","authors":["Haochen Zhao","Yuyao Kong","Yongxiu Xu","Gaopeng Gou","Hongbo Xu","Yubin Wang","Haoliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.23299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23285v1","updated":"2025-10-27T12:53:48Z","published":"2025-10-27T12:53:48Z","title":"Adaptive Stochastic Coefficients for Accelerating Diffusion Sampling","summary":"  Diffusion-based generative processes, formulated as differential equation\nsolving, frequently balance computational speed with sample quality. Our\ntheoretical investigation of ODE- and SDE-based solvers reveals complementary\nweaknesses: ODE solvers accumulate irreducible gradient error along\ndeterministic trajectories, while SDE methods suffer from amplified\ndiscretization errors when the step budget is limited. Building upon this\ninsight, we introduce AdaSDE, a novel single-step SDE solver that aims to unify\nthe efficiency of ODEs with the error resilience of SDEs. Specifically, we\nintroduce a single per-step learnable coefficient, estimated via lightweight\ndistillation, which dynamically regulates the error correction strength to\naccelerate diffusion sampling. Notably, our framework can be integrated with\nexisting solvers to enhance their capabilities. Extensive experiments\ndemonstrate state-of-the-art performance: at 5 NFE, AdaSDE achieves FID scores\nof 4.18 on CIFAR-10, 8.05 on FFHQ and 6.96 on LSUN Bedroom. Codes are available\nin https://github.com/WLU-wry02/AdaSDE.\n","authors":["Ruoyu Wang","Beier Zhu","Junzhi Li","Liangyu Yuan","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.23285v1.pdf","comment":"To appear in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23278v1","updated":"2025-10-27T12:39:50Z","published":"2025-10-27T12:39:50Z","title":"hYOLO Model: Enhancing Object Classification with Hierarchical Context\n  in YOLOv8","summary":"  Current convolution neural network (CNN) classification methods are\npredominantly focused on flat classification which aims solely to identify a\nspecified object within an image. However, real-world objects often possess a\nnatural hierarchical organization that can significantly help classification\ntasks. Capturing the presence of relations between objects enables better\ncontextual understanding as well as control over the severity of mistakes.\nConsidering these aspects, this paper proposes an end-to-end hierarchical model\nfor image detection and classification built upon the YOLO model family. A\nnovel hierarchical architecture, a modified loss function, and a performance\nmetric tailored to the hierarchical nature of the model are introduced. The\nproposed model is trained and evaluated on two different hierarchical\ncategorizations of the same dataset: a systematic categorization that\ndisregards visual similarities between objects and a categorization accounting\nfor common visual characteristics across classes. The results illustrate how\nthe suggested methodology addresses the inherent hierarchical structure present\nin real-world objects, which conventional flat classification algorithms often\noverlook.\n","authors":["Veska Tsenkova","Peter Stanchev","Daniel Petrov","Deyan Lazarov"],"pdf_url":"https://arxiv.org/pdf/2510.23278v1.pdf","comment":"39 pages, 12 figures, 4 tables, code available at\n  https://github.com/ds2run/hyolo"},{"id":"http://arxiv.org/abs/2505.18700v4","updated":"2025-10-27T12:39:19Z","published":"2025-05-24T13:48:57Z","title":"GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language\n  Models and Enhanced Reasoning Chains","summary":"  Recent advances in Visual Language Models (VLMs) have demonstrated\nexceptional performance in visual reasoning tasks. However, geo-localization\npresents unique challenges, requiring the extraction of multigranular visual\ncues from images and their integration with external world knowledge for\nsystematic reasoning. Current approaches to geo-localization tasks often lack\nrobust reasoning mechanisms and explainability, limiting their effectiveness.\nTo address these limitations, we propose the Geo Reason Enhancement (GRE)\nSuite, a novel framework that augments VLMs with structured reasoning chains\nfor accurate and interpretable location inference. The GRE Suite is\nsystematically developed across three key dimensions: dataset, model, and\nbenchmark. First, we introduce GRE30K, a high-quality geo-localization\nreasoning dataset designed to facilitate fine-grained visual and contextual\nanalysis. Next, we present the GRE model, which employs a multi-stage reasoning\nstrategy to progressively infer scene attributes, local details, and semantic\nfeatures, thereby narrowing down potential geographic regions with enhanced\nprecision. Finally, we construct the Geo Reason Evaluation Benchmark\n(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across\ndiverse urban, natural, and landmark scenes to measure both coarse-grained\n(e.g., country, continent) and fine-grained (e.g., city, street) localization\nperformance. Experimental results demonstrate that GRE significantly\noutperforms existing methods across all granularities of geo-localization\ntasks, underscoring the efficacy of reasoning-augmented VLMs in complex\ngeographic inference. Code and data will be released at\nhttps://github.com/Thorin215/GRE.\n","authors":["Chun Wang","Xiaojun Ye","Xiaoran Pan","Zihao Pan","Haofan Wang","Yiren Song"],"pdf_url":"https://arxiv.org/pdf/2505.18700v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23253v1","updated":"2025-10-27T12:15:02Z","published":"2025-10-27T12:15:02Z","title":"A Video Is Not Worth a Thousand Words","summary":"  As we become increasingly dependent on vision language models (VLMs) to\nanswer questions about the world around us, there is a significant amount of\nresearch devoted to increasing both the difficulty of video question answering\n(VQA) datasets, and the context lengths of the models that they evaluate. The\nreliance on large language models as backbones has lead to concerns about\npotential text dominance, and the exploration of interactions between\nmodalities is underdeveloped. How do we measure whether we're heading in the\nright direction, with the complexity that multi-modal models introduce? We\npropose a joint method of computing both feature attributions and modality\nscores based on Shapley values, where both the features and modalities are\narbitrarily definable. Using these metrics, we compare $6$ VLM models of\nvarying context lengths on $4$ representative datasets, focusing on\nmultiple-choice VQA. In particular, we consider video frames and whole textual\nelements as equal features in the hierarchy, and the multiple-choice VQA task\nas an interaction between three modalities: video, question and answer. Our\nresults demonstrate a dependence on text and show that the multiple-choice VQA\ntask devolves into a model's ability to ignore distractors. Code available at\nhttps://github.com/sjpollard/a-video-is-not-worth-a-thousand-words.\n","authors":["Sam Pollard","Michael Wray"],"pdf_url":"https://arxiv.org/pdf/2510.23253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.10111v2","updated":"2025-10-27T11:57:33Z","published":"2025-10-11T08:42:31Z","title":"Training-Free In-Context Forensic Chain for Image Manipulation Detection\n  and Localization","summary":"  Advances in image tampering pose serious security threats, underscoring the\nneed for effective image manipulation localization (IML). While supervised IML\nachieves strong performance, it depends on costly pixel-level annotations.\nExisting weakly supervised or training-free alternatives often underperform and\nlack interpretability. We propose the In-Context Forensic Chain (ICFC), a\ntraining-free framework that leverages multi-modal large language models\n(MLLMs) for interpretable IML tasks. ICFC integrates an objectified rule\nconstruction with adaptive filtering to build a reliable knowledge base and a\nmulti-step progressive reasoning pipeline that mirrors expert forensic\nworkflows from coarse proposals to fine-grained forensics results. This design\nenables systematic exploitation of MLLM reasoning for image-level\nclassification, pixel-level localization, and text-level interpretability.\nAcross multiple benchmarks, ICFC not only surpasses state-of-the-art\ntraining-free methods but also achieves competitive or superior performance\ncompared to weakly and fully supervised approaches.\n","authors":["Rui Chen","Bin Liu","Changtao Miao","Xinghao Wang","Yi Li","Tao Gong","Qi Chu","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2510.10111v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23241v1","updated":"2025-10-27T11:55:12Z","published":"2025-10-27T11:55:12Z","title":"Progressive Growing of Patch Size: Curriculum Learning for Accelerated\n  and Improved Medical Image Segmentation","summary":"  In this work, we introduce Progressive Growing of Patch Size, an automatic\ncurriculum learning approach for 3D medical image segmentation. Our approach\nprogressively increases the patch size during model training, resulting in an\nimproved class balance for smaller patch sizes and accelerated convergence of\nthe training process. We evaluate our curriculum approach in two settings: a\nresource-efficient mode and a performance mode, both regarding Dice score\nperformance and computational costs across 15 diverse and popular 3D medical\nimage segmentation tasks. The resource-efficient mode matches the Dice score\nperformance of the conventional constant patch size sampling baseline with a\nnotable reduction in training time to only 44%. The performance mode improves\nupon constant patch size segmentation results, achieving a statistically\nsignificant relative mean performance gain of 1.28% in Dice Score. Remarkably,\nacross all 15 tasks, our proposed performance mode manages to surpass the\nconstant patch size baseline in Dice Score performance, while simultaneously\nreducing training time to only 89%. The benefits are particularly pronounced\nfor highly imbalanced tasks such as lesion segmentation tasks. Rigorous\nexperiments demonstrate that our performance mode not only improves mean\nsegmentation performance but also reduces performance variance, yielding more\ntrustworthy model comparison. Furthermore, our findings reveal that the\nproposed curriculum sampling is not tied to a specific architecture but\nrepresents a broadly applicable strategy that consistently boosts performance\nacross diverse segmentation models, including UNet, UNETR, and SwinUNETR. In\nsummary, we show that this simple yet elegant transformation on input data\nsubstantially improves both Dice Score performance and training runtime, while\nbeing compatible across diverse segmentation backbones.\n","authors":["Stefan M. Fischer","Johannes Kiechle","Laura Daza","Lina Felsner","Richard Osuala","Daniel M. Lang","Karim Lekadir","Jan C. Peeken","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2510.23241v1.pdf","comment":"Journal Extension of \"Progressive Growing of Patch Size:\n  Resource-Efficient Curriculum Learning for Dense Prediction Tasks\"\n  (MICCAI2024) submitted to MedIA"},{"id":"http://arxiv.org/abs/2510.23240v1","updated":"2025-10-27T11:54:23Z","published":"2025-10-27T11:54:23Z","title":"Autoregressive Styled Text Image Generation, but Make it Reliable","summary":"  Generating faithful and readable styled text images (especially for Styled\nHandwritten Text generation - HTG) is an open problem with several possible\napplications across graphic design, document understanding, and image editing.\nA lot of research effort in this task is dedicated to developing strategies\nthat reproduce the stylistic characteristics of a given writer, with promising\nresults in terms of style fidelity and generalization achieved by the recently\nproposed Autoregressive Transformer paradigm for HTG. However, this method\nrequires additional inputs, lacks a proper stop mechanism, and might end up in\nrepetition loops, generating visual artifacts. In this work, we rethink the\nautoregressive formulation by framing HTG as a multimodal prompt-conditioned\ngeneration task, and tackle the content controllability issues by introducing\nspecial textual input tokens for better alignment with the visual ones.\nMoreover, we devise a Classifier-Free-Guidance-based strategy for our\nautoregressive model. Through extensive experimental validation, we demonstrate\nthat our approach, dubbed Eruku, compared to previous solutions requires fewer\ninputs, generalizes better to unseen styles, and follows more faithfully the\ntextual prompt, improving content adherence.\n","authors":["Carmine Zaccagnino","Fabio Quattrini","Vittorio Pippi","Silvia Cascianelli","Alessio Tonioni","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2510.23240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.15566v4","updated":"2025-10-27T11:34:58Z","published":"2025-09-19T04:03:44Z","title":"BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent","summary":"  In the field of AI-driven human-GUI interaction automation, while rapid\nadvances in multimodal large language models and reinforcement fine-tuning\ntechniques have yielded remarkable progress, a fundamental challenge persists:\ntheir interaction logic significantly deviates from natural human-GUI\ncommunication patterns. To fill this gap, we propose \"Blink-Think-Link\" (BTL),\na brain-inspired framework for human-GUI interaction that mimics the human\ncognitive process between users and graphical interfaces. The system decomposes\ninteractions into three biologically plausible phases: (1) Blink - rapid\ndetection and attention to relevant screen areas, analogous to saccadic eye\nmovements; (2) Think - higher-level reasoning and decision-making, mirroring\ncognitive planning; and (3) Link - generation of executable commands for\nprecise motor control, emulating human action selection mechanisms.\nAdditionally, we introduce two key technical innovations for the BTL framework:\n(1) Blink Data Generation - an automated annotation pipeline specifically\noptimized for blink data, and (2) BTL Reward -- the first rule-based reward\nmechanism that enables reinforcement learning driven by both process and\noutcome. Building upon this framework, we develop a GUI agent model named\nBTL-UI, which demonstrates competitive performance across both static GUI\nunderstanding and dynamic interaction tasks in comprehensive benchmarks. These\nresults provide conclusive empirical validation of the framework's efficacy in\ndeveloping advanced GUI Agents.\n","authors":["Shaojie Zhang","Ruoceng Zhang","Pei Fu","Shaokang Wang","Jiahui Yang","Xin Du","Shiqi Cui","Bin Qin","Ying Huang","Zhenbo Luo","Jian Luan"],"pdf_url":"https://arxiv.org/pdf/2509.15566v4.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23225v1","updated":"2025-10-27T11:23:04Z","published":"2025-10-27T11:23:04Z","title":"Through the Lens: Benchmarking Deepfake Detectors Against\n  Moiré-Induced Distortions","summary":"  Deepfake detection remains a pressing challenge, particularly in real-world\nsettings where smartphone-captured media from digital screens often introduces\nMoir\\'e artifacts that can distort detection outcomes. This study\nsystematically evaluates state-of-the-art (SOTA) deepfake detectors on\nMoir\\'e-affected videos, an issue that has received little attention. We\ncollected a dataset of 12,832 videos, spanning 35.64 hours, from the Celeb-DF,\nDFD, DFDC, UADFV, and FF++ datasets, capturing footage under diverse real-world\nconditions, including varying screens, smartphones, lighting setups, and camera\nangles. To further examine the influence of Moir\\'e patterns on deepfake\ndetection, we conducted additional experiments using our DeepMoir\\'eFake,\nreferred to as (DMF) dataset and two synthetic Moir\\'e generation techniques.\nAcross 15 top-performing detectors, our results show that Moir\\'e artifacts\ndegrade performance by as much as 25.4%, while synthetically generated Moir\\'e\npatterns lead to a 21.4% drop in accuracy. Surprisingly, demoir\\'eing methods,\nintended as a mitigation approach, instead worsened the problem, reducing\naccuracy by up to 17.2%. These findings underscore the urgent need for\ndetection models that can robustly handle Moir\\'e distortions alongside other\nrealworld challenges, such as compression, sharpening, and blurring. By\nintroducing the DMF dataset, we aim to drive future research toward closing the\ngap between controlled experiments and practical deepfake detection.\n","authors":["Razaib Tariq","Minji Heo","Simon S. Woo","Shahroz Tariq"],"pdf_url":"https://arxiv.org/pdf/2510.23225v1.pdf","comment":"48 Pages, 29 Figures, 15 Tables"},{"id":"http://arxiv.org/abs/2510.23224v1","updated":"2025-10-27T11:22:28Z","published":"2025-10-27T11:22:28Z","title":"Accurate and Scalable Multimodal Pathology Retrieval via Attentive\n  Vision-Language Alignment","summary":"  The rapid digitization of histopathology slides has opened up new\npossibilities for computational tools in clinical and research workflows. Among\nthese, content-based slide retrieval stands out, enabling pathologists to\nidentify morphologically and semantically similar cases, thereby supporting\nprecise diagnoses, enhancing consistency across observers, and assisting\nexample-based education. However, effective retrieval of whole slide images\n(WSIs) remains challenging due to their gigapixel scale and the difficulty of\ncapturing subtle semantic differences amid abundant irrelevant content. To\novercome these challenges, we present PathSearch, a retrieval framework that\nunifies fine-grained attentive mosaic representations with global-wise slide\nembeddings aligned through vision-language contrastive learning. Trained on a\ncorpus of 6,926 slide-report pairs, PathSearch captures both fine-grained\nmorphological cues and high-level semantic patterns to enable accurate and\nflexible retrieval. The framework supports two key functionalities: (1)\nmosaic-based image-to-image retrieval, ensuring accurate and efficient slide\nresearch; and (2) multi-modal retrieval, where text queries can directly\nretrieve relevant slides. PathSearch was rigorously evaluated on four public\npathology datasets and three in-house cohorts, covering tasks including\nanatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination,\nand grading across diverse organs such as breast, lung, kidney, liver, and\nstomach. External results show that PathSearch outperforms traditional\nimage-to-image retrieval frameworks. A multi-center reader study further\ndemonstrates that PathSearch improves diagnostic accuracy, boosts confidence,\nand enhances inter-observer agreement among pathologists in real clinical\nscenarios. These results establish PathSearch as a scalable and generalizable\nretrieval solution for digital pathology.\n","authors":["Hongyi Wang","Zhengjie Zhu","Jiabo Ma","Fang Wang","Yue Shi","Bo Luo","Jili Wang","Qiuyu Cai","Xiuming Zhang","Yen-Wei Chen","Lanfen Lin","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2510.23224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00578v4","updated":"2025-10-27T11:20:11Z","published":"2025-08-30T18:06:06Z","title":"C-DiffDet+: Fusing Global Scene Context with Generative Denoising for\n  High-Fidelity Car Damage Detection","summary":"  Fine-grained object detection in challenging visual domains, such as vehicle\ndamage assessment, presents a formidable challenge even for human experts to\nresolve reliably. While DiffusionDet has advanced the state-of-the-art through\nconditional denoising diffusion, its performance remains limited by local\nfeature conditioning in context-dependent scenarios. We address this\nfundamental limitation by introducing Context-Aware Fusion (CAF), which\nleverages cross-attention mechanisms to integrate global scene context with\nlocal proposal features directly. The global context is generated using a\nseparate dedicated encoder that captures comprehensive environmental\ninformation, enabling each object proposal to attend to scene-level\nunderstanding. Our framework significantly enhances the generative detection\nparadigm by enabling each object proposal to attend to comprehensive\nenvironmental information. Experimental results demonstrate an improvement over\nstate-of-the-art models on the CarDD benchmark, establishing new performance\nbenchmarks for context-aware object detection in fine-grained domains\n","authors":["Abdellah Zakaria Sellam","Ilyes Benaissa","Salah Eddine Bekhouche","Abdenour Hadid","Vito Renó","Cosimo Distante"],"pdf_url":"https://arxiv.org/pdf/2509.00578v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.10316v2","updated":"2025-10-27T10:55:25Z","published":"2025-08-14T03:44:03Z","title":"Integrating Reinforcement Learning with Visual Generative Models:\n  Foundations and Advances","summary":"  Generative models have made significant progress in synthesizing visual\ncontent, including images, videos, and 3D/4D structures. However, they are\ntypically trained with surrogate objectives such as likelihood or\nreconstruction loss, which often misalign with perceptual quality, semantic\naccuracy, or physical realism. Reinforcement learning (RL) offers a principled\nframework for optimizing non-differentiable, preference-driven, and temporally\nstructured objectives. Recent advances demonstrate its effectiveness in\nenhancing controllability, consistency, and human alignment across generative\ntasks. This survey provides a systematic overview of RL-based methods for\nvisual content generation. We review the evolution of RL from classical control\nto its role as a general-purpose optimization tool, and examine its integration\ninto image, video, and 3D/4D generation. Across these domains, RL serves not\nonly as a fine-tuning mechanism but also as a structural component for aligning\ngeneration with complex, high-level goals. We conclude with open challenges and\nfuture research directions at the intersection of RL and generative modeling.\n","authors":["Yuanzhi Liang","Yijie Fang","Rui Li","Ziqi Ni","Ruijie Su","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.10316v2.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2510.23205v1","updated":"2025-10-27T10:49:39Z","published":"2025-10-27T10:49:39Z","title":"VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D\n  Gaussian Splatting","summary":"  End-to-end autonomous driving (E2E-AD) has emerged as a promising paradigm\nthat unifies perception, prediction, and planning into a holistic, data-driven\nframework. However, achieving robustness to varying camera viewpoints, a common\nreal-world challenge due to diverse vehicle configurations, remains an open\nproblem. In this work, we propose VR-Drive, a novel E2E-AD framework that\naddresses viewpoint generalization by jointly learning 3D scene reconstruction\nas an auxiliary task to enable planning-aware view synthesis. Unlike prior\nscene-specific synthesis approaches, VR-Drive adopts a feed-forward inference\nstrategy that supports online training-time augmentation from sparse views\nwithout additional annotations. To further improve viewpoint consistency, we\nintroduce a viewpoint-mixed memory bank that facilitates temporal interaction\nacross multiple viewpoints and a viewpoint-consistent distillation strategy\nthat transfers knowledge from original to synthesized views. Trained in a fully\nend-to-end manner, VR-Drive effectively mitigates synthesis-induced noise and\nimproves planning under viewpoint shifts. In addition, we release a new\nbenchmark dataset to evaluate E2E-AD performance under novel camera viewpoints,\nenabling comprehensive analysis. Our results demonstrate that VR-Drive is a\nscalable and robust solution for the real-world deployment of end-to-end\nautonomous driving systems.\n","authors":["Hoonhee Cho","Jae-Young Kang","Giwon Lee","Hyemin Yang","Heejun Park","Seokwoo Jung","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2510.23205v1.pdf","comment":"Accepted by NeurIPS2025"},{"id":"http://arxiv.org/abs/2311.09671v2","updated":"2025-10-27T10:48:43Z","published":"2023-11-16T08:39:58Z","title":"Generalization Bounds for Robust Contrastive Learning: From Theory to\n  Practice","summary":"  Contrastive Learning first extracts features from unlabeled data, followed by\nlinear probing with labeled data. Adversarial Contrastive Learning (ACL)\nintegrates Adversarial Training into the first phase to enhance feature\nrobustness against attacks in the probing phase. While ACL has shown strong\nempirical results, its theoretical understanding remains limited. Furthermore,\nwhile a fair amount of theoretical works analyze how the unsupervised loss can\nsupport the supervised loss in the probing phase, none has examined its role to\nthe robust supervised loss. To fill this gap, our work develops rigorous\ntheories to identify which components in the unsupervised training can help\nimprove the robust supervised loss. Specifically, besides the adversarial\ncontrastive loss, we reveal that the benign one, along with a global divergence\nbetween benign and adversarial examples can also improve robustness. Proper\nexperiments are conducted to justify our findings.\n","authors":["Ngoc N. Tran","Lam Tran","Hoang Phan","Anh Bui","Tung Pham","Toan Tran","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2311.09671v2.pdf","comment":"13 pages, 1 figure, 4 tables"},{"id":"http://arxiv.org/abs/2510.23203v1","updated":"2025-10-27T10:46:22Z","published":"2025-10-27T10:46:22Z","title":"DecoDINO: 3D Human-Scene Contact Prediction with Semantic Classification","summary":"  Accurate vertex-level contact prediction between humans and surrounding\nobjects is a prerequisite for high fidelity human object interaction models\nused in robotics, AR/VR, and behavioral simulation. DECO was the first in the\nwild estimator for this task but is limited to binary contact maps and\nstruggles with soft surfaces, occlusions, children, and false-positive foot\ncontacts. We address these issues and introduce DecoDINO, a three-branch\nnetwork based on DECO's framework. It uses two DINOv2 ViT-g/14 encoders,\nclass-balanced loss weighting to reduce bias, and patch-level cross-attention\nfor improved local reasoning. Vertex features are finally passed through a\nlightweight MLP with a softmax to assign semantic contact labels. We also\ntested a vision-language model (VLM) to integrate text features, but the\nsimpler architecture performed better and was used instead. On the DAMON\nbenchmark, DecoDINO (i) raises the binary-contact F1 score by 7$\\%$, (ii)\nhalves the geodesic error, and (iii) augments predictions with object-level\nsemantic labels. Ablation studies show that LoRA fine-tuning and the dual\nencoders are key to these improvements. DecoDINO outperformed the challenge\nbaseline in both tasks of the DAMON Challenge. Our code is available at\nhttps://github.com/DavidePasero/deco/tree/main.\n","authors":["Lukas Bierling","Davide Pasero","Fleur Dolmans","Helia Ghasemi","Angelo Broere"],"pdf_url":"https://arxiv.org/pdf/2510.23203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23190v1","updated":"2025-10-27T10:27:02Z","published":"2025-10-27T10:27:02Z","title":"Evaluation of Vision-LLMs in Surveillance Video","summary":"  The widespread use of cameras in our society has created an overwhelming\namount of video data, far exceeding the capacity for human monitoring. This\npresents a critical challenge for public safety and security, as the timely\ndetection of anomalous or criminal events is crucial for effective response and\nprevention. The ability for an embodied agent to recognize unexpected events is\nfundamentally tied to its capacity for spatial reasoning. This paper\ninvestigates the spatial reasoning of vision-language models (VLMs) by framing\nanomalous action recognition as a zero-shot, language-grounded task, addressing\nthe embodied perception challenge of interpreting dynamic 3D scenes from sparse\n2D video. Specifically, we investigate whether small, pre-trained vision--LLMs\ncan act as spatially-grounded, zero-shot anomaly detectors by converting video\ninto text descriptions and scoring labels via textual entailment. We evaluate\nfour open models on UCF-Crime and RWF-2000 under prompting and\nprivacy-preserving conditions. Few-shot exemplars can improve accuracy for some\nmodels, but may increase false positives, and privacy filters -- especially\nfull-body GAN transforms -- introduce inconsistencies that degrade accuracy.\nThese results chart where current vision--LLMs succeed (simple, spatially\nsalient events) and where they falter (noisy spatial cues, identity\nobfuscation). Looking forward, we outline concrete paths to strengthen spatial\ngrounding without task-specific training: structure-aware prompts, lightweight\nspatial memory across clips, scene-graph or 3D-pose priors during description,\nand privacy methods that preserve action-relevant geometry. This positions\nzero-shot, language-grounded pipelines as adaptable building blocks for\nembodied, real-world video understanding. Our implementation for evaluating\nVLMs is publicly available at:\nhttps://github.com/pascalbenschopTU/VLLM_AnomalyRecognition\n","authors":["Pascal Benschop","Cristian Meo","Justin Dauwels","Jelte P. Mense"],"pdf_url":"https://arxiv.org/pdf/2510.23190v1.pdf","comment":"Accepted as poster in the NeurIPS 2025 Workshop on Space in Vision,\n  Language, and Embodied AI"},{"id":"http://arxiv.org/abs/2510.23184v1","updated":"2025-10-27T10:23:31Z","published":"2025-10-27T10:23:31Z","title":"Finding 3D Scene Analogies with Multimodal Foundation Models","summary":"  Connecting current observations with prior experiences helps robots adapt and\nplan in new, unseen 3D environments. Recently, 3D scene analogies have been\nproposed to connect two 3D scenes, which are smooth maps that align scene\nregions with common spatial relationships. These maps enable detailed transfer\nof trajectories or waypoints, potentially supporting demonstration transfer for\nimitation learning or task plan transfer across scenes. However, existing\nmethods for the task require additional training and fixed object vocabularies.\nIn this work, we propose to use multimodal foundation models for finding 3D\nscene analogies in a zero-shot, open-vocabulary setting. Central to our\napproach is a hybrid neural representation of scenes that consists of a sparse\ngraph based on vision-language model features and a feature field derived from\n3D shape foundation models. 3D scene analogies are then found in a\ncoarse-to-fine manner, by first aligning the graph and refining the\ncorrespondence with feature fields. Our method can establish accurate\ncorrespondences between complex scenes, and we showcase applications in\ntrajectory and waypoint transfer.\n","authors":["Junho Kim","Young Min Kim"],"pdf_url":"https://arxiv.org/pdf/2510.23184v1.pdf","comment":"Accepted to FM4RoboPlan workshop at RSS 2025"},{"id":"http://arxiv.org/abs/2505.05470v5","updated":"2025-10-27T09:57:02Z","published":"2025-05-08T17:58:45Z","title":"Flow-GRPO: Training Flow Matching Models via Online RL","summary":"  We propose Flow-GRPO, the first method to integrate online policy gradient\nreinforcement learning (RL) into flow matching models. Our approach uses two\nkey strategies: (1) an ODE-to-SDE conversion that transforms a deterministic\nOrdinary Differential Equation (ODE) into an equivalent Stochastic Differential\nEquation (SDE) that matches the original model's marginal distribution at all\ntimesteps, enabling statistical sampling for RL exploration; and (2) a\nDenoising Reduction strategy that reduces training denoising steps while\nretaining the original number of inference steps, significantly improving\nsampling efficiency without sacrificing performance. Empirically, Flow-GRPO is\neffective across multiple text-to-image tasks. For compositional generation,\nRL-tuned SD3.5-M generates nearly perfect object counts, spatial relations, and\nfine-grained attributes, increasing GenEval accuracy from $63\\%$ to $95\\%$. In\nvisual text rendering, accuracy improves from $59\\%$ to $92\\%$, greatly\nenhancing text generation. Flow-GRPO also achieves substantial gains in human\npreference alignment. Notably, very little reward hacking occurred, meaning\nrewards did not increase at the cost of appreciable image quality or diversity\ndegradation.\n","authors":["Jie Liu","Gongye Liu","Jiajun Liang","Yangguang Li","Jiaheng Liu","Xintao Wang","Pengfei Wan","Di Zhang","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2505.05470v5.pdf","comment":"Code: https://github.com/yifan123/flow_grpo"},{"id":"http://arxiv.org/abs/2505.19911v2","updated":"2025-10-27T09:54:32Z","published":"2025-05-26T12:38:58Z","title":"Attention! Your Vision Language Model Could Be Maliciously Manipulated","summary":"  Large Vision-Language Models (VLMs) have achieved remarkable success in\nunderstanding complex real-world scenarios and supporting data-driven\ndecision-making processes. However, VLMs exhibit significant vulnerability\nagainst adversarial examples, either text or image, which can lead to various\nadversarial outcomes, e.g., jailbreaking, hijacking, and hallucination, etc. In\nthis work, we empirically and theoretically demonstrate that VLMs are\nparticularly susceptible to image-based adversarial examples, where\nimperceptible perturbations can precisely manipulate each output token. To this\nend, we propose a novel attack called Vision-language model Manipulation Attack\n(VMA), which integrates first-order and second-order momentum optimization\ntechniques with a differentiable transformation mechanism to effectively\noptimize the adversarial perturbation. Notably, VMA can be a double-edged\nsword: it can be leveraged to implement various attacks, such as jailbreaking,\nhijacking, privacy breaches, Denial-of-Service, and the generation of sponge\nexamples, etc, while simultaneously enabling the injection of watermarks for\ncopyright protection. Extensive empirical evaluations substantiate the efficacy\nand generalizability of VMA across diverse scenarios and datasets. Code is\navailable at https://github.com/Trustworthy-AI-Group/VMA.\n","authors":["Xiaosen Wang","Shaokang Wang","Zhijin Ge","Yuyang Luo","Shudong Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.19911v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2507.07860v2","updated":"2025-10-27T09:35:25Z","published":"2025-07-10T15:41:35Z","title":"THUNDER: Tile-level Histopathology image UNDERstanding benchmark","summary":"  Progress in a research field can be hard to assess, in particular when many\nconcurrent methods are proposed in a short period of time. This is the case in\ndigital pathology, where many foundation models have been released recently to\nserve as feature extractors for tile-level images, being used in a variety of\ndownstream tasks, both for tile- and slide-level problems. Benchmarking\navailable methods then becomes paramount to get a clearer view of the research\nlandscape. In particular, in critical domains such as healthcare, a benchmark\nshould not only focus on evaluating downstream performance, but also provide\ninsights about the main differences between methods, and importantly, further\nconsider uncertainty and robustness to ensure a reliable usage of proposed\nmodels. For these reasons, we introduce THUNDER, a tile-level benchmark for\ndigital pathology foundation models, allowing for efficient comparison of many\nmodels on diverse datasets with a series of downstream tasks, studying their\nfeature spaces and assessing the robustness and uncertainty of predictions\ninformed by their embeddings. THUNDER is a fast, easy-to-use, dynamic benchmark\nthat can already support a large variety of state-of-the-art foundation, as\nwell as local user-defined models for direct tile-based comparison. In this\npaper, we provide a comprehensive comparison of 23 foundation models on 16\ndifferent datasets covering diverse tasks, feature analysis, and robustness.\nThe code for THUNDER is publicly available at\nhttps://github.com/MICS-Lab/thunder.\n","authors":["Pierre Marza","Leo Fillioux","Sofiène Boutaj","Kunal Mahatha","Christian Desrosiers","Pablo Piantanida","Jose Dolz","Stergios Christodoulidis","Maria Vakalopoulou"],"pdf_url":"https://arxiv.org/pdf/2507.07860v2.pdf","comment":"Accepted at NeurIPS 2025 Datasets and Benchmarks Track (Spotlight)"},{"id":"http://arxiv.org/abs/2510.23151v1","updated":"2025-10-27T09:26:27Z","published":"2025-10-27T09:26:27Z","title":"AG-Fusion: adaptive gated multimodal fusion for 3d object detection in\n  complex scenes","summary":"  Multimodal camera-LiDAR fusion technology has found extensive application in\n3D object detection, demonstrating encouraging performance. However, existing\nmethods exhibit significant performance degradation in challenging scenarios\ncharacterized by sensor degradation or environmental disturbances. We propose a\nnovel Adaptive Gated Fusion (AG-Fusion) approach that selectively integrates\ncross-modal knowledge by identifying reliable patterns for robust detection in\ncomplex scenes. Specifically, we first project features from each modality into\na unified BEV space and enhance them using a window-based attention mechanism.\nSubsequently, an adaptive gated fusion module based on cross-modal attention is\ndesigned to integrate these features into reliable BEV representations robust\nto challenging environments. Furthermore, we construct a new dataset named\nExcavator3D (E3D) focusing on challenging excavator operation scenarios to\nbenchmark performance in complex conditions. Our method not only achieves\ncompetitive performance on the standard KITTI dataset with 93.92% accuracy, but\nalso significantly outperforms the baseline by 24.88% on the challenging E3D\ndataset, demonstrating superior robustness to unreliable modal information in\ncomplex industrial scenes.\n","authors":["Sixian Liu","Chen Xu","Qiang Wang","Donghai Shi","Yiwen Li"],"pdf_url":"https://arxiv.org/pdf/2510.23151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23145v1","updated":"2025-10-27T09:21:19Z","published":"2025-10-27T09:21:19Z","title":"Implicit Modeling for Transferability Estimation of Vision Foundation\n  Models","summary":"  Transferability estimation identifies the best pre-trained models for\ndownstream tasks without incurring the high computational cost of full\nfine-tuning. This capability facilitates deployment and advances the\npre-training and fine-tuning paradigm. However, existing methods often struggle\nto accurately assess transferability for emerging pre-trained models with\ndiverse architectures, training strategies, and task alignments. In this work,\nwe propose Implicit Transferability Modeling (ITM), a novel framework that\nimplicitly models each model's intrinsic transferability, coupled with a\nDivide-and-Conquer Variational Approximation (DVA) strategy to efficiently\napproximate embedding space evolution. This design enables generalization\nacross a broader range of models and downstream tasks. Extensive experiments on\na comprehensive benchmark--spanning extensive training regimes and a wider\nvariety of model types--demonstrate that ITM consistently outperforms existing\nmethods in terms of stability, effectiveness, and efficiency.\n","authors":["Yaoyan Zheng","Huiqun Wang","Nan Zhou","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2510.23145v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23144v1","updated":"2025-10-27T09:20:59Z","published":"2025-10-27T09:20:59Z","title":"DQ3D: Depth-guided Query for Transformer-Based 3D Object Detection in\n  Traffic Scenarios","summary":"  3D object detection from multi-view images in traffic scenarios has garnered\nsignificant attention in recent years. Many existing approaches rely on object\nqueries that are generated from 3D reference points to localize objects.\nHowever, a limitation of these methods is that some reference points are often\nfar from the target object, which can lead to false positive detections. In\nthis paper, we propose a depth-guided query generator for 3D object detection\n(DQ3D) that leverages depth information and 2D detections to ensure that\nreference points are sampled from the surface or interior of the object.\nFurthermore, to address partially occluded objects in current frame, we\nintroduce a hybrid attention mechanism that fuses historical detection results\nwith depth-guided queries, thereby forming hybrid queries. Evaluation on the\nnuScenes dataset demonstrates that our method outperforms the baseline by 6.3\\%\nin terms of mean Average Precision (mAP) and 4.3\\% in the NuScenes Detection\nScore (NDS).\n","authors":["Ziyu Wang","Wenhao Li","Ji Wu"],"pdf_url":"https://arxiv.org/pdf/2510.23144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.21173v3","updated":"2025-10-27T09:18:44Z","published":"2025-09-25T13:54:34Z","title":"Can Less Precise Be More Reliable? A Systematic Evaluation of\n  Quantization's Impact on CLIP Beyond Accuracy","summary":"  The powerful zero-shot generalization capabilities of vision-language models\n(VLMs) like CLIP have enabled new paradigms for safety-related tasks such as\nout-of-distribution (OOD) detection. However, additional aspects crucial for\nthe computationally efficient and reliable deployment of CLIP are still\noverlooked. In particular, the impact of quantization on CLIP's performance\nbeyond accuracy remains underexplored. This work presents a large-scale\nevaluation of quantization on CLIP models, assessing not only in-distribution\naccuracy but a comprehensive suite of reliability metrics and revealing\ncounterintuitive results driven by pre-training source. We demonstrate that\nquantization consistently improves calibration for typically underconfident\npre-trained models, while often degrading it for overconfident variants.\nIntriguingly, this degradation in calibration does not preclude gains in other\nreliability metrics; we find that OOD detection can still improve for these\nsame poorly calibrated models. Furthermore, we identify specific\nquantization-aware training (QAT) methods that yield simultaneous gains in\nzero-shot accuracy, calibration, and OOD robustness, challenging the view of a\nstrict efficiency-performance trade-off. These findings offer critical insights\nfor navigating the multi-objective problem of deploying efficient, reliable,\nand robust VLMs by utilizing quantization beyond its conventional role.\n","authors":["Aymen Bouguerra","Daniel Montoya","Alexandra Gomez-Villa","Fabio Arnez","Chokri Mraidha"],"pdf_url":"https://arxiv.org/pdf/2509.21173v3.pdf","comment":"Preprint, under peer review"},{"id":"http://arxiv.org/abs/2510.23140v1","updated":"2025-10-27T09:17:02Z","published":"2025-10-27T09:17:02Z","title":"Fast Voxel-Wise Kinetic Modeling in Dynamic PET using a Physics-Informed\n  CycleGAN","summary":"  Tracer kinetic modeling serves a vital role in diagnosis, treatment planning,\ntracer development and oncology, but burdens practitioners with complex and\ninvasive arterial input function estimation (AIF). We adopt a physics-informed\nCycleGAN showing promise in DCE-MRI quantification to dynamic PET\nquantification. Our experiments demonstrate sound AIF predictions and parameter\nmaps closely resembling the reference.\n","authors":["Christian Salomonsen","Samuel Kuttner","Michael Kampffmeyer","Robert Jenssen","Kristoffer Wickstrøm","Jong Chul Ye","Elisabeth Wetzer"],"pdf_url":"https://arxiv.org/pdf/2510.23140v1.pdf","comment":"5 pages, 1 figure. Pre-review preprint. Submitted to MedEurIPS 2025\n  (EurIPS workshop)"},{"id":"http://arxiv.org/abs/2510.23137v1","updated":"2025-10-27T09:16:34Z","published":"2025-10-27T09:16:34Z","title":"Note on the Construction of Structure Tensor","summary":"  This note presents a theoretical discussion of two structure tensor\nconstructions: one proposed by Bigun and Granlund 1987, and the other by\nGranlund and Knutsson 1995. At first glance, these approaches may appear quite\ndifferent--the former is implemented by averaging outer products of gradient\nfilter responses, while the latter constructs the tensor from weighted outer\nproducts of tune-in frequency vectors of quadrature filters. We argue that when\nboth constructions are viewed through the common lens of Total Least Squares\n(TLS) line fitting to the power spectrum, they can be reconciled to a large\nextent, and additional benefits emerge. From this perspective, the correction\nterm introduced in Granlund and Knutsson 1995 becomes unnecessary. Omitting it\nensures that the resulting tensor remains positive semi-definite, thereby\nsimplifying the interpretation of its eigenvalues. Furthermore, this\ninterpretation allows fitting more than a single 0rientation to the input by\nreinterpreting quadrature filter responses without relying on a structure\ntensor. It also removes the constraint that responses must originate strictly\nfrom quadrature filters, allowing the use of alternative filter types and\nnon-angular tessellations. These alternatives include Gabor filters--which,\nalthough not strictly quadrature, are still suitable for structure tensor\nconstruction--even when they tessellate the spectrum in a Cartesian fashion,\nprovided they are sufficiently concentrated.\n","authors":["Josef Bigun","Fernado Alonso-Fernandez"],"pdf_url":"https://arxiv.org/pdf/2510.23137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04705v3","updated":"2025-10-27T09:09:50Z","published":"2025-07-07T06:54:44Z","title":"Identity-Preserving Text-to-Video Generation Guided by Simple yet\n  Effective Spatial-Temporal Decoupled Representations","summary":"  Identity-preserving text-to-video (IPT2V) generation, which aims to create\nhigh-fidelity videos with consistent human identity, has become crucial for\ndownstream applications. However, current end-to-end frameworks suffer a\ncritical spatial-temporal trade-off: optimizing for spatially coherent layouts\nof key elements (e.g., character identity preservation) often compromises\ninstruction-compliant temporal smoothness, while prioritizing dynamic realism\nrisks disrupting the spatial coherence of visual structures. To tackle this\nissue, we propose a simple yet effective spatial-temporal decoupled framework\nthat decomposes representations into spatial features for layouts and temporal\nfeatures for motion dynamics. Specifically, our paper proposes a semantic\nprompt optimization mechanism and stage-wise decoupled generation paradigm. The\nformer module decouples the prompt into spatial and temporal components.\nAligned with the subsequent stage-wise decoupled approach, the spatial prompts\nguide the text-to-image (T2I) stage to generate coherent spatial features,\nwhile the temporal prompts direct the sequential image-to-video (I2V) stage to\nensure motion consistency. Experimental results validate that our approach\nachieves excellent spatiotemporal consistency, demonstrating outstanding\nperformance in identity preservation, text relevance, and video quality. By\nleveraging this simple yet robust mechanism, our algorithm secures the\nrunner-up position in 2025 ACM MultiMedia Challenge. Our code is available at\nhttps://github.com/rain152/IPVG.\n","authors":["Yuji Wang","Moran Li","Xiaobin Hu","Ran Yi","Jiangning Zhang","Han Feng","Weijian Cao","Yabiao Wang","Chengjie Wang","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2507.04705v3.pdf","comment":"ACM Multimedia 2025; code URL: https://github.com/rain152/IPVG"},{"id":"http://arxiv.org/abs/2505.22453v2","updated":"2025-10-27T09:06:32Z","published":"2025-05-28T15:11:16Z","title":"First SFT, Second RL, Third UPT: Continual Improving Multi-Modal LLM\n  Reasoning via Unsupervised Post-Training","summary":"  Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL), which require expensive and manually annotated multi-modal\ndata--an ultimately unsustainable resource. This limitation has motivated a\ngrowing interest in unsupervised paradigms as a third stage of post-training\nafter SFT and RL. While recent efforts have explored this direction, their\nmethods are complex and difficult to iterate. To address this, we propose\nMM-UPT, a simple yet effective framework for unsupervised post-training of\nMLLMs, enabling continual self-improvement without any external supervision.\nThe training method of MM-UPT builds upon GRPO, replacing traditional reward\nsignals with a self-rewarding mechanism based on majority voting over multiple\nsampled responses. Our experiments demonstrate that such training method\neffectively improves the reasoning ability of Qwen2.5-VL-7B (e.g.,\n66.3\\%$\\rightarrow$72.9\\% on MathVista, 62.9\\%$\\rightarrow$68.7\\% on We-Math),\nusing standard dataset without ground truth labels. To further explore\nscalability, we extend our framework to a data self-generation setting,\ndesigning two strategies that prompt the MLLM to synthesize new training\nsamples on its own. Additional experiments show that combining these synthetic\ndata with the unsupervised training method can also boost performance,\nhighlighting a promising approach for scalable self-improvement. Overall,\nMM-UPT offers a new paradigm for autonomous enhancement of MLLMs, serving as a\ncritical third step after initial SFT and RL in the absence of external\nsupervision. Our code is available at https://github.com/waltonfuture/MM-UPT.\n","authors":["Lai Wei","Yuting Li","Chen Wang","Yue Wang","Linghe Kong","Weiran Huang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2505.22453v2.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2509.18094v3","updated":"2025-10-27T08:58:16Z","published":"2025-09-22T17:59:40Z","title":"UniPixel: Unified Object Referring and Segmentation for Pixel-Level\n  Visual Reasoning","summary":"  Recent advances in Large Multi-modal Models (LMMs) have demonstrated their\nremarkable success as general-purpose multi-modal assistants, with particular\nfocuses on holistic image- and video-language understanding. Conversely, less\nattention has been given to scaling fine-grained pixel-level understanding\ncapabilities, where the models are expected to realize pixel-level alignment\nbetween visual signals and language semantics. Some previous studies have\napplied LMMs to related tasks such as region-level captioning and referring\nexpression segmentation. However, these models are limited to performing either\nreferring or segmentation tasks independently and fail to integrate these\nfine-grained perception capabilities into visual reasoning. To bridge this gap,\nwe propose UniPixel, a large multi-modal model capable of flexibly\ncomprehending visual prompt inputs and generating mask-grounded responses. Our\nmodel distinguishes itself by seamlessly integrating pixel-level perception\nwith general visual understanding capabilities. Specifically, UniPixel\nprocesses visual prompts and generates relevant masks on demand, and performs\nsubsequent reasoning conditioning on these intermediate pointers during\ninference, thereby enabling fine-grained pixel-level reasoning. The\neffectiveness of our approach has been verified on 10 benchmarks across a\ndiverse set of tasks, including pixel-level referring/segmentation and\nobject-centric understanding in images/videos. A novel PixelQA task that\njointly requires referring, segmentation, and question answering is also\ndesigned to verify the flexibility of our method.\n","authors":["Ye Liu","Zongyang Ma","Junfu Pu","Zhongang Qi","Yang Wu","Ying Shan","Chang Wen Chen"],"pdf_url":"https://arxiv.org/pdf/2509.18094v3.pdf","comment":"NeurIPS 2025 Camera Ready. Project Page:\n  https://polyu-chenlab.github.io/unipixel/"},{"id":"http://arxiv.org/abs/2510.23124v1","updated":"2025-10-27T08:57:59Z","published":"2025-10-27T08:57:59Z","title":"DeepSalt: Bridging Laboratory and Satellite Spectra through Domain\n  Adaptation and Knowledge Distillation for Large-Scale Soil Salinity\n  Estimation","summary":"  Soil salinization poses a significant threat to both ecosystems and\nagriculture because it limits plants' ability to absorb water and, in doing so,\nreduces crop productivity. This phenomenon alters the soil's spectral\nproperties, creating a measurable relationship between salinity and light\nreflectance that enables remote monitoring. While laboratory spectroscopy\nprovides precise measurements, its reliance on in-situ sampling limits\nscalability to regional or global levels. Conversely, hyperspectral satellite\nimagery enables wide-area observation but lacks the fine-grained\ninterpretability of laboratory instruments. To bridge this gap, we introduce\nDeepSalt, a deep-learning-based spectral transfer framework that leverages\nknowledge distillation and a novel Spectral Adaptation Unit to transfer\nhigh-resolution spectral insights from laboratory-based spectroscopy to\nsatellite-based hyperspectral sensing. Our approach eliminates the need for\nextensive ground sampling while enabling accurate, large-scale salinity\nestimation, as demonstrated through comprehensive empirical benchmarks.\nDeepSalt achieves significant performance gains over methods without explicit\ndomain adaptation, underscoring the impact of the proposed Spectral Adaptation\nUnit and the knowledge distillation strategy. The model also effectively\ngeneralized to unseen geographic regions, explaining a substantial portion of\nthe salinity variance.\n","authors":["Rupasree Dey","Abdul Matin","Everett Lewark","Tanjim Bin Faruk","Andrei Bachinin","Sam Leuthold","M. Francesca Cotrufo","Shrideep Pallickara","Sangmi Lee Pallickara"],"pdf_url":"https://arxiv.org/pdf/2510.23124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23118v1","updated":"2025-10-27T08:38:52Z","published":"2025-10-27T08:38:52Z","title":"Task-Agnostic Fusion of Time Series and Imagery for Earth Observation","summary":"  We propose a task-agnostic framework for multimodal fusion of time series and\nsingle timestamp images, enabling cross-modal generation and robust downstream\nperformance. Our approach explores deterministic and learned strategies for\ntime series quantization and then leverages a masked correlation learning\nobjective, aligning discrete image and time series tokens in a unified\nrepresentation space. Instantiated in the Earth observation domain, the\npretrained model generates consistent global temperature profiles from\nsatellite imagery and is validated through counterfactual experiments. Across\ndownstream tasks, our task-agnostic pretraining outperforms task-specific\nfusion by 6\\% in R$^2$ and 2\\% in RMSE on average, and exceeds baseline methods\nby 50\\% in R$^2$ and 12\\% in RMSE. Finally, we analyze gradient sensitivity\nacross modalities, providing insights into model robustness. Code, data, and\nweights will be released under a permissive license.\n","authors":["Gianfranco Basile","Johannes Jakubik","Benedikt Blumenstiel","Thomas Brunschwiler","Juan Bernabe Moreno"],"pdf_url":"https://arxiv.org/pdf/2510.23118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23117v1","updated":"2025-10-27T08:38:17Z","published":"2025-10-27T08:38:17Z","title":"Seeing Structural Failure Before it Happens: An Image-Based\n  Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction","summary":"  Physics Informed Neural Networks (PINNs) are gaining attention for their\nability to embed physical laws into deep learning models, which is particularly\nuseful in structural engineering tasks with limited data. This paper aims to\nexplore the use of PINNs to predict the weight of small scale spaghetti\nbridges, a task relevant to understanding load limits and potential failure\nmodes in simplified structural models. Our proposed framework incorporates\nphysics-based constraints to the prediction model for improved performance. In\naddition to standard PINNs, we introduce a novel architecture named Physics\nInformed Kolmogorov Arnold Network (PIKAN), which blends universal function\napproximation theory with physical insights. The structural parameters provided\nas input to the model are collected either manually or through computer vision\nmethods. Our dataset includes 15 real bridges, augmented to 100 samples, and\nour best model achieves an $R^2$ score of 0.9603 and a mean absolute error\n(MAE) of 10.50 units. From applied perspective, we also provide a web based\ninterface for parameter entry and prediction. These results show that PINNs can\noffer reliable estimates of structural weight, even with limited data, and may\nhelp inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at\nhttps://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.\n","authors":["Omer Jauhar Khan","Sudais Khan","Hafeez Anwar"],"pdf_url":"https://arxiv.org/pdf/2510.23117v1.pdf","comment":"12 pages, 17 figures. Preprint"},{"id":"http://arxiv.org/abs/2510.23116v1","updated":"2025-10-27T08:35:49Z","published":"2025-10-27T08:35:49Z","title":"Residual Diffusion Bridge Model for Image Restoration","summary":"  Diffusion bridge models establish probabilistic paths between arbitrary\npaired distributions and exhibit great potential for universal image\nrestoration. Most existing methods merely treat them as simple variants of\nstochastic interpolants, lacking a unified analytical perspective. Besides,\nthey indiscriminately reconstruct images through global noise injection and\nremoval, inevitably distorting undegraded regions due to imperfect\nreconstruction. To address these challenges, we propose the Residual Diffusion\nBridge Model (RDBM). Specifically, we theoretically reformulate the stochastic\ndifferential equations of generalized diffusion bridge and derive the\nanalytical formulas of its forward and reverse processes. Crucially, we\nleverage the residuals from given distributions to modulate the noise injection\nand removal, enabling adaptive restoration of degraded regions while preserving\nintact others. Moreover, we unravel the fundamental mathematical essence of\nexisting bridge models, all of which are special cases of RDBM and empirically\ndemonstrate the optimality of our proposed models. Extensive experiments are\nconducted to demonstrate the state-of-the-art performance of our method both\nqualitatively and quantitatively across diverse image restoration tasks. Code\nis publicly available at https://github.com/MiliLab/RDBM.\n","authors":["Hebaixu Wang","Jing Zhang","Haoyang Chen","Haonan Guo","Di Wang","Jiayi Ma","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2510.23116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01737v2","updated":"2025-10-27T08:33:50Z","published":"2024-10-02T16:47:55Z","title":"Robust Modality-incomplete Anomaly Detection: A Modality-instructive\n  Framework with Benchmark","summary":"  Multimodal Industrial Anomaly Detection (MIAD), which utilizes 3D point\nclouds and 2D RGB images to identify abnormal regions in products, plays a\ncrucial role in industrial quality inspection. However, traditional MIAD\nsettings assume that all 2D and 3D modalities are paired, ignoring the fact\nthat multimodal data collected from the real world is often imperfect due to\nmissing modalities. Additionally, models trained on modality-incomplete data\nare prone to overfitting. Therefore, MIAD models that demonstrate robustness\nagainst modality-incomplete data are highly desirable in practice. To address\nthis, we introduce a pioneering study that comprehensively investigates\nModality-Incomplete Industrial Anomaly Detection (MIIAD), and under the\nguidance of experts, we construct the MIIAD Bench with rich modality-missing\nsettings to account for imperfect learning environments with incomplete\nmultimodal information. As expected, we find that most existing MIAD methods\nperform poorly on the MIIAD Bench, leading to significant performance\ndegradation. To tackle this challenge, we propose a novel two-stage Robust\nmodAlity-aware fusing and Detecting framewoRk, abbreviated as RADAR.\nSpecifically: i) We propose Modality-incomplete Instruction to guide the\nmultimodal Transformer to robustly adapt to various modality-incomplete\nscenarios, and implement adaptive parameter learning based on HyperNetwork. ii)\nThen, we construct a Double-Pseudo Hybrid Module to highlight the uniqueness of\nmodality combinations, mitigating overfitting issues and further enhancing the\nrobustness of the MIIAD model. Our experimental results demonstrate that the\nproposed RADAR significantly outperforms traditional MIAD methods on our newly\ncreated MIIAD dataset, proving its practical application value.\n","authors":["Bingchen Miao","Wenqiao Zhang","Juncheng Li","Wangyu Wu","Siliang Tang","Zhaocheng Li","Haochen Shi","Jun Xiao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2410.01737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13918v2","updated":"2025-10-27T08:22:57Z","published":"2025-01-23T18:55:41Z","title":"Improving Video Generation with Human Feedback","summary":"  Video generation has achieved significant advances through rectified flow\ntechniques, but issues like unsmooth motion and misalignment between videos and\nprompts persist. In this work, we develop a systematic pipeline that harnesses\nhuman feedback to mitigate these problems and refine the video generation\nmodel. Specifically, we begin by constructing a large-scale human preference\ndataset focused on modern video generation models, incorporating pairwise\nannotations across multi-dimensions. We then introduce VideoReward, a\nmulti-dimensional video reward model, and examine how annotations and various\ndesign choices impact its rewarding efficacy. From a unified reinforcement\nlearning perspective aimed at maximizing reward with KL regularization, we\nintroduce three alignment algorithms for flow-based models. These include two\ntraining-time strategies: direct preference optimization for flow (Flow-DPO)\nand reward weighted regression for flow (Flow-RWR), and an inference-time\ntechnique, Flow-NRG, which applies reward guidance directly to noisy videos.\nExperimental results indicate that VideoReward significantly outperforms\nexisting reward models, and Flow-DPO demonstrates superior performance compared\nto both Flow-RWR and supervised fine-tuning methods. Additionally, Flow-NRG\nlets users assign custom weights to multiple objectives during inference,\nmeeting personalized video quality needs.\n","authors":["Jie Liu","Gongye Liu","Jiajun Liang","Ziyang Yuan","Xiaokun Liu","Mingwu Zheng","Xiele Wu","Qiulin Wang","Menghan Xia","Xintao Wang","Xiaohong Liu","Fei Yang","Pengfei Wan","Di Zhang","Kun Gai","Yujiu Yang","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2501.13918v2.pdf","comment":"https://github.com/KwaiVGI/VideoAlign"},{"id":"http://arxiv.org/abs/2504.13593v4","updated":"2025-10-27T08:16:17Z","published":"2025-04-18T09:52:22Z","title":"KAN or MLP? Point Cloud Shows the Way Forward","summary":"  Multi-Layer Perceptrons (MLPs) have become one of the fundamental\narchitectural component in point cloud analysis due to its effective feature\nlearning mechanism. However, when processing complex geometric structures in\npoint clouds, MLPs' fixed activation functions struggle to efficiently capture\nlocal geometric features, while suffering from poor parameter efficiency and\nhigh model redundancy. In this paper, we propose PointKAN, which applies\nKolmogorov-Arnold Networks (KANs) to point cloud analysis tasks to investigate\ntheir efficacy in hierarchical feature representation. First, we introduce a\nGeometric Affine Module (GAM) to transform local features, improving the\nmodel's robustness to geometric variations. Next, in the Local Feature\nProcessing (LFP), a parallel structure extracts both group-level features and\nglobal context, providing a rich representation of both fine details and\noverall structure. Finally, these features are combined and processed in the\nGlobal Feature Processing (GFP). By repeating these operations, the receptive\nfield gradually expands, enabling the model to capture complete geometric\ninformation of the point cloud. To overcome the high parameter counts and\ncomputational inefficiency of standard KANs, we develop Efficient-KANs in the\nPointKAN-elite variant, which significantly reduces parameters while\nmaintaining accuracy. Experimental results demonstrate that PointKAN\noutperforms PointMLP on benchmark datasets such as ModelNet40, ScanObjectNN,\nand ShapeNetPart, with particularly strong performance in Few-shot Learning\ntask. Additionally, PointKAN achieves substantial reductions in parameter\ncounts and computational complexity (FLOPs). This work highlights the potential\nof KANs-based architectures in 3D vision and opens new avenues for research in\npoint cloud understanding.\n","authors":["Yan Shi","Qingdong He","Yijun Liu","Xiaoyu Liu","Jingyong Su"],"pdf_url":"https://arxiv.org/pdf/2504.13593v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23095v1","updated":"2025-10-27T08:00:46Z","published":"2025-10-27T08:00:46Z","title":"Revisiting Multimodal Positional Encoding in Vision-Language Models","summary":"  Multimodal position encoding is essential for vision-language models, yet\nthere has been little systematic investigation into multimodal position\nencoding. We conduct a comprehensive analysis of multimodal Rotary Positional\nEmbedding (RoPE) by examining its two core components: position design and\nfrequency allocation. Through extensive experiments, we identify three key\nguidelines: positional coherence, full frequency utilization, and preservation\nof textual priors-ensuring unambiguous layout, rich representation, and\nfaithful transfer from the pre-trained LLM. Based on these insights, we propose\nMulti-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and\nplug-and-play variants that require no architectural changes. Our methods\nconsistently outperform existing approaches across diverse benchmarks, with\nsignificant improvements in both general and fine-grained multimodal\nunderstanding. Code will be avaliable at\nhttps://github.com/JJJYmmm/Multimodal-RoPEs.\n","authors":["Jie Huang","Xuejing Liu","Sibo Song","Ruibing Hou","Hong Chang","Junyang Lin","Shuai Bai"],"pdf_url":"https://arxiv.org/pdf/2510.23095v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2510.23087v1","updated":"2025-10-27T07:45:17Z","published":"2025-10-27T07:45:17Z","title":"EndoWave: Rational-Wavelet 4D Gaussian Splatting for Endoscopic\n  Reconstruction","summary":"  In robot-assisted minimally invasive surgery, accurate 3D reconstruction from\nendoscopic video is vital for downstream tasks and improved outcomes. However,\nendoscopic scenarios present unique challenges, including photometric\ninconsistencies, non-rigid tissue motion, and view-dependent highlights. Most\n3DGS-based methods that rely solely on appearance constraints for optimizing\n3DGS are often insufficient in this context, as these dynamic visual artifacts\ncan mislead the optimization process and lead to inaccurate reconstructions. To\naddress these limitations, we present EndoWave, a unified spatio-temporal\nGaussian Splatting framework by incorporating an optical flow-based geometric\nconstraint and a multi-resolution rational wavelet supervision. First, we adopt\na unified spatio-temporal Gaussian representation that directly optimizes\nprimitives in a 4D domain. Second, we propose a geometric constraint derived\nfrom optical flow to enhance temporal coherence and effectively constrain the\n3D structure of the scene. Third, we propose a multi-resolution rational\northogonal wavelet as a constraint, which can effectively separate the details\nof the endoscope and enhance the rendering performance. Extensive evaluations\non two real surgical datasets, EndoNeRF and StereoMIS, demonstrate that our\nmethod EndoWave achieves state-of-the-art reconstruction quality and visual\naccuracy compared to the baseline method.\n","authors":["Taoyu Wu","Yiyi Miao","Jiaxin Guo","Ziyan Chen","Sihang Zhao","Zhuoxiao Li","Zhe Tang","Baoru Huang","Limin Yu"],"pdf_url":"https://arxiv.org/pdf/2510.23087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20625v3","updated":"2025-10-27T07:31:06Z","published":"2025-02-28T01:09:18Z","title":"T2ICount: Enhancing Cross-modal Understanding for Zero-Shot Counting","summary":"  Zero-shot object counting aims to count instances of arbitrary object\ncategories specified by text descriptions. Existing methods typically rely on\nvision-language models like CLIP, but often exhibit limited sensitivity to text\nprompts. We present T2ICount, a diffusion-based framework that leverages rich\nprior knowledge and fine-grained visual understanding from pretrained diffusion\nmodels. While one-step denoising ensures efficiency, it leads to weakened text\nsensitivity. To address this challenge, we propose a Hierarchical Semantic\nCorrection Module that progressively refines text-image feature alignment, and\na Representational Regional Coherence Loss that provides reliable supervision\nsignals by leveraging the cross-attention maps extracted from the denosing\nU-Net. Furthermore, we observe that current benchmarks mainly focus on majority\nobjects in images, potentially masking models' text sensitivity. To address\nthis, we contribute a challenging re-annotated subset of FSC147 for better\nevaluation of text-guided counting ability. Extensive experiments demonstrate\nthat our method achieves superior performance across different benchmarks. Code\nis available at https://github.com/cha15yq/T2ICount.\n","authors":["Yifei Qian","Zhongliang Guo","Bowen Deng","Chun Tong Lei","Shuai Zhao","Chun Pong Lau","Xiaopeng Hong","Michael P. Pound"],"pdf_url":"https://arxiv.org/pdf/2502.20625v3.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2509.07295v3","updated":"2025-10-27T07:29:43Z","published":"2025-09-08T23:59:32Z","title":"Reconstruction Alignment Improves Unified Multimodal Models","summary":"  Unified multimodal models (UMMs) unify visual understanding and generation\nwithin a single architecture. However, conventional training relies on\nimage-text pairs (or sequences) whose captions are typically sparse and miss\nfine-grained visual details--even when they use hundreds of words to describe a\nsimple image. We introduce Reconstruction Alignment (RecA), a\nresource-efficient post-training method that leverages visual understanding\nencoder embeddings as dense \"text prompts,\" providing rich supervision without\ncaptions. Concretely, RecA conditions a UMM on its own visual understanding\nembeddings and optimizes it to reconstruct the input image with a\nself-supervised reconstruction loss, thereby realigning understanding and\ngeneration. Despite its simplicity, RecA is broadly applicable: across\nautoregressive, masked-autoregressive, and diffusion-based UMMs, it\nconsistently improves generation and editing fidelity. With only 27 GPU-hours,\npost-training with RecA substantially improves image generation performance on\nGenEval (0.73$\\rightarrow$0.90) and DPGBench (80.93$\\rightarrow$88.15), while\nalso boosting editing benchmarks (ImgEdit 3.38$\\rightarrow$3.75, GEdit\n6.94$\\rightarrow$7.25). Notably, RecA surpasses much larger open-source models\nand applies broadly across diverse UMM architectures, establishing it as an\nefficient and general post-training alignment strategy for UMMs\n","authors":["Ji Xie","Trevor Darrell","Luke Zettlemoyer","XuDong Wang"],"pdf_url":"https://arxiv.org/pdf/2509.07295v3.pdf","comment":"34 pages, 28 figures and 11 tables; Update ablation study"},{"id":"http://arxiv.org/abs/2510.23079v1","updated":"2025-10-27T07:29:28Z","published":"2025-10-27T07:29:28Z","title":"Strategies for Robust Deep Learning Based Deformable Registration","summary":"  Deep learning based deformable registration methods have become popular in\nrecent years. However, their ability to generalize beyond training data\ndistribution can be poor, significantly hindering their usability. LUMIR brain\nregistration challenge for Learn2Reg 2025 aims to advance the field by\nevaluating the performance of the registration on contrasts and modalities\ndifferent from those included in the training set. Here we describe our\nsubmission to the challenge, which proposes a very simple idea for\nsignificantly improving robustness by transforming the images into MIND feature\nspace before feeding them into the model. In addition, a special ensembling\nstrategy is proposed that shows a small but consistent improvement.\n","authors":["Joel Honkamaa","Pekka Marttinen"],"pdf_url":"https://arxiv.org/pdf/2510.23079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23943v1","updated":"2025-10-27T23:52:46Z","published":"2025-10-27T23:52:46Z","title":"Adaptive Training of INRs via Pruning and Densification","summary":"  Encoding input coordinates with sinusoidal functions into multilayer\nperceptrons (MLPs) has proven effective for implicit neural representations\n(INRs) of low-dimensional signals, enabling the modeling of high-frequency\ndetails. However, selecting appropriate input frequencies and architectures\nwhile managing parameter redundancy remains an open challenge, often addressed\nthrough heuristics and heavy hyperparameter optimization schemes. In this\npaper, we introduce AIRe ($\\textbf{A}$daptive $\\textbf{I}$mplicit neural\n$\\textbf{Re}$presentation), an adaptive training scheme that refines the INR\narchitecture over the course of optimization. Our method uses a neuron pruning\nmechanism to avoid redundancy and input frequency densification to improve\nrepresentation capacity, leading to an improved trade-off between network size\nand reconstruction quality. For pruning, we first identify less-contributory\nneurons and apply a targeted weight decay to transfer their information to the\nremaining neurons, followed by structured pruning. Next, the densification\nstage adds input frequencies to spectrum regions where the signal underfits,\nexpanding the representational basis. Through experiments on images and SDFs,\nwe show that AIRe reduces model size while preserving, or even improving,\nreconstruction quality. Code and pretrained models will be released for public\nuse.\n","authors":["Diana Aldana","João Paulo Lima","Daniel Csillag","Daniel Perazzo","Haoan Feng","Luiz Velho","Tiago Novello"],"pdf_url":"https://arxiv.org/pdf/2510.23943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06456v3","updated":"2025-10-27T23:40:26Z","published":"2025-03-09T05:30:15Z","title":"DynCIM: Dynamic Curriculum for Imbalanced Multimodal Learning","summary":"  Multimodal learning integrates complementary information from diverse\nmodalities to enhance the decision-making process. However, the potential of\nmultimodal collaboration remains under-exploited due to disparities in data\nquality and modality representation capabilities. To address this, we introduce\nDynCIM, a novel dynamic curriculum learning framework designed to quantify the\ninherent imbalances from both sample and modality perspectives. DynCIM employs\na sample-level curriculum to dynamically assess each sample's difficulty\naccording to prediction deviation, consistency, and stability, while a\nmodality-level curriculum measures modality contributions from global and\nlocal. Furthermore, a gating-based dynamic fusion mechanism is introduced to\nadaptively adjust modality contributions, minimizing redundancy and optimizing\nfusion effectiveness. Extensive experiments on six multimodal benchmarking\ndatasets, spanning both bimodal and trimodal scenarios, demonstrate that DynCIM\nconsistently outperforms state-of-the-art methods. Our approach effectively\nmitigates modality and sample imbalances while enhancing adaptability and\nrobustness in multimodal learning tasks. Our code is available at\nhttps://github.com/Raymond-Qiancx/DynCIM.\n","authors":["Chengxuan Qian","Kai Han","Jiaxin Liu","Zhenlong Yuan","Zhengzhong Zhu","Jingchao Wang","Chongwen Lyu","Jun Chen","Zhe Liu"],"pdf_url":"https://arxiv.org/pdf/2503.06456v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11049v2","updated":"2025-10-27T23:33:48Z","published":"2025-02-16T09:23:16Z","title":"Faces of Fairness: Examining Bias in Facial Expression Recognition\n  Datasets and Models","summary":"  Building AI systems, including Facial Expression Recognition (FER), involves\ntwo critical aspects: data and model design. Both components significantly\ninfluence bias and fairness in FER tasks. Issues related to bias and fairness\nin FER datasets and models remain underexplored. This study investigates bias\nsources in FER datasets and models. Four common FER datasets--AffectNet, ExpW,\nFer2013, and RAF-DB--are analyzed. The findings demonstrate that AffectNet and\nExpW exhibit high generalizability despite data imbalances. Additionally, this\nresearch evaluates the bias and fairness of six deep models, including three\nstate-of-the-art convolutional neural network (CNN) models: MobileNet, ResNet,\nXceptionNet, as well as three transformer-based models: ViT, CLIP, and\nGPT-4o-mini. Experimental results reveal that while GPT-4o-mini and ViT achieve\nthe highest accuracy scores, they also display the highest levels of bias.\nThese findings underscore the urgent need for developing new methodologies to\nmitigate bias and ensure fairness in datasets and models, particularly in\naffective computing applications. See our implementation details at\nhttps://github.com/MMHosseini/bias_in_FER.\n","authors":["Mohammad Mehdi Hosseini","Ali Pourramezan Fard","Mohammad H. Mahoor"],"pdf_url":"https://arxiv.org/pdf/2502.11049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23930v1","updated":"2025-10-27T23:32:19Z","published":"2025-10-27T23:32:19Z","title":"PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by\n  Vision-Language Planar Priors","summary":"  Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an\nefficient representation for novel-view synthesis, achieving impressive visual\nquality. However, in scenes dominated by large and low-texture regions, common\nin indoor environments, the photometric loss used to optimize 3DGS yields\nambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome\nthis limitation, we introduce PlanarGS, a 3DGS-based framework tailored for\nindoor scene reconstruction. Specifically, we design a pipeline for\nLanguage-Prompted Planar Priors (LP3) that employs a pretrained vision-language\nsegmentation model and refines its region proposals via cross-view fusion and\ninspection with geometric priors. 3D Gaussians in our framework are optimized\nwith two additional terms: a planar prior supervision term that enforces planar\nconsistency, and a geometric prior supervision term that steers the Gaussians\ntoward the depth and normal cues. We have conducted extensive experiments on\nstandard indoor benchmarks. The results show that PlanarGS reconstructs\naccurate and detailed 3D surfaces, consistently outperforming state-of-the-art\nmethods by a large margin. Project page: https://planargs.github.io\n","authors":["Xirui Jin","Renbiao Jin","Boying Li","Danping Zou","Wenxian Yu"],"pdf_url":"https://arxiv.org/pdf/2510.23930v1.pdf","comment":"Accepted by NeurIPS 2025. Project page: https://planargs.github.io"},{"id":"http://arxiv.org/abs/2510.23929v1","updated":"2025-10-27T23:28:11Z","published":"2025-10-27T23:28:11Z","title":"TurboPortrait3D: Single-step diffusion-based fast portrait novel-view\n  synthesis","summary":"  We introduce TurboPortrait3D: a method for low-latency novel-view synthesis\nof human portraits. Our approach builds on the observation that existing\nimage-to-3D models for portrait generation, while capable of producing\nrenderable 3D representations, are prone to visual artifacts, often lack of\ndetail, and tend to fail at fully preserving the identity of the subject. On\nthe other hand, image diffusion models excel at generating high-quality images,\nbut besides being computationally expensive, are not grounded in 3D and thus\nare not directly capable of producing multi-view consistent outputs. In this\nwork, we demonstrate that image-space diffusion models can be used to\nsignificantly enhance the quality of existing image-to-avatar methods, while\nmaintaining 3D-awareness and running with low-latency. Our method takes a\nsingle frontal image of a subject as input, and applies a feedforward\nimage-to-avatar generation pipeline to obtain an initial 3D representation and\ncorresponding noisy renders. These noisy renders are then fed to a single-step\ndiffusion model which is conditioned on input image(s), and is specifically\ntrained to refine the renders in a multi-view consistent way. Moreover, we\nintroduce a novel effective training strategy that includes pre-training on a\nlarge corpus of synthetic multi-view data, followed by fine-tuning on\nhigh-quality real images. We demonstrate that our approach both qualitatively\nand quantitatively outperforms current state-of-the-art for portrait novel-view\nsynthesis, while being efficient in time.\n","authors":["Emily Kim","Julieta Martinez","Timur Bagautdinov","Jessica Hodgins"],"pdf_url":"https://arxiv.org/pdf/2510.23929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23928v1","updated":"2025-10-27T23:25:57Z","published":"2025-10-27T23:25:57Z","title":"Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in\n  Dynamic Environments","summary":"  In this paper, we propose an adaptive keyframe selection method for improved\n3D scene reconstruction in dynamic environments. The proposed method integrates\ntwo complementary modules: an error-based selection module utilizing\nphotometric and structural similarity (SSIM) errors, and a momentum-based\nupdate module that dynamically adjusts keyframe selection thresholds according\nto scene motion dynamics. By dynamically curating the most informative frames,\nour approach addresses a key data bottleneck in real-time perception. This\nallows for the creation of high-quality 3D world representations from a\ncompressed data stream, a critical step towards scalable robot learning and\ndeployment in complex, dynamic environments. Experimental results demonstrate\nsignificant improvements over traditional static keyframe selection strategies,\nsuch as fixed temporal intervals or uniform frame skipping. These findings\nhighlight a meaningful advancement toward adaptive perception systems that can\ndynamically respond to complex and evolving visual scenes. We evaluate our\nproposed adaptive keyframe selection module on two recent state-of-the-art 3D\nreconstruction networks, Spann3r and CUT3R, and observe consistent improvements\nin reconstruction quality across both frameworks. Furthermore, an extensive\nablation study confirms the effectiveness of each individual component in our\nmethod, underlining their contribution to the overall performance gains.\n","authors":["Raman Jha","Yang Zhou","Giuseppe Loianno"],"pdf_url":"https://arxiv.org/pdf/2510.23928v1.pdf","comment":"Under Review for ROBOVIS 2026"},{"id":"http://arxiv.org/abs/2509.23311v2","updated":"2025-10-27T23:22:21Z","published":"2025-09-27T13:56:12Z","title":"Seeing Symbols, Missing Cultures: Probing Vision-Language Models'\n  Reasoning on Fire Imagery and Cultural Meaning","summary":"  Vision-Language Models (VLMs) often appear culturally competent but rely on\nsuperficial pattern matching rather than genuine cultural understanding. We\nintroduce a diagnostic framework to probe VLM reasoning on fire-themed cultural\nimagery through both classification and explanation analysis. Testing multiple\nmodels on Western festivals, non-Western traditions, and emergency scenes\nreveals systematic biases: models correctly identify prominent Western\nfestivals but struggle with underrepresented cultural events, frequently\noffering vague labels or dangerously misclassifying emergencies as\ncelebrations. These failures expose the risks of symbolic shortcuts and\nhighlight the need for cultural evaluation beyond accuracy metrics to ensure\ninterpretable and fair multimodal systems.\n","authors":["Haorui Yu","Yang Zhao","Yijia Chu","Qiufeng Yi"],"pdf_url":"https://arxiv.org/pdf/2509.23311v2.pdf","comment":"8 pages, 5 figures, 4 tables. Submitted to WiNLP 2025 Workshop at\n  COLING 2025"},{"id":"http://arxiv.org/abs/2412.02076v3","updated":"2025-10-27T22:59:21Z","published":"2024-12-03T01:38:15Z","title":"Topology-Preserving Image Segmentation with Spatial-Aware Persistent\n  Feature Matching","summary":"  Topological correctness is critical for segmentation of tubular structures,\nwhich pervade in biomedical images. Existing topological segmentation loss\nfunctions are primarily based on the persistent homology of the image. They\nmatch the persistent features from the segmentation with the persistent\nfeatures from the ground truth and minimize the difference between them.\nHowever, these methods suffer from an ambiguous matching problem since the\nmatching only relies on the information in the topological space. In this work,\nwe propose an effective and efficient Spatial-Aware Topological Loss Function\nthat further leverages the information in the original spatial domain of the\nimage to assist the matching of persistent features. Extensive experiments on\nimages of various types of tubular structures show that the proposed method has\nsuperior performance in improving the topological accuracy of the segmentation\ncompared with state-of-the-art methods. Code is available at\nhttps://github.com/JRC-VPLab/SATLoss.\n","authors":["Bo Wen","Haochen Zhang","Dirk-Uwe G. Bartsch","William R. Freeman","Truong Q. Nguyen","Cheolhong An"],"pdf_url":"https://arxiv.org/pdf/2412.02076v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23907v1","updated":"2025-10-27T22:29:08Z","published":"2025-10-27T22:29:08Z","title":"DynaStride: Dynamic Stride Windowing with MMCoT for Instructional\n  Multi-Scene Captioning","summary":"  Scene-level captioning in instructional videos can enhance learning by\nrequiring an understanding of both visual cues and temporal structure. By\naligning visual cues with textual guidance, this understanding supports\nprocedural learning and multimodal reasoning, providing a richer context for\nskill acquisition. However, captions that fail to capture this structure may\nlack coherence and quality, which can create confusion and undermine the\nvideo's educational intent. To address this gap, we introduce DynaStride, a\npipeline to generate coherent, scene-level captions without requiring manual\nscene segmentation. Using the YouCookII dataset's scene annotations, DynaStride\nperforms adaptive frame sampling and multimodal windowing to capture key\ntransitions within each scene. It then employs a multimodal chain-of-thought\nprocess to produce multiple action-object pairs, which are refined and fused\nusing a dynamic stride window selection algorithm that adaptively balances\ntemporal context and redundancy. The final scene-level caption integrates\nvisual semantics and temporal reasoning in a single instructional caption.\nEmpirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,\ndemonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and\nsemantic similarity measures (BERTScore, CLIPScore). Qualitative analyses\nfurther show that DynaStride produces captions that are more temporally\ncoherent and informative, suggesting a promising direction for improving\nAI-powered instructional content generation.\n","authors":["Eddison Pham","Prisha Priyadarshini","Adrian Maliackel","Kanishk Bandi","Cristian Meo","Kevin Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.23907v1.pdf","comment":"16 pages, 15 figures, 5 Tables, submitted to AAAI AI4ED Workshop 2026"},{"id":"http://arxiv.org/abs/2405.20336v2","updated":"2025-10-27T22:19:03Z","published":"2024-05-30T17:59:39Z","title":"RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text","summary":"  In this work, we introduce a challenging task for simultaneously generating\n3D holistic body motions and singing vocals directly from textual lyrics\ninputs, advancing beyond existing works that typically address these two\nmodalities in isolation. To facilitate this, we first collect the RapVerse\ndataset, a large dataset containing synchronous rapping vocals, lyrics, and\nhigh-quality 3D holistic body meshes. With the RapVerse dataset, we investigate\nthe extent to which scaling autoregressive multimodal transformers across\nlanguage, audio, and motion can enhance the coherent and realistic generation\nof vocals and whole-body human motions. For modality unification, a\nvector-quantized variational autoencoder is employed to encode whole-body\nmotion sequences into discrete motion tokens, while a vocal-to-unit model is\nleveraged to obtain quantized audio tokens preserving content, prosodic\ninformation and singer identity. By jointly performing transformer modeling on\nthese three modalities in a unified way, our framework ensures a seamless and\nrealistic blend of vocals and human motions. Extensive experiments demonstrate\nthat our unified generation framework not only produces coherent and realistic\nsinging vocals alongside human motions directly from textual inputs, but also\nrivals the performance of specialized single-modality generation systems,\nestablishing new benchmarks for joint vocal-motion generation.\n","authors":["Jiaben Chen","Xin Yan","Yihang Chen","Siyuan Cen","Zixin Wang","Qinwei Ma","Haoyu Zhen","Kaizhi Qian","Lie Lu","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2405.20336v2.pdf","comment":"ICCV 2025, Project website: https://jiabenchen.github.io/RapVerse/"},{"id":"http://arxiv.org/abs/2510.23894v1","updated":"2025-10-27T22:05:08Z","published":"2025-10-27T22:05:08Z","title":"Improving Visual Discriminability of CLIP for Training-Free\n  Open-Vocabulary Semantic Segmentation","summary":"  Extending CLIP models to semantic segmentation remains challenging due to the\nmisalignment between their image-level pre-training objectives and the\npixel-level visual understanding required for dense prediction. While prior\nefforts have achieved encouraging results by reorganizing the final layer and\nfeatures, they often inherit the global alignment bias of preceding layers,\nleading to suboptimal segmentation performance. In this work, we propose\nLHT-CLIP, a novel training-free framework that systematically exploits the\nvisual discriminability of CLIP across layer, head, and token levels. Through\ncomprehensive analysis, we reveal three key insights: (i) the final layers\nprimarily strengthen image-text alignment with sacrifice of visual\ndiscriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),\npartly due to the emergence of anomalous tokens; (ii) a subset of attention\nheads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual\ndiscriminability across datasets; (iii) abnormal tokens display sparse and\nconsistent activation pattern compared to normal tokens. Based on these\nfindings, we propose three complementary techniques: semantic-spatial\nreweighting, selective head enhancement, and abnormal token replacement to\neffectively restore visual discriminability and improve segmentation\nperformance without any additional training, auxiliary pre-trained networks, or\nextensive hyperparameter tuning. Extensive experiments on 8 common semantic\nsegmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art\nperformance across diverse scenarios, highlighting its effectiveness and\npracticality for real-world deployment.\n","authors":["Jinxin Zhou","Jiachen Jiang","Zhihui Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.23894v1.pdf","comment":"23 pages, 10 figures, 14 tables"},{"id":"http://arxiv.org/abs/2510.23880v1","updated":"2025-10-27T21:40:31Z","published":"2025-10-27T21:40:31Z","title":"TRELLISWorld: Training-Free World Generation from Object Generators","summary":"  Text-driven 3D scene generation holds promise for a wide range of\napplications, from virtual prototyping to AR/VR and simulation. However,\nexisting methods are often constrained to single-object generation, require\ndomain-specific training, or lack support for full 360-degree viewability. In\nthis work, we present a training-free approach to 3D scene synthesis by\nrepurposing general-purpose text-to-3D object diffusion models as modular tile\ngenerators. We reformulate scene generation as a multi-tile denoising problem,\nwhere overlapping 3D regions are independently generated and seamlessly blended\nvia weighted averaging. This enables scalable synthesis of large, coherent\nscenes while preserving local semantic control. Our method eliminates the need\nfor scene-level datasets or retraining, relies on minimal heuristics, and\ninherits the generalization capabilities of object-level priors. We demonstrate\nthat our approach supports diverse scene layouts, efficient generation, and\nflexible editing, establishing a simple yet powerful foundation for\ngeneral-purpose, language-driven 3D scene construction.\n","authors":["Hanke Chen","Yuan Liu","Minchen Li"],"pdf_url":"https://arxiv.org/pdf/2510.23880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.07647v7","updated":"2025-10-27T20:14:22Z","published":"2023-04-15T22:24:05Z","title":"LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene\n  Graphs with Weak Supervision","summary":"  Supervised approaches for learning spatio-temporal scene graphs (STSG) from\nvideo are greatly hindered due to their reliance on STSG-annotated videos,\nwhich are labor-intensive to construct at scale. Is it feasible to instead use\nreadily available video captions as weak supervision? To address this question,\nwe propose LASER, a neuro-symbolic framework to enable training STSG generators\nusing only video captions. LASER employs large language models to first extract\nlogical specifications with rich spatio-temporal semantic information from\nvideo captions. LASER then trains the underlying STSG generator to align the\npredicted STSG with the specification. The alignment algorithm overcomes the\nchallenges of weak supervision by leveraging a differentiable symbolic reasoner\nand using a combination of contrastive, temporal, and semantics losses. The\noverall approach efficiently trains low-level perception models to extract a\nfine-grained STSG that conforms to the video caption. In doing so, it enables a\nnovel methodology for learning STSGs without tedious annotations. We evaluate\nour method on three video datasets: OpenPVSG, 20BN, and MUGEN. Our approach\ndemonstrates substantial improvements over fully-supervised baselines,\nachieving a unary predicate prediction accuracy of 27.78% (+12.65%) and a\nbinary recall@5 of 0.42 (+0.22) on OpenPVSG. Additionally, LASER exceeds\nbaselines by 7% on 20BN and 5.2% on MUGEN in terms of overall predicate\nprediction accuracy.\n","authors":["Jiani Huang","Ziyang Li","Mayur Naik","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2304.07647v7.pdf","comment":"Accepted at International Conference on Learning Representations\n  (ICLR) 2025"},{"id":"http://arxiv.org/abs/2510.23816v1","updated":"2025-10-27T19:56:43Z","published":"2025-10-27T19:56:43Z","title":"RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution\n  of Rare-Earth Features","summary":"  Super-resolution (SR) for remote sensing imagery often fails under\nout-of-distribution (OOD) conditions, such as rare geomorphic features captured\nby diverse sensors, producing visually plausible but physically inaccurate\nresults. We present RareFlow, a physics-aware SR framework designed for OOD\nrobustness. RareFlow's core is a dual-conditioning architecture. A Gated\nControlNet preserves fine-grained geometric fidelity from the low-resolution\ninput, while textual prompts provide semantic guidance for synthesizing complex\nfeatures. To ensure physically sound outputs, we introduce a multifaceted loss\nfunction that enforces both spectral and radiometric consistency with sensor\nproperties. Furthermore, the framework quantifies its own predictive\nuncertainty by employing a stochastic forward pass approach; the resulting\noutput variance directly identifies unfamiliar inputs, mitigating feature\nhallucination. We validate RareFlow on a new, curated benchmark of multi-sensor\nsatellite imagery. In blind evaluations, geophysical experts rated our model's\noutputs as approaching the fidelity of ground truth imagery, significantly\noutperforming state-of-the-art baselines. This qualitative superiority is\ncorroborated by quantitative gains in perceptual metrics, including a nearly\n40\\% reduction in FID. RareFlow provides a robust framework for high-fidelity\nsynthesis in data-scarce scientific domains and offers a new paradigm for\ncontrolled generation under severe domain shift.\n","authors":["Forouzan Fallah","Wenwen Li","Chia-Yu Hsu","Hyunho Lee","Yezhou Yang"],"pdf_url":"https://arxiv.org/pdf/2510.23816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.15710v2","updated":"2025-10-27T19:55:52Z","published":"2025-10-17T14:54:58Z","title":"UniMedVL: Unifying Medical Multimodal Understanding And Generation\n  Through Observation-Knowledge-Analysis","summary":"  Medical diagnostic applications require models that can process multimodal\nmedical inputs (images, patient histories, lab results) and generate diverse\noutputs including both textual reports and visual content (annotations,\nsegmentation masks, and images). Despite this need, existing medical AI systems\ndisrupt this unified process: medical image understanding models interpret\nimages but cannot generate visual outputs, while medical image generation\nmodels synthesize images but cannot provide textual explanations. This leads to\ngaps in data representation, feature integration, and task-level multimodal\ncapabilities. To this end, we propose a multi-level framework that draws\ninspiration from diagnostic workflows through the\nObservation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation\nlevel, we construct UniMed-5M, a dataset comprising over 5.6M samples that\nreformat diverse unimodal data into multimodal pairs for foundational\nobservation. At the knowledge level, we propose Progressive Curriculum Learning\nthat systematically introduces medical multimodal knowledge. At the analysis\nlevel, we introduce UniMedVL, the first medical unified multimodal model for\nthe simultaneous analysis of image understanding and generation tasks within a\nsingle architecture. UniMedVL achieves superior performance on five medical\nimage understanding benchmarks, while matching specialized models in generation\nquality across eight medical imaging modalities. Crucially, our unified\narchitecture enables bidirectional knowledge sharing: generation tasks enhance\nvisual understanding features, demonstrating that integrating traditionally\nseparate capabilities within a single medical framework unlocks improvements\nacross diverse medical vision-language tasks. Code is available at\nhttps://github.com/uni-medical/UniMedVL.\n","authors":["Junzhi Ning","Wei Li","Cheng Tang","Jiashi Lin","Chenglong Ma","Chaoyang Zhang","Jiyao Liu","Ying Chen","Shujian Gao","Lihao Liu","Yuandong Pu","Huihui Xu","Chenhui Gou","Ziyan Huang","Yi Xin","Qi Qin","Zhongying Deng","Diping Song","Bin Fu","Guang Yang","Yuanfeng Ji","Tianbin Li","Yanzhou Su","Jin Ye","Shixiang Tang","Ming Hu","Junjun He"],"pdf_url":"https://arxiv.org/pdf/2510.15710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22366v5","updated":"2025-10-27T19:52:38Z","published":"2024-10-28T19:01:18Z","title":"One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion\n  Models","summary":"  For large language models (LLMs), sparse autoencoders (SAEs) have been shown\nto decompose intermediate representations that often are not interpretable\ndirectly into sparse sums of interpretable features, facilitating better\ncontrol and subsequent analysis. However, similar analyses and approaches have\nbeen lacking for text-to-image models. We investigate the possibility of using\nSAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image\ndiffusion model. To this end, we train SAEs on the updates performed by\ntransformer blocks within SDXL Turbo's denoising U-net in its 1-step setting.\nInterestingly, we find that they generalize to 4-step SDXL Turbo and even to\nthe multi-step SDXL base model (i.e., a different model) without additional\ntraining. In addition, we show that their learned features are interpretable,\ncausally influence the generation process, and reveal specialization among the\nblocks. We do so by creating RIEBench, a representation-based image editing\nbenchmark, for editing images while they are generated by turning on and off\nindividual SAE features. This allows us to track which transformer blocks'\nfeatures are the most impactful depending on the edit category. Our work is the\nfirst investigation of SAEs for interpretability in text-to-image diffusion\nmodels and our results establish SAEs as a promising approach for understanding\nand manipulating the internal mechanisms of text-to-image models.\n","authors":["Viacheslav Surkov","Chris Wendler","Antonio Mari","Mikhail Terekhov","Justin Deschenaux","Robert West","Caglar Gulcehre","David Bau"],"pdf_url":"https://arxiv.org/pdf/2410.22366v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23807v1","updated":"2025-10-27T19:44:52Z","published":"2025-10-27T19:44:52Z","title":"Why Foundation Models in Pathology Are Failing","summary":"  In non-medical domains, foundation models (FMs) have revolutionized computer\nvision and language processing through large-scale self-supervised and\nmultimodal learning. Consequently, their rapid adoption in computational\npathology was expected to deliver comparable breakthroughs in cancer diagnosis,\nprognostication, and multimodal retrieval. However, recent systematic\nevaluations reveal fundamental weaknesses: low diagnostic accuracy, poor\nrobustness, geometric instability, heavy computational demands, and concerning\nsafety vulnerabilities. This short paper examines these shortcomings and argues\nthat they stem from deeper conceptual mismatches between the assumptions\nunderlying generic foundation modeling in mainstream AI and the intrinsic\ncomplexity of human tissue. Seven interrelated causes are identified:\nbiological complexity, ineffective self-supervision, overgeneralization,\nexcessive architectural complexity, lack of domain-specific innovation,\ninsufficient data, and a fundamental design flaw related to tissue patch size.\nThese findings suggest that current pathology foundation models remain\nconceptually misaligned with the nature of tissue morphology and call for a\nfundamental rethinking of the paradigm itself.\n","authors":["Hamid R. Tizhoosh"],"pdf_url":"https://arxiv.org/pdf/2510.23807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23798v1","updated":"2025-10-27T19:29:14Z","published":"2025-10-27T19:29:14Z","title":"A geometric and deep learning reproducible pipeline for monitoring\n  floating anthropogenic debris in urban rivers using in situ cameras","summary":"  The proliferation of floating anthropogenic debris in rivers has emerged as a\npressing environmental concern, exerting a detrimental influence on\nbiodiversity, water quality, and human activities such as navigation and\nrecreation. The present study proposes a novel methodological framework for the\nmonitoring the aforementioned waste, utilising fixed, in-situ cameras. This\nstudy provides two key contributions: (i) the continuous quantification and\nmonitoring of floating debris using deep learning and (ii) the identification\nof the most suitable deep learning model in terms of accuracy and inference\nspeed under complex environmental conditions. These models are tested in a\nrange of environmental conditions and learning configurations, including\nexperiments on biases related to data leakage. Furthermore, a geometric model\nis implemented to estimate the actual size of detected objects from a 2D image.\nThis model takes advantage of both intrinsic and extrinsic characteristics of\nthe camera. The findings of this study underscore the significance of the\ndataset constitution protocol, particularly with respect to the integration of\nnegative images and the consideration of temporal leakage. In conclusion, the\nfeasibility of metric object estimation using projective geometry coupled with\nregression corrections is demonstrated. This approach paves the way for the\ndevelopment of robust, low-cost, automated monitoring systems for urban aquatic\nenvironments.\n","authors":["Gauthier Grimmer","Romain Wenger","Clément Flint","Germain Forestier","Gilles Rixhon","Valentin Chardon"],"pdf_url":"https://arxiv.org/pdf/2510.23798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23785v1","updated":"2025-10-27T19:16:02Z","published":"2025-10-27T19:16:02Z","title":"CountFormer: A Transformer Framework for Learning Visual Repetition and\n  Structure in Class-Agnostic Object Counting","summary":"  Humans can effortlessly count diverse objects by perceiving visual repetition\nand structural relationships rather than relying on class identity. However,\nmost existing counting models fail to replicate this ability; they often\nmiscount when objects exhibit complex shapes, internal symmetry, or overlapping\ncomponents. In this work, we introduce CountFormer, a transformer-based\nframework that learns to recognize repetition and structural coherence for\nclass-agnostic object counting. Built upon the CounTR architecture, our model\nreplaces its visual encoder with the self-supervised foundation model DINOv2,\nwhich produces richer and spatially consistent feature representations. We\nfurther incorporate positional embedding fusion to preserve geometric\nrelationships before decoding these features into density maps through a\nlightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model\nachieves performance comparable to current state-of-the-art methods while\ndemonstrating superior accuracy on structurally intricate or densely packed\nscenes. Our findings indicate that integrating foundation models such as DINOv2\nenables counting systems to approach human-like structural perception,\nadvancing toward a truly general and exemplar-free counting paradigm.\n","authors":["Md Tanvir Hossain","Akif Islam","Mohd Ruhul Ameen"],"pdf_url":"https://arxiv.org/pdf/2510.23785v1.pdf","comment":"6 pages, 2 tables, 6 figures. Submitted to IEEE 5th International\n  Conference on Electrical, Computer and Telecommunication Engineering (ICECTE\n  2025)"},{"id":"http://arxiv.org/abs/2510.15870v2","updated":"2025-10-27T19:12:55Z","published":"2025-10-17T17:59:59Z","title":"OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding\n  LLM","summary":"  Advancing machine intelligence requires developing the ability to perceive\nacross multiple modalities, much as humans sense the world. We introduce\nOmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We\ncarefully study the design choices across model architecture and data curation.\nFor model architecture, we present three key innovations: (i) OmniAlignNet for\nstrengthening alignment between vision and audio embeddings in a shared\nomni-modal latent space; (ii) Temporal Embedding Grouping for capturing\nrelative temporal alignment between vision and audio signals; and (iii)\nConstrained Rotary Time Embedding for encoding absolute temporal information in\nomni-modal embeddings. We introduce a curation and synthesis pipeline that\ngenerates 24M single-modal and omni-modal conversations. We find that\nmodalities reinforce one another in both perception and reasoning. Our model,\nOmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal\nunderstanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while\nusing just 0.2T training tokens - a 6 times reduction compared to\nQwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream\napplications spanning robotics, medical AI, and smart factory.\n","authors":["Hanrong Ye","Chao-Han Huck Yang","Arushi Goel","Wei Huang","Ligeng Zhu","Yuanhang Su","Sean Lin","An-Chieh Cheng","Zhen Wan","Jinchuan Tian","Yuming Lou","Dong Yang","Zhijian Liu","Yukang Chen","Ambrish Dantrey","Ehsan Jahangiri","Sreyan Ghosh","Daguang Xu","Ehsan Hosseini-Asl","Danial Mohseni Taheri","Vidya Murali","Sifei Liu","Yao Lu","Oluwatobi Olabiyi","Yu-Chiang Frank Wang","Rafael Valle","Bryan Catanzaro","Andrew Tao","Song Han","Jan Kautz","Hongxu Yin","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2510.15870v2.pdf","comment":"Technical Report. Code: https://github.com/NVlabs/OmniVinci"},{"id":"http://arxiv.org/abs/2510.23775v1","updated":"2025-10-27T19:01:24Z","published":"2025-10-27T19:01:24Z","title":"Explainable Detection of AI-Generated Images with Artifact Localization\n  Using Faster-Than-Lies and Vision-Language Models for Edge Devices","summary":"  The increasing realism of AI-generated imagery poses challenges for verifying\nvisual authenticity. We present an explainable image authenticity detection\nsystem that combines a lightweight convolutional classifier\n(\"Faster-Than-Lies\") with a Vision-Language Model (Qwen2-VL-7B) to classify,\nlocalize, and explain artifacts in 32x32 images. Our model achieves 96.5%\naccuracy on the extended CiFAKE dataset augmented with adversarial\nperturbations and maintains an inference time of 175ms on 8-core CPUs, enabling\ndeployment on local or edge devices. Using autoencoder-based reconstruction\nerror maps, we generate artifact localization heatmaps, which enhance\ninterpretability for both humans and the VLM. We further categorize 70 visual\nartifact types into eight semantic groups and demonstrate explainable text\ngeneration for each detected anomaly. This work highlights the feasibility of\ncombining visual and linguistic reasoning for interpretable authenticity\ndetection in low-resolution imagery and outlines potential cross-domain\napplications in forensics, industrial inspection, and social media moderation.\n","authors":["Aryan Mathur","Asaduddin Ahmed","Pushti Amit Vasoya","Simeon Kandan Sonar","Yasir Z","Madesh Kuppusamy"],"pdf_url":"https://arxiv.org/pdf/2510.23775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23763v1","updated":"2025-10-27T18:49:03Z","published":"2025-10-27T18:49:03Z","title":"RoboOmni: Proactive Robot Manipulation in Omni-modal Context","summary":"  Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.\n","authors":["Siyin Wang","Jinlan Fu","Feihong Liu","Xinzhe He","Huangxuan Wu","Junhao Shi","Kexin Huang","Zhaoye Fei","Jingjing Gong","Zuxuan Wu","Yugang Jiang","See-Kiong Ng","Tat-Seng Chua","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2510.23763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06131v2","updated":"2025-10-27T18:41:08Z","published":"2025-04-08T15:23:21Z","title":"FaceCloak: Learning to Protect Face Templates","summary":"  Generative models can reconstruct face images from encoded representations\n(templates) bearing remarkable likeness to the original face, raising security\nand privacy concerns. We present \\textsc{FaceCloak}, a neural network framework\nthat protects face templates by generating smart, renewable binary cloaks. Our\nmethod proactively thwarts inversion attacks by cloaking face templates with\nunique disruptors synthesized from a single face template on the fly while\nprovably retaining biometric utility and unlinkability. Our cloaked templates\ncan suppress sensitive attributes while generalizing to novel feature\nextraction schemes and outperform leading baselines in terms of biometric\nmatching and resiliency to reconstruction attacks. \\textsc{FaceCloak}-based\nmatching is extremely fast (inference time =0.28 ms) and light (0.57 MB). We\nhave released our \\href{https://github.com/sudban3089/FaceCloak.git}{code} for\nreproducible research.\n","authors":["Sudipta Banerjee","Anubhav Jain","Chinmay Hegde","Nasir Memon"],"pdf_url":"https://arxiv.org/pdf/2504.06131v2.pdf","comment":"Accepted in IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG 2025)"},{"id":"http://arxiv.org/abs/2510.23574v1","updated":"2025-10-27T17:44:56Z","published":"2025-10-27T17:44:56Z","title":"More Than Generation: Unifying Generation and Depth Estimation via\n  Text-to-Image Diffusion Models","summary":"  Generative depth estimation methods leverage the rich visual priors stored in\npre-trained text-to-image diffusion models, demonstrating astonishing zero-shot\ncapability. However, parameter updates during training lead to catastrophic\ndegradation in the image generation capability of the pre-trained model. We\nintroduce MERGE, a unified model for image generation and depth estimation,\nstarting from a fixed pre-trained text-to-image model. MERGE demonstrates that\nthe pre-trained text-to-image model can do more than image generation, but also\nexpand to depth estimation effortlessly. Specifically, MERGE introduces a\nplay-and-plug framework that enables seamless switching between image\ngeneration and depth estimation modes through simple and pluggable converters.\nMeanwhile, we propose a Group Reuse Mechanism to encourage parameter reuse and\nimprove the utilization of the additional learnable parameters. MERGE unleashes\nthe powerful depth estimation capability of the pre-trained text-to-image model\nwhile preserving its original image generation ability. Compared to other\nunified models for image generation and depth estimation, MERGE achieves\nstate-of-the-art performance across multiple depth estimation benchmarks. The\ncode will be made available at https://github.com/H-EmbodVis/MERGE\n","authors":["Hongkai Lin","Dingkang Liang","Mingyang Du","Xin Zhou","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2510.23574v1.pdf","comment":"Accepted by NeurIPS 2025. The code will be made available at\n  https://github.com/H-EmbodVis/MERGE"},{"id":"http://arxiv.org/abs/2506.04704v3","updated":"2025-10-27T07:27:25Z","published":"2025-06-05T07:26:34Z","title":"HoliSafe: Holistic Safety Benchmarking and Modeling for Vision-Language\n  Model","summary":"  Despite emerging efforts to enhance the safety of Vision-Language Models\n(VLMs), current approaches face two main shortcomings. 1) Existing\nsafety-tuning datasets and benchmarks only partially consider how image-text\ninteractions can yield harmful content, often overlooking contextually unsafe\noutcomes from seemingly benign pairs. This narrow coverage leaves VLMs\nvulnerable to jailbreak attacks in unseen configurations. 2) Prior methods rely\nprimarily on data-centric tuning, with limited architectural innovations to\nintrinsically strengthen safety. We address these gaps by introducing a\nholistic safety dataset and benchmark, \\textbf{HoliSafe}, that spans all five\nsafe/unsafe image-text combinations, providing a more robust basis for both\ntraining and evaluation (HoliSafe-Bench). We further propose a novel modular\nframework for enhancing VLM safety with a visual guard module (VGM) designed to\nassess the harmfulness of input images for VLMs. This module endows VLMs with a\ndual functionality: they not only learn to generate safer responses but can\nalso provide an interpretable harmfulness classification to justify their\nrefusal decisions. A significant advantage of this approach is its modularity;\nthe VGM is designed as a plug-in component, allowing for seamless integration\nwith diverse pre-trained VLMs across various scales. Experiments show that\nSafe-VLM with VGM, trained on our HoliSafe, achieves state-of-the-art safety\nperformance across multiple VLM benchmarks. Additionally, the HoliSafe-Bench\nitself reveals critical vulnerabilities in existing VLM models. We hope that\nHoliSafe and VGM will spur further research into robust and interpretable VLM\nsafety, expanding future avenues for multimodal alignment.\n","authors":["Youngwan Lee","Kangsan Kim","Kwanyong Park","Ilcahe Jung","Soojin Jang","Seanie Lee","Yong-Ju Lee","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2506.04704v3.pdf","comment":"Project page: https://youngwanlee.github.io/holisafe"},{"id":"http://arxiv.org/abs/2510.09107v2","updated":"2025-10-27T07:25:00Z","published":"2025-10-10T08:00:46Z","title":"A Novel Multi-branch ConvNeXt Architecture for Identifying Subtle\n  Pathological Features in CT Scans","summary":"  Intelligent analysis of medical imaging plays a crucial role in assisting\nclinical diagnosis, especially for identifying subtle pathological features.\nThis paper introduces a novel multi-branch ConvNeXt architecture designed\nspecifically for the nuanced challenges of medical image analysis. While\napplied here to the specific problem of COVID-19 diagnosis, the methodology\noffers a generalizable framework for classifying a wide range of pathologies\nfrom CT scans. The proposed model incorporates a rigorous end-to-end pipeline,\nfrom meticulous data preprocessing and augmentation to a disciplined two-phase\ntraining strategy that leverages transfer learning effectively. The\narchitecture uniquely integrates features extracted from three parallel\nbranches: Global Average Pooling, Global Max Pooling, and a new\nAttention-weighted Pooling mechanism. The model was trained and validated on a\ncombined dataset of 2,609 CT slices derived from two distinct datasets.\nExperimental results demonstrate a superior performance on the validation set,\nachieving a final ROC-AUC of 0.9937, a validation accuracy of 0.9757, and an\nF1-score of 0.9825 for COVID-19 cases, outperforming all previously reported\nmodels on this dataset. These findings indicate that a modern, multi-branch\narchitecture, coupled with careful data handling, can achieve performance\ncomparable to or exceeding contemporary state-of-the-art models, thereby\nproving the efficacy of advanced deep learning techniques for robust medical\ndiagnostics.\n","authors":["Irash Perera","Uthayasanker Thayasivam"],"pdf_url":"https://arxiv.org/pdf/2510.09107v2.pdf","comment":"Source Code : https://github.com/Irash-Perera/MedNeXt-Branch"},{"id":"http://arxiv.org/abs/2506.11147v2","updated":"2025-10-27T07:14:11Z","published":"2025-06-11T09:55:42Z","title":"3D-RAD: A Comprehensive 3D Radiology Med-VQA Dataset with Multi-Temporal\n  Analysis and Diverse Diagnostic Tasks","summary":"  Medical Visual Question Answering (Med-VQA) holds significant potential for\nclinical decision support, yet existing efforts primarily focus on 2D imaging\nwith limited task diversity. This paper presents 3D-RAD, a large-scale dataset\ndesigned to advance 3D Med-VQA using radiology CT scans. The 3D-RAD dataset\nencompasses six diverse VQA tasks: anomaly detection, image observation,\nmedical computation, existence detection, static temporal diagnosis, and\nlongitudinal temporal diagnosis. It supports both open- and closed-ended\nquestions while introducing complex reasoning challenges, including\ncomputational tasks and multi-stage temporal analysis, to enable comprehensive\nbenchmarking. Extensive evaluations demonstrate that existing vision-language\nmodels (VLMs), especially medical VLMs exhibit limited generalization,\nparticularly in multi-temporal tasks, underscoring the challenges of real-world\n3D diagnostic reasoning. To drive future advancements, we release a\nhigh-quality training set 3D-RAD-T of 136,195 expert-aligned samples, showing\nthat fine-tuning on this dataset could significantly enhance model performance.\nOur dataset and code, aiming to catalyze multimodal medical AI research and\nestablish a robust foundation for 3D medical visual understanding, are publicly\navailable at https://github.com/Tang-xiaoxiao/3D-RAD.\n","authors":["Xiaotang Gai","Jiaxiang Liu","Yichen Li","Zijie Meng","Jian Wu","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2506.11147v2.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2503.22194v3","updated":"2025-10-27T07:06:50Z","published":"2025-03-28T07:23:12Z","title":"ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation","summary":"  We introduce ORIGEN, the first zero-shot method for 3D orientation grounding\nin text-to-image generation across multiple objects and diverse categories.\nWhile previous work on spatial grounding in image generation has mainly focused\non 2D positioning, it lacks control over 3D orientation. To address this, we\npropose a reward-guided sampling approach using a pretrained discriminative\nmodel for 3D orientation estimation and a one-step text-to-image generative\nflow model. While gradient-ascent-based optimization is a natural choice for\nreward-based guidance, it struggles to maintain image realism. Instead, we\nadopt a sampling-based approach using Langevin dynamics, which extends gradient\nascent by simply injecting random noise--requiring just a single additional\nline of code. Additionally, we introduce adaptive time rescaling based on the\nreward function to accelerate convergence. Our experiments show that ORIGEN\noutperforms both training-based and test-time guidance methods across\nquantitative metrics and user studies.\n","authors":["Yunhong Min","Daehyeon Choi","Kyeongmin Yeo","Jihyun Lee","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2503.22194v3.pdf","comment":"Project Page: https://origen2025.github.io"},{"id":"http://arxiv.org/abs/2412.18918v2","updated":"2025-10-27T06:56:08Z","published":"2024-12-25T14:37:05Z","title":"BCR-Net: Boundary-Category Refinement Network for Weakly Semi-Supervised\n  X-Ray Prohibited Item Detection with Points","summary":"  Automatic prohibited item detection in X-ray images is crucial for public\nsafety. However, most existing detection methods either rely on expensive box\nannotations to achieve high performance or use weak annotations but suffer from\nlimited accuracy. To balance annotation cost and detection performance, we\nstudy Weakly Semi-Supervised X-ray Prohibited Item Detection with Points\n(WSSPID-P) and propose a novel \\textbf{B}oundary-\\textbf{C}ategory\n\\textbf{R}efinement \\textbf{Net}work (\\textbf{BCR-Net}) that requires only a\nfew box annotations and a large number of point annotations. BCR-Net is built\nbased on Group R-CNN and introduces a new Boundary Refinement (BR) module and a\nnew Category Refinement (CR) module. The BR module develops a dual attention\nmechanism to focus on both the boundaries and salient features of prohibited\nitems. Meanwhile, the CR module incorporates contrastive branches into the\nheads of RPN and ROI by introducing a scale- and rotation-aware contrastive\nloss, enhancing intra-class consistency and inter-class separability in the\nfeature space. Based on the above designs, BCR-Net effectively addresses the\nclosely related problems of imprecise localization and inaccurate\nclassification. Experimental results on public X-ray datasets show the\neffectiveness of BCR-Net, achieving significant performance improvements to\nstate-of-the-art methods under limited annotations.\n","authors":["Sanjoeng Wong"],"pdf_url":"https://arxiv.org/pdf/2412.18918v2.pdf","comment":"The authors withdraw this preprint because an error was found in a\n  mathematical expression and the manuscript lacks evaluation on the COCO\n  dataset. We will correct the error, extend experiments to include COCO, and\n  resubmit a revised version"},{"id":"http://arxiv.org/abs/2510.16730v2","updated":"2025-10-27T06:44:38Z","published":"2025-10-19T06:51:03Z","title":"UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping\n  via a Kolmogorov-Arnold Network-Transformer Hybrid","summary":"  Coral reefs are vital yet fragile ecosystems that require accurate\nlarge-scale mapping for effective conservation. Although global products such\nas the Allen Coral Atlas provide unprecedented coverage of global coral reef\ndistri-bution, their predictions are frequently limited in spatial precision\nand semantic consistency, especially in regions requiring fine-grained boundary\ndelineation. To address these challenges, we propose UKANFormer, a novel\nse-mantic segmentation model designed to achieve high-precision mapping under\nnoisy supervision derived from Allen Coral Atlas. Building upon the UKAN\narchitecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)\nblock in the decoder, enabling the extraction of both global semantic\nstructures and local boundary details. In experiments, UKANFormer achieved a\ncoral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming\nconventional baselines under the same noisy labels setting. Remarkably, the\nmodel produces predictions that are visually and structurally more accurate\nthan the noisy labels used for training. These results challenge the notion\nthat data quality directly limits model performance, showing that architectural\ndesign can mitigate label noise and sup-port scalable mapping under imperfect\nsupervision. UKANFormer provides a foundation for ecological monitoring where\nreliable labels are scarce.\n","authors":["Tianyang Dou","Ming Li","Jiangying Qin","Xuan Liao","Jiageng Zhong","Armin Gruen","Mengyi Deng"],"pdf_url":"https://arxiv.org/pdf/2510.16730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23057v1","updated":"2025-10-27T06:39:57Z","published":"2025-10-27T06:39:57Z","title":"Seq-DeepIPC: Sequential Sensing for End-to-End Control in Legged Robot\n  Navigation","summary":"  We present Seq-DeepIPC, a sequential end-to-end perception-to-control model\nfor legged robot navigation in realworld environments. Seq-DeepIPC advances\nintelligent sensing for autonomous legged navigation by tightly integrating\nmulti-modal perception (RGB-D + GNSS) with temporal fusion and control. The\nmodel jointly predicts semantic segmentation and depth estimation, giving\nricher spatial features for planning and control. For efficient deployment on\nedge devices, we use EfficientNet-B0 as the encoder, reducing computation while\nmaintaining accuracy. Heading estimation is simplified by removing the noisy\nIMU and instead computing the bearing angle directly from consecutive GNSS\npositions. We collected a larger and more diverse dataset that includes both\nroad and grass terrains, and validated Seq-DeepIPC on a robot dog. Comparative\nand ablation studies show that sequential inputs improve perception and control\nin our models, while other baselines do not benefit. Seq-DeepIPC achieves\ncompetitive or better results with reasonable model size; although GNSS-only\nheading is less reliable near tall buildings, it is robust in open areas.\nOverall, Seq-DeepIPC extends end-to-end navigation beyond wheeled robots to\nmore versatile and temporally-aware systems. To support future research, we\nwill release the codes to our GitHub repository at\nhttps://github.com/oskarnatan/Seq-DeepIPC.\n","authors":["Oskar Natan","Jun Miura"],"pdf_url":"https://arxiv.org/pdf/2510.23057v1.pdf","comment":"Preprint notice, this manuscript has been submitted to IEEE sensors\n  journal for possible publication"},{"id":"http://arxiv.org/abs/2505.23449v3","updated":"2025-10-27T06:39:35Z","published":"2025-05-29T13:56:21Z","title":"CMIE: Combining MLLM Insights with External Evidence for Explainable\n  Out-of-Context Misinformation Detection","summary":"  Multimodal large language models (MLLMs) have demonstrated impressive\ncapabilities in visual reasoning and text generation. While previous studies\nhave explored the application of MLLM for detecting out-of-context (OOC)\nmisinformation, our empirical analysis reveals two persisting challenges of\nthis paradigm. Evaluating the representative GPT-4o model on direct reasoning\nand evidence augmented reasoning, results indicate that MLLM struggle to\ncapture the deeper relationships-specifically, cases in which the image and\ntext are not directly connected but are associated through underlying semantic\nlinks. Moreover, noise in the evidence further impairs detection accuracy. To\naddress these challenges, we propose CMIE, a novel OOC misinformation detection\nframework that incorporates a Coexistence Relationship Generation (CRG)\nstrategy and an Association Scoring (AS) mechanism. CMIE identifies the\nunderlying coexistence relationships between images and text, and selectively\nutilizes relevant evidence to enhance misinformation detection. Experimental\nresults demonstrate that our approach outperforms existing methods.\n","authors":["Fanxiao Li","Jiaying Wu","Canyuan He","Wei Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.23449v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.25776v3","updated":"2025-10-27T06:34:13Z","published":"2025-09-30T04:44:53Z","title":"Editable Noise Map Inversion: Encoding Target-image into Noise For\n  High-Fidelity Image Manipulation","summary":"  Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality and diverse images. Building on these advancements, diffusion\nmodels have also demonstrated exceptional performance in text-guided image\nediting. A key strategy for effective image editing involves inverting the\nsource image into editable noise maps associated with the target image.\nHowever, previous inversion methods face challenges in adhering closely to the\ntarget text prompt. The limitation arises because inverted noise maps, while\nenabling faithful reconstruction of the source image, restrict the flexibility\nneeded for desired edits. To overcome this issue, we propose Editable Noise Map\nInversion (ENM Inversion), a novel inversion technique that searches for\noptimal noise maps to ensure both content preservation and editability. We\nanalyze the properties of noise maps for enhanced editability. Based on this\nanalysis, our method introduces an editable noise refinement that aligns with\nthe desired edits by minimizing the difference between the reconstructed and\nedited noise maps. Extensive experiments demonstrate that ENM Inversion\noutperforms existing approaches across a wide range of image editing tasks in\nboth preservation and edit fidelity with target prompts. Our approach can also\nbe easily applied to video editing, enabling temporal consistency and content\nmanipulation across frames.\n","authors":["Mingyu Kang","Yong Suk Choi"],"pdf_url":"https://arxiv.org/pdf/2509.25776v3.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2510.23043v1","updated":"2025-10-27T06:13:07Z","published":"2025-10-27T06:13:07Z","title":"HieraMamba: Video Temporal Grounding via Hierarchical Anchor-Mamba\n  Pooling","summary":"  Video temporal grounding, the task of localizing the start and end times of a\nnatural language query in untrimmed video, requires capturing both global\ncontext and fine-grained temporal detail. This challenge is particularly\npronounced in long videos, where existing methods often compromise temporal\nfidelity by over-downsampling or relying on fixed windows. We present\nHieraMamba, a hierarchical architecture that preserves temporal structure and\nsemantic richness across scales. At its core are Anchor-MambaPooling (AMP)\nblocks, which utilize Mamba's selective scanning to produce compact anchor\ntokens that summarize video content at multiple granularities. Two\ncomplementary objectives, anchor-conditioned and segment-pooled contrastive\nlosses, encourage anchors to retain local detail while remaining globally\ndiscriminative. HieraMamba sets a new state-of-the-art on Ego4D-NLQ, MAD, and\nTACoS, demonstrating precise, temporally faithful localization in long,\nuntrimmed videos.\n","authors":["Joungbin An","Kristen Grauman"],"pdf_url":"https://arxiv.org/pdf/2510.23043v1.pdf","comment":"Project Page: https://vision.cs.utexas.edu/projects/hieramamba/"},{"id":"http://arxiv.org/abs/2510.23028v1","updated":"2025-10-27T05:49:02Z","published":"2025-10-27T05:49:02Z","title":"Nested AutoRegressive Models","summary":"  AutoRegressive (AR) models have demonstrated competitive performance in image\ngeneration, achieving results comparable to those of diffusion models. However,\ntheir token-by-token image generation mechanism remains computationally\nintensive and existing solutions such as VAR often lead to limited sample\ndiversity. In this work, we propose a Nested AutoRegressive~(NestAR) model,\nwhich proposes nested AutoRegressive architectures in generating images. NestAR\ndesigns multi-scale modules in a hierarchical order. These different scaled\nmodules are constructed in an AR architecture, where one larger-scale module is\nconditioned on outputs from its previous smaller-scale module. Within each\nmodule, NestAR uses another AR structure to generate ``patches'' of tokens. The\nproposed nested AR architecture reduces the overall complexity from\n$\\mathcal{O}(n)$ to $\\mathcal{O}(\\log n)$ in generating $n$ image tokens, as\nwell as increases image diversities. NestAR further incorporates flow matching\nloss to use continuous tokens, and develops objectives to coordinate these\nmulti-scale modules in model training. NestAR achieves competitive image\ngeneration performance while significantly lowering computational cost.\n","authors":["Hongyu Wu","Xuhui Fan","Zhangkai Wu","Longbing Cao"],"pdf_url":"https://arxiv.org/pdf/2510.23028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23023v1","updated":"2025-10-27T05:37:23Z","published":"2025-10-27T05:37:23Z","title":"UniAIDet: A Unified and Universal Benchmark for AI-Generated Image\n  Content Detection and Localization","summary":"  With the rapid proliferation of image generative models, the authenticity of\ndigital images has become a significant concern. While existing studies have\nproposed various methods for detecting AI-generated content, current benchmarks\nare limited in their coverage of diverse generative models and image\ncategories, often overlooking end-to-end image editing and artistic images. To\naddress these limitations, we introduce UniAIDet, a unified and comprehensive\nbenchmark that includes both photographic and artistic images. UniAIDet covers\na wide range of generative models, including text-to-image, image-to-image,\nimage inpainting, image editing, and deepfake models. Using UniAIDet, we\nconduct a comprehensive evaluation of various detection methods and answer\nthree key research questions regarding generalization capability and the\nrelation between detection and localization. Our benchmark and analysis provide\na robust foundation for future research.\n","authors":["Huixuan Zhang","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2510.23023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23020v1","updated":"2025-10-27T05:32:50Z","published":"2025-10-27T05:32:50Z","title":"M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance,\n  Multi-Relation Text-to-Image Benchmark","summary":"  Text-to-image models are known to struggle with generating images that\nperfectly align with textual prompts. Several previous studies have focused on\nevaluating image-text alignment in text-to-image generation. However, these\nevaluations either address overly simple scenarios, especially overlooking the\ndifficulty of prompts with multiple different instances belonging to the same\ncategory, or they introduce metrics that do not correlate well with human\nevaluation. In this study, we introduce M$^3$T2IBench, a large-scale,\nmulti-category, multi-instance, multi-relation along with an\nobject-detection-based evaluation metric, $AlignScore$, which aligns closely\nwith human evaluation. Our findings reveal that current open-source\ntext-to-image models perform poorly on this challenging benchmark.\nAdditionally, we propose the Revise-Then-Enforce approach to enhance image-text\nalignment. This training-free post-editing method demonstrates improvements in\nimage-text alignment across a broad range of diffusion models. \\footnote{Our\ncode and data has been released in supplementary material and will be made\npublicly available after the paper is accepted.}\n","authors":["Huixuan Zhang","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2510.23020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.20383v2","updated":"2025-10-27T05:22:21Z","published":"2025-04-29T03:04:09Z","title":"Neural Stereo Video Compression with Hybrid Disparity Compensation","summary":"  Disparity compensation represents the primary strategy in stereo video\ncompression (SVC) for exploiting cross-view redundancy. These mechanisms can be\nbroadly categorized into two types: one that employs explicit horizontal\nshifting, and another that utilizes an implicit cross-attention mechanism to\nreduce cross-view disparity redundancy. In this work, we propose a hybrid\ndisparity compensation (HDC) strategy that leverages explicit pixel\ndisplacement as a robust prior feature to simplify optimization and perform\nimplicit cross-attention mechanisms for subsequent warping operations, thereby\ncapturing a broader range of disparity information. Specifically, HDC first\ncomputes a similarity map by fusing the horizontally shifted cross-view\nfeatures to capture pixel displacement information. This similarity map is then\nnormalized into an \"explicit pixel-wise attention score\" to perform the\ncross-attention mechanism, implicitly aligning features from one view to\nanother. Building upon HDC, we introduce a novel end-to-end optimized neural\nstereo video compression framework, which integrates HDC-based modules into key\ncoding operations, including cross-view feature extraction and reconstruction\n(HDC-FER) and cross-view entropy modeling (HDC-EM). Extensive experiments on\nSVC benchmarks, including KITTI 2012, KITTI 2015, and Nagoya, which cover both\nautonomous driving and general scenes, demonstrate that our framework\noutperforms both neural and traditional SVC methodologies.\n","authors":["Shiyin Jiang","Zhenghao Chen","Minghao Han","Shuhang Gu"],"pdf_url":"https://arxiv.org/pdf/2504.20383v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.14376v2","updated":"2025-10-27T05:18:23Z","published":"2025-10-16T07:17:23Z","title":"DOS: Directional Object Separation in Text Embeddings for Multi-Object\n  Image Generation","summary":"  Recent progress in text-to-image (T2I) generative models has led to\nsignificant improvements in generating high-quality images aligned with text\nprompts. However, these models still struggle with prompts involving multiple\nobjects, often resulting in object neglect or object mixing. Through extensive\nstudies, we identify four problematic scenarios, Similar Shapes, Similar\nTextures, Dissimilar Background Biases, and Many Objects, where inter-object\nrelationships frequently lead to such failures. Motivated by two key\nobservations about CLIP embeddings, we propose DOS (Directional Object\nSeparation), a method that modifies three types of CLIP text embeddings before\npassing them into text-to-image models. Experimental results show that DOS\nconsistently improves the success rate of multi-object image generation and\nreduces object mixing. In human evaluations, DOS significantly outperforms four\ncompeting methods, receiving 26.24%-43.04% more votes across four benchmarks.\nThese results highlight DOS as a practical and effective solution for improving\nmulti-object image generation.\n","authors":["Dongnam Byun","Jungwon Park","Jumgmin Ko","Changin Choi","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2510.14376v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23917v2","updated":"2025-10-27T05:03:10Z","published":"2025-05-29T18:09:44Z","title":"Representational Difference Explanations","summary":"  We propose a method for discovering and visualizing the differences between\ntwo learned representations, enabling more direct and interpretable model\ncomparisons. We validate our method, which we call Representational Differences\nExplanations (RDX), by using it to compare models with known conceptual\ndifferences and demonstrate that it recovers meaningful distinctions where\nexisting explainable AI (XAI) techniques fail. Applied to state-of-the-art\nmodels on challenging subsets of the ImageNet and iNaturalist datasets, RDX\nreveals both insightful representational differences and subtle patterns in the\ndata. Although comparison is a cornerstone of scientific analysis, current\ntools in machine learning, namely post hoc XAI methods, struggle to support\nmodel comparison effectively. Our work addresses this gap by introducing an\neffective and explainable tool for contrasting model representations.\n","authors":["Neehar Kondapaneni","Oisin Mac Aodha","Pietro Perona"],"pdf_url":"https://arxiv.org/pdf/2505.23917v2.pdf","comment":"9 pages, 6 figures, 21 supplementary pages, 14 supp figs"},{"id":"http://arxiv.org/abs/2510.23009v1","updated":"2025-10-27T05:01:57Z","published":"2025-10-27T05:01:57Z","title":"UGAE: Unified Geometry and Attribute Enhancement for G-PCC Compressed\n  Point Clouds","summary":"  Lossy compression of point clouds reduces storage and transmission costs;\nhowever, it inevitably leads to irreversible distortion in geometry structure\nand attribute information. To address these issues, we propose a unified\ngeometry and attribute enhancement (UGAE) framework, which consists of three\ncore components: post-geometry enhancement (PoGE), pre-attribute enhancement\n(PAE), and post-attribute enhancement (PoAE). In PoGE, a Transformer-based\nsparse convolutional U-Net is used to reconstruct the geometry structure with\nhigh precision by predicting voxel occupancy probabilities. Building on the\nrefined geometry structure, PAE introduces an innovative enhanced\ngeometry-guided recoloring strategy, which uses a detail-aware K-Nearest\nNeighbors (DA-KNN) method to achieve accurate recoloring and effectively\npreserve high-frequency details before attribute compression. Finally, at the\ndecoder side, PoAE uses an attribute residual prediction network with a\nweighted mean squared error (W-MSE) loss to enhance the quality of\nhigh-frequency regions while maintaining the fidelity of low-frequency regions.\nUGAE significantly outperformed existing methods on three benchmark datasets:\n8iVFB, Owlii, and MVUB. Compared to the latest G-PCC test model (TMC13v29),\nUGAE achieved an average BD-PSNR gain of 9.98 dB and 90.98% BD-bitrate savings\nfor geometry under the D1 metric, as well as a 3.67 dB BD-PSNR improvement with\n56.88% BD-bitrate savings for attributes on the Y component. Additionally, it\nimproved perceptual quality significantly.\n","authors":["Pan Zhao","Hui Yuan","Chongzhen Tian","Tian Guo","Raouf Hamzaoui","Zhigeng Pan"],"pdf_url":"https://arxiv.org/pdf/2510.23009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23007v1","updated":"2025-10-27T04:57:09Z","published":"2025-10-27T04:57:09Z","title":"CoMo: Compositional Motion Customization for Text-to-Video Generation","summary":"  While recent text-to-video models excel at generating diverse scenes, they\nstruggle with precise motion control, particularly for complex, multi-subject\nmotions. Although methods for single-motion customization have been developed\nto address this gap, they fail in compositional scenarios due to two primary\nchallenges: motion-appearance entanglement and ineffective multi-motion\nblending. This paper introduces CoMo, a novel framework for\n$\\textbf{compositional motion customization}$ in text-to-video generation,\nenabling the synthesis of multiple, distinct motions within a single video.\nCoMo addresses these issues through a two-phase approach. First, in the\nsingle-motion learning phase, a static-dynamic decoupled tuning paradigm\ndisentangles motion from appearance to learn a motion-specific module. Second,\nin the multi-motion composition phase, a plug-and-play divide-and-merge\nstrategy composes these learned motions without additional training by\nspatially isolating their influence during the denoising process. To facilitate\nresearch in this new domain, we also introduce a new benchmark and a novel\nevaluation metric designed to assess multi-motion fidelity and blending.\nExtensive experiments demonstrate that CoMo achieves state-of-the-art\nperformance, significantly advancing the capabilities of controllable video\ngeneration. Our project page is at https://como6.github.io/.\n","authors":["Youcan Xu","Zhen Wang","Jiaxin Shi","Kexin Li","Feifei Shao","Jun Xiao","Yi Yang","Jun Yu","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2510.23007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23003v1","updated":"2025-10-27T04:43:20Z","published":"2025-10-27T04:43:20Z","title":"An Intelligent Water-Saving Irrigation System Based on Multi-Sensor\n  Fusion and Visual Servoing Control","summary":"  This paper introduces an intelligent water-saving irrigation system designed\nto address critical challenges in precision agriculture, such as inefficient\nwater use and poor terrain adaptability. The system integrates advanced\ncomputer vision, robotic control, and real-time stabilization technologies via\na multi-sensor fusion approach. A lightweight YOLO model, deployed on an\nembedded vision processor (K210), enables real-time plant container detection\nwith over 96% accuracy under varying lighting conditions. A simplified hand-eye\ncalibration algorithm-designed for 'handheld camera' robot arm\nconfigurations-ensures that the end effector can be precisely positioned, with\na success rate exceeding 90%. The active leveling system, driven by the\nSTM32F103ZET6 main control chip and JY901S inertial measurement data, can\nstabilize the irrigation platform on slopes up to 10 degrees, with a response\ntime of 1.8 seconds. Experimental results across three simulated agricultural\nenvironments (standard greenhouse, hilly terrain, complex lighting) demonstrate\na 30-50% reduction in water consumption compared to conventional flood\nirrigation, with water use efficiency exceeding 92% in all test cases.\n","authors":["ZhengKai Huang","YiKun Wang","ChenYu Hui"," XiaoCheng"],"pdf_url":"https://arxiv.org/pdf/2510.23003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.04016v3","updated":"2025-10-27T04:32:08Z","published":"2025-08-06T02:12:29Z","title":"S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient\n  Data and Sparse Token Distillation","summary":"  Diffusion transformers have emerged as the mainstream paradigm for video\ngeneration models. However, the use of up to billions of parameters incurs\nsignificant computational costs. Quantization offers a promising solution by\nreducing memory usage and accelerating inference. Nonetheless, we observe that\nthe joint modeling of spatial and temporal information in video diffusion\nmodels (V-DMs) leads to extremely long token sequences, which introduces high\ncalibration variance and learning challenges. To address these issues, we\npropose S$^2$Q-VDiT, a post-training quantization framework for V-DMs that\nleverages Salient data and Sparse token distillation. During the calibration\nphase, we identify that quantization performance is highly sensitive to the\nchoice of calibration data. To mitigate this, we introduce\n\\textit{Hessian-aware Salient Data Selection}, which constructs high-quality\ncalibration datasets by considering both diffusion and quantization\ncharacteristics unique to V-DMs. To tackle the learning challenges, we further\nanalyze the sparse attention patterns inherent in V-DMs. Based on this\nobservation, we propose \\textit{Attention-guided Sparse Token Distillation},\nwhich exploits token-wise attention distributions to emphasize tokens that are\nmore influential to the model's output. Under W4A6 quantization, S$^2$Q-VDiT\nachieves lossless performance while delivering $3.9\\times$ model compression\nand $1.3\\times$ inference acceleration. Code will be available at\nhttps://github.com/wlfeng0509/s2q-vdit.\n","authors":["Weilun Feng","Haotong Qin","Chuanguang Yang","Xiangqi Li","Han Yang","Yuqi Li","Zhulin An","Libo Huang","Michele Magno","Yongjun Xu"],"pdf_url":"https://arxiv.org/pdf/2508.04016v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22995v1","updated":"2025-10-27T04:25:57Z","published":"2025-10-27T04:25:57Z","title":"LoMix: Learnable Weighted Multi-Scale Logits Mixing for Medical Image\n  Segmentation","summary":"  U-shaped networks output logits at multiple spatial scales, each capturing a\ndifferent blend of coarse context and fine detail. Yet, training still treats\nthese logits in isolation - either supervising only the final,\nhighest-resolution logits or applying deep supervision with identical loss\nweights at every scale - without exploring mixed-scale combinations.\nConsequently, the decoder output misses the complementary cues that arise only\nwhen coarse and fine predictions are fused. To address this issue, we introduce\nLoMix (Logits Mixing), a NAS-inspired, differentiable plug-and-play module that\ngenerates new mixed-scale outputs and learns how exactly each of them should\nguide the training process. More precisely, LoMix mixes the multi-scale decoder\nlogits with four lightweight fusion operators: addition, multiplication,\nconcatenation, and attention-based weighted fusion, yielding a rich set of\nsynthetic mutant maps. Every original or mutant map is given a softplus loss\nweight that is co-optimized with network parameters, mimicking a one-step\narchitecture search that automatically discovers the most useful scales,\nmixtures, and operators. Plugging LoMix into recent U-shaped architectures\n(i.e., PVT-V2-B2 backbone with EMCAD decoder) on Synapse 8-organ dataset\nimproves DICE by +4.2% over single-output supervision, +2.2% over deep\nsupervision, and +1.5% over equally weighted additive fusion, all with zero\ninference overhead. When training data are scarce (e.g., one or two labeled\nscans), the advantage grows to +9.23%, underscoring LoMix's data efficiency.\nAcross four benchmarks and diverse U-shaped networks, LoMiX improves DICE by up\nto +13.5% over single-output supervision, confirming that learnable weighted\nmixed-scale fusion generalizes broadly while remaining data efficient, fully\ninterpretable, and overhead-free at inference. Our code is available at\nhttps://github.com/SLDGroup/LoMix.\n","authors":["Md Mostafijur Rahman","Radu Marculescu"],"pdf_url":"https://arxiv.org/pdf/2510.22995v1.pdf","comment":"25 pages, 13 figures, NeurIPS 2025 accepted paper"},{"id":"http://arxiv.org/abs/2510.22994v1","updated":"2025-10-27T04:19:22Z","published":"2025-10-27T04:19:22Z","title":"SceneDecorator: Towards Scene-Oriented Story Generation with Scene\n  Planning and Scene Consistency","summary":"  Recent text-to-image models have revolutionized image generation, but they\nstill struggle with maintaining concept consistency across generated images.\nWhile existing works focus on character consistency, they often overlook the\ncrucial role of scenes in storytelling, which restricts their creativity in\npractice. This paper introduces scene-oriented story generation, addressing two\nkey challenges: (i) scene planning, where current methods fail to ensure\nscene-level narrative coherence by relying solely on text descriptions, and\n(ii) scene consistency, which remains largely unexplored in terms of\nmaintaining scene consistency across multiple stories. We propose\nSceneDecorator, a training-free framework that employs VLM-Guided Scene\nPlanning to ensure narrative coherence across different scenes in a\n``global-to-local'' manner, and Long-Term Scene-Sharing Attention to maintain\nlong-term scene consistency and subject diversity across generated stories.\nExtensive experiments demonstrate the superior performance of SceneDecorator,\nhighlighting its potential to unleash creativity in the fields of arts, films,\nand games.\n","authors":["Quanjian Song","Donghao Zhou","Jingyu Lin","Fei Shen","Jiaze Wang","Xiaowei Hu","Cunjian Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2510.22994v1.pdf","comment":"Accepted by NeurIPS 2025; Project Page:\n  https://lulupig12138.github.io/SceneDecorator"},{"id":"http://arxiv.org/abs/2510.22990v1","updated":"2025-10-27T04:16:43Z","published":"2025-10-27T04:16:43Z","title":"USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked\n  Autoencoding","summary":"  Ultrasound imaging is one of the most widely used diagnostic modalities,\noffering real-time, radiation-free assessment across diverse clinical domains.\nHowever, interpretation of ultrasound images remains challenging due to high\nnoise levels, operator dependence, and limited field of view, resulting in\nsubstantial inter-observer variability. Current Deep Learning approaches are\nhindered by the scarcity of large labeled datasets and the domain gap between\ngeneral and sonographic images, which limits the transferability of models\npretrained on non-medical data. To address these challenges, we introduce the\nUltrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE),\nthe first large-scale self-supervised MAE framework pretrained exclusively on\nultrasound data. The model was pre-trained on 370,000 2D and 3D ultrasound\nimages curated from 46 open-source datasets, collectively termed OpenUS-46,\nspanning over twenty anatomical regions. This curated dataset has been made\npublicly available to facilitate further research and reproducibility. Using a\nVision Transformer encoder-decoder architecture, USF-MAE reconstructs masked\nimage patches, enabling it to learn rich, modality-specific representations\ndirectly from unlabeled data. The pretrained encoder was fine-tuned on three\npublic downstream classification benchmarks: BUS-BRA (breast cancer), MMOTU-2D\n(ovarian tumors), and GIST514-DB (gastrointestinal stromal tumors). Across all\ntasks, USF-MAE consistently outperformed conventional CNN and ViT baselines,\nachieving F1-scores of 81.6%, 79.6%, and 82.4%, respectively. Despite not using\nlabels during pretraining, USF-MAE approached the performance of the supervised\nfoundation model UltraSam on breast cancer classification and surpassed it on\nthe other tasks, demonstrating strong cross-anatomical generalization.\n","authors":["Youssef Megahed","Robin Ducharme","Mark Walker","Steven Hawken","Adrian D. C. Chan"],"pdf_url":"https://arxiv.org/pdf/2510.22990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.25744v2","updated":"2025-10-27T04:03:38Z","published":"2025-09-30T04:01:30Z","title":"Image-Plane Geometric Decoding for View-Invariant Indoor Scene\n  Reconstruction","summary":"  Volume-based indoor scene reconstruction methods offer superior\ngeneralization capability and real-time deployment potential. However, existing\nmethods rely on multi-view pixel back-projection ray intersections as weak\ngeometric constraints to determine spatial positions. This dependence results\nin reconstruction quality being heavily influenced by input view density.\nPerformance degrades in overlapping regions and unobserved areas.To address\nthese limitations, we reduce dependency on inter-view geometric constraints by\nexploiting spatial information within individual views. We propose an\nimage-plane decoding framework with three core components: Pixel-level\nConfidence Encoder, Affine Compensation Module, and Image-Plane Spatial\nDecoder. These modules decode three-dimensional structural information encoded\nin images through physical imaging processes. The framework effectively\npreserves spatial geometric features including edges, hollow structures, and\ncomplex textures. It significantly enhances view-invariant\nreconstruction.Experiments on indoor scene reconstruction datasets confirm\nsuperior reconstruction stability. Our method maintains nearly identical\nquality when view count reduces by 40%. It achieves a coefficient of variation\nof 0.24%, performance retention rate of 99.7%, and maximum performance drop of\n0.42%. These results demonstrate that exploiting intra-view spatial information\nprovides a robust solution for view-limited scenarios in practical\napplications.\n","authors":["Mingyang Li","Yimeng Fan","Changsong Liu","Lixue Xu","Xin Wang","Yanyan Liu","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.25744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22981v1","updated":"2025-10-27T04:02:52Z","published":"2025-10-27T04:02:52Z","title":"Exploring Semantic-constrained Adversarial Example with Instruction\n  Uncertainty Reduction","summary":"  Recently, semantically constrained adversarial examples (SemanticAE), which\nare directly generated from natural language instructions, have become a\npromising avenue for future research due to their flexible attacking forms. To\ngenerate SemanticAEs, current methods fall short of satisfactory attacking\nability as the key underlying factors of semantic uncertainty in human\ninstructions, such as referring diversity, descriptive incompleteness, and\nboundary ambiguity, have not been fully investigated. To tackle the issues,\nthis paper develops a multi-dimensional instruction uncertainty reduction\n(InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable,\nadaptive, and effective. Specifically, in the dimension of the sampling method,\nwe propose the residual-driven attacking direction stabilization to alleviate\nthe unstable adversarial optimization caused by the diversity of language\nreferences. By coarsely predicting the language-guided sampling process, the\noptimization process will be stabilized by the designed ResAdv-DDIM sampler,\ntherefore releasing the transferable and robust adversarial capability of\nmulti-step diffusion models. In task modeling, we propose the context-encoded\nattacking scenario constraint to supplement the missing knowledge from\nincomplete human instructions. Guidance masking and renderer integration are\nproposed to regulate the constraints of 2D/3D SemanticAE, activating stronger\nscenario-adapted attacks. Moreover, in the dimension of generator evaluation,\nwe propose the semantic-abstracted attacking evaluation enhancement by\nclarifying the evaluation boundary, facilitating the development of more\neffective SemanticAE generators. Extensive experiments demonstrate the\nsuperiority of the transfer attack performance of InSUR. Moreover, we realize\nthe reference-free generation of semantically constrained 3D adversarial\nexamples for the first time.\n","authors":["Jin Hu","Jiakai Wang","Linna Jing","Haolin Li","Haodong Liu","Haotong Qin","Aishan Liu","Ke Xu","Xianglong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.22981v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.22975v1","updated":"2025-10-27T03:56:25Z","published":"2025-10-27T03:56:25Z","title":"VoMP: Predicting Volumetric Mechanical Property Fields","summary":"  Physical simulation relies on spatially-varying mechanical properties, often\nlaboriously hand-crafted. VoMP is a feed-forward method trained to predict\nYoung's modulus ($E$), Poisson's ratio ($\\nu$), and density ($\\rho$) throughout\nthe volume of 3D objects, in any representation that can be rendered and\nvoxelized. VoMP aggregates per-voxel multi-view features and passes them to our\ntrained Geometry Transformer to predict per-voxel material latent codes. These\nlatents reside on a manifold of physically plausible materials, which we learn\nfrom a real-world dataset, guaranteeing the validity of decoded per-voxel\nmaterials. To obtain object-level training data, we propose an annotation\npipeline combining knowledge from segmented 3D datasets, material databases,\nand a vision-language model, along with a new benchmark. Experiments show that\nVoMP estimates accurate volumetric properties, far outperforming prior art in\naccuracy and speed.\n","authors":["Rishit Dagli","Donglai Xiang","Vismay Modi","Charles Loop","Clement Fuji Tsang","Anka He Chen","Anita Hu","Gavriel State","David I. W. Levin","Maria Shugrina"],"pdf_url":"https://arxiv.org/pdf/2510.22975v1.pdf","comment":"hi-res paper and other details at:\n  https://research.nvidia.com/labs/sil/projects/vomp"},{"id":"http://arxiv.org/abs/2510.22973v1","updated":"2025-10-27T03:52:45Z","published":"2025-10-27T03:52:45Z","title":"Scaling Up Occupancy-centric Driving Scene Generation: Dataset and\n  Method","summary":"  Driving scene generation is a critical domain for autonomous driving,\nenabling downstream applications, including perception and planning evaluation.\nOccupancy-centric methods have recently achieved state-of-the-art results by\noffering consistent conditioning across frames and modalities; however, their\nperformance heavily depends on annotated occupancy data, which still remains\nscarce. To overcome this limitation, we curate Nuplan-Occ, the largest semantic\noccupancy dataset to date, constructed from the widely used Nuplan benchmark.\nIts scale and diversity facilitate not only large-scale generative modeling but\nalso autonomous driving downstream applications. Based on this dataset, we\ndevelop a unified framework that jointly synthesizes high-quality semantic\noccupancy, multi-view videos, and LiDAR point clouds. Our approach incorporates\na spatio-temporal disentangled architecture to support high-fidelity spatial\nexpansion and temporal forecasting of 4D dynamic occupancy. To bridge modal\ngaps, we further propose two novel techniques: a Gaussian splatting-based\nsparse point map rendering strategy that enhances multi-view video generation,\nand a sensor-aware embedding strategy that explicitly models LiDAR sensor\nproperties for realistic multi-LiDAR simulation. Extensive experiments\ndemonstrate that our method achieves superior generation fidelity and\nscalability compared to existing approaches, and validates its practical value\nin downstream tasks. Repo:\nhttps://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2\n","authors":["Bohan Li","Xin Jin","Hu Zhu","Hongsi Liu","Ruikai Li","Jiazhe Guo","Kaiwen Cai","Chao Ma","Yueming Jin","Hao Zhao","Xiaokang Yang","Wenjun Zeng"],"pdf_url":"https://arxiv.org/pdf/2510.22973v1.pdf","comment":"https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2"},{"id":"http://arxiv.org/abs/2412.08082v2","updated":"2025-10-27T03:48:10Z","published":"2024-12-11T04:00:17Z","title":"FaceTracer: Unveiling Source Identities from Swapped Face Images and\n  Videos for Fraud Prevention","summary":"  Face-swapping techniques have advanced rapidly with the evolution of deep\nlearning, leading to widespread use and growing concerns about potential\nmisuse, especially in cases of fraud. While many efforts have focused on\ndetecting swapped face images or videos, these methods are insufficient for\ntracing the malicious users behind fraudulent activities. Intrusive\nwatermark-based approaches also fail to trace unmarked identities, limiting\ntheir practical utility. To address these challenges, we introduce FaceTracer,\nthe first non-intrusive framework specifically designed to trace the identity\nof the source person from swapped face images or videos. Specifically,\nFaceTracer leverages a disentanglement module that effectively suppresses\nidentity information related to the target person while isolating the identity\nfeatures of the source person. This allows us to extract robust identity\ninformation that can directly link the swapped face back to the original\nindividual, aiding in uncovering the actors behind fraudulent activities.\nExtensive experiments demonstrate FaceTracer's effectiveness across various\nface-swapping techniques, successfully identifying the source person in swapped\ncontent and enabling the tracing of malicious actors involved in fraudulent\nactivities. Additionally, FaceTracer shows strong transferability to unseen\nface-swapping methods including commercial applications and robustness against\ntransmission distortions and adaptive attacks.Our code is available at:\nhttps://github.com/zzy224/FaceTracer.\n","authors":["Zhongyi Zhang","Jie Zhang","Wenbo Zhou","Xinghui Zhou","Qing Guo","Weiming Zhang","Tianwei Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2412.08082v2.pdf","comment":"17 pages, 16 figures, TPAMI version"},{"id":"http://arxiv.org/abs/2505.18087v2","updated":"2025-10-27T03:45:34Z","published":"2025-05-23T16:44:21Z","title":"CXReasonBench: A Benchmark for Evaluating Structured Diagnostic\n  Reasoning in Chest X-rays","summary":"  Recent progress in Large Vision-Language Models (LVLMs) has enabled promising\napplications in medical tasks, such as report generation and visual question\nanswering. However, existing benchmarks focus mainly on the final diagnostic\nanswer, offering limited insight into whether models engage in clinically\nmeaningful reasoning. To address this, we present CheXStruct and CXReasonBench,\na structured pipeline and benchmark built on the publicly available\nMIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of\nintermediate reasoning steps directly from chest X-rays, such as segmenting\nanatomical regions, deriving anatomical landmarks and diagnostic measurements,\ncomputing diagnostic indices, and applying clinical thresholds. CXReasonBench\nleverages this pipeline to evaluate whether models can perform clinically valid\nreasoning steps and to what extent they can learn from structured guidance,\nenabling fine-grained and transparent assessment of diagnostic reasoning. The\nbenchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases,\neach paired with up to 4 visual inputs, and supports multi-path, multi-stage\nevaluation including visual grounding via anatomical region selection and\ndiagnostic measurements. Even the strongest of 12 evaluated LVLMs struggle with\nstructured reasoning and generalization, often failing to link abstract\nknowledge with anatomically grounded visual interpretation. The code is\navailable at https://github.com/ttumyche/CXReasonBench\n","authors":["Hyungyung Lee","Geon Choi","Jung-Oh Lee","Hangyul Yoon","Hyuk Gi Hong","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2505.18087v2.pdf","comment":"Accepted at NeurIPS 2025 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2510.22970v1","updated":"2025-10-27T03:44:11Z","published":"2025-10-27T03:44:11Z","title":"VALA: Learning Latent Anchors for Training-Free and Temporally\n  Consistent","summary":"  Recent advances in training-free video editing have enabled lightweight and\nprecise cross-frame generation by leveraging pre-trained text-to-image\ndiffusion models. However, existing methods often rely on heuristic frame\nselection to maintain temporal consistency during DDIM inversion, which\nintroduces manual bias and reduces the scalability of end-to-end inference. In\nthis paper, we propose~\\textbf{VALA} (\\textbf{V}ariational \\textbf{A}lignment\nfor \\textbf{L}atent \\textbf{A}nchors), a variational alignment module that\nadaptively selects key frames and compresses their latent features into\nsemantic anchors for consistent video editing. To learn meaningful assignments,\nVALA propose a variational framework with a contrastive learning objective.\nTherefore, it can transform cross-frame latent representations into compressed\nlatent anchors that preserve both content and temporal coherence. Our method\ncan be fully integrated into training-free text-to-image based video editing\nmodels. Extensive experiments on real-world video editing benchmarks show that\nVALA achieves state-of-the-art performance in inversion fidelity, editing\nquality, and temporal consistency, while offering improved efficiency over\nprior methods.\n","authors":["Zhangkai Wu","Xuhui Fan","Zhongyuan Xie","Kaize Shi","Longbing Cao"],"pdf_url":"https://arxiv.org/pdf/2510.22970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22964v1","updated":"2025-10-27T03:40:00Z","published":"2025-10-27T03:40:00Z","title":"Survey of Multimodal Geospatial Foundation Models: Techniques,\n  Applications, and Challenges","summary":"  Foundation models have transformed natural language processing and computer\nvision, and their impact is now reshaping remote sensing image analysis. With\npowerful generalization and transfer learning capabilities, they align\nnaturally with the multimodal, multi-resolution, and multi-temporal\ncharacteristics of remote sensing data. To address unique challenges in the\nfield, multimodal geospatial foundation models (GFMs) have emerged as a\ndedicated research frontier. This survey delivers a comprehensive review of\nmultimodal GFMs from a modality-driven perspective, covering five core visual\nand vision-language modalities. We examine how differences in imaging physics\nand data representation shape interaction design, and we analyze key techniques\nfor alignment, integration, and knowledge transfer to tackle modality\nheterogeneity, distribution shifts, and semantic gaps. Advances in training\nparadigms, architectures, and task-specific adaptation strategies are\nsystematically assessed alongside a wealth of emerging benchmarks.\nRepresentative multimodal visual and vision-language GFMs are evaluated across\nten downstream tasks, with insights into their architectures, performance, and\napplication scenarios. Real-world case studies, spanning land cover mapping,\nagricultural monitoring, disaster response, climate studies, and geospatial\nintelligence, demonstrate the practical potential of GFMs. Finally, we outline\npressing challenges in domain generalization, interpretability, efficiency, and\nprivacy, and chart promising avenues for future research.\n","authors":["Liling Yang","Ning Chen","Jun Yue","Yidan Liu","Jiayi Ma","Pedram Ghamisi","Antonio Plaza","Leyuan Fang"],"pdf_url":"https://arxiv.org/pdf/2510.22964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22960v1","updated":"2025-10-27T03:34:15Z","published":"2025-10-27T03:34:15Z","title":"FAME: Fairness-aware Attention-modulated Video Editing","summary":"  Training-free video editing (VE) models tend to fall back on gender\nstereotypes when rendering profession-related prompts. We propose \\textbf{FAME}\nfor \\textit{Fairness-aware Attention-modulated Video Editing} that mitigates\nprofession-related gender biases while preserving prompt alignment and temporal\nconsistency for coherent VE. We derive fairness embeddings from existing\nminority representations by softly injecting debiasing tokens into the text\nencoder. Simultaneously, FAME integrates fairness modulation into both temporal\nself attention and prompt-to-region cross attention to mitigate the motion\ncorruption and temporal inconsistency caused by directly introducing fairness\ncues. For temporal self attention, FAME introduces a region constrained\nattention mask combined with time decay weighting, which enhances intra-region\ncoherence while suppressing irrelevant inter-region interactions. For cross\nattention, it reweights tokens to region matching scores by incorporating\nfairness sensitive similarity masks derived from debiasing prompt embeddings.\nTogether, these modulations keep fairness-sensitive semantics tied to the right\nvisual regions and prevent temporal drift across frames. Extensive experiments\non new VE fairness-oriented benchmark \\textit{FairVE} demonstrate that FAME\nachieves stronger fairness alignment and semantic fidelity, surpassing existing\nVE baselines.\n","authors":["Zhangkai Wu","Xuhui Fan","Zhongyuan Xie","Kaize Shi","Zhidong Li","Longbing Cao"],"pdf_url":"https://arxiv.org/pdf/2510.22960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22946v1","updated":"2025-10-27T02:59:57Z","published":"2025-10-27T02:59:57Z","title":"LightBagel: A Light-weighted, Double Fusion Framework for Unified\n  Multimodal Understanding and Generation","summary":"  Unified multimodal models have recently shown remarkable gains in both\ncapability and versatility, yet most leading systems are still trained from\nscratch and require substantial computational resources. In this paper, we show\nthat competitive performance can be obtained far more efficiently by\nstrategically fusing publicly available models specialized for either\ngeneration or understanding. Our key design is to retain the original blocks\nwhile additionally interleaving multimodal self-attention blocks throughout the\nnetworks. This double fusion mechanism (1) effectively enables rich multi-modal\nfusion while largely preserving the original strengths of the base models, and\n(2) catalyzes synergistic fusion of high-level semantic representations from\nthe understanding encoder with low-level spatial signals from the generation\nencoder. By training with only ~ 35B tokens, this approach achieves strong\nresults across multiple benchmarks: 0.91 on GenEval for compositional\ntext-to-image generation, 82.16 on DPG-Bench for complex text-to-image\ngeneration, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By\nfully releasing the entire suite of code, model weights, and datasets, we hope\nto support future research on unified multimodal modeling.\n","authors":["Zeyu Wang","Zilong Chen","Chenhui Gou","Feng Li","Chaorui Deng","Deyao Zhu","Kunchang Li","Weihao Yu","Haoqin Tu","Haoqi Fan","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2510.22946v1.pdf","comment":"Preprint. Project page: https://ucsc-vlaa.github.io/LightBagel/"},{"id":"http://arxiv.org/abs/2510.17131v2","updated":"2025-10-27T02:58:39Z","published":"2025-10-20T03:58:46Z","title":"GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution\n  Detection","summary":"  Recent advancements have explored text-to-image diffusion models for\nsynthesizing out-of-distribution (OOD) samples, substantially enhancing the\nperformance of OOD detection. However, existing approaches typically rely on\nperturbing text-conditioned embeddings, resulting in semantic instability and\ninsufficient shift diversity, which limit generalization to realistic OOD. To\naddress these challenges, we propose GOOD, a novel and flexible framework that\ndirectly guides diffusion sampling trajectories towards OOD regions using\noff-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level\nguidance: (1) Image-level guidance based on the gradient of log partition to\nreduce input likelihood, drives samples toward low-density regions in pixel\nspace. (2) Feature-level guidance, derived from k-NN distance in the\nclassifier's latent space, promotes sampling in feature-sparse regions. Hence,\nthis dual-guidance design enables more controllable and diverse OOD sample\ngeneration. Additionally, we introduce a unified OOD score that adaptively\ncombines image and feature discrepancies, enhancing detection robustness. We\nperform thorough quantitative and qualitative analyses to evaluate the\neffectiveness of GOOD, demonstrating that training with samples generated by\nGOOD can notably enhance OOD detection performance.\n","authors":["Xin Gao","Jiyao Liu","Guanghao Li","Yueming Lyu","Jianxiong Gao","Weichen Yu","Ningsheng Xu","Liang Wang","Caifeng Shan","Ziwei Liu","Chenyang Si"],"pdf_url":"https://arxiv.org/pdf/2510.17131v2.pdf","comment":"28 pages, 16 figures, conference"},{"id":"http://arxiv.org/abs/2510.22937v1","updated":"2025-10-27T02:41:43Z","published":"2025-10-27T02:41:43Z","title":"Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics","summary":"  There has been a historic assumption that the biometrics of an individual are\nstatistically uncorrelated. We test this assumption by training Bi-Encoder\nnetworks on three verification tasks, including fingerprint-to-fingerprint\nmatching, iris-to-iris matching, and cross-modal fingerprint-to-iris matching\nusing 274 subjects with $\\sim$100k fingerprints and 7k iris images. We trained\nResNet-50 and Vision Transformer backbones in Bi-Encoder architectures such\nthat the contrastive loss between images sampled from the same individual is\nminimized. The iris ResNet architecture reaches 91 ROC AUC score for\niris-to-iris matching, providing clear evidence that the left and right irises\nof an individual are correlated. Fingerprint models reproduce the positive\nintra-subject suggested by prior work in this space. This is the first work\nattempting to use Vision Transformers for this matching. Cross-modal matching\nrises only slightly above chance, which suggests that more data and a more\nsophisticated pipeline is needed to obtain compelling results. These findings\ncontinue challenge independence assumptions of biometrics and we plan to extend\nthis work to other biometrics in the future. Code available:\nhttps://github.com/MatthewSo/bio_fingerprints_iris.\n","authors":["Matthew So","Judah Goldfeder","Mark Lis","Hod Lipson"],"pdf_url":"https://arxiv.org/pdf/2510.22937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22936v1","updated":"2025-10-27T02:40:02Z","published":"2025-10-27T02:40:02Z","title":"Positional Preservation Embedding for Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) have achieved strong performance on\nvision-language tasks, yet often suffer from inefficiencies due to redundant\nvisual tokens. Existing token merging methods reduce sequence length but\nfrequently disrupt spatial layouts and temporal continuity by disregarding\npositional relationships. In this work, we propose a novel encoding operator\ndubbed as \\textbf{P}ositional \\textbf{P}reservation \\textbf{E}mbedding\n(\\textbf{PPE}), which has the main hallmark of preservation of spatiotemporal\nstructure during visual token compression. PPE explicitly introduces the\ndisentangled encoding of 3D positions in the token dimension, enabling each\ncompressed token to encapsulate different positions from multiple original\ntokens. Furthermore, we show that PPE can effectively support cascade\nclustering -- a progressive token compression strategy that leads to better\nperformance retention. PPE is a parameter-free and generic operator that can be\nseamlessly integrated into existing token merging methods without any\nadjustments. Applied to state-of-the-art token merging framework, PPE achieves\nconsistent improvements of $2\\%\\sim5\\%$ across multiple vision-language\nbenchmarks, including MMBench (general vision understanding), TextVQA (layout\nunderstanding) and VideoMME (temporal understanding). These results demonstrate\nthat preserving positional cues is critical for efficient and effective MLLM\nreasoning.\n","authors":["Mouxiao Huang","Borui Jiang","Dehua Zheng","Hailin Hu","Kai Han","Xinghao Chen"],"pdf_url":"https://arxiv.org/pdf/2510.22936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.14610v4","updated":"2025-10-27T02:31:50Z","published":"2025-09-18T04:35:29Z","title":"Enhancing Feature Fusion of U-like Networks with Dynamic Skip\n  Connections","summary":"  U-like networks have become fundamental frameworks in medical image\nsegmentation through skip connections that bridge high-level semantics and\nlow-level spatial details. Despite their success, conventional skip connections\nexhibit two key limitations: inter-feature constraints and intra-feature\nconstraints. The inter-feature constraint refers to the static nature of\nfeature fusion in traditional skip connections, where information is\ntransmitted along fixed pathways regardless of feature content. The\nintra-feature constraint arises from the insufficient modeling of multi-scale\nfeature interactions, thereby hindering the effective aggregation of global\ncontextual information. To overcome these limitations, we propose a novel\nDynamic Skip Connection (DSC) block that fundamentally enhances cross-layer\nconnectivity through adaptive mechanisms. The DSC block integrates two\ncomplementary components. (1) Test-Time Training (TTT) module. This module\naddresses the inter-feature constraint by enabling dynamic adaptation of hidden\nrepresentations during inference, facilitating content-aware feature\nrefinement. (2) Dynamic Multi-Scale Kernel (DMSK) module. To mitigate the\nintra-feature constraint, this module adaptively selects kernel sizes based on\nglobal contextual cues, enhancing the network capacity for multi-scale feature\nintegration. The DSC block is architecture-agnostic and can be seamlessly\nincorporated into existing U-like network structures. Extensive experiments\ndemonstrate the plug-and-play effectiveness of the proposed DSC block across\nCNN-based, Transformer-based, hybrid CNN-Transformer, and Mamba-based U-like\nnetworks.\n","authors":["Yue Cao","Quansong He","Kaishen Wang","Jianlong Xiong","Zhang Yi","Tao He"],"pdf_url":"https://arxiv.org/pdf/2509.14610v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21171v2","updated":"2025-10-27T02:24:03Z","published":"2025-10-24T05:51:31Z","title":"TokenCLIP: Token-wise Prompt Learning for Zero-shot Anomaly Detection","summary":"  Adapting CLIP for anomaly detection on unseen objects has shown strong\npotential in a zero-shot manner. However, existing methods typically rely on a\nsingle textual space to align with visual semantics across diverse objects and\ndomains. The indiscriminate alignment hinders the model from accurately\ncapturing varied anomaly semantics. We propose TokenCLIP, a token-wise\nadaptation framework that enables dynamic alignment between visual and\nlearnable textual spaces for fine-grained anomaly learning. Rather than mapping\nall visual tokens to a single, token-agnostic textual space, TokenCLIP aligns\neach token with a customized textual subspace that represents its visual\ncharacteristics. Explicitly assigning a unique learnable textual space to each\ntoken is computationally intractable and prone to insufficient optimization. We\ninstead expand the token-agnostic textual space into a set of orthogonal\nsubspaces, and then dynamically assign each token to a subspace combination\nguided by semantic affinity, which jointly supports customized and efficient\ntoken-wise adaptation. To this end, we formulate dynamic alignment as an\noptimal transport problem, where all visual tokens in an image are transported\nto textual subspaces based on semantic similarity. The transport constraints of\nOT ensure sufficient optimization across subspaces and encourage them to focus\non different semantics. Solving the problem yields a transport plan that\nadaptively assigns each token to semantically relevant subspaces. A top-k\nmasking is then applied to sparsify the plan and specialize subspaces for\ndistinct visual regions. Extensive experiments demonstrate the superiority of\nTokenCLIP.\n","authors":["Qihang Zhou","Binbin Gao","Guansong Pang","Xin Wang","Jiming Chen","Shibo He"],"pdf_url":"https://arxiv.org/pdf/2510.21171v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.19333v2","updated":"2025-10-27T02:16:09Z","published":"2025-10-22T07:54:18Z","title":"A Training-Free Framework for Open-Vocabulary Image Segmentation and\n  Recognition with EfficientNet and CLIP","summary":"  This paper presents a novel training-free framework for open-vocabulary image\nsegmentation and object recognition (OVSR), which leverages EfficientNetB0, a\nconvolutional neural network, for unsupervised segmentation and CLIP, a\nvision-language model, for open-vocabulary object recognition. The proposed\nframework adopts a two stage pipeline: unsupervised image segmentation followed\nby segment-level recognition via vision-language alignment. In the first stage,\npixel-wise features extracted from EfficientNetB0 are decomposed using singular\nvalue decomposition to obtain latent representations, which are then clustered\nusing hierarchical clustering to segment semantically meaningful regions. The\nnumber of clusters is adaptively determined by the distribution of singular\nvalues. In the second stage, the segmented regions are localized and encoded\ninto image embeddings using the Vision Transformer backbone of CLIP. Text\nembeddings are precomputed using CLIP's text encoder from category-specific\nprompts, including a generic something else prompt to support open set\nrecognition. The image and text embeddings are concatenated and projected into\na shared latent feature space via SVD to enhance cross-modal alignment.\nRecognition is performed by computing the softmax over the similarities between\nthe projected image and text embeddings. The proposed method is evaluated on\nstandard benchmarks, including COCO, ADE20K, and PASCAL VOC, achieving\nstate-of-the-art performance in terms of Hungarian mIoU, precision, recall, and\nF1-score. These results demonstrate the effectiveness, flexibility, and\ngeneralizability of the proposed framework.\n","authors":["Ying Dai","Wei Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2510.19333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08222v3","updated":"2025-10-27T02:13:48Z","published":"2024-06-12T13:52:30Z","title":"Refusal as Silence: Gendered Disparities in Vision-Language Model\n  Responses","summary":"  Refusal behavior by Large Language Models is increasingly visible in content\nmoderation, yet little is known about how refusals vary by the identity of the\nuser making the request. This study investigates refusal as a sociotechnical\noutcome through a counterfactual persona design that varies gender\nidentity--including male, female, non-binary, and transgender personas--while\nkeeping the classification task and visual input constant. Focusing on a\nvision-language model (GPT-4V), we examine how identity-based language cues\ninfluence refusal in binary gender classification tasks. We find that\ntransgender and non-binary personas experience significantly higher refusal\nrates, even in non-harmful contexts. Our findings also provide methodological\nimplications for equity audits and content analysis using LLMs. Our findings\nunderscore the importance of modeling identity-driven disparities and caution\nagainst uncritical use of AI systems for content coding. This study advances\nalgorithmic fairness by reframing refusal as a communicative act that may\nunevenly regulate epistemic access and participation.\n","authors":["Sha Luo","Sang Jung Kim","Zening Duan","Kaiping Chen"],"pdf_url":"https://arxiv.org/pdf/2406.08222v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22930v1","updated":"2025-10-27T02:13:38Z","published":"2025-10-27T02:13:38Z","title":"Gen-LangSplat: Generalized Language Gaussian Splatting with Pre-Trained\n  Feature Compression","summary":"  Modeling open-vocabulary language fields in 3D is essential for intuitive\nhuman-AI interaction and querying within physical environments.\nState-of-the-art approaches, such as LangSplat, leverage 3D Gaussian Splatting\nto efficiently construct these language fields, encoding features distilled\nfrom high-dimensional models like CLIP. However, this efficiency is currently\noffset by the requirement to train a scene-specific language autoencoder for\nfeature compression, introducing a costly, per-scene optimization bottleneck\nthat hinders deployment scalability. In this work, we introduce Gen-LangSplat,\nthat eliminates this requirement by replacing the scene-wise autoencoder with a\ngeneralized autoencoder, pre-trained extensively on the large-scale ScanNet\ndataset. This architectural shift enables the use of a fixed, compact latent\nspace for language features across any new scene without any scene-specific\ntraining. By removing this dependency, our entire language field construction\nprocess achieves a efficiency boost while delivering querying performance\ncomparable to, or exceeding, the original LangSplat method. To validate our\ndesign choice, we perform a thorough ablation study empirically determining the\noptimal latent embedding dimension and quantifying representational fidelity\nusing Mean Squared Error and cosine similarity between the original and\nreprojected 512-dimensional CLIP embeddings. Our results demonstrate that\ngeneralized embeddings can efficiently and accurately support open-vocabulary\nquerying in novel 3D scenes, paving the way for scalable, real-time interactive\n3D AI applications.\n","authors":["Pranav Saxena"],"pdf_url":"https://arxiv.org/pdf/2510.22930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.15328v2","updated":"2025-10-27T02:01:46Z","published":"2025-09-18T18:18:49Z","title":"Kuramoto Orientation Diffusion Models","summary":"  Orientation-rich images, such as fingerprints and textures, often exhibit\ncoherent angular directional patterns that are challenging to model using\nstandard generative approaches based on isotropic Euclidean diffusion.\nMotivated by the role of phase synchronization in biological systems, we\npropose a score-based generative model built on periodic domains by leveraging\nstochastic Kuramoto dynamics in the diffusion process. In neural and physical\nsystems, Kuramoto models capture synchronization phenomena across coupled\noscillators -- a behavior that we re-purpose here as an inductive bias for\nstructured image generation. In our framework, the forward process performs\n\\textit{synchronization} among phase variables through globally or locally\ncoupled oscillator interactions and attraction to a global reference phase,\ngradually collapsing the data into a low-entropy von Mises distribution. The\nreverse process then performs \\textit{desynchronization}, generating diverse\npatterns by reversing the dynamics with a learned score function. This approach\nenables structured destruction during forward diffusion and a hierarchical\ngeneration process that progressively refines global coherence into fine-scale\ndetails. We implement wrapped Gaussian transition kernels and periodicity-aware\nnetworks to account for the circular geometry. Our method achieves competitive\nresults on general image benchmarks and significantly improves generation\nquality on orientation-dense datasets like fingerprints and textures.\nUltimately, this work demonstrates the promise of biologically inspired\nsynchronization dynamics as structured priors in generative modeling.\n","authors":["Yue Song","T. Anderson Keller","Sevan Brodjian","Takeru Miyato","Yisong Yue","Pietro Perona","Max Welling"],"pdf_url":"https://arxiv.org/pdf/2509.15328v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.10097v2","updated":"2025-10-27T01:56:41Z","published":"2025-10-11T08:13:46Z","title":"Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian\n  Splatting","summary":"  Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced\n3D reconstruction and novel view synthesis, but remain heavily dependent on\naccurate camera poses and dense viewpoint coverage. These requirements limit\ntheir applicability in sparse-view settings, where pose estimation becomes\nunreliable and supervision is insufficient. To overcome these challenges, we\nintroduce Gesplat, a 3DGS-based framework that enables robust novel view\nsynthesis and geometrically consistent reconstruction from unposed sparse\nimages. Unlike prior works that rely on COLMAP for sparse point cloud\ninitialization, we leverage the VGGT foundation model to obtain more reliable\ninitial poses and dense point clouds. Our approach integrates several key\ninnovations: 1) a hybrid Gaussian representation with dual position-shape\noptimization enhanced by inter-view matching consistency; 2) a graph-guided\nattribute refinement module to enhance scene details; and 3) flow-based depth\nregularization that improves depth estimation accuracy for more effective\nsupervision. Comprehensive quantitative and qualitative experiments demonstrate\nthat our approach achieves more robust performance on both forward-facing and\nlarge-scale complex datasets compared to other pose-free methods.\n","authors":["Jiahui Lu","Haihong Xiao","Xueyan Zhao","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2510.10097v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12926v3","updated":"2025-10-27T01:36:57Z","published":"2025-03-17T08:37:22Z","title":"Task-Oriented Feature Compression for Multimodal Understanding via\n  Device-Edge Co-Inference","summary":"  With the rapid development of large multimodal models (LMMs), multimodal\nunderstanding applications are emerging. As most LMM inference requests\noriginate from edge devices with limited computational capabilities, the\npredominant inference pipeline involves directly forwarding the input data to\nan edge server which handles all computations. However, this approach\nintroduces high transmission latency due to limited uplink bandwidth of edge\ndevices and significant computation latency caused by the prohibitive number of\nvisual tokens, thus hindering delay-sensitive tasks and degrading user\nexperience. To address this challenge, we propose a task-oriented feature\ncompression (TOFC) method for multimodal understanding in a device-edge\nco-inference framework, where visual features are merged by clustering and\nencoded by a learnable and selective entropy model before feature projection.\nSpecifically, we employ density peaks clustering based on K nearest neighbors\nto reduce the number of visual features, thereby minimizing both data\ntransmission and computational complexity. Subsequently, a learnable entropy\nmodel with hyperprior is utilized to encode and decode merged features, further\nreducing transmission overhead. To enhance compression efficiency, multiple\nentropy models are adaptively selected based on the characteristics of the\nvisual features, enabling a more accurate estimation of the probability\ndistribution. Comprehensive experiments on seven visual question answering\nbenchmarks validate the effectiveness of the proposed TOFC method. Results show\nthat TOFC achieves up to 52% reduction in data transmission overhead and 63%\nreduction in system latency while maintaining identical task performance,\ncompared with neural compression ELIC.\n","authors":["Cheng Yuan","Zhening Liu","Jiashu Lv","Jiawei Shao","Yufei Jiang","Jun Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2503.12926v3.pdf","comment":"Accepted by IEEE Transactions on Mobile Computing"},{"id":"http://arxiv.org/abs/2510.22916v1","updated":"2025-10-27T01:35:00Z","published":"2025-10-27T01:35:00Z","title":"Estimating Pasture Biomass from Top-View Images: A Dataset for Precision\n  Agriculture","summary":"  Accurate estimation of pasture biomass is important for decision-making in\nlivestock production systems. Estimates of pasture biomass can be used to\nmanage stocking rates to maximise pasture utilisation, while minimising the\nrisk of overgrazing and promoting overall system health. We present a\ncomprehensive dataset of 1,162 annotated top-view images of pastures collected\nacross 19 locations in Australia. The images were taken across multiple seasons\nand include a range of temperate pasture species. Each image captures a 70cm *\n30cm quadrat and is paired with on-ground measurements including biomass sorted\nby component (green, dead, and legume fraction), vegetation height, and\nNormalized Difference Vegetation Index (NDVI) from Active Optical Sensors\n(AOS). The multidimensional nature of the data, which combines visual,\nspectral, and structural information, opens up new possibilities for advancing\nthe use of precision grazing management. The dataset is released and hosted in\na Kaggle competition that challenges the international Machine Learning\ncommunity with the task of pasture biomass estimation. The dataset is available\non the official Kaggle webpage:\nhttps://www.kaggle.com/competitions/csiro-biomass\n","authors":["Qiyu Liao","Dadong Wang","Rebecca Haling","Jiajun Liu","Xun Li","Martyna Plomecka","Andrew Robson","Matthew Pringle","Rhys Pirie","Megan Walker","Joshua Whelan"],"pdf_url":"https://arxiv.org/pdf/2510.22916v1.pdf","comment":"9 pages, 2 figures, 2 tables, The dataset is available on the\n  official Kaggle webpage: https://www.kaggle.com/competitions/csiro-biomass"},{"id":"http://arxiv.org/abs/2510.10691v2","updated":"2025-10-27T01:20:52Z","published":"2025-10-12T16:38:54Z","title":"Dynamic Gaussian Splatting from Defocused and Motion-blurred Monocular\n  Videos","summary":"  This paper presents a unified framework that allows high-quality dynamic\nGaussian Splatting from both defocused and motion-blurred monocular videos. Due\nto the significant difference between the formation processes of defocus blur\nand motion blur, existing methods are tailored for either one of them, lacking\nthe ability to simultaneously deal with both of them. Although the two can be\njointly modeled as blur kernel-based convolution, the inherent difficulty in\nestimating accurate blur kernels greatly limits the progress in this direction.\nIn this work, we go a step further towards this direction. Particularly, we\npropose to estimate per-pixel reliable blur kernels using a blur prediction\nnetwork that exploits blur-related scene and camera information and is subject\nto a blur-aware sparsity constraint. Besides, we introduce a dynamic Gaussian\ndensification strategy to mitigate the lack of Gaussians for incomplete\nregions, and boost the performance of novel view synthesis by incorporating\nunseen view information to constrain scene optimization. Extensive experiments\nshow that our method outperforms the state-of-the-art methods in generating\nphotorealistic novel view synthesis from defocused and motion-blurred monocular\nvideos. Our code is available at\n\\href{https://github.com/hhhddddddd/dydeblur}{\\textcolor{cyan}{https://github.com/hhhddddddd/dydeblur}}.\n","authors":["Xuankai Zhang","Junjin Xiao","Qing Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.10691v2.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2502.10999v2","updated":"2025-10-27T00:52:27Z","published":"2025-02-16T05:30:18Z","title":"ControlText: Unlocking Controllable Fonts in Multilingual Text Rendering\n  without Font Annotations","summary":"  This work demonstrates that diffusion models can achieve font-controllable\nmultilingual text rendering using just raw images without font label\nannotations.Visual text rendering remains a significant challenge. While recent\nmethods condition diffusion on glyphs, it is impossible to retrieve exact font\nannotations from large-scale, real-world datasets, which prevents\nuser-specified font control. To address this, we propose a data-driven solution\nthat integrates the conditional diffusion model with a text segmentation model,\nutilizing segmentation masks to capture and represent fonts in pixel space in a\nself-supervised manner, thereby eliminating the need for any ground-truth\nlabels and enabling users to customize text rendering with any multilingual\nfont of their choice. The experiment provides a proof of concept of our\nalgorithm in zero-shot text and font editing across diverse fonts and\nlanguages, providing valuable insights for the community and industry toward\nachieving generalized visual text rendering. Code is available at\ngithub.com/bowen-upenn/ControlText.\n","authors":["Bowen Jiang","Yuan Yuan","Xinyi Bai","Zhuoqun Hao","Alyson Yin","Yaojie Hu","Wenyu Liao","Lyle Ungar","Camillo J. Taylor"],"pdf_url":"https://arxiv.org/pdf/2502.10999v2.pdf","comment":"The 2025 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP) Findings"},{"id":"http://arxiv.org/abs/2510.08273v5","updated":"2025-10-27T00:08:58Z","published":"2025-10-09T14:30:34Z","title":"One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion\n  Models for Text-Guided Image Inpainting","summary":"  Text-guided image inpainting aims at reconstructing the masked regions as per\ntext prompts, where the longstanding challenges lie in the preservation for\nunmasked regions, while achieving the semantics consistency between unmasked\nand inpainted masked regions. Previous arts failed to address both of them,\nalways with either of them to be remedied. Such facts, as we observed, stem\nfrom the entanglement of the hybrid (e.g., mid-and-low) frequency bands that\nencode varied image properties, which exhibit different robustness to text\nprompts during the denoising process. In this paper, we propose a\nnull-text-null frequency-aware diffusion models, dubbed \\textbf{NTN-Diff}, for\ntext-guided image inpainting, by decomposing the semantics consistency across\nmasked and unmasked regions into the consistencies as per each frequency band,\nwhile preserving the unmasked regions, to circumvent two challenges in a row.\nBased on the diffusion process, we further divide the denoising process into\nearly (high-level noise) and late (low-level noise) stages, where the\nmid-and-low frequency bands are disentangled during the denoising process. As\nobserved, the stable mid-frequency band is progressively denoised to be\nsemantically aligned during text-guided denoising process, which, meanwhile,\nserves as the guidance to the null-text denoising process to denoise\nlow-frequency band for the masked regions, followed by a subsequent text-guided\ndenoising process at late stage, to achieve the semantics consistency for\nmid-and-low frequency bands across masked and unmasked regions, while preserve\nthe unmasked regions. Extensive experiments validate the superiority of\nNTN-Diff over the state-of-the-art diffusion models to text-guided diffusion\nmodels. Our code can be accessed from https://github.com/htyjers/NTN-Diff.\n","authors":["Haipeng Liu","Yang Wang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2510.08273v5.pdf","comment":"27 pages, 11 figures, to appear at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24795v1","updated":"2025-10-27T17:57:33Z","published":"2025-10-27T17:57:33Z","title":"A Survey on Efficient Vision-Language-Action Models","summary":"  Vision-Language-Action models (VLAs) represent a significant frontier in\nembodied intelligence, aiming to bridge digital knowledge with physical-world\ninteraction. While these models have demonstrated remarkable generalist\ncapabilities, their deployment is severely hampered by the substantial\ncomputational and data requirements inherent to their underlying large-scale\nfoundation models. Motivated by the urgent need to address these challenges,\nthis survey presents the first comprehensive review of Efficient\nVision-Language-Action models (Efficient VLAs) across the entire\ndata-model-training process. Specifically, we introduce a unified taxonomy to\nsystematically organize the disparate efforts in this domain, categorizing\ncurrent techniques into three core pillars: (1) Efficient Model Design,\nfocusing on efficient architectures and model compression; (2) Efficient\nTraining, which reduces computational burdens during model learning; and (3)\nEfficient Data Collection, which addresses the bottlenecks in acquiring and\nutilizing robotic data. Through a critical review of state-of-the-art methods\nwithin this framework, this survey not only establishes a foundational\nreference for the community but also summarizes representative applications,\ndelineates key challenges, and charts a roadmap for future research. We\nmaintain a continuously updated project page to track our latest developments:\nhttps://evla-survey.github.io/\n","authors":["Zhaoshu Yu","Bo Wang","Pengpeng Zeng","Haonan Zhang","Ji Zhang","Lianli Gao","Jingkuan Song","Nicu Sebe","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2510.24795v1.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.24792v1","updated":"2025-10-27T11:00:45Z","published":"2025-10-27T11:00:45Z","title":"PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for\n  the Evaluation of Vision-Language Models","summary":"  Vision-language models (VLMs) have demonstrated remarkable progress in\nmultimodal reasoning. However, existing benchmarks remain limited in terms of\nhigh-quality, human-verified examples. Many current datasets rely on\nsynthetically generated content by large language models (LLMs). Furthermore,\nmost datasets are limited to English, as manual quality assurance of translated\nsamples is time-consuming and costly. To fill this gap, we introduce\nPISA-Bench, a multilingual benchmark derived from English examples of the\nexpert-created PISA tests, a unified framework for the assessment of student\ncompetencies in over eighty countries. Each example consists of human-extracted\ninstructions, questions, answer options, and images, enriched with question\ntype categories, and has been translated from English into five additional\nlanguages (Spanish, German, Chinese, French, and Italian), resulting in a fully\nparallel corpus covering six languages. We evaluate state-of-the-art\nvision-language models on PISA-Bench and find that especially small models\n(<20B parameters) fail to achieve high test scores. We further find substantial\nperformance degradation on non-English splits as well as high error-rates when\nmodels are tasked with spatial and geometric reasoning. By releasing the\ndataset and evaluation framework, we provide a resource for advancing research\non multilingual multimodal reasoning.\n","authors":["Patrick Haller","Fabio Barth","Jonas Golde","Georg Rehm","Alan Akbik"],"pdf_url":"https://arxiv.org/pdf/2510.24792v1.pdf","comment":"8 pages, 11 tables and figures"},{"id":"http://arxiv.org/abs/2510.24791v1","updated":"2025-10-27T10:02:53Z","published":"2025-10-27T10:02:53Z","title":"A Re-node Self-training Approach for Deep Graph-based Semi-supervised\n  Classification on Multi-view Image Data","summary":"  Recently, graph-based semi-supervised learning and pseudo-labeling have\ngained attention due to their effectiveness in reducing the need for extensive\ndata annotations. Pseudo-labeling uses predictions from unlabeled data to\nimprove model training, while graph-based methods are characterized by\nprocessing data represented as graphs. However, the lack of clear graph\nstructures in images combined with the complexity of multi-view data limits the\nefficiency of traditional and existing techniques. Moreover, the integration of\ngraph structures in multi-view data is still a challenge. In this paper, we\npropose Re-node Self-taught Graph-based Semi-supervised Learning for Multi-view\nData (RSGSLM). Our method addresses these challenges by (i) combining linear\nfeature transformation and multi-view graph fusion within a Graph Convolutional\nNetwork (GCN) framework, (ii) dynamically incorporating pseudo-labels into the\nGCN loss function to improve classification in multi-view data, and (iii)\ncorrecting topological imbalances by adjusting the weights of labeled samples\nnear class boundaries. Additionally, (iv) we introduce an unsupervised\nsmoothing loss applicable to all samples. This combination optimizes\nperformance while maintaining computational efficiency. Experimental results on\nmulti-view benchmark image datasets demonstrate that RSGSLM surpasses existing\nsemi-supervised learning approaches in multi-view contexts.\n","authors":["Jingjun Bi","Fadi Dornaika"],"pdf_url":"https://arxiv.org/pdf/2510.24791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.12345v2","updated":"2025-10-27T06:45:09Z","published":"2021-12-23T03:52:33Z","title":"Revisiting Transformation Invariant Geometric Deep Learning: An Initial\n  Representation Perspective","summary":"  Deep neural networks have achieved great success in the last decade. When\ndesigning neural networks to handle the ubiquitous geometric data such as point\nclouds and graphs, it is critical that the model can maintain invariance\ntowards various transformations such as translation, rotation, and scaling.\nMost existing graph neural network (GNN) approaches can only maintain\npermutation-invariance, failing to guarantee invariance with respect to other\ntransformations. Besides GNNs, other works design sophisticated\ntransformation-invariant layers, which are computationally expensive and\ndifficult to be extended. In this paper, we revisit why general neural networks\ncannot maintain transformation invariance. Our findings show that\ntransformation-invariant and distance-preserving initial point representations\nare sufficient to achieve transformation invariance rather than needing\nsophisticated neural layer designs. Motivated by these findings, we propose\nTransformation Invariant Neural Networks (TinvNN), a straightforward and\ngeneral plug-in for geometric data. Specifically, we realize transformation\ninvariant and distance-preserving initial point representations by modifying\nmulti-dimensional scaling and feed the representations into existing neural\nnetworks. We prove that TinvNN can strictly guarantee transformation\ninvariance, being general and flexible enough to be combined with the existing\nneural networks. Extensive experimental results on point cloud analysis and\ncombinatorial optimization demonstrate the effectiveness and general\napplicability of our method. We also extend our method into equivariance cases.\nBased on the results, we advocate that TinvNN should be considered as an\nessential baseline for further studies of transformation-invariant geometric\ndeep learning.\n","authors":["Ziwei Zhang","Xin Wang","Zeyang Zhang","Peng Cui","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2112.12345v2.pdf","comment":"13 pages; accepted by IEEE TPAMI"},{"id":"http://arxiv.org/abs/2510.24788v1","updated":"2025-10-27T05:11:44Z","published":"2025-10-27T05:11:44Z","title":"The Underappreciated Power of Vision Models for Graph Structural\n  Understanding","summary":"  Graph Neural Networks operate through bottom-up message-passing,\nfundamentally differing from human visual perception, which intuitively\ncaptures global structures first. We investigate the underappreciated potential\nof vision models for graph understanding, finding they achieve performance\ncomparable to GNNs on established benchmarks while exhibiting distinctly\ndifferent learning patterns. These divergent behaviors, combined with\nlimitations of existing benchmarks that conflate domain features with\ntopological understanding, motivate our introduction of GraphAbstract. This\nbenchmark evaluates models' ability to perceive global graph properties as\nhumans do: recognizing organizational archetypes, detecting symmetry, sensing\nconnectivity strength, and identifying critical elements. Our results reveal\nthat vision models significantly outperform GNNs on tasks requiring holistic\nstructural understanding and maintain generalizability across varying graph\nscales, while GNNs struggle with global pattern abstraction and degrade with\nincreasing graph size. This work demonstrates that vision models possess\nremarkable yet underutilized capabilities for graph structural understanding,\nparticularly for problems requiring global topological awareness and\nscale-invariant reasoning. These findings open new avenues to leverage this\nunderappreciated potential for developing more effective graph foundation\nmodels for tasks dominated by holistic pattern recognition.\n","authors":["Xinjian Zhao","Wei Pang","Zhongkai Xue","Xiangru Jian","Lei Zhang","Yaoyao Xu","Xiaozhuang Song","Shu Wu","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2510.24788v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24787v1","updated":"2025-10-27T02:31:20Z","published":"2025-10-27T02:31:20Z","title":"ESCA: Enabling Seamless Codec Avatar Execution through Algorithm and\n  Hardware Co-Optimization for Virtual Reality","summary":"  Photorealistic Codec Avatars (PCA), which generate high-fidelity human face\nrenderings, are increasingly being used in Virtual Reality (VR) environments to\nenable immersive communication and interaction through deep learning-based\ngenerative models. However, these models impose significant computational\ndemands, making real-time inference challenging on resource-constrained VR\ndevices such as head-mounted displays, where latency and power efficiency are\ncritical. To address this challenge, we propose an efficient post-training\nquantization (PTQ) method tailored for Codec Avatar models, enabling\nlow-precision execution without compromising output quality. In addition, we\ndesign a custom hardware accelerator that can be integrated into the\nsystem-on-chip of VR devices to further enhance processing efficiency. Building\non these components, we introduce ESCA, a full-stack optimization framework\nthat accelerates PCA inference on edge VR platforms. Experimental results\ndemonstrate that ESCA boosts FovVideoVDP quality scores by up to $+0.39$ over\nthe best 4-bit baseline, delivers up to $3.36\\times$ latency reduction, and\nsustains a rendering rate of 100 frames per second in end-to-end tests,\nsatisfying real-time VR requirements. These results demonstrate the feasibility\nof deploying high-fidelity codec avatars on resource-constrained devices,\nopening the door to more immersive and portable VR experiences.\n","authors":["Mingzhi Zhu","Ding Shang","Sai Qian Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24787v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2510.23606v1","updated":"2025-10-27T17:59:57Z","published":"2025-10-27T17:59:57Z","title":"Variational Masked Diffusion Models","summary":"  Masked diffusion models have recently emerged as a flexible framework for\ndiscrete generative modeling. However, a key limitation of standard masked\ndiffusion is its inability to effectively capture dependencies among tokens\nthat are predicted concurrently, leading to degraded generation quality when\ndependencies among tokens are important. To explicitly model dependencies among\ntokens, we propose Variational Masked Diffusion (VMD), a framework that\nintroduces latent variables into the masked diffusion process. Through\ncontrolled experiments on synthetic datasets, we demonstrate that VMD\nsuccessfully learns dependencies that conventional masked diffusion fails to\ncapture. We further validate the effectiveness of our approach on Sudoku\npuzzles and text datasets, where learning of dependencies among tokens improves\nglobal consistency. Across these domains, VMD enhances both generation quality\nand dependency awareness, highlighting the value of integrating variational\ninference into masked diffusion. Our code is available at:\nhttps://riccizz.github.io/VMD.\n","authors":["Yichi Zhang","Alex Schwing","Zhizhen Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.23606v1.pdf","comment":"Project Page: https://riccizz.github.io/VMD"},{"id":"http://arxiv.org/abs/2510.23605v1","updated":"2025-10-27T17:59:51Z","published":"2025-10-27T17:59:51Z","title":"Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with\n  Progressive Texture Infilling","summary":"  Current 3D/4D generation methods are usually optimized for photorealism,\nefficiency, and aesthetics. However, they often fail to preserve the semantic\nidentity of the subject across different viewpoints. Adapting generation\nmethods with one or few images of a specific subject (also known as\nPersonalization or Subject-driven generation) allows generating visual content\nthat align with the identity of the subject. However, personalized 3D/4D\ngeneration is still largely underexplored. In this work, we introduce TIRE\n(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.\nIt takes an initial 3D asset produced by an existing 3D generative model as\ninput and uses video tracking to identify the regions that need to be modified.\nThen, we adopt a subject-driven 2D inpainting model for progressively infilling\nthe identified regions. Finally, we resplat the modified 2D multi-view\nobservations back to 3D while still maintaining consistency. Extensive\nexperiments demonstrate that our approach significantly improves identity\npreservation in 3D/4D generation compared to state-of-the-art methods. Our\nproject website is available at\nhttps://zsh2000.github.io/track-inpaint-resplat.github.io/.\n","authors":["Shuhong Zheng","Ashkan Mirzaei","Igor Gilitschenski"],"pdf_url":"https://arxiv.org/pdf/2510.23605v1.pdf","comment":"NeurIPS 2025, 38 pages, 22 figures"},{"id":"http://arxiv.org/abs/2506.05314v2","updated":"2025-10-27T17:59:13Z","published":"2025-06-05T17:55:23Z","title":"Constrained Entropic Unlearning: A Primal-Dual Framework for Large\n  Language Models","summary":"  Large Language Models (LLMs) deployed in real-world settings increasingly\nface the need to unlearn sensitive, outdated, or proprietary information.\nExisting unlearning methods typically formulate forgetting and retention as a\nregularized trade-off, combining both objectives into a single scalarized loss.\nThis often leads to unstable optimization and degraded performance on retained\ndata, especially under aggressive forgetting. We propose a new formulation of\nLLM unlearning as a constrained optimization problem: forgetting is enforced\nvia a novel logit-margin flattening loss that explicitly drives the output\ndistribution toward uniformity on a designated forget set, while retention is\npreserved through a hard constraint on a separate retain set. Compared to\nentropy-based objectives, our loss is softmax-free, numerically stable, and\nmaintains non-vanishing gradients, enabling more efficient and robust\noptimization. We solve the constrained problem using a scalable primal-dual\nalgorithm that exposes the trade-off between forgetting and retention through\nthe dynamics of the dual variable, all without any extra computational\noverhead. Evaluations on the TOFU and MUSE benchmarks across diverse LLM\narchitectures demonstrate that our approach consistently matches or exceeds\nstate-of-the-art baselines, effectively removing targeted information while\npreserving downstream utility.\n","authors":["Taha Entesari","Arman Hatami","Rinat Khaziev","Anil Ramakrishna","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2506.05314v2.pdf","comment":"The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems"},{"id":"http://arxiv.org/abs/2510.16923v2","updated":"2025-10-27T17:59:01Z","published":"2025-10-19T16:38:03Z","title":"UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation\n  for End-to-end Adversarial Attacks","summary":"  Deep learning models deployed in safety critical applications like autonomous\ndriving use simulations to test their robustness against adversarial attacks in\nrealistic conditions. However, these simulations are non-differentiable,\nforcing researchers to create attacks that do not integrate simulation\nenvironmental factors, reducing attack success. To address this limitation, we\nintroduce UNDREAM, the first software framework that bridges the gap between\nphotorealistic simulators and differentiable renderers to enable end-to-end\noptimization of adversarial perturbations on any 3D objects. UNDREAM enables\nmanipulation of the environment by offering complete control over weather,\nlighting, backgrounds, camera angles, trajectories, and realistic human and\nobject movements, thereby allowing the creation of diverse scenes. We showcase\na wide array of distinct physically plausible adversarial objects that UNDREAM\nenables researchers to swiftly explore in different configurable environments.\nThis combination of photorealistic simulation and differentiable optimization\nopens new avenues for advancing research of physical adversarial attacks.\n","authors":["Mansi Phute","Matthew Hull","Haoran Wang","Alec Helbling","ShengYun Peng","Willian Lunardi","Martin Andreoni","Wenke Lee","Duen Horng Chau"],"pdf_url":"https://arxiv.org/pdf/2510.16923v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23590v1","updated":"2025-10-27T17:55:06Z","published":"2025-10-27T17:55:06Z","title":"Lightweight Robust Direct Preference Optimization","summary":"  Direct Preference Optimization (DPO) has become a popular method for\nfine-tuning large language models (LLMs) due to its stability and simplicity.\nHowever, it is also known to be sensitive to noise in the data and prone to\noverfitting. Recent works have proposed using distributionally robust\noptimization (DRO) to address potential noise and distributional shift in the\ndata. However, these methods often suffer from excessive conservatism and high\ncomputational cost. We propose DPO-PRO (DPO with Preference Robustness), a\nrobust fine-tuning algorithm based on DPO which accounts for uncertainty in the\npreference distribution through a lightweight DRO formulation. Unlike prior\nDRO-based variants, DPO-PRO focuses solely on uncertainty in preferences,\navoiding unnecessary conservatism and incurring negligible computational\noverhead. We further show that DPO-PRO is equivalent to a regularized DPO\nobjective that penalizes model overconfidence under weak preference signals. We\nevaluate DPO-PRO on standard alignment benchmarks and a real-world public\nhealth task. Experimental results show that our method consistently improves\nrobustness to noisy preference signals compared to existing DPO variants.\n","authors":["Cheol Woo Kim","Shresth Verma","Mauricio Tec","Milind Tambe"],"pdf_url":"https://arxiv.org/pdf/2510.23590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.15963v2","updated":"2025-10-27T17:51:21Z","published":"2025-10-11T20:13:59Z","title":"ESCA: Contextualizing Embodied Agents via Scene-Graph Generation","summary":"  Multi-modal large language models (MLLMs) are making rapid progress toward\ngeneral-purpose embodied agents. However, existing MLLMs do not reliably\ncapture fine-grained links between low-level visual features and high-level\ntextual semantics, leading to weak grounding and inaccurate perception. To\novercome this challenge, we propose ESCA, a framework that contextualizes\nembodied agents by grounding their perception in spatial-temporal scene graphs.\nAt its core is SGCLIP, a novel, open-domain, promptable foundation model for\ngenerating scene graphs that is based on CLIP. SGCLIP is trained on 87K+\nopen-domain videos using a neurosymbolic pipeline that aligns automatically\ngenerated captions with scene graphs produced by the model itself, eliminating\nthe need for human-labeled annotations. We demonstrate that SGCLIP excels in\nboth prompt-based inference and task-specific fine-tuning, achieving\nstate-of-the-art results on scene graph generation and action localization\nbenchmarks. ESCA with SGCLIP improves perception for embodied agents based on\nboth open-source and commercial MLLMs, achieving state of-the-art performance\nacross two embodied environments. Notably, ESCA significantly reduces agent\nperception errors and enables open-source models to surpass proprietary\nbaselines. We release the source code for SGCLIP model training at\nhttps://github.com/video-fm/LASER and for the embodied agent at\nhttps://github.com/video-fm/ESCA.\n","authors":["Jiani Huang","Amish Sethi","Matthew Kuo","Mayank Keoliya","Neelay Velingker","JungHo Jung","Ser-Nam Lim","Ziyang Li","Mayur Naik"],"pdf_url":"https://arxiv.org/pdf/2510.15963v2.pdf","comment":"Accepted as a Spotlight Paper at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23581v1","updated":"2025-10-27T17:50:19Z","published":"2025-10-27T17:50:19Z","title":"Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human\n  Animation","summary":"  Audio-driven human animation models often suffer from identity drift during\ntemporal autoregressive generation, where characters gradually lose their\nidentity over time. One solution is to generate keyframes as intermediate\ntemporal anchors that prevent degradation, but this requires an additional\nkeyframe generation stage and can restrict natural motion dynamics. To address\nthis, we propose Lookahead Anchoring, which leverages keyframes from future\ntimesteps ahead of the current generation window, rather than within it. This\ntransforms keyframes from fixed boundaries into directional beacons: the model\ncontinuously pursues these future anchors while responding to immediate audio\ncues, maintaining consistent identity through persistent guidance. This also\nenables self-keyframing, where the reference image serves as the lookahead\ntarget, eliminating the need for keyframe generation entirely. We find that the\ntemporal lookahead distance naturally controls the balance between expressivity\nand consistency: larger distances allow for greater motion freedom, while\nsmaller ones strengthen identity adherence. When applied to three recent human\nanimation models, Lookahead Anchoring achieves superior lip synchronization,\nidentity preservation, and visual quality, demonstrating improved temporal\nconditioning across several different architectures. Video results are\navailable at the following link: https://lookahead-anchoring.github.io.\n","authors":["Junyoung Seo","Rodrigo Mira","Alexandros Haliassos","Stella Bounareli","Honglie Chen","Linh Tran","Seungryong Kim","Zoe Landgraf","Jie Shen"],"pdf_url":"https://arxiv.org/pdf/2510.23581v1.pdf","comment":"Project page: https://lookahead-anchoring.github.io"},{"id":"http://arxiv.org/abs/2503.07346v2","updated":"2025-10-27T17:45:30Z","published":"2025-03-10T13:59:57Z","title":"Now you see me! Attribution Distributions Reveal What is Truly Important\n  for a Prediction","summary":"  Neural networks are regularly employed in high-stakes decision-making, where\nunderstanding and transparency is key. Attribution methods have been developed\nto gain understanding into which input features neural networks use for a\nspecific prediction. Although widely used in computer vision, these methods\noften result in unspecific saliency maps that fail to identify the relevant\ninformation that led to a decision, supported by different benchmarks results.\nHere, we revisit the common attribution pipeline and identify one cause for the\nlack of specificity in attributions as the computation of attribution of\nisolated logits. Instead, we suggest to combine attributions of multiple class\nlogits in analogy to how the softmax combines the information across logits. By\ncomputing probability distributions of attributions over classes for each\nspatial location in the image, we unleash the true capabilities of existing\nattribution methods, revealing better object- and instance-specificity and\nuncovering discriminative as well as shared features between classes. On common\nbenchmarks, including the grid-pointing game and randomization-based sanity\nchecks, we show that this reconsideration of how and where we compute\nattributions across the network improves established attribution methods while\nstaying agnostic to model architectures. We make the code publicly available:\nhttps://github.com/nilspwalter/var.\n","authors":["Nils Philipp Walter","Jilles Vreeken","Jonas Fischer"],"pdf_url":"https://arxiv.org/pdf/2503.07346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23571v1","updated":"2025-10-27T17:41:38Z","published":"2025-10-27T17:41:38Z","title":"RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim\n  Translation","summary":"  The pursuit of robot generalists - instructable agents capable of performing\ndiverse tasks across diverse environments - demands rigorous and scalable\nevaluation. Yet real-world testing of robot policies remains fundamentally\nconstrained: it is labor-intensive, slow, unsafe at scale, and difficult to\nreproduce. Existing simulation benchmarks are similarly limited, as they train\nand test policies within the same synthetic domains and cannot assess models\ntrained from real-world demonstrations or alternative simulation environments.\nAs policies expand in scope and complexity, these barriers only intensify,\nsince defining \"success\" in robotics often hinges on nuanced human judgments of\nexecution quality. In this paper, we introduce a new benchmarking framework\nthat overcomes these challenges by shifting VLA evaluation into large-scale\nsimulated environments augmented with online human feedback. Leveraging\nadvances in vision-language models, 2D-to-3D generative modeling, and\ndifferentiable rendering, our approach automatically converts video\ndemonstrations from widely used robot datasets into simulated counterparts.\nWithin these digital twins, we assess VLA policies using both automated\nVLM-guided scoring and scalable human preference judgments collected from\ncrowdworkers, transforming human involvement from tedious scene setup,\nresetting, and safety supervision into lightweight preference comparisons. To\nmeasure robustness, we systematically perturb simulated environments along\nmultiple axes, such as textures and object placements, stress-testing policy\ngeneralization under controlled variation. The result is a continuously\nevolving, reproducible, and scalable benchmark for real-world trained robot\nmanipulation policies, addressing a critical missing capability in today's\nrobotics landscape.\n","authors":["Yash Jangir","Yidi Zhang","Kashu Yamazaki","Chenyu Zhang","Kuan-Hsun Tu","Tsung-Wei Ke","Lei Ke","Yonatan Bisk","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2510.23571v1.pdf","comment":"Website: https://robotarenainf.github.io"},{"id":"http://arxiv.org/abs/2510.23564v1","updated":"2025-10-27T17:35:15Z","published":"2025-10-27T17:35:15Z","title":"ReCode: Unify Plan and Action for Universal Granularity Control","summary":"  Real-world tasks require decisions at varying granularities, and humans excel\nat this by leveraging a unified cognitive representation where planning is\nfundamentally understood as a high-level form of action. However, current Large\nLanguage Model (LLM)-based agents lack this crucial capability to operate\nfluidly across decision granularities. This limitation stems from existing\nparadigms that enforce a rigid separation between high-level planning and\nlow-level action, which impairs dynamic adaptability and limits generalization.\nWe propose ReCode (Recursive Code Generation), a novel paradigm that addresses\nthis limitation by unifying planning and action within a single code\nrepresentation. In this representation, ReCode treats high-level plans as\nabstract placeholder functions, which the agent then recursively decomposes\ninto finer-grained sub-functions until reaching primitive actions. This\nrecursive approach dissolves the rigid boundary between plan and action,\nenabling the agent to dynamically control its decision granularity.\nFurthermore, the recursive structure inherently generates rich,\nmulti-granularity training data, enabling models to learn hierarchical\ndecision-making processes. Extensive experiments show ReCode significantly\nsurpasses advanced baselines in inference performance and demonstrates\nexceptional data efficiency in training, validating our core insight that\nunifying planning and action through recursive code generation is a powerful\nand effective approach to achieving universal granularity control. The code is\navailable at https://github.com/FoundationAgents/ReCode.\n","authors":["Zhaoyang Yu","Jiayi Zhang","Huixue Su","Yufan Zhao","Yifan Wu","Mingyi Deng","Jinyu Xiang","Yizhang Lin","Lingxiao Tang","Yingchao Li","Yuyu Luo","Bang Liu","Chenglin Wu"],"pdf_url":"https://arxiv.org/pdf/2510.23564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00062v2","updated":"2025-10-27T17:35:06Z","published":"2025-05-29T13:31:51Z","title":"SafeCOMM: A Study on Safety Degradation in Fine-Tuned Telecom Large\n  Language Models","summary":"  Fine-tuning large language models (LLMs) on telecom datasets is a common\npractice to adapt general-purpose models to the telecom domain. However, little\nattention has been paid to how this process may compromise model safety. Recent\nresearch has shown that even benign fine-tuning can degrade the safety\nalignment of LLMs, causing them to respond to harmful or unethical user\nqueries. In this paper, we investigate this issue by fine-tuning LLMs on three\nrepresentative telecom datasets and show that safety degrades even for light\ntelecom domain adaptation. To this end, we introduce TeleHarm, the first\ntelecom-specific red-teaming benchmark, which we use alongside established\nDirect-Harm and HexPhi datasets to systematically assess harmful behavior. We\nfurther extend our analysis to publicly available TeleLLMs that were\ncontinually pre-trained on large telecom corpora, revealing that safety\nalignment is severely lacking, primarily due to the omission of safety-focused\ninstruction tuning. To address these issues, we evaluate three realignment\ndefenses: SafeInstruct, SafeLoRA, SafeMERGE. We show that, across all settings,\nthe proposed defenses can effectively restore safety without compromising\ntelecom task performance, leading to Safe teleCOMMunication (SafeCOMM) models.\nOur work serves as both a diagnostic study and practical guide for safety\nrealignment in telecom-tuned LLMs, underscoring the need for safety-aware\ninstruction and fine-tuning in the telecom domain.\n","authors":["Aladin Djuhera","Swanand Ravindra Kadhe","Farhan Ahmed","Syed Zawad","Fernando Koch","Walid Saad","Holger Boche"],"pdf_url":"https://arxiv.org/pdf/2506.00062v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23557v1","updated":"2025-10-27T17:31:24Z","published":"2025-10-27T17:31:24Z","title":"Minimizing Human Intervention in Online Classification","summary":"  We introduce and study an online problem arising in question answering\nsystems. In this problem, an agent must sequentially classify user-submitted\nqueries represented by $d$-dimensional embeddings drawn i.i.d. from an unknown\ndistribution. The agent may consult a costly human expert for the correct\nlabel, or guess on her own without receiving feedback. The goal is to minimize\nregret against an oracle with free expert access. When the time horizon $T$ is\nat least exponential in the embedding dimension $d$, one can learn the geometry\nof the class regions: in this regime, we propose the Conservative Hull-based\nClassifier (CHC), which maintains convex hulls of expert-labeled queries and\ncalls the expert as soon as a query lands outside all known hulls. CHC attains\n$\\mathcal{O}(\\log^d T)$ regret in $T$ and is minimax optimal for $d=1$.\nOtherwise, the geometry cannot be reliably learned without additional\ndistributional assumptions. We show that when the queries are drawn from a\nsubgaussian mixture, for $T \\le e^d$, a Center-based Classifier (CC) achieves\nregret proportional to $N\\log{N}$ where $N$ is the number of labels. To bridge\nthese regimes, we introduce the Generalized Hull-based Classifier (GHC), a\npractical extension of CHC that allows for more aggressive guessing via a\ntunable threshold parameter. Our approach is validated with experiments,\nnotably on real-world question-answering datasets using embeddings derived from\nstate-of-the-art large language models.\n","authors":["William Réveillard","Vasileios Saketos","Alexandre Proutiere","Richard Combes"],"pdf_url":"https://arxiv.org/pdf/2510.23557v1.pdf","comment":"49 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.23554v1","updated":"2025-10-27T17:28:55Z","published":"2025-10-27T17:28:55Z","title":"A U-Net and Transformer Pipeline for Multilingual Image Translation","summary":"  This paper presents an end-to-end multilingual translation pipeline that\nintegrates a custom U-Net for text detection, the Tesseract engine for text\nrecognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for\nNeural Machine Translation (NMT). Our approach first utilizes a U-Net model,\ntrained on a synthetic dataset , to accurately segment and detect text regions\nfrom an image. These detected regions are then processed by Tesseract to\nextract the source text. This extracted text is fed into a custom Transformer\nmodel trained from scratch on a multilingual parallel corpus spanning 5\nlanguages. Unlike systems reliant on monolithic pre-trained models, our\narchitecture emphasizes full customization and adaptability. The system is\nevaluated on its text detection accuracy, text recognition quality, and\ntranslation performance via BLEU scores. The complete pipeline demonstrates\npromising results, validating the viability of a custom-built system for\ntranslating text directly from images.\n","authors":["Siddharth Sahay","Radhika Agarwal"],"pdf_url":"https://arxiv.org/pdf/2510.23554v1.pdf","comment":"6 pages, 3 figures, 5 tables, and 2 algorithms. Prepared in IEEE\n  double-column format"},{"id":"http://arxiv.org/abs/2510.19223v2","updated":"2025-10-27T17:26:39Z","published":"2025-10-22T04:07:48Z","title":"Enhancing Graph Neural Networks: A Mutual Learning Approach","summary":"  Knowledge distillation (KD) techniques have emerged as a powerful tool for\ntransferring expertise from complex teacher models to lightweight student\nmodels, particularly beneficial for deploying high-performance models in\nresource-constrained devices. This approach has been successfully applied to\ngraph neural networks (GNNs), harnessing their expressive capabilities to\ngenerate node embeddings that capture structural and feature-related\ninformation. In this study, we depart from the conventional KD approach by\nexploring the potential of collaborative learning among GNNs. In the absence of\na pre-trained teacher model, we show that relatively simple and shallow GNN\narchitectures can synergetically learn efficient models capable of performing\nbetter during inference, particularly in tackling multiple tasks. We propose a\ncollaborative learning framework where ensembles of student GNNs mutually teach\neach other throughout the training process. We introduce an adaptive logit\nweighting unit to facilitate efficient knowledge exchange among models and an\nentropy enhancement technique to improve mutual learning. These components\ndynamically empower the models to adapt their learning strategies during\ntraining, optimizing their performance for downstream tasks. Extensive\nexperiments conducted on three datasets each for node and graph classification\ndemonstrate the effectiveness of our approach.\n","authors":["Paul Agbaje","Arkajyoti Mitra","Afia Anjum","Pranali Khose","Ebelechukwu Nwafor","Habeeb Olufowobi"],"pdf_url":"https://arxiv.org/pdf/2510.19223v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01213v4","updated":"2025-10-27T17:20:28Z","published":"2025-06-01T23:17:19Z","title":"On the Stability of Graph Convolutional Neural Networks: A Probabilistic\n  Perspective","summary":"  Graph convolutional neural networks (GCNNs) have emerged as powerful tools\nfor analyzing graph-structured data, achieving remarkable success across\ndiverse applications. However, the theoretical understanding of the stability\nof these models, i.e., their sensitivity to small changes in the graph\nstructure, remains in rather limited settings, hampering the development and\ndeployment of robust and trustworthy models in practice. To fill this gap, we\nstudy how perturbations in the graph topology affect GCNN outputs and propose a\nnovel formulation for analyzing model stability. Unlike prior studies that\nfocus only on worst-case perturbations, our distribution-aware formulation\ncharacterizes output perturbations across a broad range of input data. This\nway, our framework enables, for the first time, a probabilistic perspective on\nthe interplay between the statistical properties of the node data and\nperturbations in the graph topology. We conduct extensive experiments to\nvalidate our theoretical findings and demonstrate their benefits over existing\nbaselines, in terms of both representation stability and adversarial attacks on\ndownstream tasks. Our results demonstrate the practical significance of the\nproposed formulation and highlight the importance of incorporating data\ndistribution into stability analysis.\n","authors":["Ning Zhang","Henry Kenlay","Li Zhang","Mihai Cucuringu","Xiaowen Dong"],"pdf_url":"https://arxiv.org/pdf/2506.01213v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22494v2","updated":"2025-10-27T17:12:30Z","published":"2025-05-28T15:45:43Z","title":"ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type\n  Neighborhoods","summary":"  Designing protein sequences of both high fitness and novelty is a challenging\ntask in data-efficient protein engineering. Exploration beyond wild-type\nneighborhoods often leads to biologically implausible sequences or relies on\nsurrogate models that lose fidelity in novel regions. Here, we propose\nProSpero, an active learning framework in which a frozen pre-trained generative\nmodel is guided by a surrogate updated from oracle feedback. By integrating\nfitness-relevant residue selection with biologically-constrained Sequential\nMonte Carlo sampling, our approach enables exploration beyond wild-type\nneighborhoods while preserving biological plausibility. We show that our\nframework remains effective even when the surrogate is misspecified. ProSpero\nconsistently outperforms or matches existing methods across diverse protein\nengineering tasks, retrieving sequences of both high fitness and novelty.\n","authors":["Michal Kmicikiewicz","Vincent Fortuin","Ewa Szczurek"],"pdf_url":"https://arxiv.org/pdf/2505.22494v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.20094v2","updated":"2025-10-27T17:12:03Z","published":"2025-10-23T00:28:32Z","title":"On the Structure of Stationary Solutions to McKean-Vlasov Equations with\n  Applications to Noisy Transformers","summary":"  We study stationary solutions of McKean-Vlasov equations on the circle. Our\nmain contributions stem from observing an exact equivalence between solutions\nof the stationary McKean-Vlasov equation and an infinite-dimensional quadratic\nsystem of equations over Fourier coefficients, which allows explicit\ncharacterization of the stationary states in a sequence space rather than a\nfunction space. This framework provides a transparent description of local\nbifurcations, characterizing their periodicity, and resonance structures, while\naccommodating singular potentials. We derive analytic expressions that\ncharacterize the emergence, form and shape (supercritical, critical,\nsubcritical or transcritical) of bifurcations involving possibly multiple\nFourier modes and connect them with discontinuous phase transitions. We also\ncharacterize, under suitable assumptions, the detailed structure of the\nstationary bifurcating solutions that are accurate upto an arbitrary number of\nFourier modes. At the global level, we establish regularity and concavity\nproperties of the free energy landscape, proving existence, compactness, and\ncoexistence of globally minimizing stationary measures, further identifying\ndiscontinuous phase transitions with points of non-differentiability of the\nminimum free energy map. As an application, we specialize the theory to the\nNoisy Mean-Field Transformer model, where we show how changing the inverse\ntemperature parameter $\\beta$ affects the geometry of the infinitely many\nbifurcations from the uniform measure. We also explain how increasing $\\beta$\ncan lead to a rich class of approximate multi-mode stationary solutions which\ncan be seen as `metastable states'. Further, a sharp transition from continuous\nto discontinuous (first-order) phase behavior is observed as $\\beta$ increases.\n","authors":["Krishnakumar Balasubramanian","Sayan Banerjee","Philippe Rigollet"],"pdf_url":"https://arxiv.org/pdf/2510.20094v2.pdf","comment":"46 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.23535v1","updated":"2025-10-27T17:11:03Z","published":"2025-10-27T17:11:03Z","title":"Sequential Multi-Agent Dynamic Algorithm Configuration","summary":"  Dynamic algorithm configuration (DAC) is a recent trend in automated machine\nlearning, which can dynamically adjust the algorithm's configuration during the\nexecution process and relieve users from tedious trial-and-error tuning tasks.\nRecently, multi-agent reinforcement learning (MARL) approaches have improved\nthe configuration of multiple heterogeneous hyperparameters, making various\nparameter configurations for complex algorithms possible. However, many complex\nalgorithms have inherent inter-dependencies among multiple parameters (e.g.,\ndetermining the operator type first and then the operator's parameter), which\nare, however, not considered in previous approaches, thus leading to\nsub-optimal results. In this paper, we propose the sequential multi-agent DAC\n(Seq-MADAC) framework to address this issue by considering the inherent\ninter-dependencies of multiple parameters. Specifically, we propose a\nsequential advantage decomposition network, which can leverage action-order\ninformation through sequential advantage decomposition. Experiments from\nsynthetic functions to the configuration of multi-objective optimization\nalgorithms demonstrate Seq-MADAC's superior performance over state-of-the-art\nMARL methods and show strong generalization across problem classes. Seq-MADAC\nestablishes a new paradigm for the widespread dependency-aware automated\nalgorithm configuration. Our code is available at\nhttps://github.com/lamda-bbo/seq-madac.\n","authors":["Chen Lu","Ke Xue","Lei Yuan","Yao Wang","Yaoyuan Wang","Sheng Fu","Chao Qian"],"pdf_url":"https://arxiv.org/pdf/2510.23535v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.21280v2","updated":"2025-10-27T17:10:50Z","published":"2025-10-24T09:25:31Z","title":"WhaleVAD-BPN: Improving Baleen Whale Call Detection with Boundary\n  Proposal Networks and Post-processing Optimisation","summary":"  While recent sound event detection (SED) systems can identify baleen whale\ncalls in marine audio, challenges related to false positive and minority-class\ndetection persist. We propose the boundary proposal network (BPN), which\nextends an existing lightweight SED system. The BPN is inspired by work in\nimage object detection and aims to reduce the number of false positive\ndetections. It achieves this by using intermediate latent representations\ncomputed within the backbone classification model to gate the final output.\nWhen added to an existing SED system, the BPN achieves a 16.8 % absolute\nincrease in precision, as well as 21.3 % and 9.4 % improvements in the F1-score\nfor minority-class d-calls and bp-calls, respectively. We further consider two\napproaches to the selection of post-processing hyperparameters: a\nforward-search and a backward-search. By separately optimising event-level and\nframe-level hyperparameters, these two approaches lead to considerable\nperformance improvements over parameters selected using empirical methods. The\ncomplete WhaleVAD-BPN system achieves a cross-validated development F1-score of\n0.475, which is a 9.8 % absolute improvement over the baseline.\n","authors":["Christiaan M. Geldenhuys","Günther Tonitz","Thomas R. Niesler"],"pdf_url":"https://arxiv.org/pdf/2510.21280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23534v1","updated":"2025-10-27T17:10:43Z","published":"2025-10-27T17:10:43Z","title":"Direct Debiased Machine Learning via Bregman Divergence Minimization","summary":"  We develop a direct debiased machine learning framework comprising Neyman\ntargeted estimation and generalized Riesz regression. Our framework unifies\nRiesz regression for automatic debiased machine learning, covariate balancing,\ntargeted maximum likelihood estimation (TMLE), and density-ratio estimation. In\nmany problems involving causal effects or structural models, the parameters of\ninterest depend on regression functions. Plugging regression functions\nestimated by machine learning methods into the identifying equations can yield\npoor performance because of first-stage bias. To reduce such bias, debiased\nmachine learning employs Neyman orthogonal estimating equations. Debiased\nmachine learning typically requires estimation of the Riesz representer and the\nregression function. For this problem, we develop a direct debiased machine\nlearning framework with an end-to-end algorithm. We formulate estimation of the\nnuisance parameters, the regression function and the Riesz representer, as\nminimizing the discrepancy between Neyman orthogonal scores computed with known\nand unknown nuisance parameters, which we refer to as Neyman targeted\nestimation. Neyman targeted estimation includes Riesz representer estimation,\nand we measure discrepancies using the Bregman divergence. The Bregman\ndivergence encompasses various loss functions as special cases, where the\nsquared loss yields Riesz regression and the Kullback-Leibler divergence yields\nentropy balancing. We refer to this Riesz representer estimation as generalized\nRiesz regression. Neyman targeted estimation also yields TMLE as a special case\nfor regression function estimation. Furthermore, for specific pairs of models\nand Riesz representer estimation methods, we can automatically obtain the\ncovariate balancing property without explicitly solving the covariate balancing\nobjective.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2510.23534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23532v1","updated":"2025-10-27T17:09:16Z","published":"2025-10-27T17:09:16Z","title":"When No Paths Lead to Rome: Benchmarking Systematic Neural Relational\n  Reasoning","summary":"  Designing models that can learn to reason in a systematic way is an important\nand long-standing challenge. In recent years, a wide range of solutions have\nbeen proposed for the specific case of systematic relational reasoning,\nincluding Neuro-Symbolic approaches, variants of the Transformer architecture,\nand specialised Graph Neural Networks. However, existing benchmarks for\nsystematic relational reasoning focus on an overly simplified setting, based on\nthe assumption that reasoning can be reduced to composing relational paths. In\nfact, this assumption is hard-baked into the architecture of several recent\nmodels, leading to approaches that can perform well on existing benchmarks but\nare difficult to generalise to other settings. To support further progress in\nthe field of systematic relational reasoning with neural networks, we introduce\nNoRA, a new benchmark which adds several levels of difficulty and requires\nmodels to go beyond path-based reasoning.\n","authors":["Anirban Das","Irtaza Khalid","Rafael Peñaloza","Steven Schockaert"],"pdf_url":"https://arxiv.org/pdf/2510.23532v1.pdf","comment":"accepted at NeurIPS 2025 D&B track"},{"id":"http://arxiv.org/abs/2510.23530v1","updated":"2025-10-27T17:08:27Z","published":"2025-10-27T17:08:27Z","title":"Learning Linearity in Audio Consistency Autoencoders via Implicit\n  Regularization","summary":"  Audio autoencoders learn useful, compressed audio representations, but their\nnon-linear latent spaces prevent intuitive algebraic manipulation such as\nmixing or scaling. We introduce a simple training methodology to induce\nlinearity in a high-compression Consistency Autoencoder (CAE) by using data\naugmentation, thereby inducing homogeneity (equivariance to scalar gain) and\nadditivity (the decoder preserves addition) without altering the model's\narchitecture or loss function. When trained with our method, the CAE exhibits\nlinear behavior in both the encoder and decoder while preserving reconstruction\nfidelity. We test the practical utility of our learned space on music source\ncomposition and separation via simple latent arithmetic. This work presents a\nstraightforward technique for constructing structured latent spaces, enabling\nmore intuitive and efficient audio processing.\n","authors":["Bernardo Torres","Manuel Moussallam","Gabriel Meseguer-Brocal"],"pdf_url":"https://arxiv.org/pdf/2510.23530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23524v1","updated":"2025-10-27T17:02:30Z","published":"2025-10-27T17:02:30Z","title":"Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and\n  Learning Paradigms for Sustainable Intelligence","summary":"  The rapid advancement of Artificial Intelligence (AI) has led to\nunprecedented computational demands, raising significant environmental and\nethical concerns. This paper critiques the prevailing reliance on large-scale,\nstatic datasets and monolithic training paradigms, advocating for a shift\ntoward human-inspired, sustainable AI solutions. We introduce a novel\nframework, Human AI (HAI), which emphasizes incremental learning, carbon-aware\noptimization, and human-in-the-loop collaboration to enhance adaptability,\nefficiency, and accountability. By drawing parallels with biological cognition\nand leveraging dynamic architectures, HAI seeks to balance performance with\necological responsibility. We detail the theoretical foundations, system\ndesign, and operational principles that enable AI to learn continuously and\ncontextually while minimizing carbon footprints and human annotation costs. Our\napproach addresses pressing challenges in active learning, continual\nadaptation, and energy-efficient model deployment, offering a pathway toward\nresponsible, human-centered artificial intelligence.\n","authors":["KC Santosh","Rodrigue Rizk","Longwei Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23524v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.20499v2","updated":"2025-10-27T17:00:52Z","published":"2025-07-28T03:34:15Z","title":"DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain\n  Reinforcement Learning","summary":"  Cross-domain offline reinforcement learning (RL) seeks to enhance sample\nefficiency in offline RL by utilizing additional offline source datasets. A key\nchallenge is to identify and utilize source samples that are most relevant to\nthe target domain. Existing approaches address this challenge by measuring\ndomain gaps through domain classifiers, target transition dynamics modeling, or\nmutual information estimation using contrastive loss. However, these methods\noften require large target datasets, which is impractical in many real-world\nscenarios. In this work, we address cross-domain offline RL under a limited\ntarget data setting, identifying two primary challenges: (1) Dataset imbalance,\nwhich is caused by large source and small target datasets and leads to\noverfitting in neural network-based domain gap estimators, resulting in\nuninformative measurements; and (2) Partial domain overlap, where only a subset\nof the source data is closely aligned with the target domain. To overcome these\nissues, we propose DmC, a novel framework for cross-domain offline RL with\nlimited target samples. Specifically, DmC utilizes $k$-nearest neighbor\n($k$-NN) based estimation to measure domain proximity without neural network\ntraining, effectively mitigating overfitting. Then, by utilizing this domain\nproximity, we introduce a nearest-neighbor-guided diffusion model to generate\nadditional source samples that are better aligned with the target domain, thus\nenhancing policy learning with more effective source samples. Through\ntheoretical analysis and extensive experiments in diverse MuJoCo environments,\nwe demonstrate that DmC significantly outperforms state-of-the-art cross-domain\noffline RL methods, achieving substantial performance gains.\n","authors":["Linh Le Pham Van","Minh Hoang Nguyen","Duc Kieu","Hung Le","Hung The Tran","Sunil Gupta"],"pdf_url":"https://arxiv.org/pdf/2507.20499v2.pdf","comment":"accepted at ECAI 2025; offline cross-domain reinforcement learning\n  with a guided diffusion model;"},{"id":"http://arxiv.org/abs/2510.23507v1","updated":"2025-10-27T16:40:52Z","published":"2025-10-27T16:40:52Z","title":"A Deep Latent Factor Graph Clustering with Fairness-Utility Trade-off\n  Perspective","summary":"  Fair graph clustering seeks partitions that respect network structure while\nmaintaining proportional representation across sensitive groups, with\napplications spanning community detection, team formation, resource allocation,\nand social network analysis. Many existing approaches enforce rigid constraints\nor rely on multi-stage pipelines (e.g., spectral embedding followed by\n$k$-means), limiting trade-off control, interpretability, and scalability. We\nintroduce \\emph{DFNMF}, an end-to-end deep nonnegative tri-factorization\ntailored to graphs that directly optimizes cluster assignments with a soft\nstatistical-parity regularizer. A single parameter $\\lambda$ tunes the\nfairness--utility balance, while nonnegativity yields parts-based factors and\ntransparent soft memberships. The optimization uses sparse-friendly alternating\nupdates and scales near-linearly with the number of edges. Across synthetic and\nreal networks, DFNMF achieves substantially higher group balance at comparable\nmodularity, often dominating state-of-the-art baselines on the Pareto front.\nThe code is available at https://github.com/SiamakGhodsi/DFNMF.git.\n","authors":["Siamak Ghodsi","Amjad Seyedi","Tai Le Quy","Fariba Karimi","Eirini Ntoutsi"],"pdf_url":"https://arxiv.org/pdf/2510.23507v1.pdf","comment":"Accepted to IEEE Big-Data 2025 main research track. The paper is 10\n  main pages and 4 pages of Appendix"},{"id":"http://arxiv.org/abs/2510.23503v1","updated":"2025-10-27T16:36:51Z","published":"2025-10-27T16:36:51Z","title":"Bayes-Split-Edge: Bayesian Optimization for Constrained Collaborative\n  Inference in Wireless Edge Systems","summary":"  Mobile edge devices (e.g., AR/VR headsets) typically need to complete timely\ninference tasks while operating with limited on-board computing and energy\nresources. In this paper, we investigate the problem of collaborative inference\nin wireless edge networks, where energy-constrained edge devices aim to\ncomplete inference tasks within given deadlines. These tasks are carried out\nusing neural networks, and the edge device seeks to optimize inference\nperformance under energy and delay constraints. The inference process can be\nsplit between the edge device and an edge server, thereby achieving\ncollaborative inference over wireless networks. We formulate an inference\nutility optimization problem subject to energy and delay constraints, and\npropose a novel solution called Bayes-Split-Edge, which leverages Bayesian\noptimization for collaborative split inference over wireless edge networks. Our\nsolution jointly optimizes the transmission power and the neural network split\npoint. The Bayes-Split-Edge framework incorporates a novel hybrid acquisition\nfunction that balances inference task utility, sample efficiency, and\nconstraint violation penalties. We evaluate our approach using the VGG19 model\non the ImageNet-Mini dataset, and Resnet101 on Tiny-ImageNet, and real-world\nmMobile wireless channel datasets. Numerical results demonstrate that\nBayes-Split-Edge achieves up to 2.4x reduction in evaluation cost compared to\nstandard Bayesian optimization and achieves near-linear convergence. It also\noutperforms several baselines, including CMA-ES, DIRECT, exhaustive search, and\nProximal Policy Optimization (PPO), while matching exhaustive search\nperformance under tight constraints. These results confirm that the proposed\nframework provides a sample-efficient solution requiring maximum 20 function\nevaluations and constraint-aware optimization for wireless split inference in\nedge computing systems.\n","authors":["Fatemeh Zahra Safaeipour","Jacob Chakareski","Morteza Hashemi"],"pdf_url":"https://arxiv.org/pdf/2510.23503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23501v1","updated":"2025-10-27T16:35:01Z","published":"2025-10-27T16:35:01Z","title":"Towards Deep Physics-Informed Kolmogorov-Arnold Networks","summary":"  Since their introduction, Kolmogorov-Arnold Networks (KANs) have been\nsuccessfully applied across several domains, with physics-informed machine\nlearning (PIML) emerging as one of the areas where they have thrived. In the\nPIML setting, Chebyshev-based physics-informed KANs (cPIKANs) have become the\nstandard due to their computational efficiency. However, like their multilayer\nperceptron-based counterparts, cPIKANs face significant challenges when scaled\nto depth, leading to training instabilities that limit their applicability to\nseveral PDE problems. To address this, we propose a basis-agnostic, Glorot-like\ninitialization scheme that preserves activation variance and yields substantial\nimprovements in stability and accuracy over the default initialization of\ncPIKANs. Inspired by the PirateNet architecture, we further introduce\nResidual-Gated Adaptive KANs (RGA KANs), designed to mitigate divergence in\ndeep cPIKANs where initialization alone is not sufficient. Through empirical\ntests and information bottleneck analysis, we show that RGA KANs successfully\ntraverse all training phases, unlike baseline cPIKANs, which stagnate in the\ndiffusion phase in specific PDE settings. Evaluations on seven standard forward\nPDE benchmarks under a fixed training pipeline with adaptive components\ndemonstrate that RGA KANs consistently outperform parameter-matched cPIKANs and\nPirateNets - often by several orders of magnitude - while remaining stable in\nsettings where the others diverge.\n","authors":["Spyros Rigas","Fotios Anagnostopoulos","Michalis Papachristou","Georgios Alexandridis"],"pdf_url":"https://arxiv.org/pdf/2510.23501v1.pdf","comment":"73 pages, 22 figures"},{"id":"http://arxiv.org/abs/2510.23498v1","updated":"2025-10-27T16:32:56Z","published":"2025-10-27T16:32:56Z","title":"Mixed Precision Training of Neural ODEs","summary":"  Exploiting low-precision computations has become a standard strategy in deep\nlearning to address the growing computational costs imposed by ever larger\nmodels and datasets. However, naively performing all computations in low\nprecision can lead to roundoff errors and instabilities. Therefore, mixed\nprecision training schemes usually store the weights in high precision and use\nlow-precision computations only for whitelisted operations. Despite their\nsuccess, these principles are currently not reliable for training\ncontinuous-time architectures such as neural ordinary differential equations\n(Neural ODEs). This paper presents a mixed precision training framework for\nneural ODEs, combining explicit ODE solvers with a custom backpropagation\nscheme, and demonstrates its effectiveness across a range of learning tasks.\nOur scheme uses low-precision computations for evaluating the velocity,\nparameterized by the neural network, and for storing intermediate states, while\nstability is provided by a custom dynamic adjoint scaling and by accumulating\nthe solution and gradients in higher precision. These contributions address two\nkey challenges in training neural ODE: the computational cost of repeated\nnetwork evaluations and the growth of memory requirements with the number of\ntime steps or layers. Along with the paper, we publish our extendable,\nopen-source PyTorch package rampde, whose syntax resembles that of leading\npackages to provide a drop-in replacement in existing codes. We demonstrate the\nreliability and effectiveness of our scheme using challenging test cases and on\nneural ODE applications in image classification and generative models,\nachieving approximately 50% memory reduction and up to 2x speedup while\nmaintaining accuracy comparable to single-precision training.\n","authors":["Elena Celledoni","Brynjulf Owren","Lars Ruthotto","Tianjiao Nicole Yang"],"pdf_url":"https://arxiv.org/pdf/2510.23498v1.pdf","comment":"Code available at https://github.com/EmoryMLIP/rampde; 26 pages, 4\n  figures"},{"id":"http://arxiv.org/abs/2502.06545v3","updated":"2025-10-27T16:31:36Z","published":"2025-02-10T15:10:06Z","title":"Universal Sequence Preconditioning","summary":"  We study the problem of preconditioning in sequential prediction. From the\ntheoretical lens of linear dynamical systems, we show that convolving the input\nsequence corresponds to applying a polynomial to the hidden transition matrix.\nBuilding on this insight, we propose a universal preconditioning method that\nconvolves the input with coefficients from orthogonal polynomials such as\nChebyshev or Legendre. We prove that this approach reduces regret for two\ndistinct prediction algorithms and yields the first ever sublinear and\nhidden-dimension independent regret bounds (up to logarithmic factors) that\nhold for systems with marginally stable and asymmetric transition matrices.\nFinally, extensive synthetic and real-world experiments show that this simple\npreconditioning strategy improves the performance of a diverse range of\nalgorithms, including recurrent neural networks, and generalizes to signals\nbeyond linear dynamical systems.\n","authors":["Annie Marsden","Elad Hazan"],"pdf_url":"https://arxiv.org/pdf/2502.06545v3.pdf","comment":"35 pages, 3 tables, 5 figures"},{"id":"http://arxiv.org/abs/2509.04653v2","updated":"2025-10-27T16:26:55Z","published":"2025-09-04T20:40:37Z","title":"Deriving Transformer Architectures as Implicit Multinomial Regression","summary":"  While attention has been empirically shown to improve model performance, it\nlacks a rigorous mathematical justification. This short paper establishes a\nnovel connection between attention mechanisms and multinomial regression.\nSpecifically, we show that in a fixed multinomial regression setting,\noptimizing over latent features yields solutions that align with the dynamics\ninduced on features by attention blocks. In other words, the evolution of\nrepresentations through a transformer can be interpreted as a trajectory that\nrecovers the optimal features for classification.\n","authors":["Jonas A. Actor","Anthony Gruber","Eric C. Cyr"],"pdf_url":"https://arxiv.org/pdf/2509.04653v2.pdf","comment":"4 pages, additional 3 pages of references and supplementary details"},{"id":"http://arxiv.org/abs/2510.23489v1","updated":"2025-10-27T16:25:16Z","published":"2025-10-27T16:25:16Z","title":"Quantum Phase Classification of Rydberg Atom Systems Using\n  Resource-Efficient Variational Quantum Circuits and Classical Shadows","summary":"  Quantum phase transitions in Rydberg atom arrays present significant\nopportunities for studying many-body physics, yet distinguishing between\ndifferent ordered phases without explicit order parameters remains challenging.\nWe present a resource-efficient quantum machine learning approach combining\nclassical shadow tomography with variational quantum circuits (VQCs) for binary\nphase classification of Z2 and Z3 ordered phases. Our pipeline processes 500\nrandomized measurements per 51-atom chain state, reconstructs shadow operators,\nperforms PCA dimensionality reduction (514 features), and encodes features\nusing angle embedding onto a 2-qubit parameterized circuit. The circuit employs\nRY-RZ angle encoding, strong entanglement via all-to-all CZ gates, and a\nminimal 2-parameter ansatz achieving depth 7. Training via simultaneous\nperturbation stochastic approximation (SPSA) with hinge loss converged in 120\niterations. The model achieved 100% test accuracy with perfect precision,\nrecall, and F1 scores, demonstrating that minimal quantum resources suffice for\nhigh-accuracy phase classification. This work establishes pathways for\nquantum-enhanced condensed matter physics on near-term quantum devices.\n","authors":["Hemish Ahuja","Samradh Bhardwaj","Kirti Dhir","Roman Bagdasarian","Ziwoong Jang"],"pdf_url":"https://arxiv.org/pdf/2510.23489v1.pdf","comment":"7 pages, 2 tables, and 3 figures. for associated code files, see\n  https://github.com/Hemishahuja/FLIQ_Challenge_ClassiqDuQIS"},{"id":"http://arxiv.org/abs/2510.17714v2","updated":"2025-10-27T16:20:43Z","published":"2025-10-20T16:28:42Z","title":"The Marked Edge Walk: A Novel MCMC Algorithm for Sampling of Graph\n  Partitions","summary":"  Novel Markov Chain Monte Carlo (MCMC) methods have enabled the generation of\nlarge ensembles of redistricting plans through graph partitioning. However,\nexisting algorithms such as Reversible Recombination (RevReCom) and\nMetropolized Forest Recombination (MFR) are constrained to sampling from\ndistributions related to spanning trees. We introduce the marked edge walk\n(MEW), a novel MCMC algorithm for sampling from the space of graph partitions\nunder a tunable distribution. The walk operates on the space of spanning trees\nwith marked edges, allowing for calculable transition probabilities for use in\nthe Metropolis-Hastings algorithm. Empirical results on real-world dual graphs\nshow convergence under target distributions unrelated to spanning trees. For\nthis reason, MEW represents an advancement in flexible ensemble generation.\n","authors":["Atticus McWhorter","Daryl DeFord"],"pdf_url":"https://arxiv.org/pdf/2510.17714v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.05530v3","updated":"2025-10-27T16:20:28Z","published":"2025-03-07T15:54:04Z","title":"Leveraging Approximate Caching for Faster Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems.\n","authors":["Shai Bergman","Anne-Marie Kermarrec","Diana Petrescu","Rafael Pires","Mathis Randl","Martijn de Vos","Ji Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.05530v3.pdf","comment":"Accepted at Middleware '25"},{"id":"http://arxiv.org/abs/2503.05965v4","updated":"2025-10-27T16:18:11Z","published":"2025-03-07T22:09:47Z","title":"Validating LLM-as-a-Judge Systems under Rating Indeterminacy","summary":"  The LLM-as-a-judge paradigm, in which a judge LLM system replaces human\nraters in rating the outputs of other generative AI (GenAI) systems, plays a\ncritical role in scaling and standardizing GenAI evaluations. To validate such\njudge systems, evaluators assess human--judge agreement by first collecting\nmultiple human ratings for each item in a validation corpus, then aggregating\nthe ratings into a single, per-item gold label rating. For many items, however,\nrating criteria may admit multiple valid interpretations, so a human or LLM\nrater may deem multiple ratings \"reasonable\" or \"correct.\" We call this\ncondition rating indeterminacy. Problematically, many rating tasks that contain\nrating indeterminacy rely on forced-choice elicitation, whereby raters are\ninstructed to select only one rating for each item. In this paper, we introduce\na framework for validating LLM-as-a-judge systems under rating indeterminacy.\nWe draw theoretical connections between different measures of judge system\nperformance under different human--judge agreement metrics, and different\nrating elicitation and aggregation schemes. We demonstrate that differences in\nhow humans and LLMs resolve rating indeterminacy when responding to\nforced-choice rating instructions can heavily bias LLM-as-a-judge validation.\nThrough extensive experiments involving 11 real-world rating tasks and 9\ncommercial LLMs, we show that standard validation approaches that rely upon\nforced-choice ratings select judge systems that are highly suboptimal,\nperforming as much as 31% worse than judge systems selected by our approach\nthat uses multi-label \"response set\" ratings to account for rating\nindeterminacy. We conclude with concrete recommendations for more principled\napproaches to LLM-as-a-judge validation.\n","authors":["Luke Guerdan","Solon Barocas","Kenneth Holstein","Hanna Wallach","Zhiwei Steven Wu","Alexandra Chouldechova"],"pdf_url":"https://arxiv.org/pdf/2503.05965v4.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23486v1","updated":"2025-10-27T16:17:45Z","published":"2025-10-27T16:17:45Z","title":"Learning to Reason Efficiently with Discounted Reinforcement Learning","summary":"  Large reasoning models (LRMs) often consume excessive tokens, inflating\ncomputational cost and latency. We challenge the assumption that longer\nresponses improve accuracy. By penalizing reasoning tokens using a discounted\nreinforcement learning setup (interpretable as a small token cost) and\nanalyzing Blackwell optimality in restricted policy classes, we encourage\nconcise yet accurate reasoning. Experiments confirm our theoretical results\nthat this approach shortens chains of thought while preserving accuracy.\n","authors":["Alex Ayoub","Kavosh Asadi","Dale Schuurmans","Csaba Szepesvári","Karim Bouyarmane"],"pdf_url":"https://arxiv.org/pdf/2510.23486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01034v2","updated":"2025-10-27T16:17:17Z","published":"2025-06-01T14:30:46Z","title":"Less is More: Local Intrinsic Dimensions of Contextual Language Models","summary":"  Understanding the internal mechanisms of large language models (LLMs) remains\na challenging and complex endeavor. Even fundamental questions, such as how\nfine-tuning affects model behavior, often require extensive empirical\nevaluation. In this paper, we introduce a novel perspective based on the\ngeometric properties of contextual latent embeddings to study the effects of\ntraining and fine-tuning. To that end, we measure the local dimensions of a\ncontextual language model's latent space and analyze their shifts during\ntraining and fine-tuning. We show that the local dimensions provide insights\ninto the model's training dynamics and generalization ability. Specifically,\nthe mean of the local dimensions predicts when the model's training\ncapabilities are exhausted, as exemplified in a dialogue state tracking task,\noverfitting, as demonstrated in an emotion recognition task, and grokking, as\nillustrated with an arithmetic task. Furthermore, our experiments suggest a\npractical heuristic: reductions in the mean local dimension tend to accompany\nand predict subsequent performance gains. Through this exploration, we aim to\nprovide practitioners with a deeper understanding of the implications of\nfine-tuning on embedding spaces, facilitating informed decisions when\nconfiguring models for specific applications. The results of this work\ncontribute to the ongoing discourse on the interpretability, adaptability, and\ngeneralizability of LLMs by bridging the gap between intrinsic model mechanisms\nand geometric properties in the respective embeddings.\n","authors":["Benjamin Matthias Ruppik","Julius von Rohrscheidt","Carel van Niekerk","Michael Heck","Renato Vukovic","Shutong Feng","Hsien-chin Lin","Nurul Lubis","Bastian Rieck","Marcus Zibrowius","Milica Gašić"],"pdf_url":"https://arxiv.org/pdf/2506.01034v2.pdf","comment":"Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025; in press). 10 pages, with an additional 17 pages in\n  the appendix. Our code is available at\n  https://github.com/aidos-lab/Topo_LLM_public and\n  https://github.com/aidos-lab/grokking-via-lid"},{"id":"http://arxiv.org/abs/2510.23485v1","updated":"2025-10-27T16:17:09Z","published":"2025-10-27T16:17:09Z","title":"Tighter CMI-Based Generalization Bounds via Stochastic Projection and\n  Quantization","summary":"  In this paper, we leverage stochastic projection and lossy compression to\nestablish new conditional mutual information (CMI) bounds on the generalization\nerror of statistical learning algorithms. It is shown that these bounds are\ngenerally tighter than the existing ones. In particular, we prove that for\ncertain problem instances for which existing MI and CMI bounds were recently\nshown in Attias et al. [2024] and Livni [2023] to become vacuous or fail to\ndescribe the right generalization behavior, our bounds yield suitable\ngeneralization guarantees of the order of $\\mathcal{O}(1/\\sqrt{n})$, where $n$\nis the size of the training dataset. Furthermore, we use our bounds to\ninvestigate the problem of data \"memorization\" raised in those works, and which\nasserts that there are learning problem instances for which any learning\nalgorithm that has good prediction there exist distributions under which the\nalgorithm must \"memorize\" a big fraction of the training dataset. We show that\nfor every learning algorithm, there exists an auxiliary algorithm that does not\nmemorize and which yields comparable generalization error for any data\ndistribution. In part, this shows that memorization is not necessary for good\ngeneralization.\n","authors":["Milad Sefidgaran","Kimia Nadjahi","Abdellatif Zaidi"],"pdf_url":"https://arxiv.org/pdf/2510.23485v1.pdf","comment":"Accepted for oral presentation at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23484v1","updated":"2025-10-27T16:16:40Z","published":"2025-10-27T16:16:40Z","title":"T-REGS: Minimum Spanning Tree Regularization for Self-Supervised\n  Learning","summary":"  Self-supervised learning (SSL) has emerged as a powerful paradigm for\nlearning representations without labeled data, often by enforcing invariance to\ninput transformations such as rotations or blurring. Recent studies have\nhighlighted two pivotal properties for effective representations: (i) avoiding\ndimensional collapse-where the learned features occupy only a low-dimensional\nsubspace, and (ii) enhancing uniformity of the induced distribution. In this\nwork, we introduce T-REGS, a simple regularization framework for SSL based on\nthe length of the Minimum Spanning Tree (MST) over the learned representation.\nWe provide theoretical analysis demonstrating that T-REGS simultaneously\nmitigates dimensional collapse and promotes distribution uniformity on\narbitrary compact Riemannian manifolds. Several experiments on synthetic data\nand on classical SSL benchmarks validate the effectiveness of our approach at\nenhancing representation quality.\n","authors":["Julie Mordacq","David Loiseaux","Vicky Kalogeiton","Steve Oudot"],"pdf_url":"https://arxiv.org/pdf/2510.23484v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2504.17660v2","updated":"2025-10-27T16:14:05Z","published":"2025-04-24T15:29:39Z","title":"Effortless, Simulation-Efficient Bayesian Inference using Tabular\n  Foundation Models","summary":"  Simulation-based inference (SBI) offers a flexible and general approach to\nperforming Bayesian inference: In SBI, a neural network is trained on synthetic\ndata simulated from a model and used to rapidly infer posterior distributions\nfor observed data. A key goal for SBI is to achieve accurate inference with as\nfew simulations as possible, especially for expensive simulators. In this work,\nwe address this challenge by repurposing recent probabilistic foundation models\nfor tabular data: We show how tabular foundation models -- specifically TabPFN\n-- can be used as pre-trained autoregressive conditional density estimators for\nSBI. We propose Neural Posterior Estimation with Prior-data Fitted Networks\n(NPE-PFN) and show that it is competitive with current SBI approaches in terms\nof accuracy for both benchmark tasks and two complex scientific inverse\nproblems. Crucially, it often substantially outperforms them in terms of\nsimulation efficiency, sometimes requiring orders of magnitude fewer\nsimulations. NPE-PFN eliminates the need for inference network selection,\ntraining, and hyperparameter tuning. We also show that it exhibits superior\nrobustness to model misspecification and can be scaled to simulation budgets\nthat exceed the context size limit of TabPFN. NPE-PFN provides a new direction\nfor SBI, where training-free, general-purpose inference models offer efficient,\neasy-to-use, and flexible solutions for a wide range of stochastic inverse\nproblems.\n","authors":["Julius Vetter","Manuel Gloeckler","Daniel Gedon","Jakob H. Macke"],"pdf_url":"https://arxiv.org/pdf/2504.17660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23472v1","updated":"2025-10-27T16:10:32Z","published":"2025-10-27T16:10:32Z","title":"BBOPlace-Bench: Benchmarking Black-Box Optimization for Chip Placement","summary":"  Chip placement is a vital stage in modern chip design as it has a substantial\nimpact on the subsequent processes and the overall quality of the final chip.\nThe use of black-box optimization (BBO) for chip placement has a history of\nseveral decades. However, early efforts were limited by immature problem\nformulations and inefficient algorithm designs. Recent progress has shown the\neffectiveness and efficiency of BBO for chip placement, proving its potential\nto achieve state-of-the-art results. Despite these advancements, the field\nlacks a unified, BBO-specific benchmark for thoroughly assessing various\nproblem formulations and BBO algorithms. To fill this gap, we propose\nBBOPlace-Bench, the first benchmark designed specifically for evaluating and\ndeveloping BBO algorithms for chip placement tasks. It integrates three problem\nformulations of BBO for chip placement, and offers a modular, decoupled, and\nflexible framework that enables users to seamlessly implement, test, and\ncompare their own algorithms. BBOPlace-Bench integrates a wide variety of\nexisting BBO algorithms, including simulated annealing (SA), evolutionary\nalgorithms (EAs), and Bayesian optimization (BO). Experimental results show\nthat the problem formulations of mask-guided optimization and hyperparameter\noptimization exhibit superior performance than the sequence pair problem\nformulation, while EAs demonstrate better overall performance than SA and BO,\nespecially in high-dimensional search spaces, and also achieve state-of-the-art\nperformance compared to the mainstream chip placement methods. BBOPlace-Bench\nnot only facilitates the development of efficient BBO-driven solutions for chip\nplacement but also broadens the practical application scenarios (which are\nurgently needed) for the BBO community. The code of BBOPlace-Bench is available\nat https://github.com/lamda-bbo/BBOPlace-Bench.\n","authors":["Ke Xue","Ruo-Tong Chen","Rong-Xi Tan","Xi Lin","Yunqi Shi","Siyuan Xu","Mingxuan Yuan","Chao Qian"],"pdf_url":"https://arxiv.org/pdf/2510.23472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23471v1","updated":"2025-10-27T16:09:07Z","published":"2025-10-27T16:09:07Z","title":"Robust Decision Making with Partially Calibrated Forecasts","summary":"  Calibration has emerged as a foundational goal in ``trustworthy machine\nlearning'', in part because of its strong decision theoretic semantics.\nIndependent of the underlying distribution, and independent of the decision\nmaker's utility function, calibration promises that amongst all policies\nmapping predictions to actions, the uniformly best policy is the one that\n``trusts the predictions'' and acts as if they were correct. But this is true\nonly of \\emph{fully calibrated} forecasts, which are tractable to guarantee\nonly for very low dimensional prediction problems. For higher dimensional\nprediction problems (e.g. when outcomes are multiclass), weaker forms of\ncalibration have been studied that lack these decision theoretic properties. In\nthis paper we study how a conservative decision maker should map predictions\nendowed with these weaker (``partial'') calibration guarantees to actions, in a\nway that is robust in a minimax sense: i.e. to maximize their expected utility\nin the worst case over distributions consistent with the calibration\nguarantees. We characterize their minimax optimal decision rule via a duality\nargument, and show that surprisingly, ``trusting the predictions and acting\naccordingly'' is recovered in this minimax sense by \\emph{decision calibration}\n(and any strictly stronger notion of calibration), a substantially weaker and\nmore tractable condition than full calibration. For calibration guarantees that\nfall short of decision calibration, the minimax optimal decision rule is still\nefficiently computable, and we provide an empirical evaluation of a natural one\nthat applies to any regression model solved to optimize squared error.\n","authors":["Shayan Kiyani","Hamed Hassani","George Pappas","Aaron Roth"],"pdf_url":"https://arxiv.org/pdf/2510.23471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23469v1","updated":"2025-10-27T16:07:36Z","published":"2025-10-27T16:07:36Z","title":"Adaptive Dual Prompting: Hierarchical Debiasing for Fairness-aware Graph\n  Neural Networks","summary":"  In recent years, pre-training Graph Neural Networks (GNNs) through\nself-supervised learning on unlabeled graph data has emerged as a widely\nadopted paradigm in graph learning. Although the paradigm is effective for\npre-training powerful GNN models, the objective gap often exists between\npre-training and downstream tasks. To bridge this gap, graph prompting adapts\npre-trained GNN models to specific downstream tasks with extra learnable\nprompts while keeping the pre-trained GNN models frozen. As recent graph\nprompting methods largely focus on enhancing model utility on downstream tasks,\nthey often overlook fairness concerns when designing prompts for adaptation. In\nfact, pre-trained GNN models will produce discriminative node representations\nacross demographic subgroups, as downstream graph data inherently contains\nbiases in both node attributes and graph structures. To address this issue, we\npropose an Adaptive Dual Prompting (ADPrompt) framework that enhances fairness\nfor adapting pre-trained GNN models to downstream tasks. To mitigate attribute\nbias, we design an Adaptive Feature Rectification module that learns customized\nattribute prompts to suppress sensitive information at the input layer,\nreducing bias at the source. Afterward, we propose an Adaptive Message\nCalibration module that generates structure prompts at each layer, which adjust\nthe message from neighboring nodes to enable dynamic and soft calibration of\nthe information flow. Finally, ADPrompt jointly optimizes the two prompting\nmodules to adapt the pre-trained GNN while enhancing fairness. We conduct\nextensive experiments on four datasets with four pre-training strategies to\nevaluate the performance of ADPrompt. The results demonstrate that our proposed\nADPrompt outperforms seven baseline methods on node classification tasks.\n","authors":["Yuhan Yang","Xingbo Fu","Jundong Li"],"pdf_url":"https://arxiv.org/pdf/2510.23469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18242v2","updated":"2025-10-27T16:02:45Z","published":"2025-07-24T09:30:37Z","title":"Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods","summary":"  Despite their theoretical appeal, totally corrective boosting methods based\non linear programming have received limited empirical attention. In this paper,\nwe conduct the first large-scale experimental study of six LP-based boosting\nformulations, including two novel methods, NM-Boost and QRLP-Boost, across 20\ndiverse datasets. We evaluate the use of both heuristic and optimal base\nlearners within these formulations, and analyze not only accuracy, but also\nensemble sparsity, margin distribution, anytime performance, and hyperparameter\nsensitivity. We show that totally corrective methods can outperform or match\nstate-of-the-art heuristics like XGBoost and LightGBM when using shallow trees,\nwhile producing significantly sparser ensembles. We further show that these\nmethods can thin pre-trained ensembles without sacrificing performance, and we\nhighlight both the strengths and limitations of using optimal decision trees in\nthis context.\n","authors":["Fabian Akkerman","Julien Ferry","Christian Artigues","Emmanuel Hebrard","Thibaut Vidal"],"pdf_url":"https://arxiv.org/pdf/2507.18242v2.pdf","comment":"Published in TMLR, see: https://openreview.net/forum?id=lscC4PZUE4"},{"id":"http://arxiv.org/abs/2510.23463v1","updated":"2025-10-27T16:01:15Z","published":"2025-10-27T16:01:15Z","title":"Differential Privacy as a Perk: Federated Learning over Multiple-Access\n  Fading Channels with a Multi-Antenna Base Station","summary":"  Federated Learning (FL) is a distributed learning paradigm that preserves\nprivacy by eliminating the need to exchange raw data during training. In its\nprototypical edge instantiation with underlying wireless transmissions enabled\nby analog over-the-air computing (AirComp), referred to as \\emph{over-the-air\nFL (AirFL)}, the inherent channel noise plays a unique role of \\emph{frenemy}\nin the sense that it degrades training due to noisy global aggregation while\nproviding a natural source of randomness for privacy-preserving mechanisms,\nformally quantified by \\emph{differential privacy (DP)}. It remains,\nnevertheless, challenging to effectively harness such channel impairments, as\nprior arts, under assumptions of either simple channel models or restricted\ntypes of loss functions, mostly considering (local) DP enhancement with a\nsingle-round or non-convergent bound on privacy loss. In this paper, we study\nAirFL over multiple-access fading channels with a multi-antenna base station\n(BS) subject to user-level DP requirements. Despite a recent study, which\nclaimed in similar settings that artificial noise (AN) must be injected to\nensure DP in general, we demonstrate, on the contrary, that DP can be gained as\na \\emph{perk} even \\emph{without} employing any AN. Specifically, we derive a\nnovel bound on DP that converges under general bounded-domain assumptions on\nmodel parameters, along with a convergence bound with general smooth and\nnon-convex loss functions. Next, we optimize over receive beamforming and power\nallocations to characterize the optimal convergence-privacy trade-offs, which\nalso reveal explicit conditions in which DP is achievable without compromising\ntraining. Finally, our theoretical findings are validated by extensive\nnumerical results.\n","authors":["Hao Liang","Haifeng Wen","Kaishun Wu","Hong Xing"],"pdf_url":"https://arxiv.org/pdf/2510.23463v1.pdf","comment":"15 pages, 5 figures, submitted for possible publication"},{"id":"http://arxiv.org/abs/2501.19342v4","updated":"2025-10-27T15:59:24Z","published":"2025-01-31T17:43:30Z","title":"Covering Multiple Objectives with a Small Set of Solutions Using\n  Bayesian Optimization","summary":"  In multi-objective black-box optimization, the goal is typically to find\nsolutions that optimize a set of $T$ black-box objective functions, $f_1,\n\\ldots f_T$, simultaneously. Traditional approaches often seek a single\nPareto-optimal set that balances trade-offs among all objectives. In contrast,\nwe consider a problem setting that departs from this paradigm: finding a small\nset of $K < T$ solutions, that collectively \"cover\" the $T$ objectives. A set\nof solutions is defined as \"covering\" if, for each objective $f_1, \\ldots f_T$,\nthere is at least one good solution. A motivating example for this problem\nsetting occurs in drug design. For example, we may have $T$ pathogens and aim\nto identify a set of $K < T$ antibiotics such that at least one antibiotic can\nbe used to treat each pathogen. This problem, known as coverage optimization,\nhas yet to be tackled with the Bayesian optimization (BO) framework. To fill\nthis void, we develop Multi-Objective Coverage Bayesian Optimization (MOCOBO),\na BO algorithm for solving coverage optimization. Our approach is based on a\nnew acquisition function reminiscent of expected improvement in the vanilla BO\nsetup. We demonstrate the performance of our method on high-dimensional\nblack-box optimization tasks, including applications in peptide and molecular\ndesign. Results show that the coverage of the $K < T$ solutions found by MOCOBO\nmatches or nearly matches the coverage of $T$ solutions obtained by optimizing\neach objective individually. Furthermore, in in vitro experiments, the peptides\nfound by MOCOBO exhibited high potency against drug-resistant pathogens,\nfurther demonstrating the potential of MOCOBO for drug discovery. All of our\ncode is publicly available at the following link:\nhttps://github.com/nataliemaus/mocobo.\n","authors":["Natalie Maus","Kyurae Kim","Yimeng Zeng","Haydn Thomas Jones","Fangping Wan","Marcelo Der Torossian Torres","Cesar de la Fuente-Nunez","Jacob R. Gardner"],"pdf_url":"https://arxiv.org/pdf/2501.19342v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23455v1","updated":"2025-10-27T15:56:19Z","published":"2025-10-27T15:56:19Z","title":"SGFusion: Stochastic Geographic Gradient Fusion in Federated Learning","summary":"  This paper proposes Stochastic Geographic Gradient Fusion (SGFusion), a novel\ntraining algorithm to leverage the geographic information of mobile users in\nFederated Learning (FL). SGFusion maps the data collected by mobile devices\nonto geographical zones and trains one FL model per zone, which adapts well to\nthe data and behaviors of users in that zone. SGFusion models the local\ndata-based correlation among geographical zones as a hierarchical random graph\n(HRG) optimized by Markov Chain Monte Carlo sampling. At each training step,\nevery zone fuses its local gradient with gradients derived from a small set of\nother zones sampled from the HRG. This approach enables knowledge fusion and\nsharing among geographical zones in a probabilistic and stochastic gradient\nfusion process with self-attention weights, such that \"more similar\" zones have\n\"higher probabilities\" of sharing gradients with \"larger attention weights.\"\nSGFusion remarkably improves model utility without introducing undue\ncomputational cost. Extensive theoretical and empirical results using a\nheart-rate prediction dataset collected across 6 countries show that models\ntrained with SGFusion converge with upper-bounded expected errors and\nsignificantly improve utility in all countries compared to existing approaches\nwithout notable cost in system scalability.\n","authors":["Khoa Nguyen","Khang Tran","NhatHai Phan","Cristian Borcea","Rouming Jin","Issa Khalil"],"pdf_url":"https://arxiv.org/pdf/2510.23455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23449v1","updated":"2025-10-27T15:52:47Z","published":"2025-10-27T15:52:47Z","title":"Schrodinger Neural Network and Uncertainty Quantification: Quantum\n  Machine","summary":"  We introduce the Schrodinger Neural Network (SNN), a principled architecture\nfor conditional density estimation and uncertainty quantification inspired by\nquantum mechanics. The SNN maps each input to a normalized wave function on the\noutput domain and computes predictive probabilities via the Born rule. The SNN\ndeparts from standard parametric likelihood heads by learning complex\ncoefficients of a spectral expansion (e . g ., Chebyshev polynomials) whose\nsquared modulus yields the conditional density $p(y|x)=\\left| \\psi _x(y)\\right|\n{}^2$ with analytic normalization. This representation confers three practical\nadvantages: positivity and exact normalization by construction, native\nmultimodality through interference among basis modes without explicit mixture\nbookkeeping, and yields closed-form (or efficiently computable)\nfunctionals$-$such as moments and several calibration diagnostics$-$as\nquadratic forms in coefficient space. We develop the statistical and\ncomputational foundations of the SNN, including (i) training by exact\nmaximum-likelihood with unit-sphere coefficient parameterization, (ii)\nphysics-inspired quadratic regularizers (kinetic and potential energies)\nmotivated by uncertainty relations between localization and spectral\ncomplexity, (iii) scalable low-rank and separable extensions for multivariate\noutputs, (iv) operator-based extensions that represent observables,\nconstraints, and weak labels as self-adjoint matrices acting on the amplitude\nspace, and (v) a comprehensive framework for evaluating multimodal predictions.\nThe SNN provides a coherent, tractable framework to elevate probabilistic\nprediction from point estimates to physically inspired amplitude-based\ndistributions.\n","authors":["M. M. Hammad"],"pdf_url":"https://arxiv.org/pdf/2510.23449v1.pdf","comment":"29 pages, 16 figures"},{"id":"http://arxiv.org/abs/2510.23448v1","updated":"2025-10-27T15:52:23Z","published":"2025-10-27T15:52:23Z","title":"An Information-Theoretic Analysis of Out-of-Distribution Generalization\n  in Meta-Learning with Applications to Meta-RL","summary":"  In this work, we study out-of-distribution generalization in meta-learning\nfrom an information-theoretic perspective. We focus on two scenarios: (i) when\nthe testing environment mismatches the training environment, and (ii) when the\ntraining environment is broader than the testing environment. The first\ncorresponds to the standard distribution mismatch setting, while the second\nreflects a broad-to-narrow training scenario. We further formalize the\ngeneralization problem in meta-reinforcement learning and establish\ncorresponding generalization bounds. Finally, we analyze the generalization\nperformance of a gradient-based meta-reinforcement learning algorithm.\n","authors":["Xingtu Liu"],"pdf_url":"https://arxiv.org/pdf/2510.23448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.22219v2","updated":"2025-10-27T15:45:43Z","published":"2025-09-26T11:31:38Z","title":"Automatic Discovery of One Parameter Subgroups of $SO(n)$","summary":"  We introduce a novel framework for the automatic discovery of one-parameter\nsubgroups ($H_{\\gamma}$) of $SO(3)$ and, more generally, $SO(n)$. One-parameter\nsubgroups of $SO(n)$ are crucial in a wide range of applications, including\nrobotics, quantum mechanics, and molecular structure analysis. Our method\nutilizes the standard Jordan form of skew-symmetric matrices, which define the\nLie algebra of $SO(n)$, to establish a canonical form for orbits under the\naction of $H_{\\gamma}$. This canonical form is then employed to derive a\nstandardized representation for $H_{\\gamma}$-invariant functions. By learning\nthe appropriate parameters, the framework uncovers the underlying one-parameter\nsubgroup $H_{\\gamma}$. The effectiveness of the proposed approach is\ndemonstrated through tasks such as double pendulum modeling, moment of inertia\nprediction, top quark tagging and invariant polynomial regression, where it\nsuccessfully recovers meaningful subgroup structure and produces interpretable,\nsymmetry-aware representations.\n","authors":["Pavan Karjol","Vivek V Kashyap","Rohan Kashyap","Prathosh A P"],"pdf_url":"https://arxiv.org/pdf/2509.22219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06094v3","updated":"2025-10-27T15:42:48Z","published":"2025-06-06T13:54:19Z","title":"Onboard Mission Replanning for Adaptive Cooperative Multi-Robot Systems","summary":"  Cooperative autonomous robotic systems have significant potential for\nexecuting complex multi-task missions across space, air, ground, and maritime\ndomains. But they commonly operate in remote, dynamic and hazardous\nenvironments, requiring rapid in-mission adaptation without reliance on fragile\nor slow communication links to centralised compute. Fast, on-board replanning\nalgorithms are therefore needed to enhance resilience. Reinforcement Learning\nshows strong promise for efficiently solving mission planning tasks when\nformulated as Travelling Salesperson Problems (TSPs), but existing methods: 1)\nare unsuitable for replanning, where agents do not start at a single location;\n2) do not allow cooperation between agents; 3) are unable to model tasks with\nvariable durations; or 4) lack practical considerations for on-board\ndeployment. Here we define the Cooperative Mission Replanning Problem as a\nnovel variant of multiple TSP with adaptations to overcome these issues, and\ndevelop a new encoder/decoder-based model using Graph Attention Networks and\nAttention Models to solve it effectively and efficiently. Using a simple\nexample of cooperative drones, we show our replanner consistently (90% of the\ntime) maintains performance within 10% of the state-of-the-art LKH3 heuristic\nsolver, whilst running 85-370 times faster on a Raspberry Pi. This work paves\nthe way for increased resilience in autonomous multi-agent systems.\n","authors":["Elim Kwan","Rehman Qureshi","Liam Fletcher","Colin Laganier","Victoria Nockles","Richard Walters"],"pdf_url":"https://arxiv.org/pdf/2506.06094v3.pdf","comment":"9 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2510.23438v1","updated":"2025-10-27T15:41:27Z","published":"2025-10-27T15:41:27Z","title":"Coresets for Clustering Under Stochastic Noise","summary":"  We study the problem of constructing coresets for $(k, z)$-clustering when\nthe input dataset is corrupted by stochastic noise drawn from a known\ndistribution. In this setting, evaluating the quality of a coreset is\ninherently challenging, as the true underlying dataset is unobserved. To\naddress this, we investigate coreset construction using surrogate error metrics\nthat are tractable and provably related to the true clustering cost. We analyze\na traditional metric from prior work and introduce a new error metric that more\nclosely aligns with the true cost. Although our metric is defined independently\nof the noise distribution, it enables approximation guarantees that scale with\nthe noise level. We design a coreset construction algorithm based on this\nmetric and show that, under mild assumptions on the data and noise, enforcing\nan $\\varepsilon$-bound under our metric yields smaller coresets and tighter\nguarantees on the true clustering cost than those obtained via classical\nmetrics. In particular, we prove that the coreset size can improve by a factor\nof up to $\\mathrm{poly}(k)$, where $n$ is the dataset size. Experiments on\nreal-world datasets support our theoretical findings and demonstrate the\npractical advantages of our approach.\n","authors":["Lingxiao Huang","Zhize Li","Nisheeth K. Vishnoi","Runkai Yang","Haoyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.23438v1.pdf","comment":"This paper has been accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2411.13479v3","updated":"2025-10-27T15:33:38Z","published":"2024-11-20T17:26:26Z","title":"Conformal Prediction for Hierarchical Data","summary":"  We consider conformal prediction for multivariate data and focus on\nhierarchical data, where some components are linear combinations of others.\nIntuitively, the hierarchical structure can be leveraged to reduce the size of\nprediction regions for the same coverage level. We implement this intuition by\nincluding a projection step (also called a reconciliation step) in the split\nconformal prediction [SCP] procedure, and prove that the resulting prediction\nregions are indeed globally smaller. We do so both under the classic objective\nof joint coverage and under a new and challenging task: component-wise\ncoverage, for which efficiency results are more difficult to obtain. The\nassociated strategies and their analyses are based both on the literature of\nSCP and of forecast reconciliation, which we connect. We also illustrate the\ntheoretical findings, for different scales of hierarchies on simulated data.\n","authors":["Guillaume Principato","Gilles Stoltz","Yvenn Amara-Ouali","Yannig Goude","Bachir Hamrouche","Jean-Michel Poggi"],"pdf_url":"https://arxiv.org/pdf/2411.13479v3.pdf","comment":"38 pages, 3 figures"},{"id":"http://arxiv.org/abs/2510.23428v1","updated":"2025-10-27T15:33:05Z","published":"2025-10-27T15:33:05Z","title":"Improving Predictions of Molecular Properties with Graph Featurisation\n  and Heterogeneous Ensemble Models","summary":"  We explore a \"best-of-both\" approach to modelling molecular properties by\ncombining learned molecular descriptors from a graph neural network (GNN) with\ngeneral-purpose descriptors and a mixed ensemble of machine learning (ML)\nmodels. We introduce a MetaModel framework to aggregate predictions from a\ndiverse set of leading ML models. We present a featurisation scheme for\ncombining task-specific GNN-derived features with conventional molecular\ndescriptors.\n  We demonstrate that our framework outperforms the cutting-edge ChemProp model\non all regression datasets tested and 6 of 9 classification datasets. We\nfurther show that including the GNN features derived from ChemProp boosts the\nensemble model's performance on several datasets where it otherwise would have\nunderperformed. We conclude that to achieve optimal performance across a wide\nset of problems, it is vital to combine general-purpose descriptors with\ntask-specific learned features and use a diverse set of ML models to make the\npredictions.\n","authors":["Michael L. Parker","Samar Mahmoud","Bailey Montefiore","Mario Öeren","Himani Tandon","Charlotte Wharrick","Matthew D. Segall"],"pdf_url":"https://arxiv.org/pdf/2510.23428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23427v1","updated":"2025-10-27T15:33:01Z","published":"2025-10-27T15:33:01Z","title":"PrivacyGuard: A Modular Framework for Privacy Auditing in Machine\n  Learning","summary":"  The increasing deployment of Machine Learning (ML) models in sensitive\ndomains motivates the need for robust, practical privacy assessment tools.\nPrivacyGuard is a comprehensive tool for empirical differential privacy (DP)\nanalysis, designed to evaluate privacy risks in ML models through\nstate-of-the-art inference attacks and advanced privacy measurement techniques.\nTo this end, PrivacyGuard implements a diverse suite of privacy attack--\nincluding membership inference , extraction, and reconstruction attacks --\nenabling both off-the-shelf and highly configurable privacy analyses. Its\nmodular architecture allows for the seamless integration of new attacks, and\nprivacy metrics, supporting rapid adaptation to emerging research advances. We\nmake PrivacyGuard available at\nhttps://github.com/facebookresearch/PrivacyGuard.\n","authors":["Luca Melis","Matthew Grange","Iden Kalemaj","Karan Chadha","Shengyuan Hu","Elena Kashtelyan","Will Bullock"],"pdf_url":"https://arxiv.org/pdf/2510.23427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01320v3","updated":"2025-10-27T15:22:41Z","published":"2025-06-02T05:02:33Z","title":"Psi-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models","summary":"  We introduce $\\Psi$-Sampler, an SMC-based framework incorporating pCNL-based\ninitial particle sampling for effective inference-time reward alignment with a\nscore-based generative model. Inference-time reward alignment with score-based\ngenerative models has recently gained significant traction, following a broader\nparadigm shift from pre-training to post-training optimization. At the core of\nthis trend is the application of Sequential Monte Carlo (SMC) to the denoising\nprocess. However, existing methods typically initialize particles from the\nGaussian prior, which inadequately captures reward-relevant regions and results\nin reduced sampling efficiency. We demonstrate that initializing from the\nreward-aware posterior significantly improves alignment performance. To enable\nposterior sampling in high-dimensional latent spaces, we introduce the\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines\ndimension-robust proposals with gradient-informed dynamics. This approach\nenables efficient and scalable posterior sampling and consistently improves\nperformance across various reward alignment tasks, including layout-to-image\ngeneration, quantity-aware generation, and aesthetic-preference generation, as\ndemonstrated in our experiments. Project Webpage:\nhttps://psi-sampler.github.io/\n","authors":["Taehoon Yoon","Yunhong Min","Kyeongmin Yeo","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2506.01320v3.pdf","comment":"NeurIPS 2025, Spotlight Presentation"},{"id":"http://arxiv.org/abs/2505.17895v2","updated":"2025-10-27T15:19:13Z","published":"2025-05-23T13:43:14Z","title":"DataRater: Meta-Learned Dataset Curation","summary":"  The quality of foundation models depends heavily on their training data.\nConsequently, great efforts have been put into dataset curation. Yet most\napproaches rely on manual tuning of coarse-grained mixtures of large buckets of\ndata, or filtering by hand-crafted heuristics. An approach that is ultimately\nmore scalable (let alone more satisfying) is to \\emph{learn} which data is\nactually valuable for training. This type of meta-learning could allow more\nsophisticated, fine-grained, and effective curation. Our proposed\n\\emph{DataRater} is an instance of this idea. It estimates the value of\ntraining on any particular data point. This is done by meta-learning using\n`meta-gradients', with the objective of improving training efficiency on held\nout data. In extensive experiments across a range of model scales and datasets,\nwe find that using our DataRater to filter data is highly effective, resulting\nin significantly improved compute efficiency.\n","authors":["Dan A. Calian","Gregory Farquhar","Iurii Kemaev","Luisa M. Zintgraf","Matteo Hessel","Jeremy Shar","Junhyuk Oh","András György","Tom Schaul","Jeffrey Dean","Hado van Hasselt","David Silver"],"pdf_url":"https://arxiv.org/pdf/2505.17895v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.17697v3","updated":"2025-10-27T15:15:26Z","published":"2025-10-20T16:10:56Z","title":"A Principle of Targeted Intervention for Multi-Agent Reinforcement\n  Learning","summary":"  Steering cooperative multi-agent reinforcement learning (MARL) towards\ndesired outcomes is challenging, particularly when the global guidance from a\nhuman on the whole multi-agent system is impractical in a large-scale MARL. On\nthe other hand, designing external mechanisms (e.g., intrinsic rewards and\nhuman feedback) to coordinate agents mostly relies on empirical studies,\nlacking a easy-to-use research tool. In this work, we employ multi-agent\ninfluence diagrams (MAIDs) as a graphical framework to address the above\nissues. First, we introduce the concept of MARL interaction paradigms\n(orthogonal to MARL learning paradigms), using MAIDs to analyze and visualize\nboth unguided self-organization and global guidance mechanisms in MARL. Then,\nwe design a new MARL interaction paradigm, referred to as the targeted\nintervention paradigm that is applied to only a single targeted agent, so the\nproblem of global guidance can be mitigated. In implementation, we introduce a\ncausal inference technique, referred to as Pre-Strategy Intervention (PSI), to\nrealize the targeted intervention paradigm. Since MAIDs can be regarded as a\nspecial class of causal diagrams, a composite desired outcome that integrates\nthe primary task goal and an additional desired outcome can be achieved by\nmaximizing the corresponding causal effect through the PSI. Moreover, the\nbundled relevance graph analysis of MAIDs provides a tool to identify whether\nan MARL learning paradigm is workable under the design of an MARL interaction\nparadigm. In experiments, we demonstrate the effectiveness of our proposed\ntargeted intervention, and verify the result of relevance graph analysis.\n","authors":["Anjie Liu","Jianhong Wang","Samuel Kaski","Jun Wang","Mengyue Yang"],"pdf_url":"https://arxiv.org/pdf/2510.17697v3.pdf","comment":"Published in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.10268v2","updated":"2025-10-27T15:13:18Z","published":"2025-10-11T16:03:28Z","title":"Neural variational inference for cutting feedback during uncertainty\n  propagation","summary":"  In many scientific applications, uncertainty of estimates from an earlier\n(upstream) analysis needs to be propagated in subsequent (downstream) Bayesian\nanalysis, without feedback. Cutting feedback methods, also termed cut-Bayes,\nachieve this by constructing a cut-posterior distribution that prevents\nbackward information flow. Cutting feedback like nested MCMC is computationally\nchallenging while variational inference (VI) cut-Bayes methods need two\nvariational approximations and require access to the upstream data and model.\nIn this manuscript we propose, NeVI-Cut, a provably accurate and modular neural\nnetwork-based variational inference method for cutting feedback. We directly\nutilize samples from the upstream analysis without requiring access to the\nupstream data or model. This simultaneously preserves modularity of analysis\nand reduces approximation errors by avoiding a variational approximation for\nthe upstream model. We then use normalizing flows to specify the conditional\nvariational family for the downstream parameters and estimate the conditional\ncut-posterior as a variational solution of Monte Carlo average loss over all\nthe upstream samples. We provide theoretical guarantees on the NeVI-Cut\nestimate to approximate any cut-posterior. Our results are in a fixed-data\nregime and provide convergence rates of the actual variational solution,\nquantifying how richness of the neural architecture and the complexity of the\ntarget cut-posterior dictate the approximation quality. In the process, we\nestablish new results on uniform Kullback-Leibler approximation rates of\nconditional normalizing flows. Simulation studies and two real-world analyses\nillustrate how NeVI-Cut achieves significant computational gains over\ntraditional cutting feedback methods and is considerably more accurate than\nparametric variational cut approaches.\n","authors":["Jiafang Song","Sandipan Pramanik","Abhirup Datta"],"pdf_url":"https://arxiv.org/pdf/2510.10268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23409v1","updated":"2025-10-27T15:12:49Z","published":"2025-10-27T15:12:49Z","title":"Eigen-Value: Efficient Domain-Robust Data Valuation via Eigenvalue-Based\n  Approach","summary":"  Data valuation has become central in the era of data-centric AI. It drives\nefficient training pipelines and enables objective pricing in data markets by\nassigning a numeric value to each data point. Most existing data valuation\nmethods estimate the effect of removing individual data points by evaluating\nchanges in model validation performance under in-distribution (ID) settings, as\nopposed to out-of-distribution (OOD) scenarios where data follow different\npatterns. Since ID and OOD data behave differently, data valuation methods\nbased on ID loss often fail to generalize to OOD settings, particularly when\nthe validation set contains no OOD data. Furthermore, although OOD-aware\nmethods exist, they involve heavy computational costs, which hinder practical\ndeployment. To address these challenges, we introduce \\emph{Eigen-Value} (EV),\na plug-and-play data valuation framework for OOD robustness that uses only an\nID data subset, including during validation. EV provides a new spectral\napproximation of domain discrepancy, which is the gap of loss between ID and\nOOD using ratios of eigenvalues of ID data's covariance matrix. EV then\nestimates the marginal contribution of each data point to this discrepancy via\nperturbation theory, alleviating the computational burden. Subsequently, EV\nplugs into ID loss-based methods by adding an EV term without any additional\ntraining loop. We demonstrate that EV achieves improved OOD robustness and\nstable value rankings across real-world datasets, while remaining\ncomputationally lightweight. These results indicate that EV is practical for\nlarge-scale settings with domain shift, offering an efficient path to\nOOD-robust data valuation.\n","authors":["Youngjun Choi","Joonseong Kang","Sungjun Lim","Kyungwoo Song"],"pdf_url":"https://arxiv.org/pdf/2510.23409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23408v1","updated":"2025-10-27T15:11:31Z","published":"2025-10-27T15:11:31Z","title":"AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream\n  Processing Pipelines","summary":"  Data pipelines are essential in stream processing as they enable the\nefficient collection, processing, and delivery of real-time data, supporting\nrapid data analysis. In this paper, we present AutoStreamPipe, a novel\nframework that employs Large Language Models (LLMs) to automate the design,\ngeneration, and deployment of stream processing pipelines. AutoStreamPipe\nbridges the semantic gap between high-level user intent and platform-specific\nimplementations across distributed stream processing systems for structured\nmulti-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an\nextended version of GoT. AutoStreamPipe combines resilient execution\nstrategies, advanced query analysis, and HGoT to deliver pipelines with good\naccuracy. Experimental evaluations on diverse pipelines demonstrate that\nAutoStreamPipe significantly reduces development time (x6.3) and error rates\n(x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM\ncode-generation methods.\n","authors":["Abolfazl Younesi","Zahra Najafabadi Samani","Thomas Fahringer"],"pdf_url":"https://arxiv.org/pdf/2510.23408v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2501.02409v4","updated":"2025-10-27T15:06:54Z","published":"2025-01-05T01:04:23Z","title":"Interpretable Neural ODEs for Gene Regulatory Network Discovery under\n  Perturbations","summary":"  Modern high-throughput biological datasets with thousands of perturbations\nprovide the opportunity for large-scale discovery of causal graphs that\nrepresent the regulatory interactions between genes. Differentiable causal\ngraphical models have been proposed to infer a gene regulatory network (GRN)\nfrom large scale interventional datasets, capturing the causal gene regulatory\nrelationships from genetic perturbations. However, existing models are limited\nin their expressivity and scalability while failing to address the dynamic\nnature of biological processes such as cellular differentiation. We propose\nPerturbODE, a novel framework that incorporates biologically informative neural\nordinary differential equations (neural ODEs) to model cell state trajectories\nunder perturbations and derive the causal GRN from the neural ODE's parameters.\nWe demonstrate PerturbODE's efficacy in trajectory prediction and GRN inference\nacross simulated and real over-expression datasets.\n","authors":["Zaikang Lin","Sei Chang","Aaron Zweig","Minseo Kang","Elham Azizi","David A. Knowles"],"pdf_url":"https://arxiv.org/pdf/2501.02409v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.12206v2","updated":"2025-10-27T14:53:32Z","published":"2025-10-14T06:56:33Z","title":"Controllable Collision Scenario Generation via Collision Pattern\n  Prediction","summary":"  Evaluating the safety of autonomous vehicles (AVs) requires diverse,\nsafety-critical scenarios, with collisions being especially important yet rare\nand unsafe to collect in the real world. Therefore, the community has been\nfocusing on generating safety-critical scenarios in simulation. However,\ncontrolling attributes such as collision type and time-to-accident (TTA)\nremains challenging. We introduce a new task called controllable collision\nscenario generation, where the goal is to produce trajectories that realize a\nuser-specified collision type and TTA, to investigate the feasibility of\nautomatically generating desired collision scenarios. To support this task, we\npresent COLLIDE, a large-scale collision scenario dataset constructed by\ntransforming real-world driving logs into diverse collisions, balanced across\nfive representative collision types and different TTA intervals. We propose a\nframework that predicts Collision Pattern, a compact and interpretable\nrepresentation that captures the spatial configuration of the ego and the\nadversarial vehicles at impact, before rolling out full adversarial\ntrajectories. Experiments show that our approach outperforms strong baselines\nin both collision rate and controllability. Furthermore, generated scenarios\nconsistently induce higher planner failure rates, revealing limitations of\nexisting planners. We demonstrate that these scenarios fine-tune planners for\nrobustness improvements, contributing to safer AV deployment in different\ncollision scenarios. Project page is available at\nhttps://submit-user.github.io/anon2025\n","authors":["Pin-Lun Chen","Chi-Hsi Kung","Che-Han Chang","Wei-Chen Chiu","Yi-Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2510.12206v2.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2509.18001v2","updated":"2025-10-27T14:49:07Z","published":"2025-09-22T16:40:42Z","title":"Unveiling m-Sharpness Through the Structure of Stochastic Gradient Noise","summary":"  Sharpness-aware minimization (SAM) has emerged as a highly effective\ntechnique for improving model generalization, but its underlying principles are\nnot fully understood. We investigated the phenomenon known as m-sharpness,\nwhere the performance of SAM improves monotonically as the micro-batch size for\ncomputing perturbations decreases. In practice, the empirical m-sharpness\neffect underpins the deployment of SAM in distributed training, yet a rigorous\ntheoretical account has remained lacking. To provide a theoretical explanation\nfor m-sharpness, we leverage an extended Stochastic Differential Equation (SDE)\nframework and analyze the structure of stochastic gradient noise (SGN) to\ncharacterize the dynamics of various SAM variants, including n-SAM and m-SAM.\nOur findings reveal that the stochastic noise introduced during SAM\nperturbations inherently induces a variance-based sharpness regularization\neffect. Motivated by our theoretical insights, we introduce Reweighted SAM\n(RW-SAM), which employs sharpness-weighted sampling to mimic the generalization\nbenefits of m-SAM while remaining parallelizable. Comprehensive experiments\nvalidate the effectiveness of our theoretical analysis and proposed method.\n","authors":["Haocheng Luo","Mehrtash Harandi","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2509.18001v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23393v1","updated":"2025-10-27T14:47:30Z","published":"2025-10-27T14:47:30Z","title":"The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N\n  Sampling via max@k Optimisation","summary":"  The application of Reinforcement Learning with Verifiable Rewards (RLVR) to\nmathematical and coding domains has demonstrated significant improvements in\nthe reasoning and problem-solving abilities of Large Language Models. Despite\nits success in single generation problem solving, the reinforcement learning\nfine-tuning process may harm the model's exploration ability, as reflected in\ndecreased diversity of generations and a resulting degradation of performance\nduring Best-of-N sampling for large N values. In this work, we focus on\noptimizing the max@k metric, a continuous generalization of pass@k. We derive\nan unbiased on-policy gradient estimate for direct optimization of this metric.\nFurthermore, we extend our derivations to the off-policy updates, a common\nelement in modern RLVR algorithms, that allows better sample efficiency.\nEmpirically, we show that our objective effectively optimizes max@k metric in\noff-policy scenarios, aligning the model with the Best-of-N inference strategy.\n","authors":["Farid Bagirov","Mikhail Arkhipov","Ksenia Sycheva","Evgeniy Glukhov","Egor Bogomolov"],"pdf_url":"https://arxiv.org/pdf/2510.23393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23389v1","updated":"2025-10-27T14:43:19Z","published":"2025-10-27T14:43:19Z","title":"Floating-Point Neural Network Verification at the Software Level","summary":"  The behaviour of neural network components must be proven correct before\ndeployment in safety-critical systems. Unfortunately, existing neural network\nverification techniques cannot certify the absence of faults at the software\nlevel. In this paper, we show how to specify and verify that neural networks\nare safe, by explicitly reasoning about their floating-point implementation. In\ndoing so, we construct NeuroCodeBench 2.0, a benchmark comprising 912 neural\nnetwork verification examples that cover activation functions, common layers,\nand full neural networks of up to 170K parameters. Our verification suite is\nwritten in plain C and is compatible with the format of the International\nCompetition on Software Verification (SV-COMP). Thanks to it, we can conduct\nthe first rigorous evaluation of eight state-of-the-art software verifiers on\nneural network code. The results show that existing automated verification\ntools can correctly solve an average of 11% of our benchmark, while producing\naround 3% incorrect verdicts. At the same time, a historical analysis reveals\nthat the release of our benchmark has already had a significantly positive\nimpact on the latter.\n","authors":["Edoardo Manino","Bruno Farias","Rafael Sá Menezes","Fedor Shmarov","Lucas C. Cordeiro"],"pdf_url":"https://arxiv.org/pdf/2510.23389v1.pdf","comment":"Pre-print before submission to peer review"},{"id":"http://arxiv.org/abs/2505.23799v3","updated":"2025-10-27T14:42:01Z","published":"2025-05-26T16:53:47Z","title":"Estimating LLM Consistency: A User Baseline vs Surrogate Metrics","summary":"  Large language models (LLMs) are prone to hallucinations and sensitiveto\nprompt perturbations, often resulting in inconsistent or unreliablegenerated\ntext. Different methods have been proposed to mitigate suchhallucinations and\nfragility, one of which is to measure theconsistency of LLM responses -- the\nmodel's confidence in the responseor likelihood of generating a similar\nresponse when resampled. Inprevious work, measuring LLM response consistency\noften relied oncalculating the probability of a response appearing within a\npool of resampledresponses, analyzing internal states, or evaluating logits of\nresopnses.However, it was not clear how well theseapproaches approximated\nusers' perceptions of consistency of LLMresponses. To find out, we performed a\nuser study ($n=2,976$)demonstrating that current methods for measuring LLM\nresponseconsistency typically do not align well with humans' perceptions of\nLLMconsistency. We propose a logit-based ensemble method for estimatingLLM\nconsistency and show that our method matches the performance of\nthebest-performing existing metric in estimating human ratings of\nLLMconsistency. Our results suggest that methods for estimating LLMconsistency\nwithout human evaluation are sufficiently imperfect towarrant broader use of\nevaluation with human input; this would avoidmisjudging the adequacy of models\nbecause of the imperfections ofautomated consistency metrics.\n","authors":["Xiaoyuan Wu","Weiran Lin","Omer Akgul","Lujo Bauer"],"pdf_url":"https://arxiv.org/pdf/2505.23799v3.pdf","comment":"Published as a main conference paper at EMNLP 2025"},{"id":"http://arxiv.org/abs/2409.06185v2","updated":"2025-10-27T14:39:52Z","published":"2024-09-10T03:26:42Z","title":"Can Large Language Models Unlock Novel Scientific Research Ideas?","summary":"  The widespread adoption of Large Language Models (LLMs) and publicly\navailable ChatGPT have marked a significant turning point in the integration of\nArtificial Intelligence (AI) into people's everyday lives. This study examines\nthe ability of Large Language Models (LLMs) to generate future research ideas\nfrom scientific papers. Unlike tasks such as summarization or translation, idea\ngeneration lacks a clearly defined reference set or structure, making manual\nevaluation the default standard. However, human evaluation in this setting is\nextremely challenging ie: it requires substantial domain expertise, contextual\nunderstanding of the paper, and awareness of the current research landscape.\nThis makes it time-consuming, costly, and fundamentally non-scalable,\nparticularly as new LLMs are being released at a rapid pace. Currently, there\nis no automated evaluation metric specifically designed for this task. To\naddress this gap, we propose two automated evaluation metrics: Idea Alignment\nScore (IAScore) and Idea Distinctness Index. We further conducted human\nevaluation to assess the novelty, relevance, and feasibility of the generated\nfuture research ideas. This investigation offers insights into the evolving\nrole of LLMs in idea generation, highlighting both its capability and\nlimitations. Our work contributes to the ongoing efforts in evaluating and\nutilizing language models for generating future research ideas. We make our\ndatasets and codes publicly available\n","authors":["Sandeep Kumar","Tirthankar Ghosal","Vinayak Goyal","Asif Ekbal"],"pdf_url":"https://arxiv.org/pdf/2409.06185v2.pdf","comment":"EMNLP 2025 (Main)"},{"id":"http://arxiv.org/abs/2510.23384v1","updated":"2025-10-27T14:35:20Z","published":"2025-10-27T14:35:20Z","title":"Opinion Mining Based Entity Ranking using Fuzzy Logic Algorithmic\n  Approach","summary":"  Opinions are central to almost all human activities and are key influencers\nof our behaviors. In current times due to growth of social networking website\nand increase in number of e-commerce site huge amount of opinions are now\navailable on web. Given a set of evaluative statements that contain opinions\n(or sentiments) about an Entity, opinion mining aims to extract attributes and\ncomponents of the object that have been commented on in each statement and to\ndetermine whether the comments are positive, negative or neutral. While lot of\nresearch recently has been done in field of opinion mining and some of it\ndealing with ranking of entities based on review or opinion set, classifying\nopinions into finer granularity level and then ranking entities has never been\ndone before. In this paper method for opinion mining from statements at a\ndeeper level of granularity is proposed. This is done by using fuzzy logic\nreasoning, after which entities are ranked as per this information.\n","authors":["Pratik N. Kalamkar","A. G. Phakatkar"],"pdf_url":"https://arxiv.org/pdf/2510.23384v1.pdf","comment":"8 pages, 4 figures, Conference Paper"},{"id":"http://arxiv.org/abs/2510.23379v1","updated":"2025-10-27T14:29:22Z","published":"2025-10-27T14:29:22Z","title":"Symbolic Neural Generation with Applications to Lead Discovery in Drug\n  Design","summary":"  We investigate a relatively underexplored class of hybrid neurosymbolic\nmodels integrating symbolic learning with neural reasoning to construct data\ngenerators meeting formal correctness criteria. In \\textit{Symbolic Neural\nGenerators} (SNGs), symbolic learners examine logical specifications of\nfeasible data from a small set of instances -- sometimes just one. Each\nspecification in turn constrains the conditional information supplied to a\nneural-based generator, which rejects any instance violating the symbolic\nspecification. Like other neurosymbolic approaches, SNG exploits the\ncomplementary strengths of symbolic and neural methods. The outcome of an SNG\nis a triple $(H, X, W)$, where $H$ is a symbolic description of feasible\ninstances constructed from data, $X$ a set of generated new instances that\nsatisfy the description, and $W$ an associated weight. We introduce a semantics\nfor such systems, based on the construction of appropriate \\textit{base} and\n\\textit{fibre} partially-ordered sets combined into an overall partial order,\nand outline a probabilistic extension relevant to practical applications. In\nthis extension, SNGs result from searching over a weighted partial ordering. We\nimplement an SNG combining a restricted form of Inductive Logic Programming\n(ILP) with a large language model (LLM) and evaluate it on early-stage drug\ndesign. Our main interest is the description and the set of potential inhibitor\nmolecules generated by the SNG. On benchmark problems -- where drug targets are\nwell understood -- SNG performance is statistically comparable to\nstate-of-the-art methods. On exploratory problems with poorly understood\ntargets, generated molecules exhibit binding affinities on par with leading\nclinical candidates. Experts further find the symbolic specifications useful as\npreliminary filters, with several generated molecules identified as viable for\nsynthesis and wet-lab testing.\n","authors":["Ashwin Srinivasan","A Baskar","Tirtharaj Dash","Michael Bain","Sanjay Kumar Dey","Mainak Banerjee"],"pdf_url":"https://arxiv.org/pdf/2510.23379v1.pdf","comment":"37 pages, 15 figures; partial overlap of experimental results with\n  https://doi.org/10.1101/2025.02.14.634875"},{"id":"http://arxiv.org/abs/2507.00971v2","updated":"2025-10-27T14:28:11Z","published":"2025-07-01T17:20:04Z","title":"Reasoning as an Adaptive Defense for Safety","summary":"  Reasoning methods that adaptively allocate test-time compute have advanced\nLLM performance on easy to verify domains such as math and code. In this work,\nwe study how to utilize this approach to train models that exhibit a degree of\nrobustness to safety vulnerabilities, and show that doing so can provide\nbenefits. We build a recipe called $\\textit{TARS}$ (Training Adaptive Reasoners\nfor Safety), a reinforcement learning (RL) approach that trains models to\nreason about safety using chain-of-thought traces and a reward signal that\nbalances safety with task completion. To build TARS, we identify three critical\ndesign choices: (1) a ``lightweight'' warmstart SFT stage, (2) a mix of\nharmful, harmless, and ambiguous prompts to prevent shortcut behaviors such as\ntoo many refusals, and (3) a reward function to prevent degeneration of\nreasoning capabilities during training. Models trained with TARS exhibit\nadaptive behaviors by spending more compute on ambiguous queries, leading to\nbetter safety-refusal trade-offs. They also internally learn to better\ndistinguish between safe and unsafe prompts and attain greater robustness to\nboth white-box (e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our\nwork provides an effective, open recipe for training LLMs against jailbreaks\nand harmful requests by reasoning per prompt.\n","authors":["Taeyoun Kim","Fahim Tajwar","Aditi Raghunathan","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2507.00971v2.pdf","comment":"44 pages, 10 Figures, 7 Tables"},{"id":"http://arxiv.org/abs/2508.16359v2","updated":"2025-10-27T14:23:31Z","published":"2025-08-22T13:05:55Z","title":"RotaTouille: Rotation Equivariant Deep Learning for Contours","summary":"  Contours or closed planar curves are common in many domains. For example,\nthey appear as object boundaries in computer vision, isolines in meteorology,\nand the orbits of rotating machinery. In many cases when learning from contour\ndata, planar rotations of the input will result in correspondingly rotated\noutputs. It is therefore desirable that deep learning models be rotationally\nequivariant. In addition, contours are typically represented as an ordered\nsequence of edge points, where the choice of starting point is arbitrary. It is\ntherefore also desirable for deep learning methods to be equivariant under\ncyclic shifts. We present RotaTouille, a deep learning framework for learning\nfrom contour data that achieves both rotation and cyclic shift equivariance\nthrough complex-valued circular convolution. We further introduce and\ncharacterize equivariant non-linearities, coarsening layers, and global pooling\nlayers to obtain invariant representations for downstream tasks. Finally, we\ndemonstrate the effectiveness of RotaTouille through experiments in shape\nclassification, reconstruction, and contour regression.\n","authors":["Odin Hoff Gardaa","Nello Blaser"],"pdf_url":"https://arxiv.org/pdf/2508.16359v2.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2510.23371v1","updated":"2025-10-27T14:21:05Z","published":"2025-10-27T14:21:05Z","title":"Towards a Generalizable AI for Materials Discovery: Validation through\n  Immersion Coolant Screening","summary":"  Artificial intelligence (AI) has emerged as a powerful accelerator of\nmaterials discovery, yet most existing models remain problem-specific,\nrequiring additional data collection and retraining for each new property. Here\nwe introduce and validate GATE (Geometrically Aligned Transfer Encoder) -- a\ngeneralizable AI framework that jointly learns 34 physicochemical properties\nspanning thermal, electrical, mechanical, and optical domains. By aligning\nthese properties within a shared geometric space, GATE captures cross-property\ncorrelations that reduce disjoint-property bias -- a key factor causing false\nnegatives in multi-criteria screening. To demonstrate its generalizability,\nGATE -- without any problem-specific reconfiguration -- was directly applied to\nthe discovery of immersion cooling fluids for data centers, a stringent\nreal-world challenge defined by the Open Compute Project (OCP). Screening\nbillions of candidates, GATE identified 92,861 molecules as promising for\npractical deployment. Four were experimentally or literarily validated, showing\nstrong agreement with wet-lab measurements and performance comparable to or\nexceeding a commercial coolant. These results establish GATE as a ready-to-use,\ngeneralizable AI platform readily applicable across diverse materials discovery\ntasks.\n","authors":["Hyunseung Kim","Dae-Woong Jeong","Changyoung Park","Won-Ji Lee","Ha-Eun Lee","Ji-Hye Lee","Rodrigo Hormazabal","Sung Moon Ko","Sumin Lee","Soorin Yim","Chanhui Lee","Sehui Han","Sang-Ho Cha","Woohyung Lim"],"pdf_url":"https://arxiv.org/pdf/2510.23371v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.17516v3","updated":"2025-10-27T14:17:13Z","published":"2025-10-20T13:14:38Z","title":"SimBench: Benchmarking the Ability of Large Language Models to Simulate\n  Human Behaviors","summary":"  Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators.\n","authors":["Tiancheng Hu","Joachim Baumann","Lorenzo Lupo","Nigel Collier","Dirk Hovy","Paul Röttger"],"pdf_url":"https://arxiv.org/pdf/2510.17516v3.pdf","comment":"Project Website: http://simbench.tiancheng.hu/ Data:\n  https://huggingface.co/datasets/pitehu/SimBench"},{"id":"http://arxiv.org/abs/2505.15405v2","updated":"2025-10-27T14:16:21Z","published":"2025-05-21T11:47:40Z","title":"HOPSE: Scalable Higher-Order Positional and Structural Encoder for\n  Combinatorial Representations","summary":"  While Graph Neural Networks (GNNs) have proven highly effective at modeling\nrelational data, pairwise connections cannot fully capture multi-way\nrelationships naturally present in complex real-world systems. In response to\nthis, Topological Deep Learning (TDL) leverages more general combinatorial\nrepresentations -- such as simplicial or cellular complexes -- to accommodate\nhigher-order interactions. Existing TDL methods often extend GNNs through\nHigher-Order Message Passing (HOMP), but face critical \\emph{scalability\nchallenges} due to \\textit{(i)} a combinatorial explosion of message-passing\nroutes, and \\textit{(ii)} significant complexity overhead from the propagation\nmechanism. This work presents HOPSE (Higher-Order Positional and Structural\nEncoder), an alternative method to solve tasks involving higher-order\ninteractions \\emph{without message passing}. Instead, HOPSE breaks\n\\emph{arbitrary higher-order domains} into their neighborhood relationships\nusing a Hasse graph decomposition. This method shows that decoupling the\nrepresentation learning of neighborhood topology from that of attributes\nresults in lower computational complexity, casting doubt on the need for HOMP.\nThe experiments on molecular graph tasks and topological benchmarks show that\nHOPSE matches performance on traditional TDL datasets and outperforms HOMP\nmethods on topological tasks, achieving up to $7\\times$ speedups over\nHOMP-based models, opening a new path for scalable TDL.\n","authors":["Martin Carrasco","Guillermo Bernardez","Marco Montagna","Nina Miolane","Lev Telyatnikov"],"pdf_url":"https://arxiv.org/pdf/2505.15405v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23364v1","updated":"2025-10-27T14:14:09Z","published":"2025-10-27T14:14:09Z","title":"ZeroFlood: A Geospatial Foundation Model for Data-Efficient Flood\n  Susceptibility Mapping","summary":"  Flood susceptibility mapping (FSM) is vital for disaster prevention but\nremains challenging in data-scarce regions where hydrodynamic models require\ndense geophysical inputs. This work introduces ZeroFlood, a geospatial\nfoundation model framework for data-efficient FSM. The approach fine-tunes\nGeospatial Foundation Models (GFMs) with Thinking-in-Modality (TiM) reasoning,\nenabling flood prediction from basic Earth observation data such as Sentinel-1\nor Sentinel-2 imagery. Using paired EO and simulated flood maps from data-rich\nregions, ZeroFlood bridges data availability gaps through cross-modal\nrepresentation learning. Experiments with TerraMind and Prithvi GFMs show that\nTiM enhances model robustness, with the TerraMind-Large configuration achieving\nan F1 score of 67.21. The results demonstrate the feasibility of\nfoundation-model-based FSM as a scalable and data-efficient solution for flood\nrisk management.\n","authors":["Hyeongkyun Kim","Orestis Oikonomou"],"pdf_url":"https://arxiv.org/pdf/2510.23364v1.pdf","comment":"Preprint submitted to EUSAR 2026 (under review)"},{"id":"http://arxiv.org/abs/2410.07076v6","updated":"2025-10-27T14:10:54Z","published":"2024-10-09T17:19:58Z","title":"MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses","summary":"  Scientific discovery plays a pivotal role in advancing human society, and\nrecent progress in large language models (LLMs) suggests their potential to\naccelerate this process. However, it remains unclear whether LLMs can\nautonomously generate novel and valid hypotheses in chemistry. In this work, we\ninvestigate whether LLMs can discover high-quality chemistry hypotheses given\nonly a research background-comprising a question and/or a survey-without\nrestriction on the domain of the question. We begin with the observation that\nhypothesis discovery is a seemingly intractable task. To address this, we\npropose a formal mathematical decomposition grounded in a fundamental\nassumption: that most chemistry hypotheses can be composed from a research\nbackground and a set of inspirations. This decomposition leads to three\npractical subtasks-retrieving inspirations, composing hypotheses with\ninspirations, and ranking hypotheses - which together constitute a sufficient\nset of subtasks for the overall scientific discovery task. We further develop\nan agentic LLM framework, MOOSE-Chem, that is a direct implementation of this\nmathematical decomposition. To evaluate this framework, we construct a\nbenchmark of 51 high-impact chemistry papers published and online after January\n2024, each manually annotated by PhD chemists with background, inspirations,\nand hypothesis. The framework is able to rediscover many hypotheses with high\nsimilarity to the groundtruth, successfully capturing the core\ninnovations-while ensuring no data contamination since it uses an LLM with\nknowledge cutoff date prior to 2024. Finally, based on LLM's surprisingly high\naccuracy on inspiration retrieval, a task with inherently out-of-distribution\nnature, we propose a bold assumption: that LLMs may already encode latent\nscientific knowledge associations not yet recognized by humans.\n","authors":["Zonglin Yang","Wanhao Liu","Ben Gao","Tong Xie","Yuqiang Li","Wanli Ouyang","Soujanya Poria","Erik Cambria","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.07076v6.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2510.23362v1","updated":"2025-10-27T14:10:25Z","published":"2025-10-27T14:10:25Z","title":"Robust Non-negative Proximal Gradient Algorithm for Inverse Problems","summary":"  Proximal gradient algorithms (PGA), while foundational for inverse problems\nlike image reconstruction, often yield unstable convergence and suboptimal\nsolutions by violating the critical non-negativity constraint. We identify the\ngradient descent step as the root cause of this issue, which introduces\nnegative values and induces high sensitivity to hyperparameters. To overcome\nthese limitations, we propose a novel multiplicative update proximal gradient\nalgorithm (SSO-PGA) with convergence guarantees, which is designed for\nrobustness in non-negative inverse problems. Our key innovation lies in\nsuperseding the gradient descent step with a learnable sigmoid-based operator,\nwhich inherently enforces non-negativity and boundedness by transforming\ntraditional subtractive updates into multiplicative ones. This design,\naugmented by a sliding parameter for enhanced stability and convergence, not\nonly improves robustness but also boosts expressive capacity and noise\nimmunity. We further formulate a degradation model for multi-modal restoration\nand derive its SSO-PGA-based optimization algorithm, which is then unfolded\ninto a deep network to marry the interpretability of optimization with the\npower of deep learning. Extensive numerical and real-world experiments\ndemonstrate that our method significantly surpasses traditional PGA and other\nstate-of-the-art algorithms, ensuring superior performance and stability.\n","authors":["Hanzhang Wang","Zonglin Liu","Jingyi Xu","Chenyang Wang","Zhiwei Zhong","Qiangqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2510.23362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23347v1","updated":"2025-10-27T14:01:41Z","published":"2025-10-27T14:01:41Z","title":"Macroeconomic Forecasting for the G7 countries under Uncertainty Shocks","summary":"  Accurate macroeconomic forecasting has become harder amid geopolitical\ndisruptions, policy reversals, and volatile financial markets. Conventional\nvector autoregressions (VARs) overfit in high dimensional settings, while\nthreshold VARs struggle with time varying interdependencies and complex\nparameter structures. We address these limitations by extending the Sims Zha\nBayesian VAR with exogenous variables (SZBVARx) to incorporate domain-informed\nshrinkage and four newspaper based uncertainty shocks such as economic policy\nuncertainty, geopolitical risk, US equity market volatility, and US monetary\npolicy uncertainty. The framework improves structural interpretability,\nmitigates dimensionality, and imposes empirically guided regularization. Using\nG7 data, we study spillovers from uncertainty shocks to five core variables\n(unemployment, real broad effective exchange rates, short term rates, oil\nprices, and CPI inflation), combining wavelet coherence (time frequency\ndynamics) with nonlinear local projections (state dependent impulse responses).\nOut-of-sample results at 12 and 24 month horizons show that SZBVARx outperforms\n14 benchmarks, including classical VARs and leading machine learning models, as\nconfirmed by Murphy difference diagrams, multivariate Diebold Mariano tests,\nand Giacomini White predictability tests. Credible Bayesian prediction\nintervals deliver robust uncertainty quantification for scenario analysis and\nrisk management. The proposed SZBVARx offers G7 policymakers a transparent,\nwell calibrated tool for modern macroeconomic forecasting under pervasive\nuncertainty.\n","authors":["Shovon Sengupta","Sunny Kumar Singh","Tanujit Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2510.23347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11816v2","updated":"2025-10-27T14:01:38Z","published":"2025-02-17T14:06:36Z","title":"Mixing It Up: Exploring Mixer Networks for Irregular Multivariate Time\n  Series Forecasting","summary":"  Forecasting Irregular Multivariate Time Series (IMTS) has recently emerged as\na distinct research field, necessitating specialized models to address its\nunique challenges. While most forecasting literature assumes regularly spaced\nobservations without missing values, many real-world datasets - particularly in\nhealthcare, climate research, and biomechanics - violate these assumptions.\nTime Series (TS)-mixer models have achieved remarkable success in regular\nmultivariate time series forecasting. However, they remain unexplored for IMTS\ndue to their requirement for complete and evenly spaced observations. To bridge\nthis gap, we introduce IMTS-Mixer, a novel forecasting architecture designed\nspecifically for IMTS. Our approach retains the core principles of TS mixer\nmodels while introducing innovative methods to transform IMTS into fixed-size\nmatrix representations, enabling their seamless integration with mixer modules.\nWe evaluate IMTS-Mixer on a benchmark of four real-world datasets from various\ndomains. Our results demonstrate that IMTS-Mixer establishes a new\nstate-of-the-art in forecasting accuracy while also improving computational\nefficiency.\n","authors":["Christian Klötergens","Vijaya Krishna Yalavarthi","Tim Dernedde","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2502.11816v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23346v1","updated":"2025-10-27T14:01:29Z","published":"2025-10-27T14:01:29Z","title":"Block-Diagonal LoRA for Eliminating Communication Overhead in Tensor\n  Parallel LoRA Serving","summary":"  When serving a single base LLM with several different LoRA adapters\nsimultaneously, the adapters cannot simply be merged with the base model's\nweights as the adapter swapping would create overhead and requests using\ndifferent adapters could not be batched. Rather, the LoRA computations have to\nbe separated from the base LLM computations, and in a multi-device setup the\nLoRA adapters can be sharded in a way that is well aligned with the base\nmodel's tensor parallel execution, as proposed in S-LoRA. However, the S-LoRA\nsharding strategy encounters some communication overhead, which may be small in\ntheory, but can be large in practice. In this paper, we propose to constrain\ncertain LoRA factors to be block-diagonal, which allows for an alternative way\nof sharding LoRA adapters that does not require any additional communication\nfor the LoRA computations. We demonstrate in extensive experiments that our\nblock-diagonal LoRA approach is similarly parameter efficient as standard LoRA\n(i.e., for a similar number of parameters it achieves similar downstream\nperformance) and that it leads to significant end-to-end speed-up over S-LoRA.\nFor example, when serving on eight A100 GPUs, we observe up to 1.79x (1.23x)\nend-to-end speed-up with 0.87x (1.74x) the number of adapter parameters for\nLlama-3.1-70B, and up to 1.63x (1.3x) end-to-end speed-up with 0.86x (1.73x)\nthe number of adapter parameters for Llama-3.1-8B.\n","authors":["Xinyu Wang","Jonas M. Kübler","Kailash Budhathoki","Yida Wang","Matthäus Kleindessner"],"pdf_url":"https://arxiv.org/pdf/2510.23346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20445v4","updated":"2025-10-27T13:58:21Z","published":"2024-10-27T13:51:09Z","title":"TrajAgent: An LLM-Agent Framework for Trajectory Modeling via\n  Large-and-Small Model Collaboration","summary":"  Trajectory modeling, which includes research on trajectory data pattern\nmining and future prediction, has widespread applications in areas such as life\nservices, urban transportation, and public administration. Numerous methods\nhave been proposed to address specific problems within trajectory modeling.\nHowever, the heterogeneity of data and the diversity of trajectory tasks make\neffective and reliable trajectory modeling an important yet highly challenging\nendeavor, even for domain experts. \\fix In this paper, we propose\n\\textit{TrajAgent}, a agent framework powered by large language models (LLMs),\ndesigned to facilitate robust and efficient trajectory modeling through\nautomation modeling. This framework leverages and optimizes diverse specialized\nmodels to address various trajectory modeling tasks across different datasets\neffectively. \\unfix~In \\textit{TrajAgent}, we first develop \\textit{UniEnv}, an\nexecution environment with a unified data and model interface, to support the\nexecution and training of various models. Building on \\textit{UniEnv}, we\nintroduce an agentic workflow designed for automatic trajectory modeling across\nvarious trajectory tasks and data. Furthermore, we introduce collaborative\nlearning schema between LLM-based agents and small speciallized models, to\nenhance the performance of the whole framework effectively. Extensive\nexperiments on four tasks using four real-world datasets demonstrate the\neffectiveness of \\textit{TrajAgent} in automated trajectory modeling, achieving\na performance improvement of \\fix 2.38\\%-69.91\\% \\unfix over baseline methods.\nThe codes and data can be accessed via\nhttps://github.com/tsinghua-fib-lab/TrajAgent.\n","authors":["Yuwei Du","Jie Feng","Jie Zhao","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2410.20445v4.pdf","comment":"Accepted by NeurIPS 2025,\n  https://github.com/tsinghua-fib-lab/TrajAgent"},{"id":"http://arxiv.org/abs/2510.20075v3","updated":"2025-10-27T13:54:40Z","published":"2025-10-22T23:16:50Z","title":"LLMs can hide text in other text of the same length","summary":"  A meaningful text can be hidden inside another, completely different yet\nstill coherent and plausible, text of the same length. For example, a tweet\ncontaining a harsh political critique could be embedded in a tweet that\ncelebrates the same political leader, or an ordinary product review could\nconceal a secret manuscript. This uncanny state of affairs is now possible\nthanks to Large Language Models, and in this paper we present a simple and\nefficient protocol to achieve it. We show that even modest 8-billion-parameter\nopen-source LLMs are sufficient to obtain high-quality results, and a message\nas long as this abstract can be encoded and decoded locally on a laptop in\nseconds. The existence of such a protocol demonstrates a radical decoupling of\ntext from authorial intent, further eroding trust in written communication,\nalready shaken by the rise of LLM chatbots. We illustrate this with a concrete\nscenario: a company could covertly deploy an unfiltered LLM by encoding its\nanswers within the compliant responses of a safe model. This possibility raises\nurgent questions for AI safety and challenges our understanding of what it\nmeans for a Large Language Model to know something.\n","authors":["Antonio Norelli","Michael Bronstein"],"pdf_url":"https://arxiv.org/pdf/2510.20075v3.pdf","comment":"21 pages, main paper 9 pages"},{"id":"http://arxiv.org/abs/2506.02599v2","updated":"2025-10-27T13:50:39Z","published":"2025-06-03T08:24:14Z","title":"Assessing the Completeness of Traffic Scenario Categories for Automated\n  Highway Driving Functions via Cluster-based Analysis","summary":"  The ability to operate safely in increasingly complex traffic scenarios is a\nfundamental requirement for Automated Driving Systems (ADS). Ensuring the safe\nrelease of ADS functions necessitates a precise understanding of the occurring\ntraffic scenarios. To support this objective, this work introduces a pipeline\nfor traffic scenario clustering and the analysis of scenario category\ncompleteness. The Clustering Vector Quantized - Variational Autoencoder\n(CVQ-VAE) is employed for the clustering of highway traffic scenarios and\nutilized to create various catalogs with differing numbers of traffic scenario\ncategories. Subsequently, the impact of the number of categories on the\ncompleteness considerations of the traffic scenario categories is analyzed. The\nresults show an outperforming clustering performance compared to previous work.\nThe trade-off between cluster quality and the amount of required data to\nmaintain completeness is discussed based on the publicly available highD\ndataset.\n","authors":["Niklas Roßberg","Marion Neumeier","Sinan Hasirlioglu","Mohamed Essayed Bouzouraa","Michael Botsch"],"pdf_url":"https://arxiv.org/pdf/2506.02599v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23330v1","updated":"2025-10-27T13:45:55Z","published":"2025-10-27T13:45:55Z","title":"The First Star-by-star $N$-body/Hydrodynamics Simulation of Our Galaxy\n  Coupling with a Surrogate Model","summary":"  A major goal of computational astrophysics is to simulate the Milky Way\nGalaxy with sufficient resolution down to individual stars. However, the\nscaling fails due to some small-scale, short-timescale phenomena, such as\nsupernova explosions. We have developed a novel integration scheme of\n$N$-body/hydrodynamics simulations working with machine learning. This approach\nbypasses the short timesteps caused by supernova explosions using a surrogate\nmodel, thereby improving scalability. With this method, we reached 300 billion\nparticles using 148,900 nodes, equivalent to 7,147,200 CPU cores, breaking\nthrough the billion-particle barrier currently faced by state-of-the-art\nsimulations. This resolution allows us to perform the first star-by-star galaxy\nsimulation, which resolves individual stars in the Milky Way Galaxy. The\nperformance scales over $10^4$ CPU cores, an upper limit in the current\nstate-of-the-art simulations using both A64FX and X86-64 processors and NVIDIA\nCUDA GPUs.\n","authors":["Keiya Hirashima","Michiko S. Fujii","Takayuki R. Saitoh","Naoto Harada","Kentaro Nomura","Kohji Yoshikawa","Yutaka Hirai","Tetsuro Asano","Kana Moriwaki","Masaki Iwasawa","Takashi Okamoto","Junichiro Makino"],"pdf_url":"https://arxiv.org/pdf/2510.23330v1.pdf","comment":"12 pages, 7 figures, 7 tables, IEEE/ACM Supercomputing Conference\n  (SC25)"},{"id":"http://arxiv.org/abs/2510.23327v1","updated":"2025-10-27T13:44:15Z","published":"2025-10-27T13:44:15Z","title":"GRAD: Real-Time Gated Recurrent Anomaly Detection in Autonomous Vehicle\n  Sensors Using Reinforced EMA and Multi-Stage Sliding Window Techniques","summary":"  This paper introduces GRAD, a real-time anomaly detection method for\nautonomous vehicle sensors that integrates statistical analysis and deep\nlearning to ensure the reliability of sensor data. The proposed approach\ncombines the Reinforced Exponential Moving Average (REMA), which adapts\nsmoothing factors and thresholding for outlier detection, with the Multi-Stage\nSliding Window (MS-SW) technique for capturing both short- and long-term\npatterns. These features are processed using a lightweight Gated Recurrent Unit\n(GRU) model, which detects and classifies anomalies based on bias types, while\na recovery module restores damaged sensor data to ensure continuous system\noperation. GRAD has a lightweight architecture consisting of two layers of GRU\nwith a limited number of neurons that make it appropriate for real-time\napplications while maintaining high detection accuracy. The GRAD framework\nachieved remarkable performance in anomaly detection and classification. The\nmodel demonstrated an overall F1-score of 97.6% for abnormal data and 99.4% for\nnormal data, signifying its high accuracy in distinguishing between normal and\nanomalous sensor data. Regarding the anomaly classification, GRAD successfully\ncategorized different anomaly types with high precision, enabling the recovery\nmodule to accurately restore damaged sensor data. Relative to analogous\nstudies, GRAD surpasses current models by attaining a balance between elevated\ndetection accuracy and diminished computational expense. These results\ndemonstrate GRAD's potential as a reliable and efficient solution for real-time\nanomaly detection in autonomous vehicle systems, guaranteeing safe vehicle\noperation with minimal computational overhead.\n","authors":["Mohammad Hossein Jafari Naeimi","Ali Norouzi","Athena Abdi"],"pdf_url":"https://arxiv.org/pdf/2510.23327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23325v1","updated":"2025-10-27T13:42:16Z","published":"2025-10-27T13:42:16Z","title":"Multitask Multimodal Self-Supervised Learning for Medical Images","summary":"  This thesis works to address a pivotal challenge in medical image analysis:\nthe reliance on extensive labeled datasets, which are often limited due to the\nneed for expert annotation and constrained by privacy and legal issues. By\nfocusing on the development of self-supervised learning techniques and domain\nadaptation methods, this research aims to circumvent these limitations,\npresenting a novel approach to enhance the utility and efficacy of deep\nlearning in medical imaging.\n  Central to this thesis is the development of the Medformer, an innovative\nneural network architecture designed for multitask learning and deep domain\nadaptation. This model is adept at pre-training on diverse medical image\ndatasets, handling varying sizes and modalities, and is equipped with a dynamic\ninput-output adaptation mechanism. This enables efficient processing and\nintegration of a wide range of medical image types, from 2D X-rays to complex\n3D MRIs, thus mitigating the dependency on large labeled datasets.\n  Further, the thesis explores the current state of self-supervised learning in\nmedical imaging. It introduces novel pretext tasks that are capable of\nextracting meaningful information from unlabeled data, significantly advancing\nthe model's interpretative abilities. This approach is validated through\nrigorous experimentation, including the use of the MedMNIST dataset,\ndemonstrating the model's proficiency in learning generalized features\napplicable to various downstream tasks.\n  In summary, this thesis contributes to the advancement of medical image\nanalysis by offering a scalable, adaptable framework that reduces reliance on\nlabeled data. It paves the way for more accurate, efficient diagnostic tools in\nhealthcare, signifying a major step forward in the application of deep learning\nin medical imaging.\n","authors":["Cristian Simionescu"],"pdf_url":"https://arxiv.org/pdf/2510.23325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23319v1","updated":"2025-10-27T13:30:54Z","published":"2025-10-27T13:30:54Z","title":"Arabic Little STT: Arabic Children Speech Recognition Dataset","summary":"  The performance of Artificial Intelligence (AI) systems fundamentally depends\non high-quality training data. However, low-resource languages like Arabic\nsuffer from severe data scarcity. Moreover, the absence of child-specific\nspeech corpora is an essential gap that poses significant challenges. To\naddress this gap, we present our created dataset, Arabic Little STT, a dataset\nof Levantine Arabic child speech recorded in classrooms, containing 355\nutterances from 288 children (ages 6 - 13). We further conduct a systematic\nassessment of Whisper, a state-of-the-art automatic speech recognition (ASR)\nmodel, on this dataset and compare its performance with adult Arabic\nbenchmarks. Our evaluation across eight Whisper variants reveals that even the\nbest-performing model (Large_v3) struggles significantly, achieving a 0.66 word\nerror rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on\nadult datasets. These results align with other research on English speech.\nResults highlight the critical need for dedicated child speech benchmarks and\ninclusive training data in ASR development. Emphasizing that such data must be\ngoverned by strict ethical and privacy frameworks to protect sensitive child\ninformation. We hope that this study provides an initial step for future work\non equitable speech technologies for Arabic-speaking children. We hope that our\npublicly available dataset enrich the children's demographic representation in\nASR datasets.\n","authors":["Mouhand Alkadri","Dania Desouki","Khloud Al Jallad"],"pdf_url":"https://arxiv.org/pdf/2510.23319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.18924v2","updated":"2025-10-27T13:24:50Z","published":"2025-10-21T10:14:49Z","title":"Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients","summary":"  Reinforcement learning from human feedback (RLHF) or verifiable rewards\n(RLVR), the standard paradigm for aligning LLMs or building recent SOTA\nreasoning models, is highly sensitive to noise from inconsistent or erroneous\nrewards. Yet, the interaction between such noise and widely used group-based\npolicy optimization methods remains underexplored. We introduce a noise-robust\nGroup Relative Policy Optimization (GRPO) and Done Right GRPO (Dr.GRPO)\nframework that explicitly models reward corruption as Bernoulli noise. Our\nmethod applies noise correction after estimating reward flip probabilities to\ndebias the learning signal, yielding provably unbiased gradient estimates.\nTheoretical analysis shows that group-based methods inherently mitigate\nindividual-level noise, and our correction strategy amplifies this robustness.\nEmpirically, we observe consistent improvements across math and code tasks when\napplying our noise correction to standard reward model usage, with particular\ngains of up to 6.7 percentage points in accuracy on math tasks and 1.5 on code\ntasks under realistic reward model conditions. This work bridges label-noise\ncorrection from supervised learning with modern RLHF, offering both theoretical\ninsights and a practical algorithm for noisy real-world deployment.\n","authors":["Omar El Mansouri","Mohamed El Amine Seddik","Salem Lahlou"],"pdf_url":"https://arxiv.org/pdf/2510.18924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15807v2","updated":"2025-10-27T13:12:32Z","published":"2025-05-21T17:59:01Z","title":"The Atlas of In-Context Learning: How Attention Heads Shape In-Context\n  Retrieval Augmentation","summary":"  Large language models are able to exploit in-context learning to access\nexternal knowledge beyond their training data through retrieval-augmentation.\nWhile promising, its inner workings remain unclear. In this work, we shed light\non the mechanism of in-context retrieval augmentation for question answering by\nviewing a prompt as a composition of informational components. We propose an\nattribution-based method to identify specialized attention heads, revealing\nin-context heads that comprehend instructions and retrieve relevant contextual\ninformation, and parametric heads that store entities' relational knowledge. To\nbetter understand their roles, we extract function vectors and modify their\nattention weights to show how they can influence the answer generation process.\nFinally, we leverage the gained insights to trace the sources of knowledge used\nduring inference, paving the way towards more safe and transparent language\nmodels.\n","authors":["Patrick Kahardipraja","Reduan Achtibat","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2505.15807v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23295v1","updated":"2025-10-27T13:03:29Z","published":"2025-10-27T13:03:29Z","title":"Predicting symbolic ODEs from multiple trajectories","summary":"  We introduce MIO, a transformer-based model for inferring symbolic ordinary\ndifferential equations (ODEs) from multiple observed trajectories of a\ndynamical system. By combining multiple instance learning with\ntransformer-based symbolic regression, the model effectively leverages repeated\nobservations of the same system to learn more generalizable representations of\nthe underlying dynamics. We investigate different instance aggregation\nstrategies and show that even simple mean aggregation can substantially boost\nperformance. MIO is evaluated on systems ranging from one to four dimensions\nand under varying noise levels, consistently outperforming existing baselines.\n","authors":["Yakup Emre Şahin","Niki Kilbertus","Sören Becker"],"pdf_url":"https://arxiv.org/pdf/2510.23295v1.pdf","comment":"Published at: 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) Workshop: Machine Learning and the Physical Sciences"},{"id":"http://arxiv.org/abs/2510.23288v1","updated":"2025-10-27T12:59:45Z","published":"2025-10-27T12:59:45Z","title":"Learning from Frustration: Torsor CNNs on Graphs","summary":"  Most equivariant neural networks rely on a single global symmetry, limiting\ntheir use in domains where symmetries are instead local. We introduce Torsor\nCNNs, a framework for learning on graphs with local symmetries encoded as edge\npotentials-- group-valued transformations between neighboring coordinate\nframes. We establish that this geometric construction is fundamentally\nequivalent to the classical group synchronization problem, yielding: (1) a\nTorsor Convolutional Layer that is provably equivariant to local changes in\ncoordinate frames, and (2) the frustration loss--a standalone geometric\nregularizer that encourages locally equivariant representations when added to\nany NN's training objective. The Torsor CNN framework unifies and generalizes\nseveral architectures--including classical CNNs and Gauge CNNs on manifolds--\nby operating on arbitrary graphs without requiring a global coordinate system\nor smooth manifold structure. We establish the mathematical foundations of this\nframework and demonstrate its applicability to multi-view 3D recognition, where\nrelative camera poses naturally define the required edge potentials.\n","authors":["Daiyuan Li","Shreya Arya","Robert Ghrist"],"pdf_url":"https://arxiv.org/pdf/2510.23288v1.pdf","comment":"19 pages (main text + appendices), 1 figure"},{"id":"http://arxiv.org/abs/2510.18913v2","updated":"2025-10-27T12:50:13Z","published":"2025-10-21T05:53:13Z","title":"ADPO: Anchored Direct Preference Optimization","summary":"  Direct Preference Optimization (DPO) is an efficient alternative to\nreinforcement learning from human feedback (RLHF), yet it typically assumes\nhard binary labels and pairwise comparisons. Such assumptions can be brittle\nunder noisy or distribution-shifted supervision. We present Anchored Direct\nPreference Optimization (ADPO), which (i) incorporates soft preference\nprobabilities, (ii) aligns policy updates through reference anchoring that\ninduces an implicit trust region, and (iii) extends to listwise learning via\nPlackett-Luce modeling. In controlled synthetic setups covering 12 scenarios (4\nnoise types x 3 severities) and 3 model scales, ADPO exhibits relative\nimprovements ranging from 12% to 79% over a standard DPO baseline (10-seed\nmeans; 95% CIs in the Appendix). Hard labels tend to fare better under severe\nnoise, whereas soft labels yield better calibration under distribution shift;\nlistwise variants achieve the highest WinMass (expected probability mass on the\nground-truth best item) in 9/12 scenarios. Larger models amplify ADPO's\nbenefits (0.718 vs. 0.416 at hidden=256), suggesting that anchoring acts as an\neffective trust-region regularizer. We release code and configurations to\nfacilitate reproducibility.\n","authors":["Wang Zixian"],"pdf_url":"https://arxiv.org/pdf/2510.18913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.19143v4","updated":"2025-10-27T12:36:23Z","published":"2025-06-23T21:28:45Z","title":"Thought Anchors: Which LLM Reasoning Steps Matter?","summary":"  Current frontier large-language models rely on reasoning to achieve\nstate-of-the-art performance. Many existing interpretability are limited in\nthis area, as standard methods have been designed to study single forward\npasses of a model rather than the multi-token computational steps that unfold\nduring reasoning. We argue that analyzing reasoning traces at the sentence\nlevel is a promising approach to understanding reasoning processes. We\nintroduce a black-box method that measures each sentence's counterfactual\nimportance by repeatedly sampling replacement sentences from the model,\nfiltering for semantically different ones, and continuing the chain of thought\nfrom that point onwards to quantify the sentence's impact on the distribution\nof final answers. We discover that certain sentences can have an outsized\nimpact on the trajectory of the reasoning trace and final answer. We term these\nsentences \\textit{thought anchors}. These are generally planning or uncertainty\nmanagement sentences, and specialized attention heads consistently attend from\nsubsequent sentences to thought anchors. We further show that examining\nsentence-sentence causal links within a reasoning trace gives insight into a\nmodel's behavior. Such information can be used to predict a problem's\ndifficulty and the extent different question domains involve sequential or\ndiffuse reasoning. As a proof-of-concept, we demonstrate that our techniques\ntogether provide a practical toolkit for analyzing reasoning models by\nconducting a detailed case study of how the model solves a difficult math\nproblem, finding that our techniques yield a consistent picture of the\nreasoning trace's structure. We provide an open-source tool\n(thought-anchors.com) for visualizing the outputs of our methods on further\nproblems. The convergence across our methods shows the potential of\nsentence-level analysis for a deeper understanding of reasoning models.\n","authors":["Paul C. Bogdan","Uzay Macar","Neel Nanda","Arthur Conmy"],"pdf_url":"https://arxiv.org/pdf/2506.19143v4.pdf","comment":"Paul C. Bogdan and Uzay Macar contributed equally to this work, and\n  their listed order was determined by coinflip. Neel Nanda and Arthur Conmy\n  contributed equally to this work as senior authors, and their listed order\n  was determined by coinflip"},{"id":"http://arxiv.org/abs/2510.23273v1","updated":"2025-10-27T12:33:01Z","published":"2025-10-27T12:33:01Z","title":"A Novel Framework for Multi-Modal Protein Representation Learning","summary":"  Accurate protein function prediction requires integrating heterogeneous\nintrinsic signals (e.g., sequence and structure) with noisy extrinsic contexts\n(e.g., protein-protein interactions and GO term annotations). However, two key\nchallenges hinder effective fusion: (i) cross-modal distributional mismatch\namong embeddings produced by pre-trained intrinsic encoders, and (ii) noisy\nrelational graphs of extrinsic data that degrade GNN-based information\naggregation. We propose Diffused and Aligned Multi-modal Protein Embedding\n(DAMPE), a unified framework that addresses these through two core mechanisms.\nFirst, we propose Optimal Transport (OT)-based representation alignment that\nestablishes correspondence between intrinsic embedding spaces of different\nmodalities, effectively mitigating cross-modal heterogeneity. Second, we\ndevelop a Conditional Graph Generation (CGG)-based information fusion method,\nwhere a condition encoder fuses the aligned intrinsic embeddings to provide\ninformative cues for graph reconstruction. Meanwhile, our theoretical analysis\nimplies that the CGG objective drives this condition encoder to absorb\ngraph-aware knowledge into its produced protein representations. Empirically,\nDAMPE outperforms or matches state-of-the-art methods such as DPFunc on\nstandard GO benchmarks, achieving AUPR gains of 0.002-0.013 pp and Fmax gains\n0.004-0.007 pp. Ablation studies further show that OT-based alignment\ncontributes 0.043-0.064 pp AUPR, while CGG-based fusion adds 0.005-0.111 pp\nFmax. Overall, DAMPE offers a scalable and theoretically grounded approach for\nrobust multi-modal protein representation learning, substantially enhancing\nprotein function prediction.\n","authors":["Runjie Zheng","Zhen Wang","Anjie Qiao","Jiancong Xie","Jiahua Rao","Yuedong Yang"],"pdf_url":"https://arxiv.org/pdf/2510.23273v1.pdf","comment":"35 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2510.23264v1","updated":"2025-10-27T12:24:14Z","published":"2025-10-27T12:24:14Z","title":"PAHQ: Accelerating Automated Circuit Discovery through Mixed-Precision\n  Inference Optimization","summary":"  Circuit discovery, which involves identifying sparse and task-relevant\nsubnetworks in pre-trained language models, is a cornerstone of mechanistic\ninterpretability. Automated Circuit Discovery (ACDC) has emerged as a pivotal\nmethodology in circuit discovery, but its application to large language models\nis severely limited by computational inefficiency and prohibitively high memory\nrequirements. Although several accelerated approaches have been proposed, they\nprimarily rely on linear approximations to ACDC, which significantly\ncompromises analytical faithfulness. Our proposed method for accelerating\nautomated circuit discovery, Per Attention Head Quantization (PAHQ), takes a\nfundamentally different approach by optimizing the efficiency of each\nindividual patching operation. PAHQ leverages a fundamental alignment between\nactivation patching and mixed-precision quantization (MPQ): interpretability\nanalysis through patching essentially performs targeted ablation studies.\nTherefore, we can maintain high precision exclusively for investigated\ncomponents while safely reducing precision elsewhere in the network.\nPAHQ-accelerated ACDC reduces runtime by up to 80\\% and memory consumption by\nup to 30\\% compared to unaccelerated ACDC while maintaining faithfulness.\nImportantly, our method readily integrates with existing edge-based circuit\ndiscovery techniques by modifying the attention computation mechanism. This\ntraining-free approach provides a practical and novel pathway for accelerating\nmechanistic interpretability methods. Our code is available at\nhttps://github.com/626619403/PAHQ.\n","authors":["Xinhai Wang","Shu Yang","Liangyu Wang","Lin Zhang","Huanyi Xie","Lijie Hu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23261v1","updated":"2025-10-27T12:23:37Z","published":"2025-10-27T12:23:37Z","title":"Toward Interpretable Evaluation Measures for Time Series Segmentation","summary":"  Time series segmentation is a fundamental task in analyzing temporal data\nacross various domains, from human activity recognition to energy monitoring.\nWhile numerous state-of-the-art methods have been developed to tackle this\nproblem, the evaluation of their performance remains critically limited.\nExisting measures predominantly focus on change point accuracy or rely on\npoint-based measures such as Adjusted Rand Index (ARI), which fail to capture\nthe quality of the detected segments, ignore the nature of errors, and offer\nlimited interpretability. In this paper, we address these shortcomings by\nintroducing two novel evaluation measures: WARI (Weighted Adjusted Rand Index),\nthat accounts for the position of segmentation errors, and SMS (State Matching\nScore), a fine-grained measure that identifies and scores four fundamental\ntypes of segmentation errors while allowing error-specific weighting. We\nempirically validate WARI and SMS on synthetic and real-world benchmarks,\nshowing that they not only provide a more accurate assessment of segmentation\nquality but also uncover insights, such as error provenance and type, that are\ninaccessible with traditional measures.\n","authors":["Félix Chavelli","Paul Boniol","Michaël Thomazo"],"pdf_url":"https://arxiv.org/pdf/2510.23261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15496v2","updated":"2025-10-27T12:22:36Z","published":"2025-05-21T13:22:07Z","title":"Fast Rate Bounds for Multi-Task and Meta-Learning with Different Sample\n  Sizes","summary":"  We present new fast-rate PAC-Bayesian generalization bounds for multi-task\nand meta-learning in the unbalanced setting, i.e. when the tasks have training\nsets of different sizes, as is typically the case in real-world scenarios.\nPreviously, only standard-rate bounds were known for this situation, while\nfast-rate bounds were limited to the setting where all training sets are of\nequal size. Our new bounds are numerically computable as well as interpretable,\nand we demonstrate their flexibility in handling a number of cases where they\ngive stronger guarantees than previous bounds. Besides the bounds themselves,\nwe also make conceptual contributions: we demonstrate that the unbalanced\nmulti-task setting has different statistical properties than the balanced\nsituation, specifically that proofs from the balanced situation do not carry\nover to the unbalanced setting. Additionally, we shed light on the fact that\nthe unbalanced situation allows two meaningful definitions of multi-task risk,\ndepending on whether all tasks should be considered equally important or if\nsample-rich tasks should receive more weight than sample-poor ones.\n","authors":["Hossein Zakerinia","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2505.15496v2.pdf","comment":"Conference on Neural Information Processing Systems (NeurIPS), 2025"},{"id":"http://arxiv.org/abs/2510.23259v1","updated":"2025-10-27T12:22:24Z","published":"2025-10-27T12:22:24Z","title":"GCAO: Group-driven Clustering via Gravitational Attraction and\n  Optimization","summary":"  Traditional clustering algorithms often struggle with high-dimensional and\nnon-uniformly distributed data, where low-density boundary samples are easily\ndisturbed by neighboring clusters, leading to unstable and distorted clustering\nresults. To address this issue, we propose a Group-driven Clustering via\nGravitational Attraction and Optimization (GCAO) algorithm. GCAO introduces a\ngroup-level optimization mechanism that aggregates low-density boundary points\ninto collaboratively moving groups, replacing the traditional point-based\ncontraction process. By combining local density estimation with neighborhood\ntopology, GCAO constructs effective gravitational interactions between groups\nand their surroundings, enhancing boundary clarity and structural consistency.\nUsing groups as basic motion units, a gravitational contraction strategy\nensures globally stable and directionally consistent convergence. Experiments\non multiple high-dimensional datasets demonstrate that GCAO outperforms 11\nrepresentative clustering methods, achieving average improvements of 37.13%,\n52.08%, 44.98%, and 38.81% in NMI, ARI, Homogeneity, and ACC, respectively,\nwhile maintaining competitive efficiency and scalability. These results\nhighlight GCAO's superiority in preserving cluster integrity, enhancing\nboundary separability, and ensuring robust performance on complex data\ndistributions.\n","authors":["Qi Li","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23258v1","updated":"2025-10-27T12:21:33Z","published":"2025-10-27T12:21:33Z","title":"Deep Active Inference with Diffusion Policy and Multiple Timescale World\n  Model for Real-World Exploration and Navigation","summary":"  Autonomous robotic navigation in real-world environments requires exploration\nto acquire environmental information as well as goal-directed navigation in\norder to reach specified targets. Active inference (AIF) based on the\nfree-energy principle provides a unified framework for these behaviors by\nminimizing the expected free energy (EFE), thereby combining epistemic and\nextrinsic values. To realize this practically, we propose a deep AIF framework\nthat integrates a diffusion policy as the policy model and a multiple timescale\nrecurrent state-space model (MTRSSM) as the world model. The diffusion policy\ngenerates diverse candidate actions while the MTRSSM predicts their\nlong-horizon consequences through latent imagination, enabling action selection\nthat minimizes EFE. Real-world navigation experiments demonstrated that our\nframework achieved higher success rates and fewer collisions compared with the\nbaselines, particularly in exploration-demanding scenarios. These results\nhighlight how AIF based on EFE minimization can unify exploration and\ngoal-directed navigation in real-world robotic settings.\n","authors":["Riko Yokozawa","Kentaro Fujii","Yuta Nomura","Shingo Murata"],"pdf_url":"https://arxiv.org/pdf/2510.23258v1.pdf","comment":"Preprint version"},{"id":"http://arxiv.org/abs/2510.23254v1","updated":"2025-10-27T12:16:49Z","published":"2025-10-27T12:16:49Z","title":"Provable test-time adaptivity and distributional robustness of\n  in-context learning","summary":"  We study in-context learning problems where a Transformer is pretrained on\ntasks drawn from a mixture distribution $\\pi=\\sum_{\\alpha\\in\\mathcal{A}}\n\\lambda_{\\alpha} \\pi_{\\alpha}$, called the pretraining prior, in which each\nmixture component $\\pi_{\\alpha}$ is a distribution on tasks of a specific\ndifficulty level indexed by $\\alpha$. Our goal is to understand the performance\nof the pretrained Transformer when evaluated on a different test distribution\n$\\mu$, consisting of tasks of fixed difficulty $\\beta\\in\\mathcal{A}$, and with\npotential distribution shift relative to $\\pi_\\beta$, subject to the\nchi-squared divergence $\\chi^2(\\mu,\\pi_{\\beta})$ being at most $\\kappa$. In\nparticular, we consider nonparametric regression problems with random\nsmoothness, and multi-index models with random smoothness as well as random\neffective dimension. We prove that a large Transformer pretrained on sufficient\ndata achieves the optimal rate of convergence corresponding to the difficulty\nlevel $\\beta$, uniformly over test distributions $\\mu$ in the chi-squared\ndivergence ball. Thus, the pretrained Transformer is able to achieve faster\nrates of convergence on easier tasks and is robust to distribution shift at\ntest time. Finally, we prove that even if an estimator had access to the test\ndistribution $\\mu$, the convergence rate of its expected risk over $\\mu$ could\nnot be faster than that of our pretrained Transformers, thereby providing a\nmore appropriate optimality guarantee than minimax lower bounds.\n","authors":["Tianyi Ma","Tengyao Wang","Richard J. Samworth"],"pdf_url":"https://arxiv.org/pdf/2510.23254v1.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2505.16635v2","updated":"2025-10-27T12:12:05Z","published":"2025-05-22T13:07:06Z","title":"WikiDBGraph: A Data Management Benchmark Suite for Collaborative\n  Learning over Database Silos","summary":"  Relational databases are often fragmented across organizations, creating data\nsilos that hinder distributed data management and mining. Collaborative\nlearning (CL) -- techniques that enable multiple parties to train models\njointly without sharing raw data -- offers a principled approach to this\nchallenge. However, existing CL frameworks (e.g., federated and split learning)\nremain limited in real-world deployments. Current CL benchmarks and algorithms\nprimarily target the learning step under assumptions of isolated, aligned, and\njoinable databases, and they typically neglect the end-to-end data management\npipeline, especially preprocessing steps such as table joins and data\nalignment. In contrast, our analysis of the real-world corpus WikiDBs shows\nthat databases are interconnected, unaligned, and sometimes unjoinable,\nexposing a significant gap between CL algorithm design and practical\ndeployment. To close this evaluation gap, we build WikiDBGraph, a large-scale\ndataset constructed from 100{,}000 real-world relational databases linked by 17\nmillion weighted edges. Each node (database) and edge (relationship) is\nannotated with 13 and 12 properties, respectively, capturing a hybrid of\ninstance- and feature-level overlap across databases. Experiments on\nWikiDBGraph demonstrate both the effectiveness and limitations of existing CL\nmethods under realistic conditions, highlighting previously overlooked gaps in\nmanaging real-world data silos and pointing to concrete directions for\npractical deployment of collaborative learning systems.\n","authors":["Zhaomin Wu","Ziyang Wang","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2505.16635v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16315v7","updated":"2025-10-27T11:57:07Z","published":"2024-11-25T12:08:54Z","title":"Local Learning for Covariate Selection in Nonparametric Causal Effect\n  Estimation with Latent Variables","summary":"  Estimating causal effects from nonexperimental data is a fundamental problem\nin many fields of science. A key component of this task is selecting an\nappropriate set of covariates for confounding adjustment to avoid bias. Most\nexisting methods for covariate selection often assume the absence of latent\nvariables and rely on learning the global network structure among variables.\nHowever, identifying the global structure can be unnecessary and inefficient,\nespecially when our primary interest lies in estimating the effect of a\ntreatment variable on an outcome variable. To address this limitation, we\npropose a novel local learning approach for covariate selection in\nnonparametric causal effect estimation, which accounts for the presence of\nlatent variables. Our approach leverages testable independence and dependence\nrelationships among observed variables to identify a valid adjustment set for a\ntarget causal relationship, ensuring both soundness and completeness under\nstandard assumptions. We validate the effectiveness of our algorithm through\nextensive experiments on both synthetic and real-world data.\n","authors":["Zheng Li","Xichen Guo","Feng Xie","Yan Zeng","Hao Zhang","Zhi Geng"],"pdf_url":"https://arxiv.org/pdf/2411.16315v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23241v1","updated":"2025-10-27T11:55:12Z","published":"2025-10-27T11:55:12Z","title":"Progressive Growing of Patch Size: Curriculum Learning for Accelerated\n  and Improved Medical Image Segmentation","summary":"  In this work, we introduce Progressive Growing of Patch Size, an automatic\ncurriculum learning approach for 3D medical image segmentation. Our approach\nprogressively increases the patch size during model training, resulting in an\nimproved class balance for smaller patch sizes and accelerated convergence of\nthe training process. We evaluate our curriculum approach in two settings: a\nresource-efficient mode and a performance mode, both regarding Dice score\nperformance and computational costs across 15 diverse and popular 3D medical\nimage segmentation tasks. The resource-efficient mode matches the Dice score\nperformance of the conventional constant patch size sampling baseline with a\nnotable reduction in training time to only 44%. The performance mode improves\nupon constant patch size segmentation results, achieving a statistically\nsignificant relative mean performance gain of 1.28% in Dice Score. Remarkably,\nacross all 15 tasks, our proposed performance mode manages to surpass the\nconstant patch size baseline in Dice Score performance, while simultaneously\nreducing training time to only 89%. The benefits are particularly pronounced\nfor highly imbalanced tasks such as lesion segmentation tasks. Rigorous\nexperiments demonstrate that our performance mode not only improves mean\nsegmentation performance but also reduces performance variance, yielding more\ntrustworthy model comparison. Furthermore, our findings reveal that the\nproposed curriculum sampling is not tied to a specific architecture but\nrepresents a broadly applicable strategy that consistently boosts performance\nacross diverse segmentation models, including UNet, UNETR, and SwinUNETR. In\nsummary, we show that this simple yet elegant transformation on input data\nsubstantially improves both Dice Score performance and training runtime, while\nbeing compatible across diverse segmentation backbones.\n","authors":["Stefan M. Fischer","Johannes Kiechle","Laura Daza","Lina Felsner","Richard Osuala","Daniel M. Lang","Karim Lekadir","Jan C. Peeken","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2510.23241v1.pdf","comment":"Journal Extension of \"Progressive Growing of Patch Size:\n  Resource-Efficient Curriculum Learning for Dense Prediction Tasks\"\n  (MICCAI2024) submitted to MedIA"},{"id":"http://arxiv.org/abs/2508.01488v2","updated":"2025-10-27T11:55:07Z","published":"2025-08-02T21:00:55Z","title":"PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective","summary":"  In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.\n","authors":["Alain Riou","Bernardo Torres","Ben Hayes","Stefan Lattner","Gaëtan Hadjeres","Gaël Richard","Geoffroy Peeters"],"pdf_url":"https://arxiv.org/pdf/2508.01488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23237v1","updated":"2025-10-27T11:48:44Z","published":"2025-10-27T11:48:44Z","title":"Robust Iterative Learning Hidden Quantum Markov Models","summary":"  Hidden Quantum Markov Models (HQMMs) extend classical Hidden Markov Models to\nthe quantum domain, offering a powerful probabilistic framework for modeling\nsequential data with quantum coherence. However, existing HQMM learning\nalgorithms are highly sensitive to data corruption and lack mechanisms to\nensure robustness under adversarial perturbations. In this work, we introduce\nthe Adversarially Corrupted HQMM (AC-HQMM), which formalizes robustness\nanalysis by allowing a controlled fraction of observation sequences to be\nadversarially corrupted. To learn AC-HQMMs, we propose the Robust Iterative\nLearning Algorithm (RILA), a derivative-free method that integrates a Remove\nCorrupted Rows by Entropy Filtering (RCR-EF) module with an iterative\nstochastic resampling procedure for physically valid Kraus operator updates.\nRILA incorporates L1-penalized likelihood objectives to enhance stability,\nresist overfitting, and remain effective under non-differentiable conditions.\nAcross multiple HQMM and HMM benchmarks, RILA demonstrates superior convergence\nstability, corruption resilience, and preservation of physical validity\ncompared to existing algorithms, establishing a principled and efficient\napproach for robust quantum sequential learning.\n","authors":["Ning Ning"],"pdf_url":"https://arxiv.org/pdf/2510.23237v1.pdf","comment":"Quantum Computing, Bayesian Inference, Spatiotemporal Analysis,\n  Robust Learning"},{"id":"http://arxiv.org/abs/2510.23235v1","updated":"2025-10-27T11:40:14Z","published":"2025-10-27T11:40:14Z","title":"Grassmanian Interpolation of Low-Pass Graph Filters: Theory and\n  Applications","summary":"  Low-pass graph filters are fundamental for signal processing on graphs and\nother non-Euclidean domains. However, the computation of such filters for\nparametric graph families can be prohibitively expensive as computation of the\ncorresponding low-frequency subspaces, requires the repeated solution of an\neigenvalue problem. We suggest a novel algorithm of low-pass graph filter\ninterpolation based on Riemannian interpolation in normal coordinates on the\nGrassmann manifold. We derive an error bound estimate for the subspace\ninterpolation and suggest two possible applications for induced parametric\ngraph families. First, we argue that the temporal evolution of the node\nfeatures may be translated to the evolving graph topology via a similarity\ncorrection to adjust the homophily degree of the network. Second, we suggest a\ndot product graph family induced by a given static graph which allows to infer\nimproved message passing scheme for node classification facilitated by the\nfilter interpolation.\n","authors":["Anton Savostianov","Michael T. Schaub","Benjamin Stamm"],"pdf_url":"https://arxiv.org/pdf/2510.23235v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2410.02777v2","updated":"2025-10-27T11:37:07Z","published":"2024-09-17T16:00:35Z","title":"Secure and Confidential Certificates of Online Fairness","summary":"  The black-box service model enables ML service providers to serve clients\nwhile keeping their intellectual property and client data confidential.\nConfidentiality is critical for delivering ML services legally and responsibly,\nbut makes it difficult for outside parties to verify important model properties\nsuch as fairness. Existing methods that assess model fairness confidentially\nlack either (i) reliability because they certify fairness with respect to a\nstatic set of data, and therefore fail to guarantee fairness in the presence of\ndistribution shift or service provider malfeasance; and/or (ii) scalability due\nto the computational overhead of confidentiality-preserving cryptographic\nprimitives. We address these problems by introducing online fairness\ncertificates, which verify that a model is fair with respect to data received\nby the service provider online during deployment. We then present OATH, a\ndeployably efficient and scalable zero-knowledge proof protocol for\nconfidential online group fairness certification. OATH exploits statistical\nproperties of group fairness via a cut-and-choose style protocol, enabling\nscalability improvements over baselines.\n","authors":["Olive Franzese","Ali Shahin Shamsabadi","Carter Luck","Hamed Haddadi"],"pdf_url":"https://arxiv.org/pdf/2410.02777v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09384v2","updated":"2025-10-27T11:25:33Z","published":"2025-03-12T13:29:33Z","title":"Revisiting Agnostic Boosting","summary":"  Boosting is a key method in statistical learning, allowing for converting\nweak learners into strong ones. While well studied in the realizable case, the\nstatistical properties of weak-to-strong learning remain less understood in the\nagnostic setting, where there are no assumptions on the distribution of the\nlabels. In this work, we propose a new agnostic boosting algorithm with\nsubstantially improved sample complexity compared to prior works under very\ngeneral assumptions. Our approach is based on a reduction to the realizable\ncase, followed by a margin-based filtering of high-quality hypotheses.\nFurthermore, we show a nearly-matching lower bound, settling the sample\ncomplexity of agnostic boosting up to logarithmic factors.\n","authors":["Arthur da Cunha","Mikael Møller Høgsgaard","Andrea Paudice","Yuxin Sun"],"pdf_url":"https://arxiv.org/pdf/2503.09384v2.pdf","comment":"Camera-ready version: NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.21339v2","updated":"2025-10-27T11:23:40Z","published":"2025-10-24T11:08:32Z","title":"Multi-turn Training with Basic Human Feedback Helps Little on LLM\n  Reasoning","summary":"  The reasoning capabilities of Large Language Models (LLMs) are typically\ndeveloped through the single-turn reinforcement learning, whereas real-world\napplications often involve multi-turn interactions with human feedback, leading\nto a potential mismatch between training and deployment conditions. In this\nwork, we study whether multi-turn training with human feedback is necessary for\nreasoning tasks. We compare conventional single-turn training with three\nmulti-turn strategies and reach contrary conclusions to previous research. We\nfind that models trained in a single-turn setting generalize effectively to\nboth single- and multi-turn evaluations, while models trained with multi-turn\nstrategies exhibit a significant degradation in single-turn reasoning\nperformance. These results suggest that for tasks with complete information,\nrobust single-turn training remains more effective and reliable, as multi-turn\ntraining with basic feedback provides limited benefits and can even degrade\nreasoning capabilities.\n","authors":["Qiang Liu","Wuganjing Song","Zhenzhou Lin","Feifan Chen","Qiaolong Cai","Chen Li","Yongduo Sui"],"pdf_url":"https://arxiv.org/pdf/2510.21339v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05525v2","updated":"2025-10-27T11:14:40Z","published":"2025-05-08T09:17:26Z","title":"A critical assessment of reinforcement learning methods for microswimmer\n  navigation in complex flows","summary":"  Navigating in a fluid flow while being carried by it, using only information\naccessible from on-board sensors, is a problem commonly faced by small\nplanktonic organisms. It is also directly relevant to autonomous robots\ndeployed in the oceans. In the last ten years, the fluid mechanics community\nhas widely adopted reinforcement learning, often in the form of its simplest\nimplementations, to address this challenge. But it is unclear how good are the\nstrategies learned by these algorithms. In this paper, we perform a\nquantitative assessment of reinforcement learning methods applied to navigation\nin partially observable flows. We first introduce a well-posed problem of\ndirectional navigation for which a quasi-optimal policy is known analytically.\nWe then report on the poor performance and robustness of commonly used\nalgorithms (Q-Learning, Advantage Actor Critic) in flows regularly encountered\nin the literature: Taylor-Green vortices, Arnold-Beltrami-Childress flow, and\ntwo-dimensional turbulence. We show that they are vastly surpassed by PPO\n(Proximal Policy Optimization), a more advanced algorithm that has established\ndominance across a wide range of benchmarks in the reinforcement learning\ncommunity. In particular, our custom implementation of PPO matches the\ntheoretical quasi-optimal performance in turbulent flow and does so in a robust\nmanner. Reaching this result required the use of several additional techniques,\nsuch as vectorized environments and generalized advantage estimation, as well\nas hyperparameter optimization. This study demonstrates the importance of\nalgorithm selection, implementation details, and fine-tuning for discovering\ntruly smart autonomous navigation strategies in complex flows.\n","authors":["Selim Mecanna","Aurore Loisy","Christophe Eloy"],"pdf_url":"https://arxiv.org/pdf/2505.05525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23216v1","updated":"2025-10-27T11:06:00Z","published":"2025-10-27T11:06:00Z","title":"Human-Like Goalkeeping in a Realistic Football Simulation: a\n  Sample-Efficient Reinforcement Learning Approach","summary":"  While several high profile video games have served as testbeds for Deep\nReinforcement Learning (DRL), this technique has rarely been employed by the\ngame industry for crafting authentic AI behaviors. Previous research focuses on\ntraining super-human agents with large models, which is impractical for game\nstudios with limited resources aiming for human-like agents. This paper\nproposes a sample-efficient DRL method tailored for training and fine-tuning\nagents in industrial settings such as the video game industry. Our method\nimproves sample efficiency of value-based DRL by leveraging pre-collected data\nand increasing network plasticity. We evaluate our method training a goalkeeper\nagent in EA SPORTS FC 25, one of the best-selling football simulations today.\nOur agent outperforms the game's built-in AI by 10% in ball saving rate.\nAblation studies show that our method trains agents 50% faster compared to\nstandard DRL methods. Finally, qualitative evaluation from domain experts\nindicates that our approach creates more human-like gameplay compared to\nhand-crafted agents. As a testimony of the impact of the approach, the method\nis intended to replace the hand-crafted counterpart in next iterations of the\nseries.\n","authors":["Alessandro Sestini","Joakim Bergdahl","Jean-Philippe Barrette-LaPierre","Florian Fuchs","Brady Chen","Micheal Jones","Linus Gisslén"],"pdf_url":"https://arxiv.org/pdf/2510.23216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23215v1","updated":"2025-10-27T11:05:16Z","published":"2025-10-27T11:05:16Z","title":"Accelerating Eigenvalue Dataset Generation via Chebyshev Subspace Filter","summary":"  Eigenvalue problems are among the most important topics in many scientific\ndisciplines. With the recent surge and development of machine learning, neural\neigenvalue methods have attracted significant attention as a forward pass of\ninference requires only a tiny fraction of the computation time compared to\ntraditional solvers. However, a key limitation is the requirement for large\namounts of labeled data in training, including operators and their eigenvalues.\nTo tackle this limitation, we propose a novel method, named Sorting Chebyshev\nSubspace Filter (SCSF), which significantly accelerates eigenvalue data\ngeneration by leveraging similarities between operators -- a factor overlooked\nby existing methods. Specifically, SCSF employs truncated fast Fourier\ntransform sorting to group operators with similar eigenvalue distributions and\nconstructs a Chebyshev subspace filter that leverages eigenpairs from\npreviously solved problems to assist in solving subsequent ones, reducing\nredundant computations. To the best of our knowledge, SCSF is the first method\nto accelerate eigenvalue data generation. Experimental results show that SCSF\nachieves up to a $3.5\\times$ speedup compared to various numerical solvers.\n","authors":["Hong Wang","Jie Wang","Jian Luo","huanshuo dong","Yeqiu Chen","Runmin Jiang","Zhen huang"],"pdf_url":"https://arxiv.org/pdf/2510.23215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05596v2","updated":"2025-10-27T11:02:49Z","published":"2025-06-05T21:15:13Z","title":"Zero-shot protein stability prediction by inverse folding models: a free\n  energy interpretation","summary":"  Inverse folding models have proven to be highly effective zero-shot\npredictors of protein stability. Despite this success, the link between the\namino acid preferences of an inverse folding model and the free-energy\nconsiderations underlying thermodynamic stability remains incompletely\nunderstood. A better understanding would be of interest not only from a\ntheoretical perspective, but also potentially provide the basis for stronger\nzero-shot stability prediction. In this paper, we take steps to clarify the\nfree-energy foundations of inverse folding models. Our derivation reveals the\nstandard practice of likelihood ratios as a simplistic approximation and\nsuggests several paths towards better estimates of the relative stability. We\nempirically assess these approaches and demonstrate that considerable gains in\nzero-shot performance can be achieved with fairly simple means.\n","authors":["Jes Frellsen","Maher M. Kassem","Tone Bengtsen","Lars Olsen","Kresten Lindorff-Larsen","Jesper Ferkinghoff-Borg","Wouter Boomsma"],"pdf_url":"https://arxiv.org/pdf/2506.05596v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.11507v2","updated":"2025-10-27T10:57:33Z","published":"2025-10-13T15:17:08Z","title":"Automatic Music Sample Identification with Multi-Track Contrastive\n  Learning","summary":"  Sampling, the technique of reusing pieces of existing audio tracks to create\nnew music content, is a very common practice in modern music production. In\nthis paper, we tackle the challenging task of automatic sample identification,\nthat is, detecting such sampled content and retrieving the material from which\nit originates. To do so, we adopt a self-supervised learning approach that\nleverages a multi-track dataset to create positive pairs of artificial mixes,\nand design a novel contrastive learning objective. We show that such method\nsignificantly outperforms previous state-of-the-art baselines, that is robust\nto various genres, and that scales well when increasing the number of noise\nsongs in the reference database. In addition, we extensively analyze the\ncontribution of the different components of our training pipeline and\nhighlight, in particular, the need for high-quality separated stems for this\ntask.\n","authors":["Alain Riou","Joan Serrà","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2510.11507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23208v1","updated":"2025-10-27T10:54:25Z","published":"2025-10-27T10:54:25Z","title":"Increasing LLM Coding Capabilities through Diverse Synthetic Coding\n  Tasks","summary":"  Large language models (LLMs) have shown impressive promise in code\ngeneration, yet their progress remains limited by the shortage of large-scale\ndatasets that are both diverse and well-aligned with human reasoning. Most\nexisting resources pair problems with solutions, but omit the intermediate\nthought process that guides coding. To close this gap, we present a scalable\nsynthetic data generation pipeline that produces nearly 800k\ninstruction-reasoning-code-test quadruplets. Each sample combines a task, a\nstep-by-step reasoning trace, a working solution, and executable tests,\nenabling models to learn not just the what but also the how of problem solving.\nOur pipeline combines four key components: curated contest problems, web-mined\ncontent filtered by relevance classifiers, data expansion guided by reasoning\npatterns, and multi-stage execution-based validation. A genetic mutation\nalgorithm further increases task diversity while maintaining consistency\nbetween reasoning traces and code implementations. Our key finding is that\nfine-tuning LLMs on this dataset yields consistent improvements on coding\nbenchmarks. Beyond raw accuracy, reasoning-aware data can substitute for model\nscaling, generalize across architectures, and outperform leading open-source\nalternatives under identical sample budgets. Our work establishes\nreasoning-centered synthetic data generation as an efficient approach for\nadvancing coding capabilities in LLMs. We publish our dataset and generation\npipeline to facilitate further research.\n","authors":["Amal Abed","Ivan Lukic","Jörg K. H. Franke","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2510.23208v1.pdf","comment":"Presented at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) Workshop: The 4th Deep Learning for Code Workshop\n  (DL4C)"},{"id":"http://arxiv.org/abs/2311.09671v2","updated":"2025-10-27T10:48:43Z","published":"2023-11-16T08:39:58Z","title":"Generalization Bounds for Robust Contrastive Learning: From Theory to\n  Practice","summary":"  Contrastive Learning first extracts features from unlabeled data, followed by\nlinear probing with labeled data. Adversarial Contrastive Learning (ACL)\nintegrates Adversarial Training into the first phase to enhance feature\nrobustness against attacks in the probing phase. While ACL has shown strong\nempirical results, its theoretical understanding remains limited. Furthermore,\nwhile a fair amount of theoretical works analyze how the unsupervised loss can\nsupport the supervised loss in the probing phase, none has examined its role to\nthe robust supervised loss. To fill this gap, our work develops rigorous\ntheories to identify which components in the unsupervised training can help\nimprove the robust supervised loss. Specifically, besides the adversarial\ncontrastive loss, we reveal that the benign one, along with a global divergence\nbetween benign and adversarial examples can also improve robustness. Proper\nexperiments are conducted to justify our findings.\n","authors":["Ngoc N. Tran","Lam Tran","Hoang Phan","Anh Bui","Tung Pham","Toan Tran","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2311.09671v2.pdf","comment":"13 pages, 1 figure, 4 tables"},{"id":"http://arxiv.org/abs/2506.20324v2","updated":"2025-10-27T10:42:21Z","published":"2025-06-25T11:06:30Z","title":"Permutation Equivariant Neural Controlled Differential Equations for\n  Dynamic Graph Representation Learning","summary":"  Dynamic graphs exhibit complex temporal dynamics due to the interplay between\nevolving node features and changing network structures. Recently, Graph Neural\nControlled Differential Equations (Graph Neural CDEs) successfully adapted\nNeural CDEs from paths on Euclidean domains to paths on graph domains. Building\non this foundation, we introduce Permutation Equivariant Neural Graph CDEs,\nwhich project Graph Neural CDEs onto permutation equivariant function spaces.\nThis significantly reduces the model's parameter count without compromising\nrepresentational power, resulting in more efficient training and improved\ngeneralisation. We empirically demonstrate the advantages of our approach\nthrough experiments on simulated dynamical systems and real-world tasks,\nshowing improved performance in both interpolation and extrapolation scenarios.\n","authors":["Torben Berndt","Benjamin Walker","Tiexin Qin","Jan Stühmer","Andrey Kormilitzin"],"pdf_url":"https://arxiv.org/pdf/2506.20324v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.19885v2","updated":"2025-10-27T10:39:15Z","published":"2025-06-24T04:53:49Z","title":"FlightKooba: A Fast Interpretable FTP Model","summary":"  Flight trajectory prediction (FTP) and similar time series tasks typically\nrequire capturing smooth latent dynamics hidden within noisy signals. However,\nexisting deep learning models face significant challenges of high computational\ncost and insufficient interpretability due to their complex black-box nature.\nThis paper introduces FlightKooba, a novel modeling approach designed to\nextract such underlying dynamics analytically. Our framework uniquely\nintegrates HiPPO theory, Koopman operator theory, and control theory. By\nleveraging Legendre polynomial bases, it constructs Koopman operators\nanalytically, thereby avoiding large-scale parameter training. The method's\ncore strengths lie in its exceptional computational efficiency and inherent\ninterpretability. Experiments on multiple public datasets validate our design\nphilosophy: for signals exhibiting strong periodicity or clear physical laws\n(e.g., in aviation, meteorology, and traffic flow), FlightKooba delivers\ncompetitive prediction accuracy while reducing trainable parameters by several\norders of magnitude and achieving the fastest training speed. Furthermore, we\nanalyze the model's theoretical boundaries, clarifying its inherent low-pass\nfiltering characteristics that render it unsuitable for sequences dominated by\nhigh-frequency noise. In summary, FlightKooba offers a powerful, efficient, and\ninterpretable new alternative for time series analysis, particularly in\nresource-constrained environments.\n","authors":["Jing Lu","Xuan Wu","Yizhun Tian","Songhan Fan","Yali Fang"],"pdf_url":"https://arxiv.org/pdf/2506.19885v2.pdf","comment":"Version 2: Major revision of the manuscript to refine the narrative,\n  clarify the model's theoretical limitations and application scope, and\n  improve overall presentation for journal submission"},{"id":"http://arxiv.org/abs/2510.23199v1","updated":"2025-10-27T10:36:56Z","published":"2025-10-27T10:36:56Z","title":"Rate-optimal Design for Anytime Best Arm Identification","summary":"  We consider the best arm identification problem, where the goal is to\nidentify the arm with the highest mean reward from a set of $K$ arms under a\nlimited sampling budget. This problem models many practical scenarios such as\nA/B testing. We consider a class of algorithms for this problem, which is\nprovably minimax optimal up to a constant factor. This idea is a generalization\nof existing works in fixed-budget best arm identification, which are limited to\na particular choice of risk measures. Based on the framework, we propose Almost\nTracking, a closed-form algorithm that has a provable guarantee on the popular\nrisk measure $H_1$. Unlike existing algorithms, Almost Tracking does not\nrequire the total budget in advance nor does it need to discard a significant\npart of samples, which gives a practical advantage. Through experiments on\nsynthetic and real-world datasets, we show that our algorithm outperforms\nexisting anytime algorithms as well as fixed-budget algorithms.\n","authors":["Junpei Komiyama","Kyoungseok Jang","Junya Honda"],"pdf_url":"https://arxiv.org/pdf/2510.23199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23198v1","updated":"2025-10-27T10:36:15Z","published":"2025-10-27T10:36:15Z","title":"PTPP-Aware Adaptation Scaling Laws: Predicting Domain-Adaptation\n  Performance at Unseen Pre-Training Budgets","summary":"  Continual pre-training (CPT) for domain adaptation must balance target-domain\ngains with stability on the base domain. Existing CPT scaling laws typically\nassume a fixed pre-training budget, which limits their ability to forecast\nadaptation outcomes for models trained at different tokens-per-parameter\n(PTPP). We present \\emph{PTPP-aware} adaptation scaling laws that make the\npre-training budget an explicit variable, enabling accurate \\emph{prediction}\nof adaptation loss at unseen \\ptpp. On a multilingual setup (English/Arabic\n$\\rightarrow$ French), PTPP-aware formulations trained on early stages\n(\\ptpp{}=\\{15,31\\}) predict target loss at \\ptpp{}=279 and outperform a\nPTPP-agnostic \\dcpt{} transfer baseline on metrics (Huber-on-log,\nMAE$_\\mathrm{rel}$, calibration slope); full diagnostics (RMSE, MAPE) are in\nthe appendix. Beyond forecasting, we show a practical use case: planning replay\nratios and adaptation token budgets that satisfy target and forgetting\nconstraints under compute limits.\n","authors":["Etienne Goffinet","Shane Bergsma","Avraham Sheinin","Natalia Vassilieva","Shaheer Muhammad","Preslav Nakov","Gurpreet Gosal"],"pdf_url":"https://arxiv.org/pdf/2510.23198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23191v1","updated":"2025-10-27T10:30:30Z","published":"2025-10-27T10:30:30Z","title":"The Benchmarking Epistemology: Construct Validity for Evaluating Machine\n  Learning Models","summary":"  Predictive benchmarking, the evaluation of machine learning models based on\npredictive performance and competitive ranking, is a central epistemic practice\nin machine learning research and an increasingly prominent method for\nscientific inquiry. Yet, benchmark scores alone provide at best measurements of\nmodel performance relative to an evaluation dataset and a concrete learning\nproblem. Drawing substantial scientific inferences from the results, say about\ntheoretical tasks like image classification, requires additional assumptions\nabout the theoretical structure of the learning problems, evaluation functions,\nand data distributions. We make these assumptions explicit by developing\nconditions of construct validity inspired by psychological measurement theory.\nWe examine these assumptions in practice through three case studies, each\nexemplifying a typical intended inference: measuring engineering progress in\ncomputer vision with ImageNet; evaluating policy-relevant weather predictions\nwith WeatherBench; and examining limitations of the predictability of life\nevents with the Fragile Families Challenge. Our framework clarifies the\nconditions under which benchmark scores can support diverse scientific claims,\nbringing predictive benchmarking into perspective as an epistemological\npractice and a key site of conceptual and theoretical reasoning in machine\nlearning.\n","authors":["Timo Freiesleben","Sebastian Zezulka"],"pdf_url":"https://arxiv.org/pdf/2510.23191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19599v3","updated":"2025-10-27T10:30:01Z","published":"2025-04-28T09:02:24Z","title":"GVPO: Group Variance Policy Optimization for Large Language Model\n  Post-Training","summary":"  Post-training plays a crucial role in refining and aligning large language\nmodels to meet specific tasks and human preferences. While recent advancements\nin post-training techniques, such as Group Relative Policy Optimization (GRPO),\nleverage increased sampling with relative reward scoring to achieve superior\nperformance, these methods often suffer from training instability that limits\ntheir practical adoption. As a next step, we present Group Variance Policy\nOptimization (GVPO). GVPO incorporates the analytical solution to\nKL-constrained reward maximization directly into its gradient weights, ensuring\nalignment with the optimal policy. The method provides intuitive physical\ninterpretations: its gradient mirrors the mean squared error between the\ncentral distance of implicit rewards and that of actual rewards. GVPO offers\ntwo key advantages: (1) it guarantees a unique optimal solution, exactly the\nKL-constrained reward maximization objective, (2) it supports flexible sampling\ndistributions that avoids on-policy and importance sampling limitations. By\nunifying theoretical guarantees with practical adaptability, GVPO establishes a\nnew paradigm for reliable and versatile LLM post-training.\n","authors":["Kaichen Zhang","Yuzhong Hong","Junwei Bao","Hongfei Jiang","Yang Song","Dingqian Hong","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2504.19599v3.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23189v1","updated":"2025-10-27T10:27:00Z","published":"2025-10-27T10:27:00Z","title":"DREaM: Drug-Drug Relation Extraction via Transfer Learning Method","summary":"  Relation extraction between drugs plays a crucial role in identifying drug\ndrug interactions and predicting side effects. The advancement of machine\nlearning methods in relation extraction, along with the development of large\nmedical text databases, has enabled the low cost extraction of such relations\ncompared to other approaches that typically require expert knowledge. However,\nto the best of our knowledge, there are limited datasets specifically designed\nfor drug drug relation extraction currently available. Therefore, employing\ntransfer learning becomes necessary to apply machine learning methods in this\ndomain. In this study, we propose DREAM, a method that first employs a trained\nrelation extraction model to discover relations between entities and then\napplies this model to a corpus of medical texts to construct an ontology of\ndrug relationships. The extracted relations are subsequently validated using a\nlarge language model. Quantitative results indicate that the LLM agreed with 71\nof the relations extracted from a subset of PubMed abstracts. Furthermore, our\nqualitative analysis indicates that this approach can uncover ambiguities in\nthe medical domain, highlighting the challenges inherent in relation extraction\nin this field.\n","authors":["Ali Fata","Hossein Rahmani","Parinaz Soltanzadeh","Amirhossein Derakhshan","Behrouz Minaei Bidgoli"],"pdf_url":"https://arxiv.org/pdf/2510.23189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23181v1","updated":"2025-10-27T10:21:35Z","published":"2025-10-27T10:21:35Z","title":"Physics-informed diffusion models for extrapolating crystal structures\n  beyond known motifs","summary":"  Discovering materials with previously unreported crystal frameworks is key to\nachieving transformative functionality. Generative artificial intelligence\noffers a scalable means to propose candidate crystal structures, however\nexisting approaches mainly reproduce decorated variants of established motifs\nrather than uncover new configurations. Here we develop a physics-informed\ndiffusion method, supported by chemically grounded validation protocol, which\nembeds descriptors of compactness and local environment diversity to balance\nphysical plausibility with structural novelty. Conditioning on these metrics\nimproves generative performance across architectures, increasing the fraction\nof structures outside 100 most common prototypes up to 67%. When crystal\nstructure prediction (CSP) is seeded with generative structures, most\ncandidates (97%) are reconstructed by CSP, yielding 145 (66%) low-energy\nframeworks not matching any known prototypes. These results show that while\ngenerative models are not substitutes for CSP, their chemically informed,\ndiversity-guided outputs can enhance CSP efficiency, establishing a practical\ngenerative-CSP synergy for discovery-oriented exploration of chemical space.\n","authors":["Andrij Vasylenko","Federico Ottomano","Christopher M. Collins","Rahul Savani","Matthew S. Dyer","Matthew J. Rosseinsky"],"pdf_url":"https://arxiv.org/pdf/2510.23181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23176v1","updated":"2025-10-27T10:10:19Z","published":"2025-10-27T10:10:19Z","title":"TARC: Time-Adaptive Robotic Control","summary":"  Fixed-frequency control in robotics imposes a trade-off between the\nefficiency of low-frequency control and the robustness of high-frequency\ncontrol, a limitation not seen in adaptable biological systems. We address this\nwith a reinforcement learning approach in which policies jointly select control\nactions and their application durations, enabling robots to autonomously\nmodulate their control frequency in response to situational demands. We\nvalidate our method with zero-shot sim-to-real experiments on two distinct\nhardware platforms: a high-speed RC car and a quadrupedal robot. Our method\nmatches or outperforms fixed-frequency baselines in terms of rewards while\nsignificantly reducing the control frequency and exhibiting adaptive frequency\ncontrol under real-world conditions.\n","authors":["Arnav Sukhija","Lenart Treven","Jin Cheng","Florian Dörfler","Stelian Coros","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2510.23176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23171v1","updated":"2025-10-27T09:57:26Z","published":"2025-10-27T09:57:26Z","title":"Benchmarking VQE Configurations: Architectures, Initializations, and\n  Optimizers for Silicon Ground State Energy","summary":"  Quantum computing presents a promising path toward precise quantum chemical\nsimulations, particularly for systems that challenge classical methods. This\nwork investigates the performance of the Variational Quantum Eigensolver (VQE)\nin estimating the ground-state energy of the silicon atom, a relatively heavy\nelement that poses significant computational complexity. Within a hybrid\nquantum-classical optimization framework, we implement VQE using a range of\nansatz, including Double Excitation Gates, ParticleConservingU2, UCCSD, and\nk-UpCCGSD, combined with various optimizers such as gradient descent, SPSA, and\nADAM. The main contribution of this work lies in a systematic methodological\nexploration of how these configuration choices interact to influence VQE\nperformance, establishing a structured benchmark for selecting optimal settings\nin quantum chemical simulations. Key findings show that parameter\ninitialization plays a decisive role in the algorithm's stability, and that the\ncombination of a chemically inspired ansatz with adaptive optimization yields\nsuperior convergence and precision compared to conventional approaches.\n","authors":["Zakaria Boutakka","Nouhaila Innan","Muhammed Shafique","Mohamed Bennai","Z. Sakhi"],"pdf_url":"https://arxiv.org/pdf/2510.23171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02917v2","updated":"2025-10-27T09:37:07Z","published":"2025-06-25T09:56:25Z","title":"Echo State Transformer: Attention Over Finite Memories","summary":"  While Large Language Models and their underlying Transformer architecture are\nremarkably efficient, they do not reflect how our brain processes and learns a\ndiversity of cognitive tasks such as language and working memory. Furthermore,\nsequential data processing with Transformers encounters a fundamental barrier:\nquadratic complexity growth with sequence length. Motivated by these\nlimitations, our ambition is to create more efficient models that are less\nreliant on intensive computations. We introduce Echo State Transformers (EST),\na hybrid architecture that elegantly resolves this challenge while\ndemonstrating exceptional performance in classification and detection tasks.\nEST integrates the Transformer attention mechanisms with principles from\nReservoir Computing to create a fixed-size window distributed memory system.\nDrawing inspiration from Echo State Networks, the most prominent instance of\nthe Reservoir Computing paradigm, our approach leverages reservoirs (random\nrecurrent networks) as a lightweight and efficient memory. Our architecture\nintegrates a new module called ''Working Memory'' based on several reservoirs\nworking in parallel. These reservoirs work as independent working memory units\nwith distinct internal dynamics. A novelty here is that the classical reservoir\nhyperparameters, controlling the dynamics, are now trained. Thus, the EST\ndynamically adapts the reservoir memory/non-linearity trade-off. Thanks to\nthese working memory units, EST achieves constant computational complexity at\neach processing step, effectively breaking the quadratic scaling problem of\nstandard Transformers. We evaluate ESTs on a recent challenging timeseries\nbenchmark: the Time Series Library, which comprises 69 tasks across five\ncategories. Results show that ESTs ranks first overall in two of five\ncategories, outperforming strong state-of-the-art baselines on classification\nand anomaly detection tasks, while remaining competitive on short-term\nforecasting. These results position ESTs as a compelling alternative for\ntime-series classification and anomaly detection, and a practical complement to\ntransformer-style models in applications that prioritize robust representations\nand sensitive event detection.\n","authors":["Yannis Bendi-Ouis","Xavier Hinaut"],"pdf_url":"https://arxiv.org/pdf/2507.02917v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02513v2","updated":"2025-10-27T09:31:55Z","published":"2025-02-04T17:32:17Z","title":"Diffusion Generative Modeling on Lie Group Representations","summary":"  We introduce a novel class of score-based diffusion processes that operate\ndirectly in the representation space of Lie groups. Leveraging the framework of\nGeneralized Score Matching, we derive a class of Langevin dynamics that\ndecomposes as a direct sum of Lie algebra representations, enabling the\nmodeling of any target distribution on any (non-Abelian) Lie group. Standard\nscore-matching emerges as a special case of our framework when the Lie group is\nthe translation group. We prove that our generalized generative processes arise\nas solutions to a new class of paired stochastic differential equations (SDEs),\nintroduced here for the first time. We validate our approach through\nexperiments on diverse data types, demonstrating its effectiveness in\nreal-world applications such as SO(3)-guided molecular conformer generation and\nmodeling ligand-specific global SE(3) transformations for molecular docking,\nshowing improvement in comparison to Riemannian diffusion on the group itself.\nWe show that an appropriate choice of Lie group enhances learning efficiency by\nreducing the effective dimensionality of the trajectory space and enables the\nmodeling of transitions between complex data distributions.\n","authors":["Marco Bertolini","Tuan Le","Djork-Arné Clevert"],"pdf_url":"https://arxiv.org/pdf/2502.02513v2.pdf","comment":"29 pages. Accepted as a spotlight paper at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23156v1","updated":"2025-10-27T09:30:36Z","published":"2025-10-27T09:30:36Z","title":"Enabling Vibration-Based Gesture Recognition on Everyday Furniture via\n  Energy-Efficient FPGA Implementation of 1D Convolutional Networks","summary":"  The growing demand for smart home interfaces has increased interest in\nnon-intrusive sensing methods like vibration-based gesture recognition. While\nprior studies demonstrated feasibility, they often rely on complex\npreprocessing and large Neural Networks (NNs) requiring costly high-performance\nhardware, resulting in high energy usage and limited real-world deployability.\nThis study proposes an energy-efficient solution deploying compact NNs on\nlow-power Field-Programmable Gate Arrays (FPGAs) to enable real-time gesture\nrecognition with competitive accuracy. We adopt a series of optimizations: (1)\nWe replace complex spectral preprocessing with raw waveform input, eliminating\ncomplex on-board preprocessing while reducing input size by 21x without\nsacrificing accuracy. (2) We design two lightweight architectures (1D-CNN and\n1D-SepCNN) tailored for embedded FPGAs, reducing parameters from 369 million to\nas few as 216 while maintaining comparable accuracy. (3) With integer-only\nquantization and automated RTL generation, we achieve seamless FPGA deployment.\nA ping-pong buffering mechanism in 1D-SepCNN further improves deployability\nunder tight memory constraints. (4) We extend a hardware-aware search framework\nto support constraint-driven model configuration selection, considering\naccuracy, deployability, latency, and energy consumption. Evaluated on two\nswipe-direction datasets with multiple users and ordinary tables, our approach\nachieves low-latency, energy-efficient inference on the AMD Spartan-7 XC7S25\nFPGA. Under the PS data splitting setting, the selected 6-bit 1D-CNN reaches\n0.970 average accuracy across users with 9.22 ms latency. The chosen 8-bit\n1D-SepCNN further reduces latency to 6.83 ms (over 53x CPU speedup) with\nslightly lower accuracy (0.949). Both consume under 1.2 mJ per inference,\ndemonstrating suitability for long-term edge operation.\n","authors":["Koki Shibata","Tianheng Ling","Chao Qian","Tomokazu Matsui","Hirohiko Suwa","Keiichi Yasumoto","Gregor Schiele"],"pdf_url":"https://arxiv.org/pdf/2510.23156v1.pdf","comment":"9 pages, 5 figures, 5 tables, accepted by 2025 IEEE Annual Congress\n  on Artificial Intelligence of Things (IEEE AIoT)"},{"id":"http://arxiv.org/abs/2510.23151v1","updated":"2025-10-27T09:26:27Z","published":"2025-10-27T09:26:27Z","title":"AG-Fusion: adaptive gated multimodal fusion for 3d object detection in\n  complex scenes","summary":"  Multimodal camera-LiDAR fusion technology has found extensive application in\n3D object detection, demonstrating encouraging performance. However, existing\nmethods exhibit significant performance degradation in challenging scenarios\ncharacterized by sensor degradation or environmental disturbances. We propose a\nnovel Adaptive Gated Fusion (AG-Fusion) approach that selectively integrates\ncross-modal knowledge by identifying reliable patterns for robust detection in\ncomplex scenes. Specifically, we first project features from each modality into\na unified BEV space and enhance them using a window-based attention mechanism.\nSubsequently, an adaptive gated fusion module based on cross-modal attention is\ndesigned to integrate these features into reliable BEV representations robust\nto challenging environments. Furthermore, we construct a new dataset named\nExcavator3D (E3D) focusing on challenging excavator operation scenarios to\nbenchmark performance in complex conditions. Our method not only achieves\ncompetitive performance on the standard KITTI dataset with 93.92% accuracy, but\nalso significantly outperforms the baseline by 24.88% on the challenging E3D\ndataset, demonstrating superior robustness to unreliable modal information in\ncomplex industrial scenes.\n","authors":["Sixian Liu","Chen Xu","Qiang Wang","Donghai Shi","Yiwen Li"],"pdf_url":"https://arxiv.org/pdf/2510.23151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23149v1","updated":"2025-10-27T09:26:07Z","published":"2025-10-27T09:26:07Z","title":"Complexity Dependent Error Rates for Physics-informed Statistical\n  Learning via the Small-ball Method","summary":"  Physics-informed statistical learning (PISL) integrates empirical data with\nphysical knowledge to enhance the statistical performance of estimators. While\nPISL methods are widely used in practice, a comprehensive theoretical\nunderstanding of how informed regularization affects statistical properties is\nstill missing. Specifically, two fundamental questions have yet to be fully\naddressed: (1) what is the trade-off between considering soft penalties versus\nhard constraints, and (2) what is the statistical gain of incorporating\nphysical knowledge compared to purely data-driven empirical error minimisation.\nIn this paper, we address these questions for PISL in convex classes of\nfunctions under physical knowledge expressed as linear equations by developing\nappropriate complexity dependent error rates based on the small-ball method. We\nshow that, under suitable assumptions, (1) the error rates of physics-informed\nestimators are comparable to those of hard constrained empirical error\nminimisers, differing only by constant terms, and that (2) informed\npenalization can effectively reduce model complexity, akin to dimensionality\nreduction, thereby improving learning performance. This work establishes a\ntheoretical framework for evaluating the statistical properties of\nphysics-informed estimators in convex classes of functions, contributing to\nclosing the gap between statistical theory and practical PISL, with potential\napplications to cases not yet explored in the literature.\n","authors":["Diego Marcondes"],"pdf_url":"https://arxiv.org/pdf/2510.23149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23148v1","updated":"2025-10-27T09:24:51Z","published":"2025-10-27T09:24:51Z","title":"Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement\n  Learning in BabyAI","summary":"  Deep reinforcement learning agents often struggle when tasks require\nunderstanding both vision and language. Conventional architectures typically\nisolate perception (for example, CNN-based visual encoders) from\ndecision-making (policy networks). This separation can be inefficient, since\nthe policy's failures do not directly help the perception module learn what is\nimportant. To address this, we implement the Perception-Decision Interleaving\nTransformer (PDiT) architecture introduced by Mao et al. (2023), a model that\nalternates between perception and decision layers within a single transformer.\nThis interleaving allows feedback from decision-making to refine perceptual\nfeatures dynamically. In addition, we integrate a contrastive loss inspired by\nCLIP to align textual mission embeddings with visual scene features. We\nevaluate the PDiT encoders on the BabyAI GoToLocal environment and find that\nthe approach achieves more stable rewards and stronger alignment compared to a\nstandard PPO baseline. The results suggest that interleaved transformer\nencoders are a promising direction for developing more integrated autonomous\nagents.\n","authors":["Aryan Mathur","Asaduddin Ahmed"],"pdf_url":"https://arxiv.org/pdf/2510.23148v1.pdf","comment":"Undergraduate research project, IIT Palakkad, 2025"},{"id":"http://arxiv.org/abs/2510.23142v1","updated":"2025-10-27T09:19:10Z","published":"2025-10-27T09:19:10Z","title":"Rethinking GSPO: The Perplexity-Entropy Equivalence","summary":"  We provide a new perspective on GSPO's length-normalized importance ratios by\nestablishing their connection to information-theoretic quantities. We show that\nGSPO's sequence-level weight $s(\\theta) =\n(\\pi_\\theta/\\pi_{\\theta_{\\text{old}}})^{1/|y|}$ can be equivalently expressed\nas the inverse perplexity ratio\n$\\text{PPL}_{\\theta_{\\text{old}}}/\\text{PPL}_\\theta$ and as the exponential\ncross-entropy change $\\exp(\\Delta H)$. While the perplexity-entropy\nrelationship follows from standard definitions, this observation provides a\nuseful lens for understanding GSPO: the algorithm weights policy gradient\nupdates by perplexity ratios, offering an information-theoretic interpretation\nof the importance weights. This perspective helps explain GSPO's empirical\nproperties, including log-domain variance reduction through geometric averaging\nand stability in training mixture-of-experts models. We validate the\nmathematical equivalences and variance predictions through controlled\nexperiments on mathematical reasoning tasks.\n","authors":["Chi Liu"],"pdf_url":"https://arxiv.org/pdf/2510.23142v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2509.21173v3","updated":"2025-10-27T09:18:44Z","published":"2025-09-25T13:54:34Z","title":"Can Less Precise Be More Reliable? A Systematic Evaluation of\n  Quantization's Impact on CLIP Beyond Accuracy","summary":"  The powerful zero-shot generalization capabilities of vision-language models\n(VLMs) like CLIP have enabled new paradigms for safety-related tasks such as\nout-of-distribution (OOD) detection. However, additional aspects crucial for\nthe computationally efficient and reliable deployment of CLIP are still\noverlooked. In particular, the impact of quantization on CLIP's performance\nbeyond accuracy remains underexplored. This work presents a large-scale\nevaluation of quantization on CLIP models, assessing not only in-distribution\naccuracy but a comprehensive suite of reliability metrics and revealing\ncounterintuitive results driven by pre-training source. We demonstrate that\nquantization consistently improves calibration for typically underconfident\npre-trained models, while often degrading it for overconfident variants.\nIntriguingly, this degradation in calibration does not preclude gains in other\nreliability metrics; we find that OOD detection can still improve for these\nsame poorly calibrated models. Furthermore, we identify specific\nquantization-aware training (QAT) methods that yield simultaneous gains in\nzero-shot accuracy, calibration, and OOD robustness, challenging the view of a\nstrict efficiency-performance trade-off. These findings offer critical insights\nfor navigating the multi-objective problem of deploying efficient, reliable,\nand robust VLMs by utilizing quantization beyond its conventional role.\n","authors":["Aymen Bouguerra","Daniel Montoya","Alexandra Gomez-Villa","Fabio Arnez","Chokri Mraidha"],"pdf_url":"https://arxiv.org/pdf/2509.21173v3.pdf","comment":"Preprint, under peer review"},{"id":"http://arxiv.org/abs/2510.21273v2","updated":"2025-10-27T09:17:45Z","published":"2025-10-24T09:16:12Z","title":"Enforcing Calibration in Multi-Output Probabilistic Regression with\n  Pre-rank Regularization","summary":"  Probabilistic models must be well calibrated to support reliable\ndecision-making. While calibration in single-output regression is well studied,\ndefining and achieving multivariate calibration in multi-output regression\nremains considerably more challenging. The existing literature on multivariate\ncalibration primarily focuses on diagnostic tools based on pre-rank functions,\nwhich are projections that reduce multivariate prediction-observation pairs to\nunivariate summaries to detect specific types of miscalibration. In this work,\nwe go beyond diagnostics and introduce a general regularization framework to\nenforce multivariate calibration during training for arbitrary pre-rank\nfunctions. This framework encompasses existing approaches such as highest\ndensity region calibration and copula calibration. Our method enforces\ncalibration by penalizing deviations of the projected probability integral\ntransforms (PITs) from the uniform distribution, and can be added as a\nregularization term to the loss function of any probabilistic predictor.\nSpecifically, we propose a regularization loss that jointly enforces both\nmarginal and multivariate pre-rank calibration. We also introduce a new\nPCA-based pre-rank that captures calibration along directions of maximal\nvariance in the predictive distribution, while also enabling dimensionality\nreduction. Across 18 real-world multi-output regression datasets, we show that\nunregularized models are consistently miscalibrated, and that our methods\nsignificantly improve calibration across all pre-rank functions without\nsacrificing predictive accuracy.\n","authors":["Naomi Desobry","Elnura Zhalieva","Souhaib Ben Taieb"],"pdf_url":"https://arxiv.org/pdf/2510.21273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23141v1","updated":"2025-10-27T09:17:44Z","published":"2025-10-27T09:17:44Z","title":"Treble10: A high-quality dataset for far-field speech recognition,\n  dereverberation, and enhancement","summary":"  Accurate far-field speech datasets are critical for tasks such as automatic\nspeech recognition (ASR), dereverberation, speech enhancement, and source\nseparation. However, current datasets are limited by the trade-off between\nacoustic realism and scalability. Measured corpora provide faithful physics but\nare expensive, low-coverage, and rarely include paired clean and reverberant\ndata. In contrast, most simulation-based datasets rely on simplified\ngeometrical acoustics, thus failing to reproduce key physical phenomena like\ndiffraction, scattering, and interference that govern sound propagation in\ncomplex environments. We introduce Treble10, a large-scale, physically accurate\nroom-acoustic dataset. Treble10 contains over 3000 broadband room impulse\nresponses (RIRs) simulated in 10 fully furnished real-world rooms, using a\nhybrid simulation paradigm implemented in the Treble SDK that combines a\nwave-based and geometrical acoustics solver. The dataset provides six\ncomplementary subsets, spanning mono, 8th-order Ambisonics, and 6-channel\ndevice RIRs, as well as pre-convolved reverberant speech scenes paired with\nLibriSpeech utterances. All signals are simulated at 32 kHz, accurately\nmodelling low-frequency wave effects and high-frequency reflections. Treble10\nbridges the realism gap between measurement and simulation, enabling\nreproducible, physically grounded evaluation and large-scale data augmentation\nfor far-field speech tasks. The dataset is openly available via the Hugging\nFace Hub, and is intended as both a benchmark and a template for\nnext-generation simulation-driven audio research.\n","authors":["Sarabeth S. Mullins","Georg Götz","Eric Bezzam","Steven Zheng","Daniel Gert Nielsen"],"pdf_url":"https://arxiv.org/pdf/2510.23141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23136v1","updated":"2025-10-27T09:16:16Z","published":"2025-10-27T09:16:16Z","title":"A method for outlier detection based on cluster analysis and visual\n  expert criteria","summary":"  Outlier detection is an important problem occurring in a wide range of areas.\nOutliers are the outcome of fraudulent behaviour, mechanical faults, human\nerror, or simply natural deviations. Many data mining applications perform\noutlier detection, often as a preliminary step in order to filter out outliers\nand build more representative models. In this paper, we propose an outlier\ndetection method based on a clustering process. The aim behind the proposal\noutlined in this paper is to overcome the specificity of many existing outlier\ndetection techniques that fail to take into account the inherent dispersion of\ndomain objects. The outlier detection method is based on four criteria designed\nto represent how human beings (experts in each domain) visually identify\noutliers within a set of objects after analysing the clusters. This has an\nadvantage over other clustering-based outlier detection techniques that are\nfounded on a purely numerical analysis of clusters. Our proposal has been\nevaluated, with satisfactory results, on data (particularly time series) from\ntwo different domains: stabilometry, a branch of medicine studying\nbalance-related functions in human beings and electroencephalography (EEG), a\nneurological exploration used to diagnose nervous system disorders. To validate\nthe proposed method, we studied method outlier detection and efficiency in\nterms of runtime. The results of regression analyses confirm that our proposal\nis useful for detecting outlier data in different domains, with a false\npositive rate of less than 2% and a reliability greater than 99%.\n","authors":["Juan A. Lara","David Lizcano","Víctor Rampérez","Javier Soriano"],"pdf_url":"https://arxiv.org/pdf/2510.23136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11245v2","updated":"2025-10-27T09:13:03Z","published":"2025-02-16T19:45:09Z","title":"Shortcuts and Identifiability in Concept-based Models from a\n  Neuro-Symbolic Lens","summary":"  Concept-based Models are neural networks that learn a concept extractor to\nmap inputs to high-level concepts and an inference layer to translate these\ninto predictions. Ensuring these modules produce interpretable concepts and\nbehave reliably in out-of-distribution is crucial, yet the conditions for\nachieving this remain unclear. We study this problem by establishing a novel\nconnection between Concept-based Models and reasoning shortcuts (RSs), a common\nissue where models achieve high accuracy by learning low-quality concepts, even\nwhen the inference layer is fixed and provided upfront. Specifically, we extend\nRSs to the more complex setting of Concept-based Models and derive theoretical\nconditions for identifying both the concepts and the inference layer. Our\nempirical results highlight the impact of RSs and show that existing methods,\neven combined with multiple natural mitigation strategies, often fail to meet\nthese conditions in practice.\n","authors":["Samuele Bortolotti","Emanuele Marconato","Paolo Morettin","Andrea Passerini","Stefano Teso"],"pdf_url":"https://arxiv.org/pdf/2502.11245v2.pdf","comment":"Accepted at NeurIPS25"},{"id":"http://arxiv.org/abs/2507.09732v2","updated":"2025-10-27T09:12:08Z","published":"2025-07-13T18:11:26Z","title":"Continental-scale habitat distribution modelling with multimodal earth\n  observation foundation models","summary":"  Habitats integrate the abiotic conditions, vegetation composition and\nstructure that support biodiversity and sustain nature's contributions to\npeople. Most habitats face mounting pressures from human activities, which\nrequires accurate, high-resolution habitat mapping for effective conservation\nand restoration. Yet, current habitat maps often fall short in thematic or\nspatial resolution because they must (1) model several mutually exclusive\nhabitat types that co-occur across landscapes and (2) cope with severe class\nimbalance that complicates exhaustive multi-class training. Here, we evaluated\nhow high-resolution remote sensing (RS) data and Artificial Intelligence (AI)\ntools can improve habitat mapping across large geographical extents at fine\nspatial and thematic resolution. Using vegetation plots from the European\nVegetation Archive, we modelled the distribution of Level 3 EUNIS habitat types\nacross Europe and assessed multiple modelling strategies against independent\nvalidation datasets. Strategies that exploited the hierarchical nature of\nhabitat classifications resolved classification ambiguities, especially in\nfragmented habitats. Integrating satellite-borne multispectral and radar\nimagery, particularly through Earth Observation (EO) Foundation models\n(EO-FMs), enhanced within-formation discrimination and overall performance.\nFinally, ensemble machine learning that corrects class imbalance boosted\npredictive accuracy even further. Our methodological framework is transferable\nbeyond Europe and adaptable to other classification systems. Future research\nshould advance temporal modelling of habitat dynamics, extend to habitat\nsegmentation and quality assessment, and exploit next-generation EO data paired\nwith higher-quality in situ observations.\n","authors":["Sara Si-Moussi","Stephan Hennekens","Sander Mucher","Stan Los","Yoann Cartier","Borja Jiménez-Alfaro","Fabio Attorre","Jens-Christian Svenning","Wilfried Thuiller"],"pdf_url":"https://arxiv.org/pdf/2507.09732v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22453v2","updated":"2025-10-27T09:06:32Z","published":"2025-05-28T15:11:16Z","title":"First SFT, Second RL, Third UPT: Continual Improving Multi-Modal LLM\n  Reasoning via Unsupervised Post-Training","summary":"  Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL), which require expensive and manually annotated multi-modal\ndata--an ultimately unsustainable resource. This limitation has motivated a\ngrowing interest in unsupervised paradigms as a third stage of post-training\nafter SFT and RL. While recent efforts have explored this direction, their\nmethods are complex and difficult to iterate. To address this, we propose\nMM-UPT, a simple yet effective framework for unsupervised post-training of\nMLLMs, enabling continual self-improvement without any external supervision.\nThe training method of MM-UPT builds upon GRPO, replacing traditional reward\nsignals with a self-rewarding mechanism based on majority voting over multiple\nsampled responses. Our experiments demonstrate that such training method\neffectively improves the reasoning ability of Qwen2.5-VL-7B (e.g.,\n66.3\\%$\\rightarrow$72.9\\% on MathVista, 62.9\\%$\\rightarrow$68.7\\% on We-Math),\nusing standard dataset without ground truth labels. To further explore\nscalability, we extend our framework to a data self-generation setting,\ndesigning two strategies that prompt the MLLM to synthesize new training\nsamples on its own. Additional experiments show that combining these synthetic\ndata with the unsupervised training method can also boost performance,\nhighlighting a promising approach for scalable self-improvement. Overall,\nMM-UPT offers a new paradigm for autonomous enhancement of MLLMs, serving as a\ncritical third step after initial SFT and RL in the absence of external\nsupervision. Our code is available at https://github.com/waltonfuture/MM-UPT.\n","authors":["Lai Wei","Yuting Li","Chen Wang","Yue Wang","Linghe Kong","Weiran Huang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2505.22453v2.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.10099v2","updated":"2025-10-27T09:00:42Z","published":"2025-10-11T08:16:33Z","title":"Uncovering Singularities in Feynman Integrals via Machine Learning","summary":"  We introduce a machine-learning framework based on symbolic regression to\nextract the full symbol alphabet of multi-loop Feynman integrals. By targeting\nthe analytic structure rather than reduction, the method is broadly applicable\nand interpretable across different families of integrals. It successfully\nreconstructs complete symbol alphabets in nontrivial examples, demonstrating\nboth robustness and generality. Beyond accelerating computations case by case,\nit uncovers the analytic structure universally. This framework opens new\navenues for multi-loop amplitude analysis and provides a versatile tool for\nexploring scattering amplitudes.\n","authors":["Yuanche Liu","Yingxuan Xu","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.10099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23124v1","updated":"2025-10-27T08:57:59Z","published":"2025-10-27T08:57:59Z","title":"DeepSalt: Bridging Laboratory and Satellite Spectra through Domain\n  Adaptation and Knowledge Distillation for Large-Scale Soil Salinity\n  Estimation","summary":"  Soil salinization poses a significant threat to both ecosystems and\nagriculture because it limits plants' ability to absorb water and, in doing so,\nreduces crop productivity. This phenomenon alters the soil's spectral\nproperties, creating a measurable relationship between salinity and light\nreflectance that enables remote monitoring. While laboratory spectroscopy\nprovides precise measurements, its reliance on in-situ sampling limits\nscalability to regional or global levels. Conversely, hyperspectral satellite\nimagery enables wide-area observation but lacks the fine-grained\ninterpretability of laboratory instruments. To bridge this gap, we introduce\nDeepSalt, a deep-learning-based spectral transfer framework that leverages\nknowledge distillation and a novel Spectral Adaptation Unit to transfer\nhigh-resolution spectral insights from laboratory-based spectroscopy to\nsatellite-based hyperspectral sensing. Our approach eliminates the need for\nextensive ground sampling while enabling accurate, large-scale salinity\nestimation, as demonstrated through comprehensive empirical benchmarks.\nDeepSalt achieves significant performance gains over methods without explicit\ndomain adaptation, underscoring the impact of the proposed Spectral Adaptation\nUnit and the knowledge distillation strategy. The model also effectively\ngeneralized to unseen geographic regions, explaining a substantial portion of\nthe salinity variance.\n","authors":["Rupasree Dey","Abdul Matin","Everett Lewark","Tanjim Bin Faruk","Andrei Bachinin","Sam Leuthold","M. Francesca Cotrufo","Shrideep Pallickara","Sangmi Lee Pallickara"],"pdf_url":"https://arxiv.org/pdf/2510.23124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23123v1","updated":"2025-10-27T08:57:24Z","published":"2025-10-27T08:57:24Z","title":"Beyond Higher Rank: Token-wise Input-Output Projections for Efficient\n  Low-Rank Adaptation","summary":"  Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method\nwidely used in large language models (LLMs). LoRA essentially describes the\nprojection of an input space into a low-dimensional output space, with the\ndimensionality determined by the LoRA rank. In standard LoRA, all input tokens\nshare the same weights and undergo an identical input-output projection. This\nlimits LoRA's ability to capture token-specific information due to the inherent\nsemantic differences among tokens. To address this limitation, we propose\nToken-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts\nLoRA weights according to the input token, thereby learning token-wise\ninput-output projections in an end-to-end manner. Formally, the weights of\nTopLoRA can be expressed as $B\\Sigma_X A$, where $A$ and $B$ are low-rank\nmatrices (as in standard LoRA), and $\\Sigma_X$ is a diagonal matrix generated\nfrom each input token $X$. Notably, TopLoRA does not increase the rank of LoRA\nweights but achieves more granular adaptation by learning token-wise LoRA\nweights (i.e., token-wise input-output projections). Extensive experiments\nacross multiple models and datasets demonstrate that TopLoRA consistently\noutperforms LoRA and its variants. The code is available at\nhttps://github.com/Leopold1423/toplora-neurips25.\n","authors":["Shiwei Li","Xiandi Luo","Haozhao Wang","Xing Tang","Ziqiang Cui","Dugang Liu","Yuhua Li","Xiuqiang He","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2510.23123v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.09663v3","updated":"2025-10-27T08:55:30Z","published":"2025-05-14T14:52:22Z","title":"Analog Foundation Models","summary":"  Analog in-memory computing (AIMC) is a promising compute paradigm to improve\nspeed and power efficiency of neural network inference beyond the limits of\nconventional von Neumann-based architectures. However, AIMC introduces\nfundamental challenges such as noisy computations and strict constraints on\ninput and output quantization. Because of these constraints and imprecisions,\noff-the-shelf LLMs are not able to achieve 4-bit-level performance when\ndeployed on AIMC-based hardware. While researchers previously investigated\nrecovering this accuracy gap on small, mostly vision-based models, a generic\nmethod applicable to LLMs pre-trained on trillions of tokens does not yet\nexist. In this work, we introduce a general and scalable method to robustly\nadapt LLMs for execution on noisy, low-precision analog hardware. Our approach\nenables state-of-the-art models $\\unicode{x2013}$ including\nPhi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\\unicode{x2013}$ to retain\nperformance comparable to 4-bit weight, 8-bit activation baselines, despite the\npresence of analog noise and quantization constraints. Additionally, we show\nthat as a byproduct of our training methodology, analog foundation models can\nbe quantized for inference on low-precision digital hardware. Finally, we show\nthat our models also benefit from test-time compute scaling, showing better\nscaling behavior than models trained with 4-bit weight and 8-bit static input\nquantization. Our work bridges the gap between high-capacity LLMs and efficient\nanalog hardware, offering a path toward energy-efficient foundation models.\nCode is available at https://github.com/IBM/analog-foundation-models.\n","authors":["Julian Büchel","Iason Chalas","Giovanni Acampa","An Chen","Omobayode Fagbohungbe","Sidney Tsai","Kaoutar El Maghraoui","Manuel Le Gallo","Abbas Rahimi","Abu Sebastian"],"pdf_url":"https://arxiv.org/pdf/2505.09663v3.pdf","comment":"Neural Information Processing Systems (NeurIPS) 2025"},{"id":"http://arxiv.org/abs/2505.16291v2","updated":"2025-10-27T08:40:39Z","published":"2025-05-22T06:43:15Z","title":"Fairness under Competition","summary":"  Algorithmic fairness has emerged as a central issue in ML, and it has become\nstandard practice to adjust ML algorithms so that they will satisfy fairness\nrequirements such as Equal Opportunity. In this paper we consider the effects\nof adopting such fair classifiers on the overall level of ecosystem fairness.\nSpecifically, we introduce the study of fairness with competing firms, and\ndemonstrate the failure of fair classifiers in yielding fair ecosystems. Our\nresults quantify the loss of fairness in systems, under a variety of\nconditions, based on classifiers' correlation and the level of their data\noverlap. We show that even if competing classifiers are individually fair, the\necosystem's outcome may be unfair; and that adjusting biased algorithms to\nimprove their individual fairness may lead to an overall decline in ecosystem\nfairness. In addition to these theoretical results, we also provide supporting\nexperimental evidence. Together, our model and results provide a novel and\nessential call for action.\n","authors":["Ronen Gradwohl","Eilam Shapira","Moshe Tennenholtz"],"pdf_url":"https://arxiv.org/pdf/2505.16291v2.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23117v1","updated":"2025-10-27T08:38:17Z","published":"2025-10-27T08:38:17Z","title":"Seeing Structural Failure Before it Happens: An Image-Based\n  Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction","summary":"  Physics Informed Neural Networks (PINNs) are gaining attention for their\nability to embed physical laws into deep learning models, which is particularly\nuseful in structural engineering tasks with limited data. This paper aims to\nexplore the use of PINNs to predict the weight of small scale spaghetti\nbridges, a task relevant to understanding load limits and potential failure\nmodes in simplified structural models. Our proposed framework incorporates\nphysics-based constraints to the prediction model for improved performance. In\naddition to standard PINNs, we introduce a novel architecture named Physics\nInformed Kolmogorov Arnold Network (PIKAN), which blends universal function\napproximation theory with physical insights. The structural parameters provided\nas input to the model are collected either manually or through computer vision\nmethods. Our dataset includes 15 real bridges, augmented to 100 samples, and\nour best model achieves an $R^2$ score of 0.9603 and a mean absolute error\n(MAE) of 10.50 units. From applied perspective, we also provide a web based\ninterface for parameter entry and prediction. These results show that PINNs can\noffer reliable estimates of structural weight, even with limited data, and may\nhelp inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at\nhttps://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.\n","authors":["Omer Jauhar Khan","Sudais Khan","Hafeez Anwar"],"pdf_url":"https://arxiv.org/pdf/2510.23117v1.pdf","comment":"12 pages, 17 figures. Preprint"},{"id":"http://arxiv.org/abs/2510.23111v1","updated":"2025-10-27T08:31:55Z","published":"2025-10-27T08:31:55Z","title":"Neural Emulator Superiority: When Machine Learning for PDEs Surpasses\n  its Training Data","summary":"  Neural operators or emulators for PDEs trained on data from numerical solvers\nare conventionally assumed to be limited by their training data's fidelity. We\nchallenge this assumption by identifying \"emulator superiority,\" where neural\nnetworks trained purely on low-fidelity solver data can achieve higher accuracy\nthan those solvers when evaluated against a higher-fidelity reference. Our\ntheoretical analysis reveals how the interplay between emulator inductive\nbiases, training objectives, and numerical error characteristics enables\nsuperior performance during multi-step rollouts. We empirically validate this\nfinding across different PDEs using standard neural architectures,\ndemonstrating that emulators can implicitly learn dynamics that are more\nregularized or exhibit more favorable error accumulation properties than their\ntraining data, potentially surpassing training data limitations and mitigating\nnumerical artifacts. This work prompts a re-evaluation of emulator\nbenchmarking, suggesting neural emulators might achieve greater physical\nfidelity than their training source within specific operational regimes.\nProject Page: https://tum-pbs.github.io/emulator-superiority\n","authors":["Felix Koehler","Nils Thuerey"],"pdf_url":"https://arxiv.org/pdf/2510.23111v1.pdf","comment":"Accepted at NeurIPS 2025:\n  https://neurips.cc/virtual/2025/poster/116770"},{"id":"http://arxiv.org/abs/2510.09160v2","updated":"2025-10-27T08:24:49Z","published":"2025-10-10T09:03:49Z","title":"Efficient Resource-Constrained Training of Vision Transformers via\n  Subspace Optimization","summary":"  As AI increasingly shapes daily life, energy consumption and data privacy\nhave become pressing concerns. On-device learning trains models directly on\nedge devices, cutting energy consumption and safeguarding data privacy.\nHowever, the expanding scale of modern neural networks creates a major obstacle\nfor on-device training. Although prior work has concentrated on compact\nconvolutional architectures, we instead apply subspace-based training to\ntransformer models. Motivated by the idea that a model's essential information\nlies in a fixed subspace, we introduce Weight-Activation Subspace Iteration\n(WASI), a method that mitigates the memory bottleneck of backpropagation and\nboosts inference efficiency in transformer models by restricting training to\nthis subspace. Our results demonstrate that WASI maintains accuracy comparable\nto vanilla training while reducing memory usage by up to $62\\times$ and\ncomputational cost (FLOPs) by up to $2\\times$. On a Raspberry Pi 5, WASI\nachieves roughly $1.5\\times$ faster training and inference than vanilla\ntraining.\n","authors":["Le-Trung Nguyen","Enzo Tartaglione","Van-Tam Nguyen"],"pdf_url":"https://arxiv.org/pdf/2510.09160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13918v2","updated":"2025-10-27T08:22:57Z","published":"2025-01-23T18:55:41Z","title":"Improving Video Generation with Human Feedback","summary":"  Video generation has achieved significant advances through rectified flow\ntechniques, but issues like unsmooth motion and misalignment between videos and\nprompts persist. In this work, we develop a systematic pipeline that harnesses\nhuman feedback to mitigate these problems and refine the video generation\nmodel. Specifically, we begin by constructing a large-scale human preference\ndataset focused on modern video generation models, incorporating pairwise\nannotations across multi-dimensions. We then introduce VideoReward, a\nmulti-dimensional video reward model, and examine how annotations and various\ndesign choices impact its rewarding efficacy. From a unified reinforcement\nlearning perspective aimed at maximizing reward with KL regularization, we\nintroduce three alignment algorithms for flow-based models. These include two\ntraining-time strategies: direct preference optimization for flow (Flow-DPO)\nand reward weighted regression for flow (Flow-RWR), and an inference-time\ntechnique, Flow-NRG, which applies reward guidance directly to noisy videos.\nExperimental results indicate that VideoReward significantly outperforms\nexisting reward models, and Flow-DPO demonstrates superior performance compared\nto both Flow-RWR and supervised fine-tuning methods. Additionally, Flow-NRG\nlets users assign custom weights to multiple objectives during inference,\nmeeting personalized video quality needs.\n","authors":["Jie Liu","Gongye Liu","Jiajun Liang","Ziyang Yuan","Xiaokun Liu","Mingwu Zheng","Xiele Wu","Qiulin Wang","Menghan Xia","Xintao Wang","Xiaohong Liu","Fei Yang","Pengfei Wan","Di Zhang","Kun Gai","Yujiu Yang","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2501.13918v2.pdf","comment":"https://github.com/KwaiVGI/VideoAlign"},{"id":"http://arxiv.org/abs/2507.03312v3","updated":"2025-10-27T08:21:13Z","published":"2025-07-04T05:47:04Z","title":"MPX: Mixed Precision Training for JAX","summary":"  Mixed-precision training has emerged as an indispensable tool for enhancing\nthe efficiency of neural network training in recent years. Concurrently, JAX\nhas grown in popularity as a versatile machine learning toolbox. However, it\ncurrently lacks robust support for mixed-precision training. We propose MPX, a\nmixed-precision training toolbox for JAX that simplifies and accelerates the\ntraining of large-scale neural networks while preserving model accuracy. MPX\nseamlessly integrates with popular toolboxes such as Equinox and Flax, allowing\nusers to convert full-precision pipelines to mixed-precision versions with\nminimal modifications. By casting both inputs and outputs to half precision,\nand introducing a dynamic loss-scaling mechanism, MPX alleviates issues like\ngradient underflow and overflow that commonly arise in half precision\ncomputations. Its design inherits critical features from JAX's type-promotion\nbehavior, ensuring that operations take place in the correct precision and\nallowing for selective enforcement of full precision where needed (e.g., sums,\nmeans, or softmax). MPX further provides wrappers for automatic creation and\nmanagement of mixed-precision gradients and optimizers, enabling\nstraightforward integration into existing JAX training pipelines. MPX's source\ncode, documentation, and usage examples are available at\ngithub.com/Data-Science-in-Mechanical-Engineering/mixed_precision_for_JAX .\n","authors":["Alexander Gräfe","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2507.03312v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23106v1","updated":"2025-10-27T08:21:09Z","published":"2025-10-27T08:21:09Z","title":"Sampling from Energy distributions with Target Concrete Score Identity","summary":"  We introduce the Target Concrete Score Identity Sampler (TCSIS), a method for\nsampling from unnormalized densities on discrete state spaces by learning the\nreverse dynamics of a Continuous-Time Markov Chain (CTMC). Our approach builds\non a forward in time CTMC with a uniform noising kernel and relies on the\nproposed Target Concrete Score Identity, which relates the concrete score, the\nratio of marginal probabilities of two states, to a ratio of expectations of\nBoltzmann factors under the forward uniform diffusion kernel. This formulation\nenables Monte Carlo estimation of the concrete score without requiring samples\nfrom the target distribution or computation of the partition function. We\napproximate the concrete score with a neural network and propose two\nalgorithms: Self-Normalized TCSIS and Unbiased TCSIS. Finally, we demonstrate\nthe effectiveness of TCSIS on problems from statistical physics.\n","authors":["Sergei Kholkin","Francisco Vargas","Alexander Korotin"],"pdf_url":"https://arxiv.org/pdf/2510.23106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17701v3","updated":"2025-10-27T08:19:35Z","published":"2025-05-23T10:10:22Z","title":"COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary\n  Weights in Down Projection","summary":"  The growing size of large language models has created significant\ncomputational inefficiencies. To address this challenge, sparse activation\nmethods selectively deactivates non-essential parameters during inference,\nreducing computational costs in FFNN layers. While existing methods focus on\nnon-linear gating mechanisms, we hypothesize that the sparsity of the FFNN\nlayer lies globally in the form of a linear combination over its internal down\nprojection matrix. Based on this insight, we propose two methods: M-COUNTDOWN,\nleveraging indirect coefficients, and D-COUNTDOWN, utilizing direct\ncoefficients of the linear combination. Experimental results demonstrate that\nD-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5%\nideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4%\nbetter performance preservation compared to existing methods. Our specialized\nkernel implementations effectively realize these theoretical gains into\nsubstantial real-world acceleration.\n","authors":["Jaewon Cheon","Pilsung Kang"],"pdf_url":"https://arxiv.org/pdf/2505.17701v3.pdf","comment":"EMNLP 2025 (Main Track)"},{"id":"http://arxiv.org/abs/2509.15857v2","updated":"2025-10-27T08:14:06Z","published":"2025-09-19T10:47:34Z","title":"EvoBrain: Dynamic Multi-Channel EEG Graph Modeling for Time-Evolving\n  Brain Networks","summary":"  Dynamic GNNs, which integrate temporal and spatial features in\nElectroencephalography (EEG) data, have shown great potential in automating\nseizure detection. However, fully capturing the underlying dynamics necessary\nto represent brain states, such as seizure and non-seizure, remains a\nnon-trivial task and presents two fundamental challenges. First, most existing\ndynamic GNN methods are built on temporally fixed static graphs, which fail to\nreflect the evolving nature of brain connectivity during seizure progression.\nSecond, current efforts to jointly model temporal signals and graph structures\nand, more importantly, their interactions remain nascent, often resulting in\ninconsistent performance. To address these challenges, we present the first\ntheoretical analysis of these two problems, demonstrating the effectiveness and\nnecessity of explicit dynamic modeling and time-then-graph dynamic GNN method.\nBuilding on these insights, we propose EvoBrain, a novel seizure detection\nmodel that integrates a two-stream Mamba architecture with a GCN enhanced by\nLaplacian Positional Encoding, following neurological insights. Moreover,\nEvoBrain incorporates explicitly dynamic graph structures, allowing both nodes\nand edges to evolve over time. Our contributions include (a) a theoretical\nanalysis proving the expressivity advantage of explicit dynamic modeling and\ntime-then-graph over other approaches, (b) a novel and efficient model that\nsignificantly improves AUROC by 23% and F1 score by 30%, compared with the\ndynamic GNN baseline, and (c) broad evaluations of our method on the\nchallenging early seizure prediction tasks.\n","authors":["Rikuto Kotoge","Zheng Chen","Tasuku Kimura","Yasuko Matsubara","Takufumi Yanagisawa","Haruhiko Kishima","Yasushi Sakurai"],"pdf_url":"https://arxiv.org/pdf/2509.15857v2.pdf","comment":"Accepted by NeurIPS 2025 (spotlight)"},{"id":"http://arxiv.org/abs/2406.05714v8","updated":"2025-10-27T07:47:53Z","published":"2024-06-09T10:12:08Z","title":"A conversion theorem and minimax optimality for continuum contextual\n  bandits","summary":"  We study the contextual continuum bandits problem, where the learner\nsequentially receives a side information vector and has to choose an action in\na convex set, minimizing a function associated with the context. The goal is to\nminimize all the underlying functions for the received contexts, leading to the\ncontextual notion of regret, which is stronger than the standard static regret.\nAssuming that the objective functions are $\\gamma$-H\\\"older with respect to the\ncontexts, $0<\\gamma\\le 1,$ we demonstrate that any algorithm achieving a\nsub-linear static regret can be extended to achieve a sub-linear contextual\nregret. We prove a static-to-contextual regret conversion theorem that provides\nan upper bound for the contextual regret of the output algorithm as a function\nof the static regret of the input algorithm. We further study the implications\nof this general result for three fundamental cases of dependency of the\nobjective function on the action variable: (a) Lipschitz bandits, (b) convex\nbandits, (c) strongly convex and smooth bandits. For Lipschitz bandits and\n$\\gamma=1,$ combining our results with the lower bound of Slivkins (2014), we\nprove that the minimax optimal contextual regret for the noise-free adversarial\nsetting is achieved. Then, we prove that in the presence of noise, the\ncontextual regret rate as a function of the number of queries is the same for\nconvex bandits as it is for strongly convex and smooth bandits. Lastly, we\npresent a minimax lower bound, implying two key facts. First, obtaining a\nsub-linear contextual regret may be impossible over functions that are not\ncontinuous with respect to the context. Second, for convex bandits and strongly\nconvex and smooth bandits, the algorithms that we propose achieve, up to a\nlogarithmic factor, the minimax optimal rate of contextual regret as a function\nof the number of queries.\n","authors":["Arya Akhavan","Karim Lounici","Massimiliano Pontil","Alexandre B. Tsybakov"],"pdf_url":"https://arxiv.org/pdf/2406.05714v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07426v2","updated":"2025-10-27T07:43:15Z","published":"2025-03-10T15:11:07Z","title":"RePO: Understanding Preference Learning Through ReLU-Based Optimization","summary":"  Aligning large language models (LLMs) with human preferences is critical for\nreal-world deployment, yet existing methods like RLHF face computational and\nstability challenges. While DPO establishes an offline paradigm with single\nhyperparameter $\\beta$, subsequent methods like SimPO reintroduce complexity\nthrough dual parameters ($\\beta$, $\\gamma$). We propose {ReLU-based Preference\nOptimization (RePO)}, a streamlined algorithm that eliminates $\\beta$ via two\nadvances: (1) retaining SimPO's reference-free margins but removing $\\beta$\nthrough gradient analysis, and (2) adopting a ReLU-based max-margin loss that\nnaturally filters trivial pairs. Theoretically, RePO is characterized as\nSimPO's limiting case ($\\beta \\to \\infty$), where the logistic weighting\ncollapses to binary thresholding, forming a convex envelope of the 0-1 loss.\nEmpirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO\nand SimPO across multiple base models, requiring only one hyperparameter to\ntune.\n","authors":["Junkang Wu","Kexin Huang","Xue Wang","Jinyang Gao","Bolin Ding","Jiancan Wu","Xiangnan He","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2503.07426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23083v1","updated":"2025-10-27T07:36:41Z","published":"2025-10-27T07:36:41Z","title":"Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and\n  Outcome Rewards","summary":"  Generating high-quality code remains a challenge for Large Language Models\n(LLMs). For the evolution of reasoning models on this task, reward models are a\nnecessary intermediate step. These models judge outcomes or intermediate steps.\nDecoder-only transformer models can be turned into reward models by introducing\na regression layer and supervised fine-tuning. While it is known that\nreflection capabilities generally increase with the size of a model, we want to\ninvestigate whether state-of-the-art small language models like the Phi-4\nfamily can be turned into usable reward models blending the consideration of\nprocess rewards and outcome rewards.\n  Targeting this goal, we construct a dataset of code samples with correctness\nlabels derived from the APPS coding challenge benchmark. We then train a\nvalue-head model to estimate the success probability of intermediate outputs.\nOur evaluation shows that small LLMs are capable of serving as effective reward\nmodels or code evaluation critics, successfully identifying correct solutions\namong multiple candidates. Using this critic, we achieve over a 20% improvement\nin the search capability of the most accurate code out of multiple generations.\n","authors":["Jan Niklas Groeneveld","Xi Qin","Alexander Schaefer","Yaad Oren"],"pdf_url":"https://arxiv.org/pdf/2510.23083v1.pdf","comment":"Accepted and to be presented at NeurIPS 2025 Workshop: Foundations of\n  Reasoning in Language Models"},{"id":"http://arxiv.org/abs/2509.07295v3","updated":"2025-10-27T07:29:43Z","published":"2025-09-08T23:59:32Z","title":"Reconstruction Alignment Improves Unified Multimodal Models","summary":"  Unified multimodal models (UMMs) unify visual understanding and generation\nwithin a single architecture. However, conventional training relies on\nimage-text pairs (or sequences) whose captions are typically sparse and miss\nfine-grained visual details--even when they use hundreds of words to describe a\nsimple image. We introduce Reconstruction Alignment (RecA), a\nresource-efficient post-training method that leverages visual understanding\nencoder embeddings as dense \"text prompts,\" providing rich supervision without\ncaptions. Concretely, RecA conditions a UMM on its own visual understanding\nembeddings and optimizes it to reconstruct the input image with a\nself-supervised reconstruction loss, thereby realigning understanding and\ngeneration. Despite its simplicity, RecA is broadly applicable: across\nautoregressive, masked-autoregressive, and diffusion-based UMMs, it\nconsistently improves generation and editing fidelity. With only 27 GPU-hours,\npost-training with RecA substantially improves image generation performance on\nGenEval (0.73$\\rightarrow$0.90) and DPGBench (80.93$\\rightarrow$88.15), while\nalso boosting editing benchmarks (ImgEdit 3.38$\\rightarrow$3.75, GEdit\n6.94$\\rightarrow$7.25). Notably, RecA surpasses much larger open-source models\nand applies broadly across diverse UMM architectures, establishing it as an\nefficient and general post-training alignment strategy for UMMs\n","authors":["Ji Xie","Trevor Darrell","Luke Zettlemoyer","XuDong Wang"],"pdf_url":"https://arxiv.org/pdf/2509.07295v3.pdf","comment":"34 pages, 28 figures and 11 tables; Update ablation study"},{"id":"http://arxiv.org/abs/2510.09107v2","updated":"2025-10-27T07:25:00Z","published":"2025-10-10T08:00:46Z","title":"A Novel Multi-branch ConvNeXt Architecture for Identifying Subtle\n  Pathological Features in CT Scans","summary":"  Intelligent analysis of medical imaging plays a crucial role in assisting\nclinical diagnosis, especially for identifying subtle pathological features.\nThis paper introduces a novel multi-branch ConvNeXt architecture designed\nspecifically for the nuanced challenges of medical image analysis. While\napplied here to the specific problem of COVID-19 diagnosis, the methodology\noffers a generalizable framework for classifying a wide range of pathologies\nfrom CT scans. The proposed model incorporates a rigorous end-to-end pipeline,\nfrom meticulous data preprocessing and augmentation to a disciplined two-phase\ntraining strategy that leverages transfer learning effectively. The\narchitecture uniquely integrates features extracted from three parallel\nbranches: Global Average Pooling, Global Max Pooling, and a new\nAttention-weighted Pooling mechanism. The model was trained and validated on a\ncombined dataset of 2,609 CT slices derived from two distinct datasets.\nExperimental results demonstrate a superior performance on the validation set,\nachieving a final ROC-AUC of 0.9937, a validation accuracy of 0.9757, and an\nF1-score of 0.9825 for COVID-19 cases, outperforming all previously reported\nmodels on this dataset. These findings indicate that a modern, multi-branch\narchitecture, coupled with careful data handling, can achieve performance\ncomparable to or exceeding contemporary state-of-the-art models, thereby\nproving the efficacy of advanced deep learning techniques for robust medical\ndiagnostics.\n","authors":["Irash Perera","Uthayasanker Thayasivam"],"pdf_url":"https://arxiv.org/pdf/2510.09107v2.pdf","comment":"Source Code : https://github.com/Irash-Perera/MedNeXt-Branch"},{"id":"http://arxiv.org/abs/2506.14436v5","updated":"2025-10-27T07:13:12Z","published":"2025-06-17T11:55:08Z","title":"MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant\n  Multi-Task Adaptation","summary":"  Adapting large-scale foundation models in multi-task scenarios often suffers\nfrom task conflict and oblivion. To mitigate such issues, we propose a novel\n''model MoE-ization'' strategy that leads to a conflict- and oblivion-resistant\nmulti-task adaptation method. Given a weight matrix of a pre-trained model, our\nmethod applies SVD to it and introduces a learnable router to adjust its\nsingular values based on tasks and samples. Accordingly, the weight matrix\nbecomes a Mixture of Orthogonal Rank-one Experts (MoORE), in which each expert\ncorresponds to the outer product of a left singular vector and the\ncorresponding right one. We can improve the model capacity by imposing a\nlearnable orthogonal transform on the right singular vectors. Unlike low-rank\nadaptation (LoRA) and its MoE-driven variants, MoORE guarantees the experts'\northogonality and maintains the column space of the original weight matrix.\nThese two properties make the adapted model resistant to the conflicts among\nthe new tasks and the oblivion of its original tasks, respectively. Experiments\non various datasets demonstrate that MoORE outperforms existing multi-task\nadaptation methods consistently, showing its superiority in terms of conflict-\nand oblivion-resistance. The code of the experiments is available at\nhttps://github.com/DaShenZi721/MoORE.\n","authors":["Shen Yuan","Yin Zheng","Taifeng Wang","Binbin Liu","Hongteng Xu"],"pdf_url":"https://arxiv.org/pdf/2506.14436v5.pdf","comment":"26 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.22194v3","updated":"2025-10-27T07:06:50Z","published":"2025-03-28T07:23:12Z","title":"ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation","summary":"  We introduce ORIGEN, the first zero-shot method for 3D orientation grounding\nin text-to-image generation across multiple objects and diverse categories.\nWhile previous work on spatial grounding in image generation has mainly focused\non 2D positioning, it lacks control over 3D orientation. To address this, we\npropose a reward-guided sampling approach using a pretrained discriminative\nmodel for 3D orientation estimation and a one-step text-to-image generative\nflow model. While gradient-ascent-based optimization is a natural choice for\nreward-based guidance, it struggles to maintain image realism. Instead, we\nadopt a sampling-based approach using Langevin dynamics, which extends gradient\nascent by simply injecting random noise--requiring just a single additional\nline of code. Additionally, we introduce adaptive time rescaling based on the\nreward function to accelerate convergence. Our experiments show that ORIGEN\noutperforms both training-based and test-time guidance methods across\nquantitative metrics and user studies.\n","authors":["Yunhong Min","Daehyeon Choi","Kyeongmin Yeo","Jihyun Lee","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2503.22194v3.pdf","comment":"Project Page: https://origen2025.github.io"},{"id":"http://arxiv.org/abs/2510.17103v2","updated":"2025-10-27T06:46:29Z","published":"2025-10-20T02:28:08Z","title":"Adapting to Stochastic and Adversarial Losses in Episodic MDPs with\n  Aggregate Bandit Feedback","summary":"  We study online learning in finite-horizon episodic Markov decision processes\n(MDPs) under the challenging aggregate bandit feedback model, where the learner\nobserves only the cumulative loss incurred in each episode, rather than\nindividual losses at each state-action pair. While prior work in this setting\nhas focused exclusively on worst-case analysis, we initiate the study of\nbest-of-both-worlds (BOBW) algorithms that achieve low regret in both\nstochastic and adversarial environments. We propose the first BOBW algorithms\nfor episodic tabular MDPs with aggregate bandit feedback. In the case of known\ntransitions, our algorithms achieve $O(\\log T)$ regret in stochastic settings\nand ${O}(\\sqrt{T})$ regret in adversarial ones. Importantly, we also establish\nmatching lower bounds, showing the optimality of our algorithms in this\nsetting. We further extend our approach to unknown-transition settings by\nincorporating confidence-based techniques. Our results rely on a combination of\nFTRL over occupancy measures, self-bounding techniques, and new loss estimators\ninspired by recent advances in online shortest path problems. Along the way, we\nalso provide the first individual-gap-dependent lower bounds and demonstrate\nnear-optimal BOBW algorithms for shortest path problems with bandit feedback.\n","authors":["Shinji Ito","Kevin Jamieson","Haipeng Luo","Arnab Maiti","Taira Tsuchiya"],"pdf_url":"https://arxiv.org/pdf/2510.17103v2.pdf","comment":"49 pages"},{"id":"http://arxiv.org/abs/2510.23053v1","updated":"2025-10-27T06:31:35Z","published":"2025-10-27T06:31:35Z","title":"AirFed: Federated Graph-Enhanced Multi-Agent Reinforcement Learning for\n  Multi-UAV Cooperative Mobile Edge Computing","summary":"  Multiple Unmanned Aerial Vehicles (UAVs) cooperative Mobile Edge Computing\n(MEC) systems face critical challenges in coordinating trajectory planning,\ntask offloading, and resource allocation while ensuring Quality of Service\n(QoS) under dynamic and uncertain environments. Existing approaches suffer from\nlimited scalability, slow convergence, and inefficient knowledge sharing among\nUAVs, particularly when handling large-scale IoT device deployments with\nstringent deadline constraints. This paper proposes AirFed, a novel federated\ngraph-enhanced multi-agent reinforcement learning framework that addresses\nthese challenges through three key innovations. First, we design dual-layer\ndynamic Graph Attention Networks (GATs) that explicitly model spatial-temporal\ndependencies among UAVs and IoT devices, capturing both service relationships\nand collaborative interactions within the network topology. Second, we develop\na dual-Actor single-Critic architecture that jointly optimizes continuous\ntrajectory control and discrete task offloading decisions. Third, we propose a\nreputation-based decentralized federated learning mechanism with\ngradient-sensitive adaptive quantization, enabling efficient and robust\nknowledge sharing across heterogeneous UAVs. Extensive experiments demonstrate\nthat AirFed achieves 42.9% reduction in weighted cost compared to\nstate-of-the-art baselines, attains over 99% deadline satisfaction and 94.2%\nIoT device coverage rate, and reduces communication overhead by 54.5%.\nScalability analysis confirms robust performance across varying UAV numbers,\nIoT device densities, and system scales, validating AirFed's practical\napplicability for large-scale UAV-MEC deployments.\n","authors":["Zhiyu Wang","Suman Raj","Rajkumar Buyya"],"pdf_url":"https://arxiv.org/pdf/2510.23053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23051v1","updated":"2025-10-27T06:26:45Z","published":"2025-10-27T06:26:45Z","title":"SwiftTS: A Swift Selection Framework for Time Series Pre-trained Models\n  via Multi-task Meta-Learning","summary":"  Pre-trained models exhibit strong generalization to various downstream tasks.\nHowever, given the numerous models available in the model hub, identifying the\nmost suitable one by individually fine-tuning is time-consuming. In this paper,\nwe propose \\textbf{SwiftTS}, a swift selection framework for time series\npre-trained models. To avoid expensive forward propagation through all\ncandidates, SwiftTS adopts a learning-guided approach that leverages historical\ndataset-model performance pairs across diverse horizons to predict model\nperformance on unseen datasets. It employs a lightweight dual-encoder\narchitecture that embeds time series and candidate models with rich\ncharacteristics, computing patchwise compatibility scores between data and\nmodel embeddings for efficient selection. To further enhance the generalization\nacross datasets and horizons, we introduce a horizon-adaptive expert\ncomposition module that dynamically adjusts expert weights, and the\ntransferable cross-task learning with cross-dataset and cross-horizon task\nsampling to enhance out-of-distribution (OOD) robustness. Extensive experiments\non 14 downstream datasets and 8 pre-trained models demonstrate that SwiftTS\nachieves state-of-the-art performance in time series pre-trained model\nselection.\n","authors":["Tengxue Zhang","Biao Ouyang","Yang Shu","Xinyang Chen","Chenjuan Guo","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2510.23051v1.pdf","comment":"10 pages,6 figures"},{"id":"http://arxiv.org/abs/2509.19921v2","updated":"2025-10-27T06:25:33Z","published":"2025-09-24T09:22:04Z","title":"On the Fragility of Contribution Score Computation in Federated Learning","summary":"  This paper investigates the fragility of contribution evaluation in federated\nlearning, a critical mechanism for ensuring fairness and incentivizing\nparticipation. We argue that contribution scores are susceptible to significant\ndistortions from two fundamental perspectives: architectural sensitivity and\nintentional manipulation. First, we explore how different model aggregation\nmethods impact these scores. While most research assumes a basic averaging\napproach, we demonstrate that advanced techniques, including those designed to\nhandle unreliable or diverse clients, can unintentionally yet significantly\nalter the final scores. Second, we explore vulnerabilities posed by poisoning\nattacks, where malicious participants strategically manipulate their model\nupdates to inflate their own contribution scores or reduce the importance of\nother participants. Through extensive experiments across diverse datasets and\nmodel architectures, implemented within the Flower framework, we rigorously\nshow that both the choice of aggregation method and the presence of attackers\nare potent vectors for distorting contribution scores, highlighting a critical\nneed for more robust evaluation schemes.\n","authors":["Balazs Pejo","Marcell Frank","Krisztian Varga","Peter Veliczky","Gergely Biczok"],"pdf_url":"https://arxiv.org/pdf/2509.19921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23049v1","updated":"2025-10-27T06:24:56Z","published":"2025-10-27T06:24:56Z","title":"Advantage Shaping as Surrogate Reward Maximization: Unifying Pass@K\n  Policy Gradients","summary":"  This note reconciles two seemingly distinct approaches to policy gradient\noptimization for the Pass@K objective in reinforcement learning with verifiable\nrewards: (1) direct REINFORCE-style methods, and (2) advantage-shaping\ntechniques that directly modify GRPO. We show that these are two sides of the\nsame coin. By reverse-engineering existing advantage-shaping algorithms, we\nreveal that they implicitly optimize surrogate rewards. We specifically\ninterpret practical ``hard-example up-weighting'' modifications to GRPO as\nreward-level regularization. Conversely, starting from surrogate reward\nobjectives, we provide a simple recipe for deriving both existing and new\nadvantage-shaping methods. This perspective provides a lens for RLVR policy\ngradient optimization beyond our original motivation of Pass@K.\n","authors":["Christos Thrampoulidis","Sadegh Mahdavi","Wenlong Deng"],"pdf_url":"https://arxiv.org/pdf/2510.23049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04678v4","updated":"2025-10-27T06:19:56Z","published":"2024-02-07T09:09:14Z","title":"FaithLM: Towards Faithful Explanations for Large Language Models","summary":"  Large language models (LLMs) increasingly produce natural language\nexplanations, yet these explanations often lack faithfulness, and they do not\nreliably reflect the evidence the model uses to decide. We introduce FaithLM, a\nmodel-agnostic framework that evaluates and improves the faithfulness of LLM\nexplanations without token masking or task-specific heuristics. FaithLM\nformalizes explanation faithfulness as an intervention property: a faithful\nexplanation should yield a prediction shift when its content is contradicted.\nTheoretical analysis shows that the resulting contrary-hint score is a sound\nand discriminative estimator of faithfulness. Building on this principle,\nFaithLM iteratively refines both the elicitation prompt and the explanation to\nmaximize the measured score. Experiments on three multi-domain datasets and\nmultiple LLM backbones demonstrate that FaithLM consistently increases\nfaithfulness and produces explanations more aligned with human rationales than\nstrong self-explanation baselines. These findings highlight that\nintervention-based evaluation, coupled with iterative optimization, provides a\nprincipled route toward faithful and reliable LLM explanations.\n","authors":["Yu-Neng Chuang","Guanchu Wang","Chia-Yuan Chang","Ruixiang Tang","Shaochen Zhong","Fan Yang","Mengnan Du","Xuanting Cai","Vladimir Braverman","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2402.04678v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13868v3","updated":"2025-10-27T06:15:12Z","published":"2025-03-18T03:35:29Z","title":"Out-of-Distribution Generalization in Time Series: A Survey","summary":"  Time series frequently manifest distribution shifts, diverse latent features,\nand non-stationary learning dynamics, particularly in open and evolving\nenvironments. These characteristics pose significant challenges for\nout-of-distribution (OOD) generalization. While substantial progress has been\nmade, a systematic synthesis of advancements remains lacking. To address this\ngap, we present the first comprehensive review of OOD generalization\nmethodologies for time series, organized to delineate the field's evolutionary\ntrajectory and contemporary research landscape. We organize our analysis across\nthree foundational dimensions: data distribution, representation learning, and\nOOD evaluation. For each dimension, we present several popular algorithms in\ndetail. Furthermore, we highlight key application scenarios, emphasizing their\nreal-world impact. Finally, we identify persistent challenges and propose\nfuture research directions. A detailed summary of the methods reviewed for the\ngeneralization of OOD in time series can be accessed at\nhttps://tsood-generalization.com.\n","authors":["Xin Wu","Fei Teng","Xingwang Li","Ji Zhang","Tianrui Li","Qiang Duan"],"pdf_url":"https://arxiv.org/pdf/2503.13868v3.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2510.23040v1","updated":"2025-10-27T06:08:19Z","published":"2025-10-27T06:08:19Z","title":"LLM Meets Diffusion: A Hybrid Framework for Crystal Material Generation","summary":"  Recent advances in generative modeling have shown significant promise in\ndesigning novel periodic crystal structures. Existing approaches typically rely\non either large language models (LLMs) or equivariant denoising models, each\nwith complementary strengths: LLMs excel at handling discrete atomic types but\noften struggle with continuous features such as atomic positions and lattice\nparameters, while denoising models are effective at modeling continuous\nvariables but encounter difficulties in generating accurate atomic\ncompositions. To bridge this gap, we propose CrysLLMGen, a hybrid framework\nthat integrates an LLM with a diffusion model to leverage their complementary\nstrengths for crystal material generation. During sampling, CrysLLMGen first\nemploys a fine-tuned LLM to produce an intermediate representation of atom\ntypes, atomic coordinates, and lattice structure. While retaining the predicted\natom types, it passes the atomic coordinates and lattice structure to a\npre-trained equivariant diffusion model for refinement. Our framework\noutperforms state-of-the-art generative models across several benchmark tasks\nand datasets. Specifically, CrysLLMGen not only achieves a balanced performance\nin terms of structural and compositional validity but also generates more\nstable and novel materials compared to LLM-based and denoisingbased models\nFurthermore, CrysLLMGen exhibits strong conditional generation capabilities,\neffectively producing materials that satisfy user-defined constraints. Code is\navailable at https://github.com/kdmsit/crysllmgen\n","authors":["Subhojyoti Khastagir","Kishalay Das","Pawan Goyal","Seung-Cheol Lee","Satadeep Bhattacharjee","Niloy Ganguly"],"pdf_url":"https://arxiv.org/pdf/2510.23040v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23039v1","updated":"2025-10-27T06:05:45Z","published":"2025-10-27T06:05:45Z","title":"Sublinear Sketches for Approximate Nearest Neighbor and Kernel Density\n  Estimation","summary":"  Approximate Nearest Neighbor (ANN) search and Approximate Kernel Density\nEstimation (A-KDE) are fundamental problems at the core of modern machine\nlearning, with broad applications in data analysis, information systems, and\nlarge-scale decision making. In massive and dynamic data streams, a central\nchallenge is to design compact sketches that preserve essential structural\nproperties of the data while enabling efficient queries.\n  In this work, we develop new sketching algorithms that achieve sublinear\nspace and query time guarantees for both ANN and A-KDE for a dynamic stream of\ndata. For ANN in the streaming model, under natural assumptions, we design a\nsublinear sketch that requires only $\\mathcal{O}(n^{1+\\rho-\\eta})$ memory by\nstoring only a sublinear ($n^{-\\eta}$) fraction of the total inputs, where\n$\\rho$ is a parameter of the LSH family, and $0<\\eta<1$. Our method supports\nsublinear query time, batch queries, and extends to the more general Turnstile\nmodel. While earlier works have focused on Exact NN, this is the first result\non ANN that achieves near-optimal trade-offs between memory size and\napproximation error.\n  Next, for A-KDE in the Sliding-Window model, we propose a sketch of size\n$\\mathcal{O}\\left(RW \\cdot \\frac{1}{\\sqrt{1+\\epsilon} - 1} \\log^2 N\\right)$,\nwhere $R$ is the number of sketch rows, $W$ is the LSH range, $N$ is the window\nsize, and $\\epsilon$ is the approximation error. This, to the best of our\nknowledge, is the first theoretical sublinear sketch guarantee for A-KDE in the\nSliding-Window model.\n  We complement our theoretical results with experiments on various real-world\ndatasets, which show that the proposed sketches are lightweight and achieve\nconsistently low error in practice.\n","authors":["Ved Danait","Srijan Das","Sujoy Bhore"],"pdf_url":"https://arxiv.org/pdf/2510.23039v1.pdf","comment":"28 pages, 11 figures"},{"id":"http://arxiv.org/abs/2510.23038v1","updated":"2025-10-27T06:03:37Z","published":"2025-10-27T06:03:37Z","title":"Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated\n  Reinforcement Learning","summary":"  Large Language Models (LLMs) are widely used as judges to evaluate response\nquality, providing a scalable alternative to human evaluation. However, most\nLLM judges operate solely on intrinsic text-based reasoning, limiting their\nability to verify complex constraints or perform accurate computation.\nMotivated by the success of tool-integrated reasoning (TIR) in numerous tasks,\nwe propose TIR-Judge, an end-to-end RL framework for training LLM judges that\nintegrates a code executor for precise evaluation. TIR-Judge is built on three\nprinciples: (i) diverse training across verifiable and non-verifiable domains,\n(ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii)\niterative RL that bootstraps directly from the initial model without\ndistillation. On seven public benchmarks, TIR-Judge surpasses strong\nreasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and\nachieves listwise performance comparable to Claude-Opus-4 despite having only\n8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled\njudge trajectories, matches the performance of distilled variants,\ndemonstrating that tool-augmented judges can self-evolve through iterative\nreinforcement learning.\n","authors":["Ran Xu","Jingjing Chen","Jiayu Ye","Yu Wu","Jun Yan","Carl Yang","Hongkun Yu"],"pdf_url":"https://arxiv.org/pdf/2510.23038v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2510.23027v1","updated":"2025-10-27T05:47:48Z","published":"2025-10-27T05:47:48Z","title":"Towards Stable and Effective Reinforcement Learning for\n  Mixture-of-Experts","summary":"  Recent advances in reinforcement learning (RL) have substantially improved\nthe training of large-scale language models, leading to significant gains in\ngeneration quality and reasoning ability. However, most existing research\nfocuses on dense models, while RL training for Mixture-of-Experts (MoE)\narchitectures remains underexplored. To address the instability commonly\nobserved in MoE training, we propose a novel router-aware approach to optimize\nimportance sampling (IS) weights in off-policy RL. Specifically, we design a\nrescaling strategy guided by router logits, which effectively reduces gradient\nvariance and mitigates training divergence. Experimental results demonstrate\nthat our method significantly improves both the convergence stability and the\nfinal performance of MoE models, highlighting the potential of RL algorithmic\ninnovations tailored to MoE architectures and providing a promising direction\nfor efficient training of large-scale expert models.\n","authors":["Di Zhang","Xun Wu","Shaohan Huang","Yaru Hao","Li Dong","Zewen Chi","Zhifang Sui","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2510.23027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23019v1","updated":"2025-10-27T05:32:48Z","published":"2025-10-27T05:32:48Z","title":"Sentinel: Dynamic Knowledge Distillation for Personalized Federated\n  Intrusion Detection in Heterogeneous IoT Networks","summary":"  Federated learning (FL) offers a privacy-preserving paradigm for machine\nlearning, but its application in intrusion detection systems (IDS) within IoT\nnetworks is challenged by severe class imbalance, non-IID data, and high\ncommunication overhead.These challenges severely degrade the performance of\nconventional FL methods in real-world network traffic classification. To\novercome these limitations, we propose Sentinel, a personalized federated IDS\n(pFed-IDS) framework that incorporates a dual-model architecture on each\nclient, consisting of a personalized teacher and a lightweight shared student\nmodel. This design effectively balances deep local adaptation with efficient\nglobal model consensus while preserving client privacy by transmitting only the\ncompact student model, thus reducing communication costs. Sentinel integrates\nthree key mechanisms to ensure robust performance: bidirectional knowledge\ndistillation with adaptive temperature scaling, multi-faceted feature\nalignment, and class-balanced loss functions. Furthermore, the server employs\nnormalized gradient aggregation with equal client weighting to enhance fairness\nand mitigate client drift. Extensive experiments on the IoTID20 and 5GNIDD\nbenchmark datasets demonstrate that Sentinel significantly outperforms\nstate-of-the-art federated methods, establishing a new performance benchmark,\nespecially under extreme data heterogeneity, while maintaining communication\nefficiency.\n","authors":["Gurpreet Singh","Keshav Sood","P. Rajalakshmi","Yong Xiang"],"pdf_url":"https://arxiv.org/pdf/2510.23019v1.pdf","comment":"This is a preprint version of a paper currently under review for\n  possible publication in IEEE TDSC"},{"id":"http://arxiv.org/abs/2510.23015v1","updated":"2025-10-27T05:22:35Z","published":"2025-10-27T05:22:35Z","title":"Coupled Flow Matching","summary":"  We introduce Coupled Flow Matching (CPFM), a framework that integrates\ncontrollable dimensionality reduction and high-fidelity reconstruction. CPFM\nlearns coupled continuous flows for both the high-dimensional data x and the\nlow-dimensional embedding y, which enables sampling p(y|x) via a latent-space\nflow and p(x|y) via a data-space flow. Unlike classical dimension-reduction\nmethods, where information discarded during compression is often difficult to\nrecover, CPFM preserves the knowledge of residual information within the\nweights of a flow network. This design provides bespoke controllability: users\nmay decide which semantic factors to retain explicitly in the latent space,\nwhile the complementary information remains recoverable through the flow\nnetwork. Coupled flow matching builds on two components: (i) an extended\nGromov-Wasserstein optimal transport objective that establishes a probabilistic\ncorrespondence between data and embeddings, and (ii) a dual-conditional\nflow-matching network that extrapolates the correspondence to the underlying\nspace. Experiments on multiple benchmarks show that CPFM yields semantically\nrich embeddings and reconstructs data with higher fidelity than existing\nbaselines.\n","authors":["Wenxi Cai","Yuheng Wang","Naichen Shi"],"pdf_url":"https://arxiv.org/pdf/2510.23015v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.12650v2","updated":"2025-10-27T05:20:40Z","published":"2025-08-18T06:25:41Z","title":"Score-informed Neural Operator for Enhancing Ordering-based Causal\n  Discovery","summary":"  Ordering-based approaches to causal discovery identify topological orders of\ncausal graphs, providing scalable alternatives to combinatorial search methods.\nUnder the Additive Noise Model (ANM) assumption, recent causal ordering methods\nbased on score matching require an accurate estimation of the Hessian diagonal\nof the log-densities. In this paper, we aim to improve the approximation of the\nHessian diagonal of the log-densities, thereby enhancing the performance of\nordering-based causal discovery algorithms. Existing approaches that rely on\nStein gradient estimators are computationally expensive and memory-intensive,\nwhile diffusion-model-based methods remain unstable due to the second-order\nderivatives of score models. To alleviate these problems, we propose\nScore-informed Neural Operator (SciNO), a probabilistic generative model in\nsmooth function spaces designed to stably approximate the Hessian diagonal and\nto preserve structural information during the score modeling. Empirical results\nshow that SciNO reduces order divergence by 42.7% on synthetic graphs and by\n31.5% on real-world datasets on average compared to DiffAN, while maintaining\nmemory efficiency and scalability. Furthermore, we propose a probabilistic\ncontrol algorithm for causal reasoning with autoregressive models that\nintegrates SciNO's probability estimates with autoregressive model priors,\nenabling reliable data-driven causal ordering informed by semantic information.\nConsequently, the proposed method enhances causal reasoning abilities of LLMs\nwithout additional fine-tuning or prompt engineering.\n","authors":["Jiyeon Kang","Songseong Kim","Chanhui Lee","Doyeong Hwang","Joanie Hayoun Chung","Yunkyung Ko","Sumin Lee","Sungwoong Kim","Sungbin Lim"],"pdf_url":"https://arxiv.org/pdf/2508.12650v2.pdf","comment":"Accepted to NeurIPS 2025. 36 pages, 18 figures, 12 tables"},{"id":"http://arxiv.org/abs/2510.23013v1","updated":"2025-10-27T05:16:10Z","published":"2025-10-27T05:16:10Z","title":"MoEMeta: Mixture-of-Experts Meta Learning for Few-Shot Relational\n  Learning","summary":"  Few-shot knowledge graph relational learning seeks to perform reasoning over\nrelations given only a limited number of training examples. While existing\napproaches largely adopt a meta-learning framework for enabling fast adaptation\nto new relations, they suffer from two key pitfalls. First, they learn relation\nmeta-knowledge in isolation, failing to capture common relational patterns\nshared across tasks. Second, they struggle to effectively incorporate local,\ntask-specific contexts crucial for rapid adaptation. To address these\nlimitations, we propose MoEMeta, a novel meta-learning framework that\ndisentangles globally shared knowledge from task-specific contexts to enable\nboth effective generalization and rapid adaptation. MoEMeta introduces two key\ninnovations: (i) a mixture-of-experts (MoE) model that learns globally shared\nrelational prototypes to enhance generalization, and (ii) a task-tailored\nadaptation mechanism that captures local contexts for fast task-specific\nadaptation. By balancing global generalization with local adaptability, MoEMeta\nsignificantly advances few-shot relational learning. Extensive experiments and\nanalyses on three KG benchmarks demonstrate that MoEMeta consistently\noutperforms existing baselines, achieving state-of-the-art performance.\n","authors":["Han Wu","Jie Yin"],"pdf_url":"https://arxiv.org/pdf/2510.23013v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23012v1","updated":"2025-10-27T05:16:04Z","published":"2025-10-27T05:16:04Z","title":"Softmax is $1/2$-Lipschitz: A tight bound across all $\\ell_p$ norms","summary":"  The softmax function is a basic operator in machine learning and\noptimization, used in classification, attention mechanisms, reinforcement\nlearning, game theory, and problems involving log-sum-exp terms. Existing\nrobustness guarantees of learning models and convergence analysis of\noptimization algorithms typically consider the softmax operator to have a\nLipschitz constant of $1$ with respect to the $\\ell_2$ norm. In this work, we\nprove that the softmax function is contractive with the Lipschitz constant\n$1/2$, uniformly across all $\\ell_p$ norms with $p \\ge 1$. We also show that\nthe local Lipschitz constant of softmax attains $1/2$ for $p = 1$ and $p =\n\\infty$, and for $p \\in (1,\\infty)$, the constant remains strictly below $1/2$\nand the supremum $1/2$ is achieved only in the limit. To our knowledge, this is\nthe first comprehensive norm-uniform analysis of softmax Lipschitz continuity.\nWe demonstrate how the sharper constant directly improves a range of existing\ntheoretical results on robustness and convergence. We further validate the\nsharpness of the $1/2$ Lipschitz constant of the softmax operator through\nempirical studies on attention-based architectures (ViT, GPT-2, Qwen3-8B) and\non stochastic policies in reinforcement learning.\n","authors":["Pravin Nair"],"pdf_url":"https://arxiv.org/pdf/2510.23012v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2509.23964v2","updated":"2025-10-27T05:14:37Z","published":"2025-09-28T16:41:56Z","title":"Detecting and Rectifying Noisy Labels: A Similarity-based Approach","summary":"  Label noise in datasets could significantly damage the performance and\nrobustness of deep neural networks (DNNs) trained on these datasets. As the\nsize of modern DNNs grows, there is a growing demand for automated tools for\ndetecting such errors. In this paper, we propose post-hoc, model-agnostic noise\ndetection and rectification methods utilizing the penultimate feature from a\nDNN. Our idea is based on the observation that the similarity between the\npenultimate feature of a mislabeled data point and its true class data points\nis higher than that for data points from other classes, making the probability\nof label occurrence within a tight, similar cluster informative for detecting\nand rectifying errors. Through theoretical and empirical analyses, we\ndemonstrate that our approach achieves high detection performance across\ndiverse, realistic noise scenarios and can automatically rectify these errors\nto improve dataset quality. Our implementation is available at\nhttps://anonymous.4open.science/r/noise-detection-and-rectification-AD8E.\n","authors":["Dang Huu-Tien","Minh-Phuong Nguyen","Naoya Inoue"],"pdf_url":"https://arxiv.org/pdf/2509.23964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23917v2","updated":"2025-10-27T05:03:10Z","published":"2025-05-29T18:09:44Z","title":"Representational Difference Explanations","summary":"  We propose a method for discovering and visualizing the differences between\ntwo learned representations, enabling more direct and interpretable model\ncomparisons. We validate our method, which we call Representational Differences\nExplanations (RDX), by using it to compare models with known conceptual\ndifferences and demonstrate that it recovers meaningful distinctions where\nexisting explainable AI (XAI) techniques fail. Applied to state-of-the-art\nmodels on challenging subsets of the ImageNet and iNaturalist datasets, RDX\nreveals both insightful representational differences and subtle patterns in the\ndata. Although comparison is a cornerstone of scientific analysis, current\ntools in machine learning, namely post hoc XAI methods, struggle to support\nmodel comparison effectively. Our work addresses this gap by introducing an\neffective and explainable tool for contrasting model representations.\n","authors":["Neehar Kondapaneni","Oisin Mac Aodha","Pietro Perona"],"pdf_url":"https://arxiv.org/pdf/2505.23917v2.pdf","comment":"9 pages, 6 figures, 21 supplementary pages, 14 supp figs"},{"id":"http://arxiv.org/abs/2510.22993v1","updated":"2025-10-27T04:18:59Z","published":"2025-10-27T04:18:59Z","title":"Can Language Models Compose Skills In-Context?","summary":"  Composing basic skills from simple tasks to accomplish composite tasks is\ncrucial for modern intelligent systems. We investigate the in-context\ncomposition ability of language models to perform composite tasks that combine\nbasic skills demonstrated in in-context examples. This is more challenging than\nthe standard setting, where skills and their composition can be learned in\ntraining. We conduct systematic experiments on various representative\nopen-source language models, utilizing linguistic and logical tasks designed to\nprobe composition abilities. The results reveal that simple task examples can\nhave a surprising negative impact on the performance, because the models\ngenerally struggle to recognize and assemble the skills correctly, even with\nChain-of-Thought examples. Theoretical analysis further shows that it is\ncrucial to align examples with the corresponding steps in the composition. This\ninspires a method for the probing tasks, whose improved performance provides\npositive support for our insights.\n","authors":["Zidong Liu","Zhuoyan Xu","Zhenmei Shi","Yingyu Liang"],"pdf_url":"https://arxiv.org/pdf/2510.22993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22991v1","updated":"2025-10-27T04:17:41Z","published":"2025-10-27T04:17:41Z","title":"Adaptive Forests For Classification","summary":"  Random Forests (RF) and Extreme Gradient Boosting (XGBoost) are two of the\nmost widely used and highly performing classification and regression models.\nThey aggregate equally weighted CART trees, generated randomly in RF or\nsequentially in XGBoost. In this paper, we propose Adaptive Forests (AF), a\nnovel approach that adaptively selects the weights of the underlying CART\nmodels. AF combines (a) the Optimal Predictive-Policy Trees (OP2T) framework to\nprescribe tailored, input-dependent unequal weights to trees and (b) Mixed\nInteger Optimization (MIO) to refine weight candidates dynamically, enhancing\noverall performance. We demonstrate that AF consistently outperforms RF,\nXGBoost, and other weighted RF in binary and multi-class classification\nproblems over 20+ real-world datasets.\n","authors":["Dimitris Bertsimas","Yubing Cui"],"pdf_url":"https://arxiv.org/pdf/2510.22991v1.pdf","comment":"Under review at JMLR"},{"id":"http://arxiv.org/abs/2507.20975v3","updated":"2025-10-27T04:17:01Z","published":"2025-07-28T16:37:56Z","title":"Locally Adaptive Conformal Inference for Operator Models","summary":"  Operator models are regression algorithms between Banach spaces of functions.\nThey have become an increasingly critical tool for spatiotemporal forecasting\nand physics emulation, especially in high-stakes scenarios where robust,\ncalibrated uncertainty quantification is required. We introduce Local Sliced\nConformal Inference (LSCI), a distribution-free framework for generating\nfunction-valued, locally adaptive prediction sets for operator models. We prove\nfinite-sample validity and derive a data-dependent upper bound on the coverage\ngap under local exchangeability. On synthetic Gaussian-process tasks and real\napplications (air quality monitoring, energy demand forecasting, and weather\nprediction), LSCI yields tighter sets with stronger adaptivity compared to\nconformal baselines. We also empirically demonstrate robustness against biased\npredictions and certain out-of-distribution noise regimes.\n","authors":["Trevor Harris","Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2507.20975v3.pdf","comment":"9 pages, 2 figures, 2 tables, Preprint"},{"id":"http://arxiv.org/abs/2510.22984v1","updated":"2025-10-27T04:08:39Z","published":"2025-10-27T04:08:39Z","title":"Equivariant Neural Networks for General Linear Symmetries on Lie\n  Algebras","summary":"  Encoding symmetries is a powerful inductive bias for improving the\ngeneralization of deep neural networks. However, most existing equivariant\nmodels are limited to simple symmetries like rotations, failing to address the\nbroader class of general linear transformations, GL(n), that appear in many\nscientific domains. We introduce Reductive Lie Neurons (ReLNs), a novel neural\nnetwork architecture exactly equivariant to these general linear symmetries.\nReLNs are designed to operate directly on a wide range of structured inputs,\nincluding general n-by-n matrices. ReLNs introduce a novel adjoint-invariant\nbilinear layer to achieve stable equivariance for both Lie-algebraic features\nand matrix-valued inputs, without requiring redesign for each subgroup. This\narchitecture overcomes the limitations of prior equivariant networks that only\napply to compact groups or simple vector data. We validate ReLNs' versatility\nacross a spectrum of tasks: they outperform existing methods on algebraic\nbenchmarks with sl(3) and sp(4) symmetries and achieve competitive results on a\nLorentz-equivariant particle physics task. In 3D drone state estimation with\ngeometric uncertainty, ReLNs jointly process velocities and covariances,\nyielding significant improvements in trajectory accuracy. ReLNs provide a\npractical and general framework for learning with broad linear group symmetries\non Lie algebras and matrix-valued data. Project page:\nhttps://reductive-lie-neuron.github.io/\n","authors":["Chankyo Kim","Sicheng Zhao","Minghan Zhu","Tzu-Yuan Lin","Maani Ghaffari"],"pdf_url":"https://arxiv.org/pdf/2510.22984v1.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.22982v1","updated":"2025-10-27T04:03:28Z","published":"2025-10-27T04:03:28Z","title":"QoSGMAA: A Robust Multi-Order Graph Attention and Adversarial Framework\n  for Sparse QoS Prediction","summary":"  With the rapid advancement of internet technologies, network services have\nbecome critical for delivering diverse and reliable applications to users.\nHowever, the exponential growth in the number of available services has\nresulted in many similar offerings, posing significant challenges in selecting\noptimal services. Predicting Quality of Service (QoS) accurately thus becomes a\nfundamental prerequisite for ensuring reliability and user satisfaction.\nHowever, existing QoS prediction methods often fail to capture rich contextual\ninformation and exhibit poor performance under extreme data sparsity and\nstructural noise. To bridge this gap, we propose a novel architecture, QoSMGAA,\nspecifically designed to enhance prediction accuracy in complex and noisy\nnetwork service environments. QoSMGAA integrates a multi-order attention\nmechanism to aggregate extensive contextual data and predict missing QoS values\neffectively. Additionally, our method incorporates adversarial neural networks\nto perform autoregressive supervised learning based on transformed interaction\nmatrices. To capture complex, higher-order interactions among users and\nservices, we employ a discrete sampling technique leveraging the Gumbel-Softmax\nmethod to generate informative negative samples. Comprehensive experimental\nvalidation conducted on large-scale real-world datasets demonstrates that our\nproposed model significantly outperforms existing baseline methods,\nhighlighting its strong potential for practical deployment in service selection\nand recommendation scenarios.\n","authors":["Guanchen Du","Jianlong Xu","Mingtong Li","Ruiqi Wang","Qianqing Guo","Caiyi Chen","Qingcao Dai","Yuxiang Zeng"],"pdf_url":"https://arxiv.org/pdf/2510.22982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22980v1","updated":"2025-10-27T04:00:42Z","published":"2025-10-27T04:00:42Z","title":"How Muon's Spectral Design Benefits Generalization: A Study on\n  Imbalanced Data","summary":"  The growing adoption of spectrum-aware matrix-valued optimizers such as Muon\nand Shampoo in deep learning motivates a systematic study of their\ngeneralization properties and, in particular, when they might outperform\ncompetitive algorithms. We approach this question by introducing appropriate\nsimplifying abstractions as follows: First, we use imbalanced data as a\ntestbed. Second, we study the canonical form of such optimizers, which is\nSpectral Gradient Descent (SpecGD) -- each update step is $UV^T$ where $U\\Sigma\nV^T$ is the truncated SVD of the gradient. Third, within this framework we\nidentify a canonical setting for which we precisely quantify when SpecGD\noutperforms vanilla Euclidean GD. For a Gaussian mixture data model and both\nlinear and bilinear models, we show that unlike GD, which prioritizes learning\ndominant principal components of the data first, SpecGD learns all principal\ncomponents of the data at equal rates. We demonstrate how this translates to a\ngrowing gap in balanced accuracy favoring SpecGD early in training and further\nshow that the gap remains consistent even when the GD counterpart uses adaptive\nstep-sizes via normalization. By extending the analysis to deep linear models,\nwe show that depth amplifies these effects. We empirically verify our\ntheoretical findings on a variety of imbalanced datasets. Our experiments\ncompare practical variants of spectral methods, like Muon and Shampoo, against\ntheir Euclidean counterparts and Adam. The results validate our findings that\nthese spectral optimizers achieve superior generalization by promoting a more\nbalanced learning of the data's underlying components.\n","authors":["Bhavya Vasudeva","Puneesh Deora","Yize Zhao","Vatsal Sharan","Christos Thrampoulidis"],"pdf_url":"https://arxiv.org/pdf/2510.22980v1.pdf","comment":"32 pages, 28 figures"},{"id":"http://arxiv.org/abs/2510.22977v1","updated":"2025-10-27T03:58:29Z","published":"2025-10-27T03:58:29Z","title":"The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool\n  Hallucination","summary":"  Enhancing the reasoning capabilities of Large Language Models (LLMs) is a key\nstrategy for building Agents that \"think then act.\" However, recent\nobservations, like OpenAI's o3, suggest a paradox: stronger reasoning often\ncoincides with increased hallucination, yet no prior work has systematically\nexamined whether reasoning enhancement itself causes tool hallucination. To\naddress this gap, we pose the central question: Does strengthening reasoning\nincrease tool hallucination? To answer this, we introduce SimpleToolHalluBench,\na diagnostic benchmark measuring tool hallucination in two failure modes: (i)\nno tool available, and (ii) only distractor tools available. Through controlled\nexperiments, we establish three key findings. First, we demonstrate a causal\nrelationship: progressively enhancing reasoning through RL increases tool\nhallucination proportionally with task performance gains. Second, this effect\ntranscends overfitting - training on non-tool tasks (e.g., mathematics) still\namplifies subsequent tool hallucination. Third, the effect is method-agnostic,\nappearing when reasoning is instilled via supervised fine-tuning and when it is\nmerely elicited at inference by switching from direct answers to step-by-step\nthinking. We also evaluate mitigation strategies including Prompt Engineering\nand Direct Preference Optimization (DPO), revealing a fundamental\nreliability-capability trade-off: reducing hallucination consistently degrades\nutility. Mechanistically, Reasoning RL disproportionately collapses\ntool-reliability-related representations, and hallucinations surface as\namplified divergences concentrated in late-layer residual streams. These\nfindings reveal that current reasoning enhancement methods inherently amplify\ntool hallucination, highlighting the need for new training objectives that\njointly optimize for capability and reliability.\n","authors":["Chenlong Yin","Zeyang Sha","Shiwen Cui","Changhua Meng"],"pdf_url":"https://arxiv.org/pdf/2510.22977v1.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.22976v1","updated":"2025-10-27T03:57:29Z","published":"2025-10-27T03:57:29Z","title":"Analysis of accuracy and efficiency of neural networks to simulate\n  Navier-Stokes fluid flows with obstacles","summary":"  Conventional fluid simulations can be time consuming and energy intensive. We\nresearched the viability of a neural network for simulating incompressible\nfluids in a randomized obstacle-heavy environment, as an alternative to the\nnumerical simulation of the Navier-Stokes equation. We hypothesized that the\nneural network predictions would have a relatively low error for simulations\nover a small number of time steps, but errors would eventually accumulate to\nthe point that the output would become very noisy. Over a rich set of obstacle\nconfigurations, we achieved a root mean square error of 0.32% on our training\ndataset and 0.36% on a testing dataset. These errors only grew to 1.45% and\n2.34% at t = 10 and, 2.11% and 4.16% at timestep t = 20. We also found that our\nselected neural network was approximately 8,800 times faster at predicting the\nflow than a conventional simulation. Our findings indicate neural networks can\nbe extremely useful at simulating fluids in obstacle-heavy environments. Useful\napplications include modeling forest fire smoke, pipe fluid flow, and\nunderwater/flood currents.\n","authors":["Rui Hespanha","Elliot McGuire","João Hespanha"],"pdf_url":"https://arxiv.org/pdf/2510.22976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22975v1","updated":"2025-10-27T03:56:25Z","published":"2025-10-27T03:56:25Z","title":"VoMP: Predicting Volumetric Mechanical Property Fields","summary":"  Physical simulation relies on spatially-varying mechanical properties, often\nlaboriously hand-crafted. VoMP is a feed-forward method trained to predict\nYoung's modulus ($E$), Poisson's ratio ($\\nu$), and density ($\\rho$) throughout\nthe volume of 3D objects, in any representation that can be rendered and\nvoxelized. VoMP aggregates per-voxel multi-view features and passes them to our\ntrained Geometry Transformer to predict per-voxel material latent codes. These\nlatents reside on a manifold of physically plausible materials, which we learn\nfrom a real-world dataset, guaranteeing the validity of decoded per-voxel\nmaterials. To obtain object-level training data, we propose an annotation\npipeline combining knowledge from segmented 3D datasets, material databases,\nand a vision-language model, along with a new benchmark. Experiments show that\nVoMP estimates accurate volumetric properties, far outperforming prior art in\naccuracy and speed.\n","authors":["Rishit Dagli","Donglai Xiang","Vismay Modi","Charles Loop","Clement Fuji Tsang","Anka He Chen","Anita Hu","Gavriel State","David I. W. Levin","Maria Shugrina"],"pdf_url":"https://arxiv.org/pdf/2510.22975v1.pdf","comment":"hi-res paper and other details at:\n  https://research.nvidia.com/labs/sil/projects/vomp"},{"id":"http://arxiv.org/abs/2502.06309v4","updated":"2025-10-27T03:53:07Z","published":"2025-02-10T09:56:15Z","title":"Analog In-memory Training on General Non-ideal Resistive Elements: The\n  Impact of Response Functions","summary":"  As the economic and environmental costs of training and deploying large\nvision or language models increase dramatically, analog in-memory computing\n(AIMC) emerges as a promising energy-efficient solution. However, the training\nperspective, especially its training dynamic, is underexplored. In AIMC\nhardware, the trainable weights are represented by the conductance of resistive\nelements and updated using consecutive electrical pulses. While the conductance\nchanges by a constant in response to each pulse, in reality, the change is\nscaled by asymmetric and non-linear response functions, leading to a non-ideal\ntraining dynamic. This paper provides a theoretical foundation for\ngradient-based training on AIMC hardware with non-ideal response functions. We\ndemonstrate that asymmetric response functions negatively impact Analog SGD by\nimposing an implicit penalty on the objective. To overcome the issue, we\npropose Residual Learning algorithm, which provably converges exactly to a\ncritical point by solving a bilevel optimization problem. We demonstrate that\nthe proposed method can be extended to address other hardware imperfections,\nsuch as limited response granularity. As we know, it is the first paper to\ninvestigate the impact of a class of generic non-ideal response functions. The\nconclusion is supported by simulations validating our theoretical insights.\n","authors":["Zhaoxian Wu","Quan Xiao","Tayfun Gokmen","Omobayode Fagbohungbe","Tianyi Chen"],"pdf_url":"https://arxiv.org/pdf/2502.06309v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.08221v3","updated":"2025-10-27T03:53:00Z","published":"2025-08-11T17:39:45Z","title":"Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning","summary":"  Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO.\n","authors":["Zihe Liu","Jiashun Liu","Yancheng He","Weixun Wang","Jiaheng Liu","Ling Pan","Xinyu Hu","Shaopan Xiong","Ju Huang","Jian Hu","Shengyi Huang","Johan Obando-Ceron","Siran Yang","Jiamang Wang","Wenbo Su","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2508.08221v3.pdf","comment":"26 pages, 21 figures"},{"id":"http://arxiv.org/abs/2502.14704v3","updated":"2025-10-27T03:36:48Z","published":"2025-02-20T16:29:37Z","title":"Not All Data are Good Labels: On the Self-supervised Labeling for Time\n  Series Forecasting","summary":"  Time Series Forecasting (TSF) is a crucial task in various domains, yet\nexisting TSF models rely heavily on high-quality data and insufficiently\nexploit all available data. This paper explores a novel self-supervised\napproach to re-label time series datasets by inherently constructing candidate\ndatasets. During the optimization of a simple reconstruction network,\nintermediates are used as pseudo labels in a self-supervised paradigm,\nimproving generalization for any predictor. We introduce the Self-Correction\nwith Adaptive Mask (SCAM), which discards overfitted components and selectively\nreplaces them with pseudo labels generated from reconstructions. Additionally,\nwe incorporate Spectral Norm Regularization (SNR) to further suppress\noverfitting from a loss landscape perspective. Our experiments on eleven\nreal-world datasets demonstrate that SCAM consistently improves the performance\nof various backbone models. This work offers a new perspective on constructing\ndatasets and enhancing the generalization of TSF models through self-supervised\nlearning. The code is available at https://github.com/SuDIS-ZJU/SCAM.\n","authors":["Yuxuan Yang","Dalin Zhang","Yuxuan Liang","Hua Lu","Gang Chen","Huan Li"],"pdf_url":"https://arxiv.org/pdf/2502.14704v3.pdf","comment":"Accepted by NeurIPS'25 (spotlight)"},{"id":"http://arxiv.org/abs/2304.03720v2","updated":"2025-10-27T03:24:00Z","published":"2023-04-07T16:34:25Z","title":"Representer Theorems for Metric and Preference Learning: Geometric\n  Insights and Algorithms","summary":"  We develop a mathematical framework to address a broad class of metric and\npreference learning problems within a Hilbert space. We obtain a novel\nrepresenter theorem for the simultaneous task of metric and preference\nlearning. Our key observation is that the representer theorem for this task can\nbe derived by regularizing the problem with respect to the norm inherent in the\ntask structure. For the general task of metric learning, our framework leads to\na simple and self-contained representer theorem and offers new geometric\ninsights into the derivation of representer theorems for this task. In the case\nof Reproducing Kernel Hilbert Spaces (RKHSs), we illustrate how our representer\ntheorem can be used to express the solution of the learning problems in terms\nof finite kernel terms similar to classical representer theorems. Lastly, our\nrepresenter theorem leads to a novel nonlinear algorithm for metric and\npreference learning. We compare our algorithm against challenging baseline\nmethods on real-world rank inference benchmarks, where it achieves competitive\nperformance. Notably, our approach significantly outperforms vanilla ideal\npoint methods and surpasses strong baselines across multiple datasets. Code\navailable at: https://github.com/PeymanMorteza/Metric-Preference-Learning-RKHS\n","authors":["Peyman Morteza"],"pdf_url":"https://arxiv.org/pdf/2304.03720v2.pdf","comment":"A shorter version appeared in AISTATS 2025"},{"id":"http://arxiv.org/abs/2510.22955v1","updated":"2025-10-27T03:23:11Z","published":"2025-10-27T03:23:11Z","title":"SARNet: A Spike-Aware consecutive validation Framework for Accurate\n  Remaining Useful Life Prediction","summary":"  Accurate prediction of remaining useful life (RUL) is essential to enhance\nsystem reliability and reduce maintenance risk. Yet many strong contemporary\nmodels are fragile around fault onset and opaque to engineers: short,\nhigh-energy spikes are smoothed away or misread, fixed thresholds blunt\nsensitivity, and physics-based explanations are scarce. To remedy this, we\nintroduce SARNet (Spike-Aware Consecutive Validation Framework), which builds\non a Modern Temporal Convolutional Network (ModernTCN) and adds spike-aware\ndetection to provide physics-informed interpretability. ModernTCN forecasts\ndegradation-sensitive indicators; an adaptive consecutive threshold validates\ntrue spikes while suppressing noise. Failure-prone segments then receive\ntargeted feature engineering (spectral slopes, statistical derivatives, energy\nratios), and the final RUL is produced by a stacked RF--LGBM regressor. Across\nbenchmark-ported datasets under an event-triggered protocol, SARNet\nconsistently lowers error compared to recent baselines (RMSE 0.0365, MAE\n0.0204) while remaining lightweight, robust, and easy to deploy.\n","authors":["Junhao Fan","Wenrui Liang","Wei-Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.22955v1.pdf","comment":"5 pages, 2 figures, 3 tables. Equal contribution by Junhao Fan and\n  Wenrui Liang. Corresponding author: Wei-Qiang Zhang. Submitted to ICASSP 2026"},{"id":"http://arxiv.org/abs/2510.13887v3","updated":"2025-10-27T03:20:35Z","published":"2025-10-14T02:58:10Z","title":"Incomplete Multi-view Clustering via Hierarchical Semantic Alignment and\n  Cooperative Completion","summary":"  Incomplete multi-view data, where certain views are entirely missing for some\nsamples, poses significant challenges for traditional multi-view clustering\nmethods. Existing deep incomplete multi-view clustering approaches often rely\non static fusion strategies or two-stage pipelines, leading to suboptimal\nfusion results and error propagation issues. To address these limitations, this\npaper proposes a novel incomplete multi-view clustering framework based on\nHierarchical Semantic Alignment and Cooperative Completion (HSACC). HSACC\nachieves robust cross-view fusion through a dual-level semantic space design.\nIn the low-level semantic space, consistency alignment is ensured by maximizing\nmutual information across views. In the high-level semantic space, adaptive\nview weights are dynamically assigned based on the distributional affinity\nbetween individual views and an initial fused representation, followed by\nweighted fusion to generate a unified global representation. Additionally,\nHSACC implicitly recovers missing views by projecting aligned latent\nrepresentations into high-dimensional semantic spaces and jointly optimizes\nreconstruction and clustering objectives, enabling cooperative learning of\ncompletion and clustering. Experimental results demonstrate that HSACC\nsignificantly outperforms state-of-the-art methods on five benchmark datasets.\nAblation studies validate the effectiveness of the hierarchical alignment and\ndynamic weighting mechanisms, while parameter analysis confirms the model's\nrobustness to hyperparameter variations.\n","authors":["Xiaojian Ding","Lin Zhao","Xian Li","Xiaoying Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.13887v3.pdf","comment":"13 pages, conference paper. Accepted to NeurIPS 2025, not yet\n  published"},{"id":"http://arxiv.org/abs/2510.22953v1","updated":"2025-10-27T03:16:15Z","published":"2025-10-27T03:16:15Z","title":"Manifold Approximation leads to Robust Kernel Alignment","summary":"  Centered kernel alignment (CKA) is a popular metric for comparing\nrepresentations, determining equivalence of networks, and neuroscience\nresearch. However, CKA does not account for the underlying manifold and relies\non numerous heuristics that cause it to behave differently at different scales\nof data. In this work, we propose Manifold approximated Kernel Alignment (MKA),\nwhich incorporates manifold geometry into the alignment task. We derive a\ntheoretical framework for MKA. We perform empirical evaluations on synthetic\ndatasets and real-world examples to characterize and compare MKA to its\ncontemporaries. Our findings suggest that manifold-aware kernel alignment\nprovides a more robust foundation for measuring representations, with potential\napplications in representation learning.\n","authors":["Mohammad Tariqul Islam","Du Liu","Deblina Sarkar"],"pdf_url":"https://arxiv.org/pdf/2510.22953v1.pdf","comment":"9 pages, 5 figures + supplementary"},{"id":"http://arxiv.org/abs/2504.17959v3","updated":"2025-10-27T03:10:19Z","published":"2025-04-24T22:08:29Z","title":"CIVIL: Causal and Intuitive Visual Imitation Learning","summary":"  Today's robots attempt to learn new tasks by imitating human examples. These\nrobots watch the human complete the task, and then try to match the actions\ntaken by the human expert. However, this standard approach to visual imitation\nlearning is fundamentally limited: the robot observes what the human does, but\nnot why the human chooses those behaviors. Without understanding which features\nof the system or environment factor into the human's decisions, robot learners\noften misinterpret the human's examples. In practice, this results in causal\nconfusion, inefficient learning, and robot policies that fail when the\nenvironment changes. We therefore propose a shift in perspective: instead of\nasking human teachers just to show what actions the robot should take, we also\nenable humans to intuitively indicate why they made those decisions. Under our\nparadigm human teachers attach markers to task-relevant objects and use natural\nlanguage prompts to describe their state representation. Our proposed\nalgorithm, CIVIL, leverages this augmented demonstration data to filter the\nrobot's visual observations and extract a feature representation that aligns\nwith the human teacher. CIVIL then applies these causal features to train a\ntransformer-based policy that -- when tested on the robot -- is able to emulate\nhuman behaviors without being confused by visual distractors or irrelevant\nitems. Our simulations and real-world experiments demonstrate that robots\ntrained with CIVIL learn both what actions to take and why to take those\nactions, resulting in better performance than state-of-the-art baselines. From\nthe human's perspective, our user study reveals that this new training paradigm\nactually reduces the total time required for the robot to learn the task, and\nalso improves the robot's performance in previously unseen scenarios. See\nvideos at our project website: https://civil2025.github.io\n","authors":["Yinlong Dai","Robert Ramirez Sanchez","Ryan Jeronimus","Shahabedin Sagheb","Cara M. Nunez","Heramb Nemlekar","Dylan P. Losey"],"pdf_url":"https://arxiv.org/pdf/2504.17959v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22951v1","updated":"2025-10-27T03:09:45Z","published":"2025-10-27T03:09:45Z","title":"Hankel Singular Value Regularization for Highly Compressible State Space\n  Models","summary":"  Deep neural networks using state space models as layers are well suited for\nlong-range sequence tasks but can be challenging to compress after training. We\nuse that regularizing the sum of Hankel singular values of state space models\nleads to a fast decay of these singular values and thus to compressible models.\nTo make the proposed Hankel singular value regularization scalable, we develop\nan algorithm to efficiently compute the Hankel singular values during training\niterations by exploiting the specific block-diagonal structure of the system\nmatrices that is we use in our state space model parametrization. Experiments\non Long Range Arena benchmarks demonstrate that the regularized state space\nlayers are up to 10$\\times$ more compressible than standard state space layers\nwhile maintaining high accuracy.\n","authors":["Paul Schwerdtner","Jules Berman","Benjamin Peherstorfer"],"pdf_url":"https://arxiv.org/pdf/2510.22951v1.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2509.11173v3","updated":"2025-10-27T03:04:50Z","published":"2025-09-14T09:11:49Z","title":"Your Compiler is Backdooring Your Model: Understanding and Exploiting\n  Compilation Inconsistency Vulnerabilities in Deep Learning Compilers","summary":"  Deep learning (DL) compilers are core infrastructure in modern DL systems,\noffering flexibility and scalability beyond vendor-specific libraries. This\nwork uncovers a fundamental vulnerability in their design: can an official,\nunmodified compiler alter a model's semantics during compilation and introduce\nhidden backdoors? We study both adversarial and natural settings. In the\nadversarial case, we craft benign models where triggers have no effect\npre-compilation but become effective backdoors after compilation. Tested on six\nmodels, three commercial compilers, and two hardware platforms, our attack\nyields 100% success on triggered inputs while preserving normal accuracy and\nremaining undetected by state-of-the-art detectors. The attack generalizes\nacross compilers, hardware, and floating-point settings. In the natural\nsetting, we analyze the top 100 HuggingFace models (including one with 220M+\ndownloads) and find natural triggers in 31 models. This shows that compilers\ncan introduce risks even without adversarial manipulation.\n  Our results reveal an overlooked threat: unmodified DL compilers can silently\nalter model semantics. To our knowledge, this is the first work to expose\ninherent security risks in DL compiler design, opening a new direction for\nsecure and trustworthy ML.\n","authors":["Simin Chen","Jinjun Peng","Yixin He","Junfeng Yang","Baishakhi Ray"],"pdf_url":"https://arxiv.org/pdf/2509.11173v3.pdf","comment":"This paper is accepted to IEEE S&P 2026, the code is available at\n  https://github.com/SeekingDream/DLCompilerAttack"},{"id":"http://arxiv.org/abs/2510.22941v1","updated":"2025-10-27T02:55:09Z","published":"2025-10-27T02:55:09Z","title":"Hazard-Responsive Digital Twin for Climate-Driven Urban Resilience and\n  Equity","summary":"  Compounding climate hazards, such as wildfire-induced outages and urban\nheatwaves, challenge the stability and equity of cities. We present a\nHazard-Responsive Digital Twin (H-RDT) that combines physics-informed neural\nnetwork modeling, multimodal data fusion, and equity-aware risk analytics for\nurban-scale response. In a synthetic district with diverse building archetypes\nand populations, a simulated wildfire-outage-heatwave cascade shows that H-RDT\nmaintains stable indoor temperature predictions (approximately 31 to 33 C)\nunder partial sensor loss, reproducing outage-driven surges and recovery. The\nreinforcement learning based fusion module adaptively reweights IoT, UAV, and\nsatellite inputs to sustain spatiotemporal coverage, while the equity-adjusted\nmapping isolates high-vulnerability clusters (schools, clinics, low-income\nhousing). Prospective interventions, such as preemptive cooling-center\nactivation and microgrid sharing, reduce population-weighted thermal risk by 11\nto 13 percent, shrink the 95th-percentile (tail) risk by 7 to 17 percent, and\ncut overheating hours by up to 9 percent. Beyond the synthetic demonstration,\nthe framework establishes a transferable foundation for real-city\nimplementation, linking physical hazard modeling with social equity and\ndecision intelligence. The H-RDT advances digital urban resilience toward\nadaptive, learning-based, and equity-centered decision support for climate\nadaptation.\n","authors":["Zhenglai Shen","Hongyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.22941v1.pdf","comment":"52 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.22940v1","updated":"2025-10-27T02:51:51Z","published":"2025-10-27T02:51:51Z","title":"RL-AUX: Reinforcement Learning for Auxiliary Task Generation","summary":"  Auxiliary Learning (AL) is a special case of Multi-task Learning (MTL) in\nwhich a network trains on auxiliary tasks to improve performance on its main\ntask. This technique is used to improve generalization and, ultimately,\nperformance on the network's main task. AL has been demonstrated to improve\nperformance across multiple domains, including navigation, image\nclassification, and natural language processing. One weakness of AL is the need\nfor labeled auxiliary tasks, which can require human effort and domain\nexpertise to generate. Meta Learning techniques have been used to solve this\nissue by learning an additional auxiliary task generation network that can\ncreate helpful tasks for the primary network. The most prominent techniques\nrely on Bi-Level Optimization, which incurs computational cost and increased\ncode complexity. To avoid the need for Bi-Level Optimization, we present an\nRL-based approach to dynamically create auxiliary tasks. In this framework, an\nRL agent is tasked with selecting auxiliary labels for every data point in a\ntraining set. The agent is rewarded when their selection improves the\nperformance on the primary task. We also experiment with learning optimal\nstrategies for weighing the auxiliary loss per data point. On the 20-Superclass\nCIFAR100 problem, our RL approach outperforms human-labeled auxiliary tasks and\nperforms as well as a prominent Bi-Level Optimization technique. Our weight\nlearning approaches significantly outperform all of these benchmarks. For\nexample, a Weight-Aware RL-based approach helps the VGG16 architecture achieve\n80.9% test accuracy while the human-labeled auxiliary task setup achieved\n75.53%. The goal of this work is to (1) prove that RL is a viable approach to\ndynamically generate auxiliary tasks and (2) demonstrate that per-sample\nauxiliary task weights can be learned alongside the auxiliary task labels and\ncan achieve strong results.\n","authors":["Judah Goldfeder","Matthew So","Hod Lipson"],"pdf_url":"https://arxiv.org/pdf/2510.22940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18530v3","updated":"2025-10-27T02:48:09Z","published":"2025-04-25T17:54:27Z","title":"Scaling Laws For Scalable Oversight","summary":"  Scalable oversight, the process by which weaker AI systems supervise stronger\nones, has been proposed as a key strategy to control future superintelligent\nsystems. However, it is still unclear how scalable oversight itself scales. To\naddress this gap, we propose a framework that quantifies the probability of\nsuccessful oversight as a function of the capabilities of the overseer and the\nsystem being overseen. Specifically, our framework models oversight as a game\nbetween capability-mismatched players; the players have oversight-specific Elo\nscores that are a piecewise-linear function of their general intelligence, with\ntwo plateaus corresponding to task incompetence and task saturation. We\nvalidate our framework with a modified version of the game Nim and then apply\nit to four oversight games: Mafia, Debate, Backdoor Code and Wargames. For each\ngame, we find scaling laws that approximate how domain performance depends on\ngeneral AI system capability. We then build on our findings in a theoretical\nstudy of Nested Scalable Oversight (NSO), a process in which trusted models\noversee untrusted stronger models, which then become the trusted models in the\nnext step. We identify conditions under which NSO succeeds and derive\nnumerically (and in some cases analytically) the optimal number of oversight\nlevels to maximize the probability of oversight success. We also apply our\ntheory to our four oversight games, where we find that NSO success rates at a\ngeneral Elo gap of 400 are 13.5% for Mafia, 51.7% for Debate, 10.0% for\nBackdoor Code, and 9.4% for Wargames; these rates decline further when\noverseeing stronger systems.\n","authors":["Joshua Engels","David D. Baek","Subhash Kantamneni","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2504.18530v3.pdf","comment":"33 pages, 17 figures; The first three authors contributed equally"},{"id":"http://arxiv.org/abs/2510.23940v1","updated":"2025-10-27T23:47:51Z","published":"2025-10-27T23:47:51Z","title":"Modeling Biological Multifunctionality with Echo State Networks","summary":"  In this work, a three-dimensional multicomponent reaction-diffusion model has\nbeen developed, combining excitable-system dynamics with diffusion processes\nand sharing conceptual features with the FitzHugh-Nagumo model. Designed to\ncapture the spatiotemporal behavior of biological systems, particularly\nelectrophysiological processes, the model was solved numerically to generate\ntime-series data. These data were subsequently used to train and evaluate an\nEcho State Network (ESN), which successfully reproduced the system's dynamic\nbehavior. The results demonstrate that simulating biological dynamics using\ndata-driven, multifunctional ESN models is both feasible and effective.\n","authors":["Anastasia-Maria Leventi-Peetz","Jörg-Volker Peetz","Kai Weber","Nikolaos Zacharis"],"pdf_url":"https://arxiv.org/pdf/2510.23940v1.pdf","comment":"26 pages, 17 figures, 6 tables, 23 references"},{"id":"http://arxiv.org/abs/2510.23936v1","updated":"2025-10-27T23:41:42Z","published":"2025-10-27T23:41:42Z","title":"A data free neural operator enabling fast inference of 2D and 3D Navier\n  Stokes equations","summary":"  Ensemble simulations of high-dimensional flow models (e.g., Navier Stokes\ntype PDEs) are computationally prohibitive for real time applications. Neural\noperators enable fast inference but are limited by costly data requirements and\npoor generalization to 3D flows. We present a data-free operator network for\nthe Navier Stokes equations that eliminates the need for paired solution data\nand enables robust, real time inference for large ensemble forecasting. The\nphysics-grounded architecture takes initial and boundary conditions as well as\nforcing functions, yielding solutions robust to high variability and\nperturbations. Across 2D benchmarks and 3D test cases, the method surpasses\nprior neural operators in accuracy and, for ensembles, achieves greater\nefficiency than conventional numerical solvers. Notably, it delivers accurate\nsolutions of the three dimensional Navier Stokes equations, a regime not\npreviously demonstrated for data free neural operators. By uniting a\nnumerically grounded architecture with the scalability of machine learning,\nthis approach establishes a practical pathway toward data free, high fidelity\nPDE surrogates for end to end scientific simulation and prediction.\n","authors":["Junho Choi","Teng-Yuan Chang","Namjung Kim","Youngjoon Hong"],"pdf_url":"https://arxiv.org/pdf/2510.23936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23935v1","updated":"2025-10-27T23:38:00Z","published":"2025-10-27T23:38:00Z","title":"Understanding Fairness and Prediction Error through Subspace\n  Decomposition and Influence Analysis","summary":"  Machine learning models have achieved widespread success but often inherit\nand amplify historical biases, resulting in unfair outcomes. Traditional\nfairness methods typically impose constraints at the prediction level, without\naddressing underlying biases in data representations. In this work, we propose\na principled framework that adjusts data representations to balance predictive\nutility and fairness. Using sufficient dimension reduction, we decompose the\nfeature space into target-relevant, sensitive, and shared components, and\ncontrol the fairness-utility trade-off by selectively removing sensitive\ninformation. We provide a theoretical analysis of how prediction error and\nfairness gaps evolve as shared subspaces are added, and employ influence\nfunctions to quantify their effects on the asymptotic behavior of parameter\nestimates. Experiments on both synthetic and real-world datasets validate our\ntheoretical insights and show that the proposed method effectively improves\nfairness while preserving predictive performance.\n","authors":["Enze Shi","Pankaj Bhagwat","Zhixian Yang","Linglong Kong","Bei Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.23935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23931v1","updated":"2025-10-27T23:33:21Z","published":"2025-10-27T23:33:21Z","title":"Differential Privacy: Gradient Leakage Attacks in Federated Learning\n  Environments","summary":"  Federated Learning (FL) allows for the training of Machine Learning models in\na collaborative manner without the need to share sensitive data. However, it\nremains vulnerable to Gradient Leakage Attacks (GLAs), which can reveal private\ninformation from the shared model updates. In this work, we investigate the\neffectiveness of Differential Privacy (DP) mechanisms - specifically, DP-SGD\nand a variant based on explicit regularization (PDP-SGD) - as defenses against\nGLAs. To this end, we evaluate the performance of several computer vision\nmodels trained under varying privacy levels on a simple classification task,\nand then analyze the quality of private data reconstructions obtained from the\nintercepted gradients in a simulated FL environment. Our results demonstrate\nthat DP-SGD significantly mitigates the risk of gradient leakage attacks,\nalbeit with a moderate trade-off in model utility. In contrast, PDP-SGD\nmaintains strong classification performance but proves ineffective as a\npractical defense against reconstruction attacks. These findings highlight the\nimportance of empirically evaluating privacy mechanisms beyond their\ntheoretical guarantees, particularly in distributed learning scenarios where\ninformation leakage may represent an unassumable critical threat to data\nsecurity and privacy.\n","authors":["Miguel Fernandez-de-Retana","Unai Zulaika","Rubén Sánchez-Corcuera","Aitor Almeida"],"pdf_url":"https://arxiv.org/pdf/2510.23931v1.pdf","comment":"17 pages, 12 figures"},{"id":"http://arxiv.org/abs/2506.01374v2","updated":"2025-10-27T23:22:07Z","published":"2025-06-02T07:02:46Z","title":"REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving","summary":"  While model serving has unlocked unprecedented capabilities, the high cost of\nserving large-scale models continues to be a significant barrier to widespread\naccessibility and rapid innovation. Compiler optimizations have long driven\nsubstantial performance improvements, but existing compilers struggle with\nneural workloads due to the exponentially large and highly interdependent space\nof possible transformations. Although existing stochastic search techniques can\nbe effective, they are often sample-inefficient and fail to leverage the\nstructural context underlying compilation decisions. We set out to investigate\nthe research question of whether reasoning with large language models (LLMs),\nwithout any retraining, can leverage the context-aware decision space of\ncompiler optimizations to significantly improve sample efficiency. To that end,\nwe introduce a novel compilation framework (dubbed Reasoning Compiler) that\nformulates optimization as a sequential, context-aware decision process guided\nby a large language model and structured Monte Carlo tree search (MCTS). The\nLLM acts as a proposal mechanism, suggesting hardware-informed transformations\nthat reflect the current program state and accumulated performance feedback.\nMCTS incorporates the LLM-generated proposals to balance exploration and\nexploitation, facilitating structured, context-sensitive traversal of the\nexpansive compiler optimization space. By achieving substantial speedups with\nmarkedly fewer samples than leading neural compilers, our approach demonstrates\nthe potential of LLM-guided reasoning to transform the landscape of compiler\noptimization.\n","authors":["Sujun Tang","Christopher Priebe","Rohan Mahapatra","Lianhui Qin","Hadi Esmaeilzadeh"],"pdf_url":"https://arxiv.org/pdf/2506.01374v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2507.02089v2","updated":"2025-10-27T23:20:30Z","published":"2025-07-02T19:07:37Z","title":"Sample Complexity Bounds for Linear Constrained MDPs with a Generative\n  Model","summary":"  We consider infinite-horizon $\\gamma$-discounted (linear) constrained Markov\ndecision processes (CMDPs) where the objective is to find a policy that\nmaximizes the expected cumulative reward subject to expected cumulative\nconstraints. Given access to a generative model, we propose to solve CMDPs with\na primal-dual framework that can leverage any black-box unconstrained MDP\nsolver. For linear CMDPs with feature dimension $d$, we instantiate the\nframework by using mirror descent value iteration\n(\\texttt{MDVI})~\\citep{kitamura2023regularization} an example MDP solver. We\nprovide sample complexity bounds for the resulting CMDP algorithm in two cases:\n(i) relaxed feasibility, where small constraint violations are allowed, and\n(ii) strict feasibility, where the output policy is required to exactly satisfy\nthe constraint. For (i), we prove that the algorithm can return an\n$\\epsilon$-optimal policy with high probability by using\n$\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^4\\epsilon^2}\\right)$ samples. For (ii),\nwe show that the algorithm requires\n$\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^6\\epsilon^2\\zeta^2}\\right)$ samples,\nwhere $\\zeta$ is the problem-dependent Slater constant that characterizes the\nsize of the feasible region. Furthermore, we prove a lower-bound of\n$\\Omega\\left(\\frac{d^2}{(1-\\gamma)^5\\epsilon^2\\zeta^2}\\right)$ for the strict\nfeasibility setting. We note that our upper bounds under both settings exhibit\na near-optimal dependence on $d$, $\\epsilon$, and $\\zeta$. Finally, we\ninstantiate our framework for tabular CMDPs and show that it can be used to\nrecover near-optimal sample complexities in this setting.\n","authors":["Xingtu Liu","Lin F. Yang","Sharan Vaswani"],"pdf_url":"https://arxiv.org/pdf/2507.02089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23926v1","updated":"2025-10-27T23:14:59Z","published":"2025-10-27T23:14:59Z","title":"Improving the Straight-Through Estimator with Zeroth-Order Information","summary":"  We study the problem of training neural networks with quantized parameters.\nLearning low-precision quantized parameters by enabling computation of\ngradients via the Straight-Through Estimator (STE) can be challenging. While\nthe STE enables back-propagation, which is a first-order method, recent works\nhave explored the use of zeroth-order (ZO) gradient descent for fine-tuning. We\nnote that the STE provides high-quality biased gradients, and ZO gradients are\nunbiased but can be expensive. We thus propose First-Order-Guided Zeroth-Order\nGradient Descent (FOGZO) that reduces STE bias while reducing computations\nrelative to ZO methods. Empirically, we show FOGZO improves the tradeoff\nbetween quality and training time in Quantization-Aware Pre-Training.\nSpecifically, versus STE at the same number of iterations, we show a 1-8\\%\naccuracy improvement for DeiT Tiny/Small, 1-2\\% accuracy improvement on ResNet\n18/50, and 1-22 perplexity point improvement for LLaMA models with up to 0.3\nbillion parameters. For the same loss, FOGZO yields a 796$\\times$ reduction in\ncomputation versus n-SPSA for a 2-layer MLP on MNIST. Code is available at\nhttps://github.com/1733116199/fogzo.\n","authors":["Ningfeng Yang","Tor M. Aamodt"],"pdf_url":"https://arxiv.org/pdf/2510.23926v1.pdf","comment":"39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)"},{"id":"http://arxiv.org/abs/2510.23921v1","updated":"2025-10-27T23:05:12Z","published":"2025-10-27T23:05:12Z","title":"Breaking the Benchmark: Revealing LLM Bias via Minimal Contextual\n  Augmentation","summary":"  Large Language Models have been shown to demonstrate stereotypical biases in\ntheir representations and behavior due to the discriminative nature of the data\nthat they have been trained on. Despite significant progress in the development\nof methods and models that refrain from using stereotypical information in\ntheir decision-making, recent work has shown that approaches used for bias\nalignment are brittle. In this work, we introduce a novel and general\naugmentation framework that involves three plug-and-play steps and is\napplicable to a number of fairness evaluation benchmarks. Through application\nof augmentation to a fairness evaluation dataset (Bias Benchmark for Question\nAnswering (BBQ)), we find that Large Language Models (LLMs), including\nstate-of-the-art open and closed weight models, are susceptible to\nperturbations to their inputs, showcasing a higher likelihood to behave\nstereotypically. Furthermore, we find that such models are more likely to have\nbiased behavior in cases where the target demographic belongs to a community\nless studied by the literature, underlining the need to expand the fairness and\nsafety research to include more diverse communities.\n","authors":["Kaveh Eskandari Miandoab","Mahammed Kamruzzaman","Arshia Gharooni","Gene Louis Kim","Vasanth Sarathy","Ninareh Mehrabi"],"pdf_url":"https://arxiv.org/pdf/2510.23921v1.pdf","comment":"9 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2510.15141v2","updated":"2025-10-27T23:02:56Z","published":"2025-10-16T20:59:46Z","title":"Beyond PCA: Manifold Dimension Estimation via Local Graph Structure","summary":"  Local principal component analysis (Local PCA) has proven to be an effective\ntool for estimating the intrinsic dimension of a manifold. More recently,\ncurvature-adjusted PCA (CA-PCA) has improved upon this approach by explicitly\naccounting for the curvature of the underlying manifold, rather than assuming\nlocal flatness. Building on these insights, we propose a general framework for\nmanifold dimension estimation that captures the manifold's local graph\nstructure by integrating PCA with regression-based techniques. Within this\nframework, we introduce two representative estimators: quadratic embedding (QE)\nand total least squares (TLS). Experiments on both synthetic and real-world\ndatasets demonstrate that these methods perform competitively with, and often\noutperform, state-of-the-art alternatives.\n","authors":["Zelong Bi","Pierre Lafaye de Micheaux"],"pdf_url":"https://arxiv.org/pdf/2510.15141v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02793v2","updated":"2025-10-27T23:02:35Z","published":"2025-06-03T12:16:46Z","title":"Doubly-Robust Estimation of Counterfactual Policy Mean Embeddings","summary":"  Estimating the distribution of outcomes under counterfactual policies is\ncritical for decision-making in domains such as recommendation, advertising,\nand healthcare. We propose and analyze a novel framework-Counterfactual Policy\nMean Embedding (CPME)-that represents the entire counterfactual outcome\ndistribution in a reproducing kernel Hilbert space (RKHS), enabling flexible\nand nonparametric distributional off-policy evaluation. We introduce both a\nplug-in estimator and a doubly robust estimator; the latter enjoys improved\nconvergence rates by correcting for bias in both the outcome embedding and\npropensity models. Building on this, we develop a doubly robust kernel test\nstatistic for hypothesis testing, which achieves asymptotic normality and thus\nenables computationally efficient testing and straightforward construction of\nconfidence intervals. Our framework also supports sampling from the\ncounterfactual distribution. Numerical simulations illustrate the practical\nbenefits of CPME over existing methods.\n","authors":["Houssam Zenati","Bariscan Bozkurt","Arthur Gretton"],"pdf_url":"https://arxiv.org/pdf/2506.02793v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04536v3","updated":"2025-10-27T22:48:13Z","published":"2025-06-05T01:01:18Z","title":"NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to\n  Capture Experimental Variability in Biological Neuron Models","summary":"  Characterizing the cellular properties of neurons is fundamental to\nunderstanding their function in the brain. In this quest, the generation of\nbio-realistic models is central towards integrating multimodal cellular data\nsets and establishing causal relationships. However, current modeling\napproaches remain constrained by the limited availability and intrinsic\nvariability of experimental neuronal data. The deterministic formalism of\nbio-realistic models currently precludes accounting for the natural variability\nobserved experimentally. While deep learning is becoming increasingly relevant\nin this space, it fails to capture the full biophysical complexity of neurons,\ntheir nonlinear voltage dynamics, and variability. To address these\nshortcomings, we introduce NOBLE, a neural operator framework that learns a\nmapping from a continuous frequency-modulated embedding of interpretable neuron\nfeatures to the somatic voltage response induced by current injection. Trained\non synthetic data generated from bio-realistic neuron models, NOBLE predicts\ndistributions of neural dynamics accounting for the intrinsic experimental\nvariability. Unlike conventional bio-realistic neuron models, interpolating\nwithin the embedding space offers models whose dynamics are consistent with\nexperimentally observed responses. NOBLE enables the efficient generation of\nsynthetic neurons that closely resemble experimental data and exhibit\ntrial-to-trial variability, offering a $4200\\times$ speedup over the numerical\nsolver. NOBLE is the first scaled-up deep learning framework that validates its\ngeneralization with real experimental data. To this end, NOBLE captures\nfundamental neural properties in a unique and emergent manner that opens the\ndoor to a better understanding of cellular composition and computations,\nneuromorphic architectures, large-scale brain circuits, and general neuroAI\napplications.\n","authors":["Luca Ghafourpour","Valentin Duruisseaux","Bahareh Tolooshams","Philip H. Wong","Costas A. Anastassiou","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2506.04536v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23914v1","updated":"2025-10-27T22:42:53Z","published":"2025-10-27T22:42:53Z","title":"Geometry-Inspired Unified Framework for Discounted and Average Reward\n  MDPs","summary":"  The theoretical analysis of Markov Decision Processes (MDPs) is commonly\nsplit into two cases - the average-reward case and the discounted-reward case -\nwhich, while sharing similarities, are typically analyzed separately. In this\nwork, we extend a recently introduced geometric interpretation of MDPs for the\ndiscounted-reward case to the average-reward case, thereby unifying both. This\nallows us to extend a major result known for the discounted-reward case to the\naverage-reward case: under a unique and ergodic optimal policy, the Value\nIteration algorithm achieves a geometric convergence rate.\n","authors":["Arsenii Mustafin","Xinyi Sheng","Dominik Baumann"],"pdf_url":"https://arxiv.org/pdf/2510.23914v1.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2505.21923v2","updated":"2025-10-27T22:42:49Z","published":"2025-05-28T03:16:08Z","title":"FALCON: An ML Framework for Fully Automated Layout-Constrained Analog\n  Circuit Design","summary":"  Designing analog circuits from performance specifications is a complex,\nmulti-stage process encompassing topology selection, parameter inference, and\nlayout feasibility. We introduce FALCON, a unified machine learning framework\nthat enables fully automated, specification-driven analog circuit synthesis\nthrough topology selection and layout-constrained optimization. Given a target\nperformance, FALCON first selects an appropriate circuit topology using a\nperformance-driven classifier guided by human design heuristics. Next, it\nemploys a custom, edge-centric graph neural network trained to map circuit\ntopology and parameters to performance, enabling gradient-based parameter\ninference through the learned forward model. This inference is guided by a\ndifferentiable layout cost, derived from analytical equations capturing\nparasitic and frequency-dependent effects, and constrained by design rules. We\ntrain and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave\ncircuits, generated and simulated using Cadence Spectre across 20\nexpert-designed topologies. Through this evaluation, FALCON demonstrates >99%\naccuracy in topology inference, <10% relative error in performance prediction,\nand efficient layout-aware design that completes in under 1 second per\ninstance. Together, these results position FALCON as a practical and extensible\nfoundation model for end-to-end analog circuit design automation.\n","authors":["Asal Mehradfar","Xuzhe Zhao","Yilun Huang","Emir Ceyani","Yankai Yang","Shihao Han","Hamidreza Aghasi","Salman Avestimehr"],"pdf_url":"https://arxiv.org/pdf/2505.21923v2.pdf","comment":"Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.23912v1","updated":"2025-10-27T22:39:34Z","published":"2025-10-27T22:39:34Z","title":"Key and Value Weights Are Probably All You Need: On the Necessity of the\n  Query, Key, Value weight Triplet in Decoder-Only Transformers","summary":"  The Query, Key, Value weight triplet is a building block of current attention\nmechanisms in state-of-the-art LLMs. We theoretically investigate whether this\ntriplet can be reduced, proving under simplifying assumptions that the Query\nweights are redundant, thereby reducing the number of non-embedding/lm-head\nparameters by over 8%. We validate the theory on full-complexity GPT-3 small\narchitectures (with layer normalization, skip connections, and weight decay)\ntrained from scratch, demonstrating that the reduced model achieves comparable\nvalidation loss to standard baselines. These findings motivate the\ninvestigation of the Query weight redundancy at scale.\n","authors":["Marko Karbevski","Antonij Mijoski"],"pdf_url":"https://arxiv.org/pdf/2510.23912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23907v1","updated":"2025-10-27T22:29:08Z","published":"2025-10-27T22:29:08Z","title":"DynaStride: Dynamic Stride Windowing with MMCoT for Instructional\n  Multi-Scene Captioning","summary":"  Scene-level captioning in instructional videos can enhance learning by\nrequiring an understanding of both visual cues and temporal structure. By\naligning visual cues with textual guidance, this understanding supports\nprocedural learning and multimodal reasoning, providing a richer context for\nskill acquisition. However, captions that fail to capture this structure may\nlack coherence and quality, which can create confusion and undermine the\nvideo's educational intent. To address this gap, we introduce DynaStride, a\npipeline to generate coherent, scene-level captions without requiring manual\nscene segmentation. Using the YouCookII dataset's scene annotations, DynaStride\nperforms adaptive frame sampling and multimodal windowing to capture key\ntransitions within each scene. It then employs a multimodal chain-of-thought\nprocess to produce multiple action-object pairs, which are refined and fused\nusing a dynamic stride window selection algorithm that adaptively balances\ntemporal context and redundancy. The final scene-level caption integrates\nvisual semantics and temporal reasoning in a single instructional caption.\nEmpirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,\ndemonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and\nsemantic similarity measures (BERTScore, CLIPScore). Qualitative analyses\nfurther show that DynaStride produces captions that are more temporally\ncoherent and informative, suggesting a promising direction for improving\nAI-powered instructional content generation.\n","authors":["Eddison Pham","Prisha Priyadarshini","Adrian Maliackel","Kanishk Bandi","Cristian Meo","Kevin Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.23907v1.pdf","comment":"16 pages, 15 figures, 5 Tables, submitted to AAAI AI4ED Workshop 2026"},{"id":"http://arxiv.org/abs/2510.23906v1","updated":"2025-10-27T22:26:20Z","published":"2025-10-27T22:26:20Z","title":"Group Interventions on Deep Networks for Causal Discovery in Subsystems","summary":"  Causal discovery uncovers complex relationships between variables, enhancing\npredictions, decision-making, and insights into real-world systems, especially\nin nonlinear multivariate time series. However, most existing methods primarily\nfocus on pairwise cause-effect relationships, overlooking interactions among\ngroups of variables, i.e., subsystems and their collective causal influence. In\nthis study, we introduce gCDMI, a novel multi-group causal discovery method\nthat leverages group-level interventions on trained deep neural networks and\nemploys model invariance testing to infer causal relationships. Our approach\ninvolves three key steps. First, we use deep learning to jointly model the\nstructural relationships among groups of all time series. Second, we apply\ngroup-wise interventions to the trained model. Finally, we conduct model\ninvariance testing to determine the presence of causal links among variable\ngroups. We evaluate our method on simulated datasets, demonstrating its\nsuperior performance in identifying group-level causal relationships compared\nto existing methods. Additionally, we validate our approach on real-world\ndatasets, including brain networks and climate ecosystems. Our results\nhighlight that applying group-level interventions to deep learning models,\ncombined with invariance testing, can effectively reveal complex causal\nstructures, offering valuable insights for domains such as neuroscience and\nclimate science.\n","authors":["Wasim Ahmad","Maha Shadaydeh","Joachim Denzler"],"pdf_url":"https://arxiv.org/pdf/2510.23906v1.pdf","comment":"Submitted to IEEE Access. We are working on the revised version"},{"id":"http://arxiv.org/abs/2510.23905v1","updated":"2025-10-27T22:23:53Z","published":"2025-10-27T22:23:53Z","title":"Inferring Group Intent as a Cooperative Game. An NLP-based Framework for\n  Trajectory Analysis using Graph Transformer Neural Network","summary":"  This paper studies group target trajectory intent as the outcome of a\ncooperative game where the complex-spatio trajectories are modeled using an\nNLP-based generative model. In our framework, the group intent is specified by\nthe characteristic function of a cooperative game, and allocations for players\nin the cooperative game are specified by either the core, the Shapley value, or\nthe nucleolus. The resulting allocations induce probability distributions that\ngovern the coordinated spatio-temporal trajectories of the targets that reflect\nthe group's underlying intent. We address two key questions: (1) How can the\nintent of a group trajectory be optimally formalized as the characteristic\nfunction of a cooperative game? (2) How can such intent be inferred from noisy\nobservations of the targets? To answer the first question, we introduce a\nFisher-information-based characteristic function of the cooperative game, which\nyields probability distributions that generate coordinated spatio-temporal\npatterns. As a generative model for these patterns, we develop an NLP-based\ngenerative model built on formal grammar, enabling the creation of realistic\nmulti-target trajectory data. To answer the second question, we train a Graph\nTransformer Neural Network (GTNN) to infer group trajectory intent-expressed as\nthe characteristic function of the cooperative game-from observational data\nwith high accuracy. The self-attention function of the GTNN depends on the\ntrack estimates. Thus, the formulation and algorithms provide a multi-layer\napproach that spans target tracking (Bayesian signal processing) and the GTNN\n(for group intent inference).\n","authors":["Yiming Zhang","Vikram Krishnamurthy","Shashwat Jain"],"pdf_url":"https://arxiv.org/pdf/2510.23905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23901v1","updated":"2025-10-27T22:17:09Z","published":"2025-10-27T22:17:09Z","title":"RS-ORT: A Reduced-Space Branch-and-Bound Algorithm for Optimal\n  Regression Trees","summary":"  Mixed-integer programming (MIP) has emerged as a powerful framework for\nlearning optimal decision trees. Yet, existing MIP approaches for regression\ntasks are either limited to purely binary features or become computationally\nintractable when continuous, large-scale data are involved. Naively binarizing\ncontinuous features sacrifices global optimality and often yields needlessly\ndeep trees. We recast the optimal regression-tree training as a two-stage\noptimization problem and propose Reduced-Space Optimal Regression Trees\n(RS-ORT) - a specialized branch-and-bound (BB) algorithm that branches\nexclusively on tree-structural variables. This design guarantees the\nalgorithm's convergence and its independence from the number of training\nsamples. Leveraging the model's structure, we introduce several bound\ntightening techniques - closed-form leaf prediction, empirical threshold\ndiscretization, and exact depth-1 subtree parsing - that combine with\ndecomposable upper and lower bounding strategies to accelerate the training.\nThe BB node-wise decomposition enables trivial parallel execution, further\nalleviating the computational intractability even for million-size datasets.\nBased on the empirical studies on several regression benchmarks containing both\nbinary and continuous features, RS-ORT also delivers superior training and\ntesting performance than state-of-the-art methods. Notably, on datasets with up\nto 2,000,000 samples with continuous features, RS-ORT can obtain guaranteed\ntraining performance with a simpler tree structure and a better generalization\nability in four hours.\n","authors":["Cristobal Heredia","Pedro Chumpitaz-Flores","Kaixun Hua"],"pdf_url":"https://arxiv.org/pdf/2510.23901v1.pdf","comment":"20 pages, 1 figure, uses ICLR 2026 LaTeX style. Submitted to arXiv as\n  a preprint version"},{"id":"http://arxiv.org/abs/2509.00103v2","updated":"2025-10-27T22:13:12Z","published":"2025-08-27T21:09:51Z","title":"Pre-trained knowledge elevates large language models beyond traditional\n  chemical reaction optimizers","summary":"  Modern optimization in experimental chemistry employs algorithmic search\nthrough black-box parameter spaces. Here we demonstrate that pre-trained\nknowledge in large language models (LLMs) fundamentally changes this paradigm.\nUsing six fully enumerated categorical reaction datasets (768-5,684\nexperiments), we benchmark LLM-guided optimization (LLM-GO) against Bayesian\noptimization (BO) and random sampling. Frontier LLMs consistently match or\nexceed BO performance across five single-objective datasets, with advantages\ngrowing as parameter complexity increases and high-performing conditions become\nscarce (<5% of space). BO retains superiority only for explicit multi-objective\ntrade-offs. To understand these contrasting behaviors, we introduce a\ntopology-agnostic information theory framework quantifying sampling diversity\nthroughout optimization campaigns. This analysis reveals that LLMs maintain\nsystematically higher exploration Shannon entropy than BO across all datasets\nwhile achieving superior performance, with advantages most pronounced in\nsolution-scarce parameter spaces where high-entropy exploration typically\nfails-suggesting that pre-trained domain knowledge enables more effective\nnavigation of chemical parameter space rather than replacing structured\nexploration strategies. To enable transparent benchmarking and community\nvalidation, we release Iron Mind (https://gomes.andrew.cmu.edu/iron-mind), a\nno-code platform for side-by-side evaluation of human, algorithmic, and LLM\noptimization campaigns with public leaderboards and complete trajectories. Our\nfindings establish that LLM-GO excels precisely where traditional methods\nstruggle: complex categorical spaces requiring domain understanding rather than\nmathematical optimization.\n","authors":["Robert MacKnight","Jose Emilio Regio","Jeffrey G. Ethier","Luke A. Baldwin","Gabe Gomes"],"pdf_url":"https://arxiv.org/pdf/2509.00103v2.pdf","comment":"27 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.23891v1","updated":"2025-10-27T22:00:49Z","published":"2025-10-27T22:00:49Z","title":"PRO: Enabling Precise and Robust Text Watermark for Open-Source LLMs","summary":"  Text watermarking for large language models (LLMs) enables model owners to\nverify text origin and protect intellectual property. While watermarking\nmethods for closed-source LLMs are relatively mature, extending them to\nopen-source models remains challenging, as developers cannot control the\ndecoding process. Consequently, owners of open-source LLMs lack practical means\nto verify whether text was generated by their models. A core difficulty lies in\nembedding watermarks directly into model weights without hurting detectability.\nA promising idea is to distill watermarks from a closed-source model into an\nopen one, but this suffers from (i) poor detectability due to mismatch between\nlearned and predefined patterns, and (ii) fragility to downstream modifications\nsuch as fine-tuning or model merging. To overcome these limitations, we propose\nPRO, a Precise and Robust text watermarking method for open-source LLMs. PRO\njointly trains a watermark policy model with the LLM, producing patterns that\nare easier for the model to learn and more consistent with detection criteria.\nA regularization term further simulates downstream perturbations and penalizes\ndegradation in watermark detectability, ensuring robustness under model edits.\nExperiments on open-source LLMs (e.g., LLaMA-3.2, LLaMA-3, Phi-2) show that PRO\nsubstantially improves both watermark detectability and resilience to model\nmodifications.\n","authors":["Jiaqi Xue","Yifei Zhao","Mansour Al Ghanim","Shangqian Gao","Ruimin Sun","Qian Lou","Mengxin Zheng"],"pdf_url":"https://arxiv.org/pdf/2510.23891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.10712v2","updated":"2025-10-27T21:57:37Z","published":"2025-09-12T22:06:57Z","title":"MinatoLoader: Accelerating Machine Learning Training Through Efficient\n  Data Preprocessing","summary":"  Data loaders are used by Machine Learning (ML) frameworks like PyTorch and\nTensorFlow to apply transformations to data before feeding it into the\naccelerator. This operation is called data preprocessing. Data preprocessing\nplays an important role in the ML training workflow because if it is\ninefficiently pipelined with the training, it can yield high GPU idleness,\nresulting in important training delays. Unfortunately, existing data loaders\nturn out to waste GPU resources, with $76\\%$ GPU idleness when using the\nPyTorch data loader, for example. One key source of inefficiency is the\nvariability in preprocessing time across samples within the same dataset.\nExisting data loaders are oblivious to this variability, and they construct\nbatches without any consideration of slow or fast samples. In this case, the\nentire batch is delayed by a single slow sample, stalling the training pipeline\nand resulting in head-of-line blocking.\n  To address these inefficiencies, we present MinatoLoader, a general-purpose\ndata loader for PyTorch that accelerates training and improves GPU utilization.\nMinatoLoader is designed for a single-server setup, containing multiple GPUs.\nIt continuously prepares data in the background and actively constructs batches\nby prioritizing fast-to-preprocess samples, while slower samples are processed\nin parallel.\n  We evaluate MinatoLoader on servers with V100 and A100 GPUs. On a machine\nwith four A100 GPUs, MinatoLoader improves the training time of a wide range of\nworkloads by up to $7.5\\times$ ($3.6\\times$ on average) over PyTorch DataLoader\nand Pecan, and up to $3\\times$ ($2.2\\times$ on average) over DALI. It also\nincreases average GPU utilization from 46.4\\% with PyTorch to 90.45\\%, while\npreserving model accuracy and enabling faster convergence.\n","authors":["Rahma Nouaji","Stella Bitchebe","Ricardo Macedo","Oana Balmau"],"pdf_url":"https://arxiv.org/pdf/2509.10712v2.pdf","comment":"Paper accepted at EuroSys 2026"},{"id":"http://arxiv.org/abs/2505.14969v2","updated":"2025-10-27T21:48:48Z","published":"2025-05-20T23:12:16Z","title":"STree: Speculative Tree Decoding for Hybrid State-Space Models","summary":"  Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree.\n","authors":["Yangchao Wu","Zongyue Qin","Alex Wong","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2505.14969v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23881v1","updated":"2025-10-27T21:43:39Z","published":"2025-10-27T21:43:39Z","title":"Generating Creative Chess Puzzles","summary":"  While Generative AI rapidly advances in various domains, generating truly\ncreative, aesthetic, and counter-intuitive outputs remains a challenge. This\npaper presents an approach to tackle these difficulties in the domain of chess\npuzzles. We start by benchmarking Generative AI architectures, and then\nintroduce an RL framework with novel rewards based on chess engine search\nstatistics to overcome some of those shortcomings. The rewards are designed to\nenhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.\nOur RL approach dramatically increases counter-intuitive puzzle generation by\n10x, from 0.22\\% (supervised) to 2.5\\%, surpassing existing dataset rates\n(2.1\\%) and the best Lichess-trained model (0.4\\%). Our puzzles meet novelty\nand diversity benchmarks, retain aesthetic themes, and are rated by human\nexperts as more creative, enjoyable, and counter-intuitive than composed book\npuzzles, even approaching classic compositions. Our final outcome is a curated\nbooklet of these AI-generated puzzles, which is acknowledged for creativity by\nthree world-renowned experts.\n","authors":["Xidong Feng","Vivek Veeriah","Marcus Chiam","Michael Dennis","Ryan Pachauri","Thomas Tumiel","Federico Barbero","Johan Obando-Ceron","Jiaxin Shi","Satinder Singh","Shaobo Hou","Nenad Tomašev","Tom Zahavy"],"pdf_url":"https://arxiv.org/pdf/2510.23881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23879v1","updated":"2025-10-27T21:39:25Z","published":"2025-10-27T21:39:25Z","title":"Artificial Intelligence Based Predictive Maintenance for Electric Buses","summary":"  Predictive maintenance (PdM) is crucial for optimizing efficiency and\nminimizing downtime of electric buses. While these vehicles provide\nenvironmental benefits, they pose challenges for PdM due to complex electric\ntransmission and battery systems. Traditional maintenance, often based on\nscheduled inspections, struggles to capture anomalies in multi-dimensional\nreal-time CAN Bus data. This study employs a graph-based feature selection\nmethod to analyze relationships among CAN Bus parameters of electric buses and\ninvestigates the prediction performance of targeted alarms using artificial\nintelligence techniques. The raw data collected over two years underwent\nextensive preprocessing to ensure data quality and consistency. A hybrid\ngraph-based feature selection tool was developed by combining statistical\nfiltering (Pearson correlation, Cramer's V, ANOVA F-test) with\noptimization-based community detection algorithms (InfoMap, Leiden, Louvain,\nFast Greedy). Machine learning models, including SVM, Random Forest, and\nXGBoost, were optimized through grid and random search with data balancing via\nSMOTEEN and binary search-based down-sampling. Model interpretability was\nachieved using LIME to identify the features influencing predictions. The\nresults demonstrate that the developed system effectively predicts vehicle\nalarms, enhances feature interpretability, and supports proactive maintenance\nstrategies aligned with Industry 4.0 principles.\n","authors":["Ayse Irmak Ercevik","Ahmet Murat Ozbayoglu"],"pdf_url":"https://arxiv.org/pdf/2510.23879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09583v2","updated":"2025-10-27T21:37:51Z","published":"2025-02-13T18:41:55Z","title":"Learning to Coordinate with Experts","summary":"  When deployed in the real world, AI agents will inevitably face challenges\nthat exceed their individual capabilities. Leveraging assistance from experts,\nwhether humans or highly capable AI systems, can significantly improve both\nsafety and performance in such situations. Since expert assistance is costly, a\ncentral challenge is determining when to consult an expert. In this paper, we\nexplore a novel variant of this problem, termed YRC-0, in which an agent must\nlearn to collaborate with an expert in new environments in an unsupervised\nmanner--that is, without interacting with the expert during training. This\nsetting motivates the development of low-cost, robust approaches for training\nexpert-leveraging agents. To support research in this area, we introduce\nYRC-Bench, an open-source benchmark that instantiates YRC-0 across diverse\nenvironments. YRC-Bench provides a standardized Gym-like API, simulated\nexperts, an evaluation pipeline, and implementations of popular baselines.\nToward tackling YRC-0, we propose a validation strategy and evaluate a range of\nlearning methods, offering insights that can inform future research. Codebase:\ngithub.com/modanesh/YRC-Bench\n","authors":["Mohamad H. Danesh","Nguyen X. Khanh","Tu Trinh","Benjamin Plaut"],"pdf_url":"https://arxiv.org/pdf/2502.09583v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.18239v2","updated":"2025-10-27T21:18:47Z","published":"2025-10-21T02:53:17Z","title":"LIME: Link-based user-item Interaction Modeling with decoupled xor\n  attention for Efficient test time scaling","summary":"  Scaling large recommendation systems requires advancing three major\nfrontiers: processing longer user histories, expanding candidate sets, and\nincreasing model capacity. While promising, transformers' computational cost\nscales quadratically with the user sequence length and linearly with the number\nof candidates. This trade-off makes it prohibitively expensive to expand\ncandidate sets or increase sequence length at inference, despite the\nsignificant performance improvements.\n  We introduce \\textbf{LIME}, a novel architecture that resolves this\ntrade-off. Through two key innovations, LIME fundamentally reduces\ncomputational complexity. First, low-rank ``link embeddings\" enable\npre-computation of attention weights by decoupling user and candidate\ninteractions, making the inference cost nearly independent of candidate set\nsize. Second, a linear attention mechanism, \\textbf{LIME-XOR}, reduces the\ncomplexity with respect to user sequence length from quadratic ($O(N^2)$) to\nlinear ($O(N)$).\n  Experiments on public and industrial datasets show LIME achieves near-parity\nwith state-of-the-art transformers but with a 10$\\times$ inference speedup on\nlarge candidate sets or long sequence lengths. When tested on a major\nrecommendation platform, LIME improved user engagement while maintaining\nminimal inference costs with respect to candidate set size and user history\nlength, establishing a new paradigm for efficient and expressive recommendation\nsystems.\n","authors":["Yunjiang Jiang","Ayush Agarwal","Yang Liu","Bi Xue"],"pdf_url":"https://arxiv.org/pdf/2510.18239v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2510.23868v1","updated":"2025-10-27T21:18:19Z","published":"2025-10-27T21:18:19Z","title":"GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and\n  UNA","summary":"  I propose \\textbf{G}roup-relative \\textbf{I}mplicit \\textbf{F}ine\n\\textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning\nLLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT\nminimizes the discrepancy between implicit and explicit reward models. It\ncombines three key ideas: (1) the online multi-response generation and\nnormalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the\nimplicit-explicit reward alignment principle of UNA. By jointly normalizing the\nimplicit and explicit rewards, GIFT eliminates an otherwise intractable term\nthat prevents effective use of implicit rewards. This normalization transforms\nthe complex reward maximization objective into a simple mean squared error\n(MSE) loss between the normalized reward functions, converting a non-convex\noptimization problem into a convex, stable, and analytically differentiable\nformulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy\nand thus retains exploration capability. Compared to GRPO, it requires fewer\nhyperparameters, converges faster, and generalizes better with significantly\nreduced training overfitting. Empirically, GIFT achieves superior reasoning and\nalignment performance on mathematical benchmarks while remaining\ncomputationally efficient.\n","authors":["Zhichao Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23866v1","updated":"2025-10-27T21:17:03Z","published":"2025-10-27T21:17:03Z","title":"A PDE-Informed Latent Diffusion Model for 2-m Temperature Downscaling","summary":"  This work presents a physics-conditioned latent diffusion model tailored for\ndynamical downscaling of atmospheric data, with a focus on reconstructing\nhigh-resolution 2-m temperature fields. Building upon a pre-existing diffusion\narchitecture and employing a residual formulation against a reference UNet, we\nintegrate a partial differential equation (PDE) loss term into the model's\ntraining objective. The PDE loss is computed in the full resolution (pixel)\nspace by decoding the latent representation and is designed to enforce physical\nconsistency through a finite-difference approximation of an effective\nadvection-diffusion balance. Empirical observations indicate that conventional\ndiffusion training already yields low PDE residuals, and we investigate how\nfine-tuning with this additional loss further regularizes the model and\nenhances the physical plausibility of the generated fields. The entirety of our\ncodebase is available on Github, for future reference and development.\n","authors":["Paul Rosu","Muchang Bahng","Erick Jiang","Rico Zhu","Vahid Tarokh"],"pdf_url":"https://arxiv.org/pdf/2510.23866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.05024v3","updated":"2025-10-27T21:05:19Z","published":"2025-10-06T17:02:59Z","title":"Inoculation Prompting: Instructing LLMs to misbehave at train-time\n  improves test-time alignment","summary":"  Large language models are sometimes trained with imperfect oversight signals,\nleading to undesired behaviors such as reward hacking and sycophancy. Improving\noversight quality can be expensive or infeasible, motivating methods that\nimprove learned behavior despite an imperfect training signal. We introduce\nInoculation Prompting (IP), a simple but counterintuitive technique that\nprevents learning of an undesired behavior by modifying training prompts to\nexplicitly request it. For example, to inoculate against reward hacking, we\nmodify the prompts used in supervised fine-tuning to request code that only\nworks on provided test cases but fails on other inputs. Across four settings we\nfind that IP reduces the learning of undesired behavior without substantially\nreducing the learning of desired capabilities. We also show that prompts which\nmore strongly elicit the undesired behavior prior to fine-tuning more\neffectively inoculate against the behavior when used during training; this\nserves as a heuristic to identify promising inoculation prompts. Overall, IP is\na simple yet effective way to control how models generalize from fine-tuning,\npreventing learning of undesired behaviors without substantially disrupting\ndesired capabilities.\n","authors":["Nevan Wichers","Aram Ebtekar","Ariana Azarbal","Victor Gillioz","Christine Ye","Emil Ryd","Neil Rathi","Henry Sleight","Alex Mallen","Fabien Roger","Samuel Marks"],"pdf_url":"https://arxiv.org/pdf/2510.05024v3.pdf","comment":"v2 Updates references. v3 Updates references; Adds IFEval results;\n  Improves appendix readability; Adds author contributions"},{"id":"http://arxiv.org/abs/2411.10959v3","updated":"2025-10-27T20:57:45Z","published":"2024-11-17T04:43:04Z","title":"Program Evaluation with Remotely Sensed Outcomes","summary":"  Economists often estimate treatment effects in experiments using remotely\nsensed variables (RSVs), e.g., satellite images or mobile phone activity, in\nplace of directly measured economic outcomes. A common practice is to use an\nobservational sample to train a predictor of the economic outcome from the RSV,\nand then use these predictions as the outcomes in the experiment. We show that\nthis method is biased whenever the RSV is a post-outcome variable, meaning that\nvariation in the economic outcome causes variation in the RSV. For example,\nchanges in poverty or environmental quality cause changes in satellite images,\nbut not vice versa. As our main result, we nonparametrically identify the\ntreatment effect by formalizing the intuition underlying common practice: the\nconditional distribution of the RSV given the outcome and treatment is stable\nacross samples. Our identifying formula reveals that efficient inference\nrequires predictions of three quantities from the RSV -- the outcome,\ntreatment, and sample indicator -- whereas common practice only predicts the\noutcome. Valid inference does not require any rate conditions on RSV\npredictions, justifying the use of complex deep learning algorithms with\nunknown statistical properties. We reanalyze the effect of an anti-poverty\nprogram in India using satellite images.\n","authors":["Ashesh Rambachan","Rahul Singh","Davide Viviano"],"pdf_url":"https://arxiv.org/pdf/2411.10959v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14527v2","updated":"2025-10-27T20:50:49Z","published":"2025-02-20T13:01:07Z","title":"Inter-turbine Modelling of Wind-Farm Power using Multi-task Learning","summary":"  Because of the global need to increase power production from renewable energy\nresources, developments in the online monitoring of the associated\ninfrastructure is of interest to reduce operation and maintenance costs.\nHowever, challenges exist for data-driven approaches to this problem, such as\nincomplete or limited histories of labelled damage-state data, operational and\nenvironmental variability, or the desire for the quantification of uncertainty\nto support risk management.\n  This work first introduces a probabilistic regression model for predicting\nwind-turbine power, which adjusts for wake effects learnt from data. Spatial\ncorrelations in the learned model parameters for different tasks (turbines) are\nthen leveraged in a hierarchical Bayesian model (an approach to multi-task\nlearning) to develop a \"metamodel\", which can be used to make power-predictions\nwhich adjust for turbine location - including on previously unobserved turbines\nnot included in the training data. The results show that the metamodel is able\nto outperform a series of benchmark models, and demonstrates a novel strategy\nfor making efficient use of data for inference in populations of structures, in\nparticular where correlations exist in the variable(s) of interest (such as\nthose from wind-turbine wake-effects).\n","authors":["Simon M. Brealy","Lawrence A. Bull","Pauline Beltrando","Anders Sommer","Nikolaos Dervilis","Keith Worden"],"pdf_url":"https://arxiv.org/pdf/2502.14527v2.pdf","comment":"Preprint submitted to Mechanical Systems and Signal Processing. A\n  shortened version of this article has submitted to the Wind Energy Science\n  Conference 2025"},{"id":"http://arxiv.org/abs/2510.23831v1","updated":"2025-10-27T20:17:34Z","published":"2025-10-27T20:17:34Z","title":"Testing-driven Variable Selection in Bayesian Modal Regression","summary":"  We propose a Bayesian variable selection method in the framework of modal\nregression for heavy-tailed responses. An efficient expectation-maximization\nalgorithm is employed to expedite parameter estimation. A test statistic is\nconstructed to exploit the shape of the model error distribution to effectively\nseparate informative covariates from unimportant ones. Through simulations, we\ndemonstrate and evaluate the efficacy of the proposed method in identifying\nimportant covariates in the presence of non-Gaussian model errors. Finally, we\napply the proposed method to analyze two datasets arising in genetic and\nepigenetic studies.\n","authors":["Jiasong Duan","Hongmei Zhang","Xianzheng Huang"],"pdf_url":"https://arxiv.org/pdf/2510.23831v1.pdf","comment":"30 pages, 2 figures, preprint under review"},{"id":"http://arxiv.org/abs/2304.07647v7","updated":"2025-10-27T20:14:22Z","published":"2023-04-15T22:24:05Z","title":"LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene\n  Graphs with Weak Supervision","summary":"  Supervised approaches for learning spatio-temporal scene graphs (STSG) from\nvideo are greatly hindered due to their reliance on STSG-annotated videos,\nwhich are labor-intensive to construct at scale. Is it feasible to instead use\nreadily available video captions as weak supervision? To address this question,\nwe propose LASER, a neuro-symbolic framework to enable training STSG generators\nusing only video captions. LASER employs large language models to first extract\nlogical specifications with rich spatio-temporal semantic information from\nvideo captions. LASER then trains the underlying STSG generator to align the\npredicted STSG with the specification. The alignment algorithm overcomes the\nchallenges of weak supervision by leveraging a differentiable symbolic reasoner\nand using a combination of contrastive, temporal, and semantics losses. The\noverall approach efficiently trains low-level perception models to extract a\nfine-grained STSG that conforms to the video caption. In doing so, it enables a\nnovel methodology for learning STSGs without tedious annotations. We evaluate\nour method on three video datasets: OpenPVSG, 20BN, and MUGEN. Our approach\ndemonstrates substantial improvements over fully-supervised baselines,\nachieving a unary predicate prediction accuracy of 27.78% (+12.65%) and a\nbinary recall@5 of 0.42 (+0.22) on OpenPVSG. Additionally, LASER exceeds\nbaselines by 7% on 20BN and 5.2% on MUGEN in terms of overall predicate\nprediction accuracy.\n","authors":["Jiani Huang","Ziyang Li","Mayur Naik","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2304.07647v7.pdf","comment":"Accepted at International Conference on Learning Representations\n  (ICLR) 2025"},{"id":"http://arxiv.org/abs/2506.06549v3","updated":"2025-10-27T20:00:10Z","published":"2025-06-06T21:41:17Z","title":"GeoClip: Geometry-Aware Clipping for Differentially Private SGD","summary":"  Differentially private stochastic gradient descent (DP-SGD) is the most\nwidely used method for training machine learning models with provable privacy\nguarantees. A key challenge in DP-SGD is setting the per-sample gradient\nclipping threshold, which significantly affects the trade-off between privacy\nand utility. While recent adaptive methods improve performance by adjusting\nthis threshold during training, they operate in the standard coordinate system\nand fail to account for correlations across the coordinates of the gradient. We\npropose GeoClip, a geometry-aware framework that clips and perturbs gradients\nin a transformed basis aligned with the geometry of the gradient distribution.\nGeoClip adaptively estimates this transformation using only previously released\nnoisy gradients, incurring no additional privacy cost. We provide convergence\nguarantees for GeoClip and derive a closed-form solution for the optimal\ntransformation that minimizes the amount of noise added while keeping the\nprobability of gradient clipping under control. Experiments on both tabular and\nimage datasets demonstrate that GeoClip consistently outperforms existing\nadaptive clipping methods under the same privacy budget.\n","authors":["Atefeh Gilani","Naima Tasnim","Lalitha Sankar","Oliver Kosut"],"pdf_url":"https://arxiv.org/pdf/2506.06549v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23818v1","updated":"2025-10-27T19:59:46Z","published":"2025-10-27T19:59:46Z","title":"ScaLoRA: Optimally Scaled Low-Rank Adaptation for Efficient High-Rank\n  Fine-Tuning","summary":"  As large language models (LLMs) continue to scale in size, the computational\noverhead has become a major bottleneck for task-specific fine-tuning. While\nlow-rank adaptation (LoRA) effectively curtails this cost by confining the\nweight updates to a low-dimensional subspace, such a restriction can hinder\neffectiveness and slow convergence. This contribution deals with these\nlimitations by accumulating progressively a high-rank weight update from\nconsecutive low-rank increments. Specifically, the per update optimal low-rank\nmatrix is identified to minimize the loss function and closely approximate full\nfine-tuning. To endow efficient and seamless optimization without restarting,\nthis optimal choice is formed by appropriately scaling the columns of the\noriginal low-rank matrix. Rigorous performance guarantees reveal that the\noptimal scaling can be found analytically. Extensive numerical tests with\npopular LLMs scaling up to 12 billion parameters demonstrate a consistent\nperformance gain and fast convergence relative to state-of-the-art LoRA\nvariants on diverse tasks including natural language understanding, commonsense\nreasoning, and mathematical problem solving.\n","authors":["Yilang Zhang","Xiaodong Yang","Yiwei Cai","Georgios B. Giannakis"],"pdf_url":"https://arxiv.org/pdf/2510.23818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23817v1","updated":"2025-10-27T19:56:46Z","published":"2025-10-27T19:56:46Z","title":"Combining SHAP and Causal Analysis for Interpretable Fault Detection in\n  Industrial Processes","summary":"  Industrial processes generate complex data that challenge fault detection\nsystems, often yielding opaque or underwhelming results despite advanced\nmachine learning techniques. This study tackles such difficulties using the\nTennessee Eastman Process, a well-established benchmark known for its intricate\ndynamics, to develop an innovative fault detection framework. Initial attempts\nwith standard models revealed limitations in both performance and\ninterpretability, prompting a shift toward a more tractable approach. By\nemploying SHAP (SHapley Additive exPlanations), we transform the problem into a\nmore manageable and transparent form, pinpointing the most critical process\nfeatures driving fault predictions. This reduction in complexity unlocks the\nability to apply causal analysis through Directed Acyclic Graphs, generated by\nmultiple algorithms, to uncover the underlying mechanisms of fault propagation.\nThe resulting causal structures align strikingly with SHAP findings,\nconsistently highlighting key process elements-like cooling and separation\nsystems-as pivotal to fault development. Together, these methods not only\nenhance detection accuracy but also provide operators with clear, actionable\ninsights into fault origins, a synergy that, to our knowledge, has not been\npreviously explored in this context. This dual approach bridges predictive\npower with causal understanding, offering a robust tool for monitoring complex\nmanufacturing environments and paving the way for smarter, more interpretable\nfault detection in industrial systems.\n","authors":["Pedro Cortes dos Santos","Matheus Becali Rocha","Renato A Krohling"],"pdf_url":"https://arxiv.org/pdf/2510.23817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03727v3","updated":"2025-10-27T19:54:43Z","published":"2025-01-07T12:16:26Z","title":"Detecting Neurocognitive Disorders through Analyses of Topic Evolution\n  and Cross-modal Consistency in Visual-Stimulated Narratives","summary":"  Early detection of neurocognitive disorders (NCDs) is crucial for timely\nintervention and disease management. Given that language impairments manifest\nearly in NCD progression, visual-stimulated narrative (VSN)-based analysis\noffers a promising avenue for NCD detection. Current VSN-based NCD detection\nmethods primarily focus on linguistic microstructures (e.g., lexical diversity)\nthat are closely tied to bottom-up, stimulus-driven cognitive processes. While\nthese features illuminate basic language abilities, the higher-order linguistic\nmacrostructures (e.g., topic development) that may reflect top-down,\nconcept-driven cognitive abilities remain underexplored. These macrostructural\npatterns are crucial for NCD detection, yet challenging to quantify due to\ntheir abstract and complex nature. To bridge this gap, we propose two novel\nmacrostructural approaches: (1) a Dynamic Topic Model (DTM) to track topic\nevolution over time, and (2) a Text-Image Temporal Alignment Network (TITAN) to\nmeasure cross-modal consistency between narrative and visual stimuli.\nExperimental results show the effectiveness of the proposed approaches in NCD\ndetection, with TITAN achieving superior performance across three corpora:\nADReSS (F1=0.8889), ADReSSo (F1=0.8504), and CU-MARVEL-RABBIT (F1=0.7238).\nFeature contribution analysis reveals that macrostructural features (e.g.,\ntopic variability, topic change rate, and topic consistency) constitute the\nmost significant contributors to the model's decision pathways, outperforming\nthe investigated microstructural features. These findings underscore the value\nof macrostructural analysis for understanding linguistic-cognitive interactions\nassociated with NCDs.\n","authors":["Jinchao Li","Yuejiao Wang","Junan Li","Jiawen Kang","Bo Zheng","Ka Ho Wong","Brian Mak","Helene H. Fung","Jean Woo","Man-Wai Mak","Timothy Kwok","Vincent Mok","Xianmin Gong","Xixin Wu","Xunying Liu","Patrick C. M. Wong","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2501.03727v3.pdf","comment":"16 pages, 5 figures, accepted by \"IEEE Journal of Selected Topics in\n  Signal Processing\""},{"id":"http://arxiv.org/abs/2410.22366v5","updated":"2025-10-27T19:52:38Z","published":"2024-10-28T19:01:18Z","title":"One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion\n  Models","summary":"  For large language models (LLMs), sparse autoencoders (SAEs) have been shown\nto decompose intermediate representations that often are not interpretable\ndirectly into sparse sums of interpretable features, facilitating better\ncontrol and subsequent analysis. However, similar analyses and approaches have\nbeen lacking for text-to-image models. We investigate the possibility of using\nSAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image\ndiffusion model. To this end, we train SAEs on the updates performed by\ntransformer blocks within SDXL Turbo's denoising U-net in its 1-step setting.\nInterestingly, we find that they generalize to 4-step SDXL Turbo and even to\nthe multi-step SDXL base model (i.e., a different model) without additional\ntraining. In addition, we show that their learned features are interpretable,\ncausally influence the generation process, and reveal specialization among the\nblocks. We do so by creating RIEBench, a representation-based image editing\nbenchmark, for editing images while they are generated by turning on and off\nindividual SAE features. This allows us to track which transformer blocks'\nfeatures are the most impactful depending on the edit category. Our work is the\nfirst investigation of SAEs for interpretability in text-to-image diffusion\nmodels and our results establish SAEs as a promising approach for understanding\nand manipulating the internal mechanisms of text-to-image models.\n","authors":["Viacheslav Surkov","Chris Wendler","Antonio Mari","Mikhail Terekhov","Justin Deschenaux","Robert West","Caglar Gulcehre","David Bau"],"pdf_url":"https://arxiv.org/pdf/2410.22366v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23810v1","updated":"2025-10-27T19:50:02Z","published":"2025-10-27T19:50:02Z","title":"A Physics-informed Multi-resolution Neural Operator","summary":"  The predictive accuracy of operator learning frameworks depends on the\nquality and quantity of available training data (input-output function pairs),\noften requiring substantial amounts of high-fidelity data, which can be\nchallenging to obtain in some real-world engineering applications. These\ndatasets may be unevenly discretized from one realization to another, with the\ngrid resolution varying across samples. In this study, we introduce a\nphysics-informed operator learning approach by extending the Resolution\nIndependent Neural Operator (RINO) framework to a fully data-free setup,\naddressing both challenges simultaneously. Here, the arbitrarily (but\nsufficiently finely) discretized input functions are projected onto a latent\nembedding space (i.e., a vector space of finite dimensions), using pre-trained\nbasis functions. The operator associated with the underlying partial\ndifferential equations (PDEs) is then approximated by a simple multi-layer\nperceptron (MLP), which takes as input a latent code along with spatiotemporal\ncoordinates to produce the solution in the physical space. The PDEs are\nenforced via a finite difference solver in the physical space. The validation\nand performance of the proposed method are benchmarked on several numerical\nexamples with multi-resolution data, where input functions are sampled at\nvarying resolutions, including both coarse and fine discretizations.\n","authors":["Sumanta Roy","Bahador Bahmani","Ioannis G. Kevrekidis","Michael D. Shields"],"pdf_url":"https://arxiv.org/pdf/2510.23810v1.pdf","comment":"26 pages, 14 figures, 4 tables"},{"id":"http://arxiv.org/abs/2510.23804v1","updated":"2025-10-27T19:38:46Z","published":"2025-10-27T19:38:46Z","title":"How do simple rotations affect the implicit bias of Adam?","summary":"  Adaptive gradient methods such as Adam and Adagrad are widely used in machine\nlearning, yet their effect on the generalization of learned models -- relative\nto methods like gradient descent -- remains poorly understood. Prior work on\nbinary classification suggests that Adam exhibits a ``richness bias,'' which\ncan help it learn nonlinear decision boundaries closer to the Bayes-optimal\ndecision boundary relative to gradient descent. However, the coordinate-wise\npreconditioning scheme employed by Adam renders the overall method sensitive to\northogonal transformations of feature space. We show that this sensitivity can\nmanifest as a reversal of Adam's competitive advantage: even small rotations of\nthe underlying data distribution can make Adam forfeit its richness bias and\nconverge to a linear decision boundary that is farther from the Bayes-optimal\ndecision boundary than the one learned by gradient descent. To alleviate this\nissue, we show that a recently proposed reparameterization method -- which\napplies an orthogonal transformation to the optimization objective -- endows\nany first-order method with equivariance to data rotations, and we empirically\ndemonstrate its ability to restore Adam's bias towards rich decision\nboundaries.\n","authors":["Adela DePavia","Vasileios Charisopoulos","Rebecca Willett"],"pdf_url":"https://arxiv.org/pdf/2510.23804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07918v2","updated":"2025-10-27T19:38:29Z","published":"2025-06-09T16:31:06Z","title":"CausalPFN: Amortized Causal Effect Estimation via In-Context Learning","summary":"  Causal effect estimation from observational data is fundamental across\nvarious applications. However, selecting an appropriate estimator from dozens\nof specialized methods demands substantial manual effort and domain expertise.\nWe present CausalPFN, a single transformer that amortizes this workflow:\ntrained once on a large library of simulated data-generating processes that\nsatisfy ignorability, it infers causal effects for new observational datasets\nout of the box. CausalPFN combines ideas from Bayesian causal inference with\nthe large-scale training protocol of prior-fitted networks (PFNs), learning to\nmap raw observations directly to causal effects without any task-specific\nadjustment. Our approach achieves superior average performance on heterogeneous\nand average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC).\nMoreover, it shows competitive performance for real-world policy making on\nuplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to\nsupport reliable decision-making based on Bayesian principles. This\nready-to-use model requires no further training or tuning and takes a step\ntoward automated causal inference (https://github.com/vdblm/CausalPFN/).\n","authors":["Vahid Balazadeh","Hamidreza Kamkari","Valentin Thomas","Benson Li","Junwei Ma","Jesse C. Cresswell","Rahul G. Krishnan"],"pdf_url":"https://arxiv.org/pdf/2506.07918v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02937v2","updated":"2025-10-27T19:36:33Z","published":"2025-06-26T23:48:03Z","title":"FoGE: Fock Space inspired encoding for graph prompting","summary":"  Recent results show that modern Large Language Models (LLM) are indeed\ncapable of understanding and answering questions about structured data such as\ngraphs. This new paradigm can lead to solutions that require less supervision\nwhile, at the same time, providing a model that can generalize and answer\nquestions beyond the training labels. Existing proposals often use some\ndescription of the graph to create an ``augmented'' prompt fed to the LLM. For\na chosen class of graphs, if a well-tailored graph encoder is deployed to play\ntogether with a pre-trained LLM, the model can answer graph-related questions\nwell. Existing solutions to graph-based prompts range from graph serialization\nto graph transformers. In this work, we show that the use of a parameter-free\ngraph encoder based on Fock space representations, a concept borrowed from\nmathematical physics, is remarkably versatile in this problem setting. The\nsimple construction, inherited directly from the theory with a few small\nadjustments, can provide rich and informative graph encodings, for a wide range\nof different graphs. We investigate the use of this idea for prefix-tuned\nprompts leveraging the capabilities of a pre-trained, frozen LLM. The\nmodifications lead to a model that can answer graph-related questions -- from\nsimple graphs to proteins to hypergraphs -- effectively and with minimal, if\nany, adjustments to the architecture. Our work significantly simplifies\nexisting solutions and generalizes well to multiple different graph-based\nstructures effortlessly.\n","authors":["Sotirios Panagiotis Chytas","Rudrasis Chakraborty","Vikas Singh"],"pdf_url":"https://arxiv.org/pdf/2507.02937v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23802v1","updated":"2025-10-27T19:35:39Z","published":"2025-10-27T19:35:39Z","title":"Learning Interpretable Features in Audio Latent Spaces via Sparse\n  Autoencoders","summary":"  While sparse autoencoders (SAEs) successfully extract interpretable features\nfrom language models, applying them to audio generation faces unique\nchallenges: audio's dense nature requires compression that obscures semantic\nmeaning, and automatic feature characterization remains limited. We propose a\nframework for interpreting audio generative models by mapping their latent\nrepresentations to human-interpretable acoustic concepts. We train SAEs on\naudio autoencoder latents, then learn linear mappings from SAE features to\ndiscretized acoustic properties (pitch, amplitude, and timbre). This enables\nboth controllable manipulation and analysis of the AI music generation process,\nrevealing how acoustic properties emerge during synthesis. We validate our\napproach on continuous (DiffRhythm-VAE) and discrete (EnCodec, WavTokenizer)\naudio latent spaces, and analyze DiffRhythm, a state-of-the-art text-to-music\nmodel, to demonstrate how pitch, timbre, and loudness evolve throughout\ngeneration. While our work is only done on audio modality, our framework can be\nextended to interpretable analysis of visual latent space generation models.\n","authors":["Nathan Paek","Yongyi Zang","Qihui Yang","Randal Leistikow"],"pdf_url":"https://arxiv.org/pdf/2510.23802v1.pdf","comment":"Accepted to NeurIPS 2025 Mechanistic Interpretability Workshop"},{"id":"http://arxiv.org/abs/2510.03095v3","updated":"2025-10-27T19:32:07Z","published":"2025-10-03T15:25:08Z","title":"Distilled Protein Backbone Generation","summary":"  Diffusion- and flow-based generative models have recently demonstrated strong\nperformance in protein backbone generation tasks, offering unprecedented\ncapabilities for de novo protein design. However, while achieving notable\nperformance in generation quality, these models are limited by their generating\nspeed, often requiring hundreds of iterative steps in the reverse-diffusion\nprocess. This computational bottleneck limits their practical utility in\nlarge-scale protein discovery, where thousands to millions of candidate\nstructures are needed. To address this challenge, we explore the techniques of\nscore distillation, which has shown great success in reducing the number of\nsampling steps in the vision domain while maintaining high generation quality.\nHowever, a straightforward adaptation of these methods results in unacceptably\nlow designability. Through extensive study, we have identified how to\nappropriately adapt Score identity Distillation (SiD), a state-of-the-art score\ndistillation strategy, to train few-step protein backbone generators which\nsignificantly reduce sampling time, while maintaining comparable performance to\ntheir pretrained teacher model. In particular, multistep generation combined\nwith inference time noise modulation is key to the success. We demonstrate that\nour distilled few-step generators achieve more than a 20-fold improvement in\nsampling speed, while achieving similar levels of designability, diversity, and\nnovelty as the Proteina teacher model. This reduction in inference cost enables\nlarge-scale in silico protein design, thereby bringing diffusion-based models\ncloser to real-world protein engineering applications. The PyTorch\nimplementation is available at https://github.com/LY-Xie/SiD_Protein\n","authors":["Liyang Xie","Haoran Zhang","Zhendong Wang","Wesley Tansey","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.03095v3.pdf","comment":"PyTorch implementation: https://github.com/LY-Xie/SiD_Protein"},{"id":"http://arxiv.org/abs/2510.23794v1","updated":"2025-10-27T19:27:04Z","published":"2025-10-27T19:27:04Z","title":"Revealing the Potential of Learnable Perturbation Ensemble Forecast\n  Model for Tropical Cyclone Prediction","summary":"  Tropical cyclones (TCs) are highly destructive and inherently uncertain\nweather systems. Ensemble forecasting helps quantify these uncertainties, yet\ntraditional systems are constrained by high computational costs and limited\ncapability to fully represent atmospheric nonlinearity. FuXi-ENS introduces a\nlearnable perturbation scheme for ensemble generation, representing a novel\nAI-based forecasting paradigm. Here, we systematically compare FuXi-ENS with\nECMWF-ENS using all 90 global TCs in 2018, examining their performance in\nTC-related physical variables, track and intensity forecasts, and the\nassociated dynamical and thermodynamical fields. FuXi-ENS demonstrates clear\nadvantages in predicting TC-related physical variables, and achieves more\naccurate track forecasts with reduced ensemble spread, though it still\nunderestimates intensity relative to observations. Further dynamical and\nthermodynamical analyses reveal that FuXi-ENS better captures large-scale\ncirculation, with moisture turbulent energy more tightly concentrated around\nthe TC warm core, whereas ECMWF-ENS exhibits a more dispersed distribution.\nThese findings highlight the potential of learnable perturbations to improve TC\nforecasting skill and provide valuable insights for advancing AI-based ensemble\nprediction of extreme weather events that have significant societal impacts.\n","authors":["Jun Liu","Tao Zhou","Jiarui Li","Xiaohui Zhong","Peng Zhang","Jie Feng","Lei Chen","Hao Li"],"pdf_url":"https://arxiv.org/pdf/2510.23794v1.pdf","comment":"30 pages, 21 figures, 1 table"},{"id":"http://arxiv.org/abs/2505.14323v2","updated":"2025-10-27T19:24:41Z","published":"2025-05-20T13:09:22Z","title":"Securing Transfer-Learned Networks with Reverse Homomorphic Encryption","summary":"  The growing body of literature on training-data reconstruction attacks raises\nsignificant concerns about deploying neural network classifiers trained on\nsensitive data. However, differentially private (DP) training (e.g. using\nDP-SGD) can defend against such attacks with large training datasets causing\nonly minimal loss of network utility. Folklore, heuristics, and (albeit\npessimistic) DP bounds suggest this fails for networks trained with small\nper-class datasets, yet to the best of our knowledge the literature offers no\ncompelling evidence. We directly demonstrate this vulnerability by\nsignificantly extending reconstruction attack capabilities under a realistic\nadversary threat model for few-shot transfer learned image classifiers. We\ndesign new white-box and black-box attacks and find that DP-SGD is unable to\ndefend against these without significant classifier utility loss. To address\nthis, we propose a novel homomorphic encryption (HE) method that protects\ntraining data without degrading model's accuracy. Conventional HE secures\nmodel's input data and requires costly homomorphic implementation of the entire\nclassifier. In contrast, our new scheme is computationally efficient and\nprotects training data rather than input data. This is achieved by means of a\nsimple role-reversal where classifier input data is unencrypted but\ntransfer-learned weights are encrypted. Classifier outputs remain encrypted,\nthus preventing both white-box and black-box (and any other) training-data\nreconstruction attacks. Under this new scheme only a trusted party with a\nprivate decryption key can obtain the classifier class decisions.\n","authors":["Robert Allison","Tomasz Maciążek","Henry Bourne"],"pdf_url":"https://arxiv.org/pdf/2505.14323v2.pdf","comment":"added protection via RHE and black box attacks"},{"id":"http://arxiv.org/abs/2506.09923v2","updated":"2025-10-27T19:22:43Z","published":"2025-06-11T16:43:36Z","title":"Apollo: A Posteriori Label-Only Membership Inference Attack Towards\n  Machine Unlearning","summary":"  Machine Unlearning (MU) aims to update Machine Learning (ML) models following\nrequests to remove training samples and their influences on a trained model\nefficiently without retraining the original ML model from scratch. While MU\nitself has been employed to provide privacy protection and regulatory\ncompliance, it can also increase the attack surface of the model. Existing\nprivacy inference attacks towards MU that aim to infer properties of the\nunlearned set rely on the weaker threat model that assumes the attacker has\naccess to both the unlearned model and the original model, limiting their\nfeasibility toward real-life scenarios. We propose a novel privacy attack, A\nPosteriori Label-Only Membership Inference Attack towards MU, Apollo, that\ninfers whether a data sample has been unlearned, following a strict threat\nmodel where an adversary has access to the label-output of the unlearned model\nonly. We demonstrate that our proposed attack, while requiring less access to\nthe target model compared to previous attacks, can achieve relatively high\nprecision on the membership status of the unlearned samples.\n","authors":["Liou Tang","James Joshi","Ashish Kundu"],"pdf_url":"https://arxiv.org/pdf/2506.09923v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23786v1","updated":"2025-10-27T19:18:36Z","published":"2025-10-27T19:18:36Z","title":"Relaxed Sequence Sampling for Diverse Protein Design","summary":"  Protein design using structure prediction models such as AlphaFold2 has shown\nremarkable success, but existing approaches like relaxed sequence optimization\n(RSO) rely on single-path gradient descent and ignore sequence-space\nconstraints, limiting diversity and designability. We introduce Relaxed\nSequence Sampling (RSS), a Markov chain Monte Carlo (MCMC) framework that\nintegrates structural and evolutionary information for protein design. RSS\noperates in continuous logit space, combining gradient-guided exploration with\nprotein language model-informed jumps. Its energy function couples\nAlphaFold2-derived structural objectives with ESM2-derived sequence priors,\nbalancing accuracy and biological plausibility. In an in silico protein binder\ndesign task, RSS produces 5$\\times$ more designable structures and 2-3$\\times$\ngreater structural diversity than RSO baselines, at equal computational cost.\nThese results highlight RSS as a principled approach for efficiently exploring\nthe protein design landscape.\n","authors":["Joohwan Ko","Aristofanis Rontogiannis","Yih-En Andrew Ban","Axel Elaldi","Nicholas Franklin"],"pdf_url":"https://arxiv.org/pdf/2510.23786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.00230v3","updated":"2025-10-27T19:09:31Z","published":"2025-06-30T19:54:34Z","title":"PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense\n  Spatial Networks for Encrypted Lossy Image Reconstruction","summary":"  Reconstructing high-quality images from low-resolution inputs using Residual\nDense Spatial Networks (RDSNs) is crucial yet challenging. It is even more\nchallenging in centralized training where multiple collaborating parties are\ninvolved, as it poses significant privacy risks, including data leakage and\ninference attacks, as well as high computational and communication costs. We\npropose a novel Privacy-Preserving Federated Learning-based RDSN (PPFL-RDSN)\nframework specifically tailored for encrypted lossy image reconstruction.\nPPFL-RDSN integrates Federated Learning (FL), local differential privacy, and\nrobust model watermarking techniques to ensure that data remains secure on\nlocal clients/devices, safeguards privacy-sensitive information, and maintains\nmodel authenticity without revealing underlying data. Empirical evaluations\nshow that PPFL-RDSN achieves comparable performance to the state-of-the-art\ncentralized methods while reducing computational burdens, and effectively\nmitigates security and privacy vulnerabilities, making it a practical solution\nfor secure and privacy-preserving collaborative computer vision applications.\n","authors":["Peilin He","James Joshi"],"pdf_url":"https://arxiv.org/pdf/2507.00230v3.pdf","comment":"Accepted to be published on the 7th IEEE International Conference on\n  Trust, Privacy and Security in Intelligent Systems, and Applications, Nov.\n  11-14, 2025, Pittsburgh, PA, USA.\n  https://www.sis.pitt.edu/lersais/conference/tps/2025/"},{"id":"http://arxiv.org/abs/2510.23772v1","updated":"2025-10-27T19:00:02Z","published":"2025-10-27T19:00:02Z","title":"Evaluating In Silico Creativity: An Expert Review of AI Chess\n  Compositions","summary":"  The rapid advancement of Generative AI has raised significant questions\nregarding its ability to produce creative and novel outputs. Our recent work\ninvestigates this question within the domain of chess puzzles and presents an\nAI system designed to generate puzzles characterized by aesthetic appeal,\nnovelty, counter-intuitive and unique solutions. We briefly discuss our method\nbelow and refer the reader to the technical paper for more details. To assess\nour system's creativity, we presented a curated booklet of AI-generated puzzles\nto three world-renowned experts: International Master for chess compositions\nAmatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All\nthree are noted authors on chess aesthetics and the evolving role of computers\nin the game. They were asked to select their favorites and explain what made\nthem appealing, considering qualities such as their creativity, level of\nchallenge, or aesthetic design.\n","authors":["Vivek Veeriah","Federico Barbero","Marcus Chiam","Xidong Feng","Michael Dennis","Ryan Pachauri","Thomas Tumiel","Johan Obando-Ceron","Jiaxin Shi","Shaobo Hou","Satinder Singh","Nenad Tomašev","Tom Zahavy"],"pdf_url":"https://arxiv.org/pdf/2510.23772v1.pdf","comment":"Accepted at the Creative AI Track, NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.24089v2","updated":"2025-10-27T18:58:38Z","published":"2025-05-30T00:23:01Z","title":"Practical Bayes-Optimal Membership Inference Attacks","summary":"  We develop practical and theoretically grounded membership inference attacks\n(MIAs) against both independent and identically distributed (i.i.d.) data and\ngraph-structured data. Building on the Bayesian decision-theoretic framework of\nSablayrolles et al., we derive the Bayes-optimal membership inference rule for\nnode-level MIAs against graph neural networks, addressing key open questions\nabout optimal query strategies in the graph setting. We introduce BASE and\nG-BASE, tractable approximations of the Bayes-optimal membership inference.\nG-BASE achieves superior performance compared to previously proposed\nclassifier-based node-level MIA attacks. BASE, which is also applicable to\nnon-graph data, matches or exceeds the performance of prior state-of-the-art\nMIAs, such as LiRA and RMIA, at a significantly lower computational cost.\nFinally, we show that BASE and RMIA are equivalent under a specific\nhyperparameter setting, providing a principled, Bayes-optimal justification for\nthe RMIA attack.\n","authors":["Marcus Lassila","Johan Östman","Khac-Hoang Ngo","Alexandre Graell i Amat"],"pdf_url":"https://arxiv.org/pdf/2505.24089v2.pdf","comment":"In NeurIPS 2025, 10 pages plus 15 pages of appendices"},{"id":"http://arxiv.org/abs/2506.06964v2","updated":"2025-10-27T18:56:23Z","published":"2025-06-08T01:59:30Z","title":"Offline RL by Reward-Weighted Fine-Tuning for Conversation Optimization","summary":"  Offline reinforcement learning (RL) is a variant of RL where the policy is\nlearned from a previously collected dataset of trajectories and rewards. In our\nwork, we propose a practical approach to offline RL with large language models\n(LLMs). We recast the problem as reward-weighted fine-tuning, which can be\nsolved using similar techniques to supervised fine-tuning (SFT). To showcase\nthe value of our approach, we apply it to learning short-horizon\nquestion-answering policies of a fixed length, where the agent reasons about\npotential answers or asks clarifying questions. Our work stands in a stark\ncontrast to state-of-the-art methods in this domain, based on SFT and direct\npreference optimization, which have additional hyper-parameters and do not\ndirectly optimize for rewards. We compare to them empirically, and report major\ngains in both optimized rewards and language quality.\n","authors":["Subhojyoti Mukherjee","Viet Dac Lai","Raghavendra Addanki","Ryan Rossi","Seunghyun Yoon","Trung Bui","Anup Rao","Jayakumar Subramanian","Branislav Kveton"],"pdf_url":"https://arxiv.org/pdf/2506.06964v2.pdf","comment":"Accepted at NeurIPS 2025 (main conference)"},{"id":"http://arxiv.org/abs/2510.23756v1","updated":"2025-10-27T18:41:25Z","published":"2025-10-27T18:41:25Z","title":"Explaining Robustness to Catastrophic Forgetting Through Incremental\n  Concept Formation","summary":"  Catastrophic forgetting remains a central challenge in continual learning,\nwhere models are required to integrate new knowledge over time without losing\nwhat they have previously learned. In prior work, we introduced Cobweb/4V, a\nhierarchical concept formation model that exhibited robustness to catastrophic\nforgetting in visual domains. Motivated by this robustness, we examine three\nhypotheses regarding the factors that contribute to such stability: (1)\nadaptive structural reorganization enhances knowledge retention, (2) sparse and\nselective updates reduce interference, and (3) information-theoretic learning\nbased on sufficiency statistics provides advantages over gradient-based\nbackpropagation. To test these hypotheses, we compare Cobweb/4V with neural\nbaselines, including CobwebNN, a neural implementation of the Cobweb framework\nintroduced in this work. Experiments on datasets of varying complexity (MNIST,\nFashion-MNIST, MedMNIST, and CIFAR-10) show that adaptive restructuring\nenhances learning plasticity, sparse updates help mitigate interference, and\nthe information-theoretic learning process preserves prior knowledge without\nrevisiting past data. Together, these findings provide insight into mechanisms\nthat can mitigate catastrophic forgetting and highlight the potential of\nconcept-based, information-theoretic approaches for building stable and\nadaptive continual learning systems.\n","authors":["Nicki Barari","Edward Kim","Christopher MacLellan"],"pdf_url":"https://arxiv.org/pdf/2510.23756v1.pdf","comment":"18 pages, 5 figures, Advances in Cognitive Systems 2025"},{"id":"http://arxiv.org/abs/2510.23751v1","updated":"2025-10-27T18:37:57Z","published":"2025-10-27T18:37:57Z","title":"Debiasing Reward Models by Representation Learning with Guarantees","summary":"  Recent alignment techniques, such as reinforcement learning from human\nfeedback, have been widely adopted to align large language models with human\npreferences by learning and leveraging reward models. In practice, these models\noften exploit spurious correlations, involving, e.g., response length,\ndiscrimination, sycophancy, and conceptual bias, which is a problem that has\nreceived increasing attention. In this work, we propose a principled framework\nthat mitigates these biases in reward models while preserving the underlying\nfactors that reflect intended preferences. We first provide a formulation of\nthe data-generating process, assuming that the observed data (e.g., text) is\ngenerated from both spurious and non-spurious latent variables. We show that,\ninterestingly, these non-spurious latent variables can be theoretically\nidentified from data, regardless of whether a surrogate for the spurious latent\nvariables is available. This further inspires a practical method that uses\nvariational inference to recover these variables and leverages them to train\nreward models. Experiments on synthetic and real-world datasets demonstrate\nthat our method effectively mitigates spurious correlation issues and yields\nmore robust reward models.\n","authors":["Ignavier Ng","Patrick Blöbaum","Siddharth Bhandari","Kun Zhang","Shiva Kasiviswanathan"],"pdf_url":"https://arxiv.org/pdf/2510.23751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23749v1","updated":"2025-10-27T18:28:56Z","published":"2025-10-27T18:28:56Z","title":"Re-envisioning Euclid Galaxy Morphology: Identifying and Interpreting\n  Features with Sparse Autoencoders","summary":"  Sparse Autoencoders (SAEs) can efficiently identify candidate monosemantic\nfeatures from pretrained neural networks for galaxy morphology. We demonstrate\nthis on Euclid Q1 images using both supervised (Zoobot) and new self-supervised\n(MAE) models. Our publicly released MAE achieves superhuman image\nreconstruction performance. While a Principal Component Analysis (PCA) on the\nsupervised model primarily identifies features already aligned with the Galaxy\nZoo decision tree, SAEs can identify interpretable features outside of this\nframework. SAE features also show stronger alignment than PCA with Galaxy Zoo\nlabels. Although challenges in interpretability remain, SAEs provide a powerful\nengine for discovering astrophysical phenomena beyond the confines of\nhuman-defined classification.\n","authors":["John F. Wu","Michael Walmsley"],"pdf_url":"https://arxiv.org/pdf/2510.23749v1.pdf","comment":"Accepted to NeurIPS Machine Learning and the Physical Sciences\n  Workshop"},{"id":"http://arxiv.org/abs/2510.23746v1","updated":"2025-10-27T18:25:36Z","published":"2025-10-27T18:25:36Z","title":"Test-Time Tuned Language Models Enable End-to-end De Novo Molecular\n  Structure Generation from MS/MS Spectra","summary":"  Tandem Mass Spectrometry enables the identification of unknown compounds in\ncrucial fields such as metabolomics, natural product discovery and\nenvironmental analysis. However, current methods rely on database matching from\npreviously observed molecules, or on multi-step pipelines that require\nintermediate fragment or fingerprint prediction. This makes finding the correct\nmolecule highly challenging, particularly for compounds absent from reference\ndatabases. We introduce a framework that, by leveraging test-time tuning,\nenhances the learning of a pre-trained transformer model to address this gap,\nenabling end-to-end de novo molecular structure generation directly from the\ntandem mass spectra and molecular formulae, bypassing manual annotations and\nintermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on\ntwo popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.\nTest-time tuning on experimental spectra allows the model to dynamically adapt\nto novel spectra, and the relative performance gain over conventional\nfine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground\ntruth, the generated molecular candidates remain structurally accurate,\nproviding valuable guidance for human interpretation and more reliable\nidentification.\n","authors":["Laura Mismetti","Marvin Alberts","Andreas Krause","Mara Graziani"],"pdf_url":"https://arxiv.org/pdf/2510.23746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23745v1","updated":"2025-10-27T18:25:21Z","published":"2025-10-27T18:25:21Z","title":"Bayesian neural networks with interpretable priors from Mercer kernels","summary":"  Quantifying the uncertainty in the output of a neural network is essential\nfor deployment in scientific or engineering applications where decisions must\nbe made under limited or noisy data. Bayesian neural networks (BNNs) provide a\nframework for this purpose by constructing a Bayesian posterior distribution\nover the network parameters. However, the prior, which is of key importance in\nany Bayesian setting, is rarely meaningful for BNNs. This is because the\ncomplexity of the input-to-output map of a BNN makes it difficult to understand\nhow certain distributions enforce any interpretable constraint on the output\nspace. Gaussian processes (GPs), on the other hand, are often preferred in\nuncertainty quantification tasks due to their interpretability. The drawback is\nthat GPs are limited to small datasets without advanced techniques, which often\nrely on the covariance kernel having a specific structure. To address these\nchallenges, we introduce a new class of priors for BNNs, called Mercer priors,\nsuch that the resulting BNN has samples which approximate that of a specified\nGP. The method works by defining a prior directly over the network parameters\nfrom the Mercer representation of the covariance kernel, and does not rely on\nthe network having a specific structure. In doing so, we can exploit the\nscalability of BNNs in a meaningful Bayesian way.\n","authors":["Alex Alberts","Ilias Bilionis"],"pdf_url":"https://arxiv.org/pdf/2510.23745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02703v2","updated":"2025-10-27T18:21:22Z","published":"2025-06-03T09:56:43Z","title":"Data Leakage and Deceptive Performance: A Critical Examination of Credit\n  Card Fraud Detection Methodologies","summary":"  The art and science of Quranic recitation (Tajweed), a discipline governed by\nmeticulous phonetic, rhythmic, and theological principles, confronts\nsubstantial educational challenges in today's digital age. Although modern\ntechnology offers unparalleled opportunities for learning, existing automated\nsystems for evaluating recitation have struggled to gain broad acceptance or\ndemonstrate educational effectiveness. This literature review examines this\ncrucial disparity, offering a thorough analysis of scholarly research, digital\nplatforms, and commercial tools developed over the past twenty years. Our\nanalysis uncovers a fundamental flaw in current approaches that adapt Automatic\nSpeech Recognition (ASR) systems, which emphasize word identification over\nqualitative acoustic evaluation. These systems suffer from limitations such as\nreliance on biased datasets, demographic disparities, and an inability to\ndeliver meaningful feedback for improvement. Challenging these data-centric\nmethodologies, we advocate for a paradigm shift toward a knowledge-based\ncomputational framework. By leveraging the unchanging nature of the Quranic\ntext and the well-defined rules of Tajweed, we propose that an effective\nevaluation system should be built upon rule-based acoustic modeling centered on\ncanonical pronunciation principles and articulation points (Makhraj), rather\nthan depending on statistical patterns derived from flawed or biased data. The\nreview concludes that the future of automated Quranic recitation assessment\nlies in hybrid systems that combine linguistic expertise with advanced audio\nprocessing. Such an approach paves the way for developing reliable, fair, and\npedagogically effective tools that can authentically assist learners across the\nglobe.\n","authors":["Mohammed Hilal Al-Kharusi","Khizar Hayat","Khalil Bader Al Ruqeishi","Haroon Rashid Lone"],"pdf_url":"https://arxiv.org/pdf/2506.02703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19053v2","updated":"2025-10-27T18:03:20Z","published":"2025-05-25T09:17:10Z","title":"Structured Reinforcement Learning for Combinatorial Decision-Making","summary":"  Reinforcement learning (RL) is increasingly applied to real-world problems\ninvolving complex and structured decisions, such as routing, scheduling, and\nassortment planning. These settings challenge standard RL algorithms, which\nstruggle to scale, generalize, and exploit structure in the presence of\ncombinatorial action spaces. We propose Structured Reinforcement Learning\n(SRL), a novel actor-critic paradigm that embeds combinatorial\noptimization-layers into the actor neural network. We enable end-to-end\nlearning of the actor via Fenchel-Young losses and provide a geometric\ninterpretation of SRL as a primal-dual algorithm in the dual of the moment\npolytope. Across six environments with exogenous and endogenous uncertainty,\nSRL matches or surpasses the performance of unstructured RL and imitation\nlearning on static tasks and improves over these baselines by up to 92% on\ndynamic problems, with improved stability and convergence speed.\n","authors":["Heiko Hoppe","Léo Baty","Louis Bouvier","Axel Parmentier","Maximilian Schiffer"],"pdf_url":"https://arxiv.org/pdf/2505.19053v2.pdf","comment":"29 pages, 8 figures, accepted at the 39th Annual Conference on Neural\n  Information Processing Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.23727v1","updated":"2025-10-27T18:03:11Z","published":"2025-10-27T18:03:11Z","title":"MUStReason: A Benchmark for Diagnosing Pragmatic Reasoning in Video-LMs\n  for Multimodal Sarcasm Detection","summary":"  Sarcasm is a specific type of irony which involves discerning what is said\nfrom what is meant. Detecting sarcasm depends not only on the literal content\nof an utterance but also on non-verbal cues such as speaker's tonality, facial\nexpressions and conversational context. However, current multimodal models\nstruggle with complex tasks like sarcasm detection, which require identifying\nrelevant cues across modalities and pragmatically reasoning over them to infer\nthe speaker's intention. To explore these limitations in VideoLMs, we introduce\nMUStReason, a diagnostic benchmark enriched with annotations of\nmodality-specific relevant cues and underlying reasoning steps to identify\nsarcastic intent. In addition to benchmarking sarcasm classification\nperformance in VideoLMs, using MUStReason we quantitatively and qualitatively\nevaluate the generated reasoning by disentangling the problem into perception\nand reasoning, we propose PragCoT, a framework that steers VideoLMs to focus on\nimplied intentions over literal meaning, a property core to detecting sarcasm.\n","authors":["Anisha Saha","Varsha Suresh","Timothy Hospedales","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2510.23727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00919v2","updated":"2025-10-27T18:01:43Z","published":"2025-01-01T18:33:48Z","title":"Geometry matters: insights from Ollivier Ricci Curvature and Ricci Flow\n  into representational alignment through Ollivier-Ricci Curvature and Ricci\n  Flow","summary":"  Representational similarity analysis (RSA) is widely used to analyze the\nalignment between humans and neural networks; however, conclusions based on\nthis approach can be misleading without considering the underlying\nrepresentational geometry. Our work introduces a framework using Ollivier Ricci\nCurvature and Ricci Flow to analyze the fine-grained local structure of\nrepresentations. This approach is agnostic to the source of the\nrepresentational space, enabling a direct geometric comparison between human\nbehavioral judgments and a model's vector embeddings. We apply it to compare\nhuman similarity judgments for 2D and 3D face stimuli with a baseline 2D native\nnetwork (VGG-Face) and a variant of it aligned to human behavior. Our results\nsuggest that geometry-aware analysis provides a more sensitive characterization\nof discrepancies and geometric dissimilarities in the underlying\nrepresentations that remain only partially captured by RSA. Notably, we reveal\ngeometric inconsistencies in the alignment when moving from 2D to 3D viewing\nconditions.This highlights how incorporating geometric information can expose\nalignment differences missed by traditional metrics, offering deeper insight\ninto representational organization.\n","authors":["Nahid Torbati","Michael Gaebler","Simon M. Hofmann","Nico Scherf"],"pdf_url":"https://arxiv.org/pdf/2501.00919v2.pdf","comment":"Presented at NeuReps workshop, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2510.23702v1","updated":"2025-10-27T18:00:00Z","published":"2025-10-27T18:00:00Z","title":"In Search of the Unknown Unknowns: A Multi-Metric Distance Ensemble for\n  Out of Distribution Anomaly Detection in Astronomical Surveys","summary":"  Distance-based methods involve the computation of distance values between\nfeatures and are a well-established paradigm in machine learning. In anomaly\ndetection, anomalies are identified by their large distance from normal data\npoints. However, the performance of these methods often hinges on a single,\nuser-selected distance metric (e.g., Euclidean), which may not be optimal for\nthe complex, high-dimensional feature spaces common in astronomy. Here, we\nintroduce a novel anomaly detection method, Distance Multi-Metric Anomaly\nDetection (DiMMAD), which uses an ensemble of distance metrics to find\nnovelties.\n  Using multiple distance metrics is effectively equivalent to using different\ngeometries in the feature space. By using a robust ensemble of diverse distance\nmetrics, we overcome the metric-selection problem, creating an anomaly score\nthat is not reliant on any single definition of distance. We demonstrate this\nmulti-metric approach as a tool for simple, interpretable scientific discovery\non astronomical time series -- (1) with simulated data for the upcoming Vera C.\nRubin Observatory Legacy Survey of Space and Time, and (2) real data from the\nZwicky Transient Facility.\n  We find that DiMMAD excels at out-of-distribution anomaly detection --\nanomalies in the data that might be new classes -- and beats other\nstate-of-the-art methods in the goal of maximizing the diversity of new classes\ndiscovered. For rare in-distribution anomaly detection, DiMMAD performs\nsimilarly to other methods, but may allow for improved interpretability. All\nour code is open source: DiMMAD is implemented within DistClassiPy:\nhttps://github.com/sidchaini/distclassipy/, while all code to reproduce the\nresults of this paper is available here: https://github.com/sidchaini/dimmad/.\n","authors":["Siddharth Chaini","Federica B. Bianco","Ashish Mahabal"],"pdf_url":"https://arxiv.org/pdf/2510.23702v1.pdf","comment":"9 pages, 5 figures, Accepted at the 2025 Machine Learning and the\n  Physical Sciences (ML4PS) workshop at NeurIPS"},{"id":"http://arxiv.org/abs/2510.23693v1","updated":"2025-10-27T17:59:48Z","published":"2025-10-27T17:59:48Z","title":"On the Societal Impact of Machine Learning","summary":"  This PhD thesis investigates the societal impact of machine learning (ML). ML\nincreasingly informs consequential decisions and recommendations, significantly\naffecting many aspects of our lives. As these data-driven systems are often\ndeveloped without explicit fairness considerations, they carry the risk of\ndiscriminatory effects. The contributions in this thesis enable more\nappropriate measurement of fairness in ML systems, systematic decomposition of\nML systems to anticipate bias dynamics, and effective interventions that reduce\nalgorithmic discrimination while maintaining system utility. I conclude by\ndiscussing ongoing challenges and future research directions as ML systems,\nincluding generative artificial intelligence, become increasingly integrated\ninto society. This work offers a foundation for ensuring that ML's societal\nimpact aligns with broader social values.\n","authors":["Joachim Baumann"],"pdf_url":"https://arxiv.org/pdf/2510.23693v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2510.23590v1","updated":"2025-10-27T17:55:06Z","published":"2025-10-27T17:55:06Z","title":"Lightweight Robust Direct Preference Optimization","summary":"  Direct Preference Optimization (DPO) has become a popular method for\nfine-tuning large language models (LLMs) due to its stability and simplicity.\nHowever, it is also known to be sensitive to noise in the data and prone to\noverfitting. Recent works have proposed using distributionally robust\noptimization (DRO) to address potential noise and distributional shift in the\ndata. However, these methods often suffer from excessive conservatism and high\ncomputational cost. We propose DPO-PRO (DPO with Preference Robustness), a\nrobust fine-tuning algorithm based on DPO which accounts for uncertainty in the\npreference distribution through a lightweight DRO formulation. Unlike prior\nDRO-based variants, DPO-PRO focuses solely on uncertainty in preferences,\navoiding unnecessary conservatism and incurring negligible computational\noverhead. We further show that DPO-PRO is equivalent to a regularized DPO\nobjective that penalizes model overconfidence under weak preference signals. We\nevaluate DPO-PRO on standard alignment benchmarks and a real-world public\nhealth task. Experimental results show that our method consistently improves\nrobustness to noisy preference signals compared to existing DPO variants.\n","authors":["Cheol Woo Kim","Shresth Verma","Mauricio Tec","Milind Tambe"],"pdf_url":"https://arxiv.org/pdf/2510.23590v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2509.02709"},{"id":"http://arxiv.org/abs/2407.11654v3","updated":"2025-10-27T17:52:25Z","published":"2024-07-16T12:21:29Z","title":"R-SFLLM: Jamming Resilient Framework for Split Federated Learning with\n  Large Language Models","summary":"  Split federated learning (SFL) is a compute-efficient paradigm in distributed\nmachine learning (ML), where components of large ML models are outsourced to\nremote servers. A significant challenge in SFL, particularly when deployed over\nwireless channels, is the susceptibility of transmitted model parameters to\nadversarial jamming that could jeopardize the learning process. This is\nparticularly pronounced for embedding parameters in large language models\n(LLMs) and vision language models (VLMs), which are learned feature vectors\nessential for domain understanding. In this paper, rigorous insights are\nprovided into the influence of jamming embeddings in SFL by deriving an\nexpression for the ML training loss divergence and showing that it is\nupper-bounded by the mean squared error (MSE). Based on this analysis, a\nphysical layer framework is developed for resilient SFL with LLMs (R-SFLLM)\nover wireless networks. R-SFLLM leverages wireless sensing data to gather\ninformation on the jamming directions-of-arrival (DoAs) for the purpose of\ndevising a novel, sensing-assisted anti-jamming strategy while jointly\noptimizing beamforming, user scheduling, and resource allocation. Extensive\nexperiments using both LLMs and VLMs demonstrate R-SFLLM's effectiveness,\nachieving close-to-baseline performance across various natural language\nprocessing (NLP) and computer vision (CV) tasks, datasets, and modalities. The\nproposed methodology further introduces an adversarial training component,\nwhere controlled noise exposure significantly enhances the model's resilience\nto perturbed parameters during training. The results show that more\nnoise-sensitive models, such as RoBERTa, benefit from this feature, especially\nwhen resource allocation is unfair. It is also shown that worst-case jamming in\nparticular translates into worst-case model outcomes, thereby necessitating the\nneed for jamming-resilient SFL protocols.\n","authors":["Aladin Djuhera","Vlad C. Andrei","Xinyang Li","Ullrich J. Mönich","Holger Boche","Walid Saad"],"pdf_url":"https://arxiv.org/pdf/2407.11654v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12362v2","updated":"2025-10-27T17:31:15Z","published":"2024-04-18T17:45:19Z","title":"KV-weights are all you need for skipless transformers","summary":"  He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks.\n","authors":["Nils Graef"],"pdf_url":"https://arxiv.org/pdf/2404.12362v2.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.23685v1","updated":"2025-10-27T16:17:10Z","published":"2025-10-27T16:17:10Z","title":"Parallel BiLSTM-Transformer networks for forecasting chaotic dynamics","summary":"  The nonlinear nature of chaotic systems results in extreme sensitivity to\ninitial conditions and highly intricate dynamical behaviors, posing fundamental\nchallenges for accurately predicting their evolution. To overcome the\nlimitation that conventional approaches fail to capture both local features and\nglobal dependencies in chaotic time series simultaneously, this study proposes\na parallel predictive framework integrating Transformer and Bidirectional Long\nShort-Term Memory (BiLSTM) networks. The hybrid model employs a dual-branch\narchitecture, where the Transformer branch mainly captures long-range\ndependencies while the BiLSTM branch focuses on extracting local temporal\nfeatures. The complementary representations from the two branches are fused in\na dedicated feature-fusion layer to enhance predictive accuracy. As\nillustrating examples, the model's performance is systematically evaluated on\ntwo representative tasks in the Lorenz system. The first is autonomous\nevolution prediction, in which the model recursively extrapolates system\ntrajectories from the time-delay embeddings of the state vector to evaluate\nlong-term tracking accuracy and stability. The second is inference of\nunmeasured variable, where the model reconstructs the unobserved states from\nthe time-delay embeddings of partial observations to assess its\nstate-completion capability. The results consistently indicate that the\nproposed hybrid framework outperforms both single-branch architectures across\ntasks, demonstrating its robustness and effectiveness in chaotic system\nprediction.\n","authors":["Junwen Ma","Mingyu Ge","Yisen Wang","Yong Zhang","Weicheng Fu"],"pdf_url":"https://arxiv.org/pdf/2510.23685v1.pdf","comment":"9 pages,7 figures"},{"id":"http://arxiv.org/abs/2507.18242v2","updated":"2025-10-27T16:02:45Z","published":"2025-07-24T09:30:37Z","title":"Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods","summary":"  Despite their theoretical appeal, totally corrective boosting methods based\non linear programming have received limited empirical attention. In this paper,\nwe conduct the first large-scale experimental study of six LP-based boosting\nformulations, including two novel methods, NM-Boost and QRLP-Boost, across 20\ndiverse datasets. We evaluate the use of both heuristic and optimal base\nlearners within these formulations, and analyze not only accuracy, but also\nensemble sparsity, margin distribution, anytime performance, and hyperparameter\nsensitivity. We show that totally corrective methods can outperform or match\nstate-of-the-art heuristics like XGBoost and LightGBM when using shallow trees,\nwhile producing significantly sparser ensembles. We further show that these\nmethods can thin pre-trained ensembles without sacrificing performance, and we\nhighlight both the strengths and limitations of using optimal decision trees in\nthis context.\n","authors":["Fabian Akkerman","Julien Ferry","Christian Artigues","Emmanuel Hebrard","Thibaut Vidal"],"pdf_url":"https://arxiv.org/pdf/2507.18242v2.pdf","comment":"Published in Transactions on Machine Learning Research (2025), see:\n  https://openreview.net/forum?id=lscC4PZUE4"},{"id":"http://arxiv.org/abs/2402.01635v2","updated":"2025-10-27T15:39:45Z","published":"2024-02-02T18:54:18Z","title":"Conditional Mean and Variance Estimation via \\textit{k}-NN Algorithm\n  with Automated Variance Selection","summary":"  We introduce a novel \\textit{k}-nearest neighbor (\\textit{k}-NN) regression\nmethod for joint estimation of the conditional mean and variance. The proposed\nalgorithm preserves the computational efficiency and manifold-learning\ncapabilities of classical non-parametric \\textit{k}-NN models, while\nintegrating a data-driven variable selection step that improves empirical\nperformance. By accurately estimating both conditional mean and variance\nregression functions, the method effectively reconstructs the conditional\ndistribution and density functions for multiple families of\nscale-and-localization generative models. We show that our estimator can\nachieve fast convergence rates, and we derive practical rules for selecting the\nsmoothing parameter~$k$ that enhance the precision of the algorithm in finite\nsample regimes. Extensive simulations for low, moderate and large-dimensional\ncovariate spaces, together with a real-world biomedical application,\ndemonstrate that the proposed method can consistently outperform the\nconventional \\textit{k-NN} regression algorithm while being more interpretable\nin the model output.\n","authors":["Marcos Matabuena","Juan C. Vidal","Oscar Hernan Madrid Padilla","Jukka-Pekka Onnela"],"pdf_url":"https://arxiv.org/pdf/2402.01635v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23684v1","updated":"2025-10-27T15:38:35Z","published":"2025-10-27T15:38:35Z","title":"VIKING: Deep variational inference with stochastic projections","summary":"  Variational mean field approximations tend to struggle with contemporary\noverparametrized deep neural networks. Where a Bayesian treatment is usually\nassociated with high-quality predictions and uncertainties, the practical\nreality has been the opposite, with unstable training, poor predictive power,\nand subpar calibration. Building upon recent work on reparametrizations of\nneural networks, we propose a simple variational family that considers two\nindependent linear subspaces of the parameter space. These represent functional\nchanges inside and outside the support of training data. This allows us to\nbuild a fully-correlated approximate posterior reflecting the\noverparametrization that tunes easy-to-interpret hyperparameters. We develop\nscalable numerical routines that maximize the associated evidence lower bound\n(ELBO) and sample from the approximate posterior. Empirically, we observe\nstate-of-the-art performance across tasks, models, and datasets compared to a\nwide array of baseline methods. Our results show that approximate Bayesian\ninference applied to deep neural networks is far from a lost cause when\nconstructing inference mechanisms that reflect the geometry of\nreparametrizations.\n","authors":["Samuel G. Fadel","Hrittik Roy","Nicholas Krämer","Yevgen Zainchkovskyy","Stas Syrota","Alejandro Valverde Mahou","Carl Henrik Ek","Søren Hauberg"],"pdf_url":"https://arxiv.org/pdf/2510.23684v1.pdf","comment":"NeurIPS 2025 (poster)"},{"id":"http://arxiv.org/abs/2510.23427v1","updated":"2025-10-27T15:33:01Z","published":"2025-10-27T15:33:01Z","title":"PrivacyGuard: A Modular Framework for Privacy Auditing in Machine\n  Learning","summary":"  The increasing deployment of Machine Learning (ML) models in sensitive\ndomains motivates the need for robust, practical privacy assessment tools.\nPrivacyGuard is a comprehensive tool for empirical differential privacy (DP)\nanalysis, designed to evaluate privacy risks in ML models through\nstate-of-the-art inference attacks and advanced privacy measurement techniques.\nTo this end, PrivacyGuard implements a diverse suite of privacy attack --\nincluding membership inference , extraction, and reconstruction attacks --\nenabling both off-the-shelf and highly configurable privacy analyses. Its\nmodular architecture allows for the seamless integration of new attacks, and\nprivacy metrics, supporting rapid adaptation to emerging research advances. We\nmake PrivacyGuard available at\nhttps://github.com/facebookresearch/PrivacyGuard.\n","authors":["Luca Melis","Matthew Grange","Iden Kalemaj","Karan Chadha","Shengyuan Hu","Elena Kashtelyan","Will Bullock"],"pdf_url":"https://arxiv.org/pdf/2510.23427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23682v1","updated":"2025-10-27T15:25:35Z","published":"2025-10-27T15:25:35Z","title":"Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust\n  Multi-Objective AI Agents","summary":"  Large language models show promise as autonomous decision-making agents, yet\ntheir deployment in high-stakes domains remains fraught with risk. Without\narchitectural safeguards, LLM agents exhibit catastrophic brittleness:\nidentical capabilities produce wildly different outcomes depending solely on\nprompt framing. We present Chimera, a neuro-symbolic-causal architecture that\nintegrates three complementary components - an LLM strategist, a formally\nverified symbolic constraint engine, and a causal inference module for\ncounterfactual reasoning. We benchmark Chimera against baseline architectures\n(LLM-only, LLM with symbolic constraints) across 52-week simulations in a\nrealistic e-commerce environment featuring price elasticity, trust dynamics,\nand seasonal demand. Under organizational biases toward either volume or margin\noptimization, LLM-only agents fail catastrophically (total loss of \\$99K in\nvolume scenarios) or destroy brand trust (-48.6% in margin scenarios). Adding\nsymbolic constraints prevents disasters but achieves only 43-87% of Chimera's\nprofit. Chimera consistently delivers the highest returns (\\$1.52M and \\$1.96M\nrespectively, some cases +\\$2.2M) while improving brand trust (+1.8% and\n+10.8%, some cases +20.86%), demonstrating prompt-agnostic robustness. Our TLA+\nformal verification proves zero constraint violations across all scenarios.\nThese results establish that architectural design not prompt engineering\ndetermines the reliability of autonomous agents in production environments. We\nprovide open-source implementations and interactive demonstrations for\nreproducibility.\n","authors":["Gokturk Aytug Akarlar"],"pdf_url":"https://arxiv.org/pdf/2510.23682v1.pdf","comment":"35 pages, 15 figures, 2 tables. Keywords: Large Language Models,\n  Autonomous Agents, Neuro-Symbolic AI, Causal Inference, Formal Verification,\n  Multi-Objective Optimization. Open-source code and interactive demo available"},{"id":"http://arxiv.org/abs/2402.01342v2","updated":"2025-10-27T15:23:58Z","published":"2024-02-02T11:57:50Z","title":"Improving Model Fusion by Training-time Neuron Alignment with Fixed\n  Neuron Anchors","summary":"  Model fusion aims to integrate several deep neural network (DNN) models'\nknowledge into one by fusing parameters, and it has promising applications,\nsuch as improving the generalization of foundation models and parameter\naveraging in federated learning. However, models under different settings\n(data, hyperparameter, etc.) have diverse neuron permutations; in other words,\nfrom the perspective of loss landscape, they reside in different loss basins,\nthus hindering model fusion performances. To alleviate this issue, previous\nstudies highlighted the role of permutation invariance and have developed\nmethods to find correct network permutations for neuron alignment after\ntraining. Orthogonal to previous attempts, this paper studies training-time\nneuron alignment, improving model fusion without the need for post-matching.\nTraining-time alignment is cheaper than post-alignment and is applicable in\nvarious model fusion scenarios. Starting from fundamental hypotheses and\ntheorems, a simple yet lossless algorithm called TNA-PFN is introduced. TNA-PFN\nutilizes partially fixed neuron weights as anchors to reduce the potential of\ntraining-time permutations, and it is empirically validated in reducing the\nbarriers of linear mode connectivity and multi-model fusion. It is also\nvalidated that TNA-PFN can improve the fusion of pretrained models under the\nsetting of model soup (vision transformers) and ColD fusion (pretrained\nlanguage models). Based on TNA-PFN, two federated learning methods, FedPFN and\nFedPNU, are proposed, showing the prospects of training-time neuron alignment.\nFedPFN and FedPNU reach state-of-the-art performances in federated learning\nunder heterogeneous settings and can be compatible with the server-side\nalgorithm.\n","authors":["Zexi Li","Zhiqi Li","Jie Lin","Tao Shen","Jun Xiao","Yike Guo","Tao Lin","Chao Wu"],"pdf_url":"https://arxiv.org/pdf/2402.01342v2.pdf","comment":"IEEE Transactions on Pattern Analysis and Machine Intelligence"},{"id":"http://arxiv.org/abs/2505.13112v3","updated":"2025-10-27T15:23:43Z","published":"2025-05-19T13:39:56Z","title":"Attention-based clustering","summary":"  Transformers have emerged as a powerful neural network architecture capable\nof tackling a wide range of learning tasks. In this work, we provide a\ntheoretical analysis of their ability to automatically extract structure from\ndata in an unsupervised setting. In particular, we demonstrate their\nsuitability for clustering when the input data is generated from a Gaussian\nmixture model. To this end, we study a simplified two-head attention layer and\ndefine a population risk whose minimization with unlabeled data drives the head\nparameters to align with the true mixture centroids. This phenomenon highlights\nthe ability of attention-based layers to capture underlying distributional\nstructure. We further examine an attention layer with key, query, and value\nmatrices fixed to the identity, and show that, even without any trainable\nparameters, it can perform in-context quantization, revealing the surprising\ncapacity of transformer-based methods to adapt dynamically to input-specific\ndistributions.\n","authors":["Rodrigo Maulen-Soto","Pierre Marion","Claire Boyer"],"pdf_url":"https://arxiv.org/pdf/2505.13112v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.01268v3","updated":"2025-10-27T15:06:24Z","published":"2025-09-29T10:04:35Z","title":"AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical\n  Guarantees","summary":"  We study the problem of determining whether a piece of text has been authored\nby a human or by a large language model (LLM). Existing state of the art\nlogits-based detectors make use of statistics derived from the log-probability\nof the observed text evaluated using the distribution function of a given\nsource LLM. However, relying solely on log probabilities can be sub-optimal. In\nresponse, we introduce AdaDetectGPT -- a novel classifier that adaptively\nlearns a witness function from training data to enhance the performance of\nlogits-based detectors. We provide statistical guarantees on its true positive\nrate, false positive rate, true negative rate and false negative rate.\nExtensive numerical studies show AdaDetectGPT nearly uniformly improves the\nstate-of-the-art method in various combination of datasets and LLMs, and the\nimprovement can reach up to 37\\%. A python implementation of our method is\navailable at https://github.com/Mamba413/AdaDetectGPT.\n","authors":["Hongyi Zhou","Jin Zhu","Pingfan Su","Kai Ye","Ying Yang","Shakeel A O B Gavioli-Akilagun","Chengchun Shi"],"pdf_url":"https://arxiv.org/pdf/2510.01268v3.pdf","comment":"Accepted by NeurIPS2025"},{"id":"http://arxiv.org/abs/2510.23681v1","updated":"2025-10-27T15:05:12Z","published":"2025-10-27T15:05:12Z","title":"Informed Initialization for Bayesian Optimization and Active Learning","summary":"  Bayesian Optimization is a widely used method for optimizing expensive\nblack-box functions, relying on probabilistic surrogate models such as Gaussian\nProcesses. The quality of the surrogate model is crucial for good optimization\nperformance, especially in the few-shot setting where only a small number of\nbatches of points can be evaluated. In this setting, the initialization plays a\ncritical role in shaping the surrogate's predictive quality and guiding\nsubsequent optimization. Despite this, practitioners typically rely on\n(quasi-)random designs to cover the input space. However, such approaches\nneglect two key factors: (a) space-filling designs may not be desirable to\nreduce predictive uncertainty, and (b) efficient hyperparameter learning during\ninitialization is essential for high-quality prediction, which may conflict\nwith space-filling designs. To address these limitations, we propose\nHyperparameter-Informed Predictive Exploration (HIPE), a novel acquisition\nstrategy that balances predictive uncertainty reduction with hyperparameter\nlearning using information-theoretic principles. We derive a closed-form\nexpression for HIPE in the Gaussian Process setting and demonstrate its\neffectiveness through extensive experiments in active learning and few-shot BO.\nOur results show that HIPE outperforms standard initialization strategies in\nterms of predictive accuracy, hyperparameter identification, and subsequent\noptimization performance, particularly in large-batch, few-shot settings\nrelevant to many real-world Bayesian Optimization applications.\n","authors":["Carl Hvarfner","David Eriksson","Eytan Bakshy","Max Balandat"],"pdf_url":"https://arxiv.org/pdf/2510.23681v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2510.23288v1","updated":"2025-10-27T12:59:45Z","published":"2025-10-27T12:59:45Z","title":"Learning from Frustration: Torsor CNNs on Graphs","summary":"  Most equivariant neural networks rely on a single global symmetry, limiting\ntheir use in domains where symmetries are instead local. We introduce Torsor\nCNNs, a framework for learning on graphs with local symmetries encoded as edge\npotentials -- group-valued transformations between neighboring coordinate\nframes. We establish that this geometric construction is fundamentally\nequivalent to the classical group synchronization problem, yielding: (1) a\nTorsor Convolutional Layer that is provably equivariant to local changes in\ncoordinate frames, and (2) the frustration loss -- a standalone geometric\nregularizer that encourages locally equivariant representations when added to\nany NN's training objective. The Torsor CNN framework unifies and generalizes\nseveral architectures -- including classical CNNs and Gauge CNNs on manifolds\n-- by operating on arbitrary graphs without requiring a global coordinate\nsystem or smooth manifold structure. We establish the mathematical foundations\nof this framework and demonstrate its applicability to multi-view 3D\nrecognition, where relative camera poses naturally define the required edge\npotentials.\n","authors":["Daiyuan Li","Shreya Arya","Robert Ghrist"],"pdf_url":"https://arxiv.org/pdf/2510.23288v1.pdf","comment":"19 pages (main text + appendices), 1 figure"},{"id":"http://arxiv.org/abs/2305.05239v2","updated":"2025-10-27T04:29:26Z","published":"2023-05-09T08:00:23Z","title":"Learnable Behavior Control: Breaking Atari Human World Records via\n  Sample-Efficient Behavior Selection","summary":"  The exploration problem is one of the main challenges in deep reinforcement\nlearning (RL). Recent promising works tried to handle the problem with\npopulation-based methods, which collect samples with diverse behaviors derived\nfrom a population of different exploratory policies. Adaptive policy selection\nhas been adopted for behavior control. However, the behavior selection space is\nlargely limited by the predefined policy population, which further limits\nbehavior diversity. In this paper, we propose a general framework called\nLearnable Behavioral Control (LBC) to address the limitation, which a) enables\na significantly enlarged behavior selection space via formulating a hybrid\nbehavior mapping from all policies; b) constructs a unified learnable process\nfor behavior selection. We introduce LBC into distributed off-policy\nactor-critic methods and achieve behavior control via optimizing the selection\nof the behavior mappings with bandit-based meta-controllers. Our agents have\nachieved 10077.52% mean human normalized score and surpassed 24 human world\nrecords within 1B training frames in the Arcade Learning Environment, which\ndemonstrates our significant state-of-the-art (SOTA) performance without\ndegrading the sample efficiency.\n","authors":["Jiajun Fan","Yuzheng Zhuang","Yuecheng Liu","Jianye Hao","Bin Wang","Jiangcheng Zhu","Hao Wang","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2305.05239v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22938v1","updated":"2025-10-27T02:47:20Z","published":"2025-10-27T02:47:20Z","title":"AQCat25: Unlocking spin-aware, high-fidelity machine learning potentials\n  for heterogeneous catalysis","summary":"  Large-scale datasets have enabled highly accurate machine learning\ninteratomic potentials (MLIPs) for general-purpose heterogeneous catalysis\nmodeling. There are, however, some limitations in what can be treated with\nthese potentials because of gaps in the underlying training data. To extend\nthese capabilities, we introduce AQCat25, a complementary dataset of 13.5\nmillion density functional theory (DFT) single point calculations designed to\nimprove the treatment of systems where spin polarization and/or higher fidelity\nare critical. We also investigate methodologies for integrating new datasets,\nsuch as AQCat25, with the broader Open Catalyst 2020 (OC20) dataset to create\nspin-aware models without sacrificing generalizability. We find that directly\ntuning a general model on AQCat25 leads to catastrophic forgetting of the\noriginal dataset's knowledge. Conversely, joint training strategies prove\neffective for improving accuracy on the new data without sacrificing general\nperformance. This joint approach introduces a challenge, as the model must\nlearn from a dataset containing both mixed-fidelity calculations and\nmixed-physics (spin-polarized vs. unpolarized). We show that explicitly\nconditioning the model on this system-specific metadata, for example by using\nFeature-wise Linear Modulation (FiLM), successfully addresses this challenge\nand further enhances model accuracy. Ultimately, our work establishes an\neffective protocol for bridging DFT fidelity domains to advance the predictive\npower of foundational models in catalysis.\n","authors":["Omar Allam","Brook Wander","Aayush R. Singh"],"pdf_url":"https://arxiv.org/pdf/2510.22938v1.pdf","comment":"32 pages, 17 figures"},{"id":"http://arxiv.org/abs/2509.12057v2","updated":"2025-10-27T02:43:37Z","published":"2025-09-15T15:38:44Z","title":"Foundational theory for optimal decision tree problems. II. Optimal\n  hypersurface decision tree algorithm","summary":"  Decision trees are a ubiquitous model for classification and regression tasks\ndue to their interpretability and efficiency. However, solving the optimal\ndecision tree (ODT) problem remains a challenging combinatorial optimization\ntask. Even for the simplest splitting rules--axis-parallel hyperplanes--it is\nNP-hard to optimize. In Part I of this series, we rigorously defined the proper\ndecision tree model through four axioms and, based on these, introduced four\nformal definitions of the ODT problem. From these definitions, we derived four\ngeneric algorithms capable of solving ODT problems for arbitrary decision trees\nsatisfying the axioms. We also analyzed the combinatorial geometric properties\nof hypersurfaces, showing that decision trees defined by polynomial\nhypersurface splitting rules satisfy the proper axioms that we proposed.\n  In this second paper (Part II) of this two-part series, building on the\nalgorithmic and geometric foundations established in Part I, we introduce the\nfirst hypersurface decision tree (HODT) algorithm. To the best of our\nknowledge, existing optimal decision tree methods are, to date, limited to\nhyperplane splitting rules--a special case of hypersurfaces--and rely on\ngeneral-purpose solvers. In contrast, our HODT algorithm addresses the general\nhypersurface decision tree model without requiring external solvers.\n  Using synthetic datasets generated from ground-truth hyperplane decision\ntrees, we vary tree size, data size, dimensionality, and label and feature\nnoise. Results showing that our algorithm recovers the ground truth more\naccurately than axis-parallel trees and exhibits greater robustness to noise.\nWe also analyzed generalization performance across 30 real-world datasets,\nshowing that HODT can achieve up to 30% higher accuracy than the\nstate-of-the-art optimal axis-parallel decision tree algorithm when tree\ncomplexity is properly controlled.\n","authors":["Xi He"],"pdf_url":"https://arxiv.org/pdf/2509.12057v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23672v1","updated":"2025-10-27T02:42:25Z","published":"2025-10-27T02:42:25Z","title":"DBLoss: Decomposition-based Loss Function for Time Series Forecasting","summary":"  Time series forecasting holds significant value in various domains such as\neconomics, traffic, energy, and AIOps, as accurate predictions facilitate\ninformed decision-making. However, the existing Mean Squared Error (MSE) loss\nfunction sometimes fails to accurately capture the seasonality or trend within\nthe forecasting horizon, even when decomposition modules are used in the\nforward propagation to model the trend and seasonality separately. To address\nthese challenges, we propose a simple yet effective Decomposition-Based Loss\nfunction called DBLoss. This method uses exponential moving averages to\ndecompose the time series into seasonal and trend components within the\nforecasting horizon, and then calculates the loss for each of these components\nseparately, followed by weighting them. As a general loss function, DBLoss can\nbe combined with any deep learning forecasting model. Extensive experiments\ndemonstrate that DBLoss significantly improves the performance of\nstate-of-the-art models across diverse real-world datasets and provides a new\nperspective on the design of time series loss functions.\n","authors":["Xiangfei Qiu","Xingjian Wu","Hanyin Cheng","Xvyuan Liu","Chenjuan Guo","Jilin Hu","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2510.23672v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.22937v1","updated":"2025-10-27T02:41:43Z","published":"2025-10-27T02:41:43Z","title":"Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics","summary":"  There has been a historic assumption that the biometrics of an individual are\nstatistically uncorrelated. We test this assumption by training Bi-Encoder\nnetworks on three verification tasks, including fingerprint-to-fingerprint\nmatching, iris-to-iris matching, and cross-modal fingerprint-to-iris matching\nusing 274 subjects with $\\sim$100k fingerprints and 7k iris images. We trained\nResNet-50 and Vision Transformer backbones in Bi-Encoder architectures such\nthat the contrastive loss between images sampled from the same individual is\nminimized. The iris ResNet architecture reaches 91 ROC AUC score for\niris-to-iris matching, providing clear evidence that the left and right irises\nof an individual are correlated. Fingerprint models reproduce the positive\nintra-subject suggested by prior work in this space. This is the first work\nattempting to use Vision Transformers for this matching. Cross-modal matching\nrises only slightly above chance, which suggests that more data and a more\nsophisticated pipeline is needed to obtain compelling results. These findings\ncontinue challenge independence assumptions of biometrics and we plan to extend\nthis work to other biometrics in the future. Code available:\nhttps://github.com/MatthewSo/bio_fingerprints_iris.\n","authors":["Matthew So","Judah Goldfeder","Mark Lis","Hod Lipson"],"pdf_url":"https://arxiv.org/pdf/2510.22937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.15390v2","updated":"2025-10-27T02:39:13Z","published":"2025-08-21T09:26:48Z","title":"Exploiting Vocabulary Frequency Imbalance in Language Model Pre-training","summary":"  Large language models are trained with tokenizers, and the resulting token\ndistribution is highly imbalanced: a few words dominate the stream while most\noccur rarely. Recent practice favors ever-larger vocabularies, but it is\nunclear where the benefit comes from. To this end, we perform a controlled\nstudy that scales the vocabulary of the language model from 24K to 196K while\nholding data, computation, and optimization unchanged. We begin by quantifying\nthe complexity of tokenized text -- formalized via Kolmogorov complexity -- and\nshow that larger vocabularies reduce this complexity. Above 24K, every common\nword is already tokenized as a single token, so enlarging vocabulary only\ndeepens the relative token-frequency imbalance. Word-level loss decomposition\nshows that larger vocabularies reduce cross-entropy loss almost exclusively by\nlowering uncertainty on the 2,500 most frequent words, even though loss on the\nrare tail rises. The same frequent words cover roughly 75% of tokens in\ndownstream benchmarks, so this training advantage transfers intact. We further\nshow that enlarging model parameters with a fixed vocabulary yields the same\nfrequent-word benefit. Our results recast \"bigger vocabularies help\" as\n\"lowering complexity of tokenized text helps,\" offering a simple, principled\nknob for tokenizer-model co-design and clarifying the loss dynamics that govern\nlanguage model scaling in pre-training.\n","authors":["Woojin Chung","Jeonghoon Kim"],"pdf_url":"https://arxiv.org/pdf/2508.15390v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2509.11226v2","updated":"2025-10-27T02:38:54Z","published":"2025-09-14T12:01:02Z","title":"Foundational theory for optimal decision tree problems. I. Algorithmic\n  and geometric foundations","summary":"  In the first paper (part I) of this series of two, we introduce four novel\ndefinitions of the ODT problems: three for size-constrained trees and one for\ndepth-constrained trees. These definitions are stated unambiguously through\nexecutable recursive programs, satisfying all criteria we propose for a formal\nspecification. In this sense, they resemble the \"standard form\" used in the\nstudy of general-purpose solvers.\n  Grounded in algebraic programming theory-a relational formalism for deriving\ncorrect-by-construction algorithms from specifications-we can not only\nestablish the existence or nonexistence of dynamic programming solutions but\nalso derive them constructively whenever they exist. Consequently, the four\ngeneric problem definitions yield four novel optimal algorithms for ODT\nproblems with arbitrary splitting rules that satisfy the axioms and objective\nfunctions of a given form. These algorithms encompass the known\ndepth-constrained, axis-parallel ODT algorithm as the special case, while\nproviding a unified, efficient, and elegant solution for the general ODT\nproblem.\n  In Part II, we present the first optimal hypersurface decision tree algorithm\nand provide comprehensive experiments against axis-parallel decision tree\nalgorithms, including heuristic CART and state-of-the-art optimal methods. The\nresults demonstrate the significant potential of decision trees with flexible\nsplitting rules. Moreover, our framework is readily extendable to support\nalgorithms for constructing even more flexible decision trees, including those\nwith mixed splitting rules.\n","authors":["Xi He"],"pdf_url":"https://arxiv.org/pdf/2509.11226v2.pdf","comment":"62 pages, Correct typos, include discussion on optimal decision tree\n  problem over binary feature data"},{"id":"http://arxiv.org/abs/2505.18190v3","updated":"2025-10-27T02:38:13Z","published":"2025-05-19T14:59:11Z","title":"PhySense: Sensor Placement Optimization for Accurate Physics Sensing","summary":"  Physics sensing plays a central role in many scientific and engineering\ndomains, which inherently involves two coupled tasks: reconstructing dense\nphysical fields from sparse observations and optimizing scattered sensor\nplacements to observe maximum information. While deep learning has made rapid\nadvances in sparse-data reconstruction, existing methods generally omit\noptimization of sensor placements, leaving the mutual enhancement between\nreconstruction and placement on the shelf. To change this suboptimal practice,\nwe propose PhySense, a synergistic two-stage framework that learns to jointly\nreconstruct physical fields and to optimize sensor placements, both aiming for\naccurate physics sensing. The first stage involves a flow-based generative\nmodel enhanced by cross-attention to adaptively fuse sparse observations.\nLeveraging the reconstruction feedback, the second stage performs sensor\nplacement via projected gradient descent to satisfy spatial constraints. We\nfurther prove that the learning objectives of the two stages are consistent\nwith classical variance-minimization principles, providing theoretical\nguarantees. Extensive experiments across three challenging benchmarks,\nespecially a 3D geometry dataset, indicate PhySense achieves state-of-the-art\nphysics sensing accuracy and discovers informative sensor placements previously\nunconsidered. Code is available at this repository:\nhttps://github.com/thuml/PhySense.\n","authors":["Yuezhou Ma","Haixu Wu","Hang Zhou","Huikun Weng","Jianmin Wang","Mingsheng Long"],"pdf_url":"https://arxiv.org/pdf/2505.18190v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.08010v4","updated":"2025-10-27T02:38:07Z","published":"2025-10-09T09:47:40Z","title":"Accelerated Evolving Set Processes for Local PageRank Computation","summary":"  This work proposes a novel framework based on nested evolving set processes\nto accelerate Personalized PageRank (PPR) computation. At each stage of the\nprocess, we employ a localized inexact proximal point iteration to solve a\nsimplified linear system. We show that the time complexity of such localized\nmethods is upper bounded by $\\min\\{\\tilde{\\mathcal{O}}(R^2/\\epsilon^2),\n\\tilde{\\mathcal{O}}(m)\\}$ to obtain an $\\epsilon$-approximation of the PPR\nvector, where $m$ denotes the number of edges in the graph and $R$ is a\nconstant defined via nested evolving set processes. Furthermore, the algorithms\ninduced by our framework require solving only\n$\\tilde{\\mathcal{O}}(1/\\sqrt{\\alpha})$ such linear systems, where $\\alpha$ is\nthe damping factor. When $1/\\epsilon^2\\ll m$, this implies the existence of an\nalgorithm that computes an $\\ epsilon $-approximation of the PPR vector with an\noverall time complexity of $\\tilde{\\mathcal{O}}\\left(R^2 /\n(\\sqrt{\\alpha}\\epsilon^2)\\right)$, independent of the underlying graph size.\nOur result resolves an open conjecture from existing literature. Experimental\nresults on real-world graphs validate the efficiency of our methods,\ndemonstrating significant convergence in the early stages.\n","authors":["Binbin Huang","Luo Luo","Yanghua Xiao","Deqing Yang","Baojian Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.08010v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.12491v2","updated":"2025-10-27T02:16:45Z","published":"2025-08-17T20:16:44Z","title":"Cost-Aware Contrastive Routing for LLMs","summary":"  We study cost-aware routing for large language models across diverse and\ndynamic pools of models. Existing approaches often overlook prompt-specific\ncontext, rely on expensive model profiling, assume a fixed set of experts, or\nuse inefficient trial-and-error strategies. We introduce Cost-Spectrum\nContrastive Routing (CSCR), a lightweight framework that maps both prompts and\nmodels into a shared embedding space to enable fast, cost-sensitive selection.\nCSCR uses compact, fast-to-compute logit footprints for open-source models and\nperplexity fingerprints for black-box APIs. A contrastive encoder is trained to\nfavor the cheapest accurate expert within adaptive cost bands. At inference\ntime, routing reduces to a single k-NN lookup via a FAISS index, requiring no\nretraining when the expert pool changes and enabling microsecond latency.\nAcross multiple benchmarks, CSCR consistently outperforms baselines, improving\nthe accuracy-cost tradeoff by up to 25%, while generalizing robustly to unseen\nLLMs and out-of-distribution prompts.\n","authors":["Reza Shirkavand","Shangqian Gao","Peiran Yu","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2508.12491v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03604v2","updated":"2025-10-27T02:10:58Z","published":"2025-02-05T20:47:44Z","title":"Bilevel ZOFO: Bridging Parameter-Efficient and Zeroth-Order Techniques\n  for Efficient LLM Fine-Tuning and Meta-Training","summary":"  Fine-tuning pre-trained Large Language Models (LLMs) for downstream tasks\nusing First-Order (FO) optimizers presents significant computational\nchallenges. Parameter-Efficient Fine-Tuning (PEFT) methods address these by\nfreezing most model parameters and training only a small subset. However, PEFT\noften underperforms compared to full fine-tuning when high task-specific\naccuracy is required. Zeroth-Order (ZO) methods fine-tune the entire\npre-trained model without back-propagation, estimating gradients through\nforward passes only. While memory-efficient, ZO methods suffer from slow\nconvergence and high sensitivity to prompt selection. We bridge these two\nworlds with Bilevel-ZOFO, a bilevel optimization method that couples fast,\nlocal FO-PEFT adaptation at the inner level with stable, memory-efficient ZO\nupdates of the full backbone at the outer level. The FO-PEFT inner loop\nperforms fast, low-memory local adaptation that reduces the variance of ZO\nestimates and stabilizes the search, guiding the outer ZO updates of the full\nbackbone and reducing prompt sensitivity. In the mean time, the outer ZO\nprovides better generalization ability for PEFT. We provide theoretical\nconvergence guarantees and empirically demonstrate that Bilevel-ZOFO\nsignificantly outperforms existing ZO and FO-PEFT methods, achieving 2-4 times\nfaster training while maintaining similar memory efficiency. Additionally, we\nshow by updating the backbone with ZO and adapting only a tiny FO-PEFT block\nper task, Bilevel-ZOFO combines full-model capacity with few-shot efficiency,\nmaking it a very efficient meta-learning algorithm that quickly adapts to new\ntasks.\n","authors":["Reza Shirkavand","Peiran Yu","Qi He","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2502.03604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24801v1","updated":"2025-10-27T23:19:48Z","published":"2025-10-27T23:19:48Z","title":"Fortytwo: Swarm Inference with Peer-Ranked Consensus","summary":"  As centralized AI hits compute ceilings and diminishing returns from\never-larger training runs, meeting demand requires an inference layer that\nscales horizontally in both capacity and capability. We present Fortytwo, a\nnovel protocol that leverages swarm intelligence principles and distributed\npairwise ranking consensus to achieve superior performance in AI inference. Our\napproach reimagines collaboration among AI nodes using swarm inference: a\npeer-ranked, reputation-weighted consensus across heterogeneous models that\nsurfaces the highest-quality responses. Using pairwise ranking with a custom\nBradley-Terry-style aggregation model, we demonstrate that swarm inference\nsubstantially outperforms majority voting, achieving 85.90% on GPQA Diamond\nversus 68.69% for majority voting with the same model set - an improvement of\n+17.21 percentage points (approximately +25.1% relative). The protocol\nincorporates on-chain reputation so node influence adapts to demonstrated\naccuracy over time, yielding a meritocratic consensus that filters low-quality\nor malicious participants. To resist Sybil attacks, Fortytwo employs\nproof-of-capability in its consensus: nodes must successfully complete\ncalibration/test requests and stake reputation to enter ranking rounds, making\nmulti-identity attacks economically unattractive while preserving openness.\nAcross six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and\nAIME, our evaluation indicates higher accuracy and strong resilience to\nadversarial and noisy free-form prompting (e.g., prompt-injection degradation\nof only 0.12% versus 6.20% for a monolithic single-model baseline), while\nretaining practical deployability. Together, these results establish a\nfoundation for decentralized AI systems - democratizing access to high-quality\ninference through collective intelligence without sacrificing reliability or\nsecurity.\n","authors":["Vladyslav Larin","Ihor Naumenko","Aleksei Ivashov","Ivan Nikitin","Alexander Firsov"],"pdf_url":"https://arxiv.org/pdf/2510.24801v1.pdf","comment":null}]},"2025-10-28T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2510.24718v1","updated":"2025-10-28T17:59:58Z","published":"2025-10-28T17:59:58Z","title":"Generative View Stitching","summary":"  Autoregressive video diffusion models are capable of long rollouts that are\nstable and consistent with history, but they are unable to guide the current\ngeneration with conditioning from the future. In camera-guided video generation\nwith a predefined camera trajectory, this limitation leads to collisions with\nthe generated scene, after which autoregression quickly collapses. To address\nthis, we propose Generative View Stitching (GVS), which samples the entire\nsequence in parallel such that the generated scene is faithful to every part of\nthe predefined camera trajectory. Our main contribution is a sampling algorithm\nthat extends prior work on diffusion stitching for robot planning to video\ngeneration. While such stitching methods usually require a specially trained\nmodel, GVS is compatible with any off-the-shelf video model trained with\nDiffusion Forcing, a prevalent sequence diffusion framework that we show\nalready provides the affordances necessary for stitching. We then introduce\nOmni Guidance, a technique that enhances the temporal consistency in stitching\nby conditioning on both the past and future, and that enables our proposed\nloop-closing mechanism for delivering long-range coherence. Overall, GVS\nachieves camera-guided video generation that is stable, collision-free,\nframe-to-frame consistent, and closes loops for a variety of predefined camera\npaths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best\nviewed as videos at https://andrewsonga.github.io/gvs.\n","authors":["Chonghyuk Song","Michal Stary","Boyuan Chen","George Kopanas","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2510.24718v1.pdf","comment":"Project website: https://andrewsonga.github.io/gvs"},{"id":"http://arxiv.org/abs/2510.24717v1","updated":"2025-10-28T17:59:57Z","published":"2025-10-28T17:59:57Z","title":"Uniform Discrete Diffusion with Metric Path for Video Generation","summary":"  Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA\n","authors":["Haoge Deng","Ting Pan","Fan Zhang","Yang Liu","Zhuoyan Luo","Yufeng Cui","Wenxuan Wang","Chunhua Shen","Shiguang Shan","Zhaoxiang Zhang","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2510.24717v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2510.24711v1","updated":"2025-10-28T17:59:02Z","published":"2025-10-28T17:59:02Z","title":"Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance","summary":"  Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.\n","authors":["Yujie Wei","Shiwei Zhang","Hangjie Yuan","Yujin Han","Zhekai Chen","Jiayu Wang","Difan Zou","Xihui Liu","Yingya Zhang","Yu Liu","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2510.24711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24709v1","updated":"2025-10-28T17:57:05Z","published":"2025-10-28T17:57:05Z","title":"Does Object Binding Naturally Emerge in Large Pretrained Vision\n  Transformers?","summary":"  Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.\n","authors":["Yihao Li","Saeed Salehi","Lyle Ungar","Konrad P. Kording"],"pdf_url":"https://arxiv.org/pdf/2510.24709v1.pdf","comment":"Accepted as a Spotlight at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24688v1","updated":"2025-10-28T17:49:42Z","published":"2025-10-28T17:49:42Z","title":"MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with\n  Relation-Aware Fusion for 3D Object Detection","summary":"  Infrastructure-based perception plays a crucial role in intelligent\ntransportation systems, offering global situational awareness and enabling\ncooperative autonomy. However, existing camera-based detection models often\nunderperform in such scenarios due to challenges such as multi-view\ninfrastructure setup, diverse camera configurations, degraded visual inputs,\nand various road layouts. We introduce MIC-BEV, a Transformer-based\nbird's-eye-view (BEV) perception framework for infrastructure-based\nmulti-camera 3D object detection. MIC-BEV flexibly supports a variable number\nof cameras with heterogeneous intrinsic and extrinsic parameters and\ndemonstrates strong robustness under sensor degradation. The proposed\ngraph-enhanced fusion module in MIC-BEV integrates multi-view image features\ninto the BEV space by exploiting geometric relationships between cameras and\nBEV cells alongside latent visual cues. To support training and evaluation, we\nintroduce M2I, a synthetic dataset for infrastructure-based object detection,\nfeaturing diverse camera configurations, road layouts, and environmental\nconditions. Extensive experiments on both M2I and the real-world dataset\nRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D\nobject detection. It also remains robust under challenging conditions,\nincluding extreme weather and sensor degradation. These results highlight the\npotential of MIC-BEV for real-world deployment. The dataset and source code are\navailable at: https://github.com/HandsomeYun/MIC-BEV.\n","authors":["Yun Zhang","Zhaoliang Zheng","Johnson Liu","Zhiyu Huang","Zewei Zhou","Zonglin Meng","Tianhui Cai","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2510.24688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07862v2","updated":"2025-10-28T17:37:03Z","published":"2025-02-11T17:19:44Z","title":"ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise\n  and Compute Resources","summary":"  Multimodal deep learning systems are deployed in dynamic scenarios due to the\nrobustness afforded by multiple sensing modalities. Nevertheless, they struggle\nwith varying compute resource availability (due to multi-tenancy, device\nheterogeneity, etc.) and fluctuating quality of inputs (from sensor feed\ncorruption, environmental noise, etc.). Statically provisioned multimodal\nsystems cannot adapt when compute resources change over time, while existing\ndynamic networks struggle with strict compute budgets. Additionally, both\nsystems often neglect the impact of variations in modality quality.\nConsequently, modalities suffering substantial corruption may needlessly\nconsume resources better allocated towards other modalities. We propose ADMN, a\nlayer-wise Adaptive Depth Multimodal Network capable of tackling both\nchallenges: it adjusts the total number of active layers across all modalities\nto meet strict compute resource constraints and continually reallocates layers\nacross input modalities according to their modality quality. Our evaluations\nshowcase ADMN can match the accuracy of state-of-the-art networks while\nreducing up to 75% of their floating-point operations.\n","authors":["Jason Wu","Yuyang Yuan","Kang Yang","Lance Kaplan","Mani Srivastava"],"pdf_url":"https://arxiv.org/pdf/2502.07862v2.pdf","comment":"Accepted to Neurips 2025"},{"id":"http://arxiv.org/abs/2505.20426v3","updated":"2025-10-28T17:35:54Z","published":"2025-05-26T18:20:22Z","title":"MMPerspective: Do MLLMs Understand Perspective? A Comprehensive\n  Benchmark for Perspective Perception, Reasoning, and Robustness","summary":"  Understanding perspective is fundamental to human visual perception, yet the\nextent to which multimodal large language models (MLLMs) internalize\nperspective geometry remains unclear. We introduce MMPerspective, the first\nbenchmark specifically designed to systematically evaluate MLLMs' understanding\nof perspective through 10 carefully crafted tasks across three complementary\ndimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark\ncomprises 2,711 real-world and synthetic image instances with 5,083\nquestion-answer pairs that probe key capabilities, such as vanishing point\nperception and counting, perspective type reasoning, line relationship\nunderstanding in 3D space, invariance to perspective-preserving\ntransformations, etc. Through a comprehensive evaluation of 43 state-of-the-art\nMLLMs, we uncover significant limitations: while models demonstrate competence\non surface-level perceptual tasks, they struggle with compositional reasoning\nand maintaining spatial consistency under perturbations. Our analysis further\nreveals intriguing patterns between model architecture, scale, and perspective\ncapabilities, highlighting both robustness bottlenecks and the benefits of\nchain-of-thought prompting. MMPerspective establishes a valuable testbed for\ndiagnosing and advancing spatial understanding in vision-language systems.\nResources available at: https://yunlong10.github.io/MMPerspective/\n","authors":["Yolo Yunlong Tang","Pinxin Liu","Zhangyun Tan","Mingqian Feng","Rui Mao","Chao Huang","Jing Bi","Yunzhong Xiao","Susan Liang","Hang Hua","Ali Vosoughi","Luchuan Song","Zeliang Zhang","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2505.20426v3.pdf","comment":"Accepted to NeurIPS 2025 DB Track"},{"id":"http://arxiv.org/abs/2510.24667v1","updated":"2025-10-28T17:35:02Z","published":"2025-10-28T17:35:02Z","title":"SAGE: Structure-Aware Generative Video Transitions between Diverse Clips","summary":"  Video transitions aim to synthesize intermediate frames between two clips,\nbut naive approaches such as linear blending introduce artifacts that limit\nprofessional use or break temporal coherence. Traditional techniques\n(cross-fades, morphing, frame interpolation) and recent generative inbetweening\nmethods can produce high-quality plausible intermediates, but they struggle\nwith bridging diverse clips involving large temporal gaps or significant\nsemantic differences, leaving a gap for content-aware and visually coherent\ntransitions. We address this challenge by drawing on artistic workflows,\ndistilling strategies such as aligning silhouettes and interpolating salient\nfeatures to preserve structure and perceptual continuity. Building on this, we\npropose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot\napproach that combines structural guidance, provided via line maps and motion\nflow, with generative synthesis, enabling smooth, semantically consistent\ntransitions without fine-tuning. Extensive experiments and comparison with\ncurrent alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate\nthat SAGE outperforms both classical and generative baselines on quantitative\nmetrics and user studies for producing transitions between diverse clips. Code\nto be released on acceptance.\n","authors":["Mia Kan","Yilin Liu","Niloy Mitra"],"pdf_url":"https://arxiv.org/pdf/2510.24667v1.pdf","comment":"Website: https://kan32501.github.io/sage.github.io/"},{"id":"http://arxiv.org/abs/2510.24657v1","updated":"2025-10-28T17:22:44Z","published":"2025-10-28T17:22:44Z","title":"Group Relative Attention Guidance for Image Editing","summary":"  Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.\n","authors":["Xuanpu Zhang","Xuesong Niu","Ruidong Chen","Dan Song","Jianhao Zeng","Penghui Du","Haoxiang Cao","Kai Wu","An-an Liu"],"pdf_url":"https://arxiv.org/pdf/2510.24657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24653v1","updated":"2025-10-28T17:18:43Z","published":"2025-10-28T17:18:43Z","title":"Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making\n  Datasets in Digital Pathology","summary":"  Interpretation of giga-pixel whole-slide images (WSIs) is an important but\ndifficult task for pathologists. Their diagnostic accuracy is estimated to\naverage around 70%. Adding a second pathologist does not substantially improve\ndecision consistency. The field lacks adequate behavioral data to explain\ndiagnostic errors and inconsistencies. To fill in this gap, we present\nPathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual\nsearch and decision-making processes of the full diagnostic workflow during\ncancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse\ninteraction, stimulus tracking, viewport navigation, and diagnostic decision\ndata (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data\ncollection process emphasizes ecological validity through an\napplication-grounded testbed, called PTAH. In total, we recorded 171,909\nfixations, 263,320 saccades, and 1,867,362 mouse interaction events. In\naddition, such data could also be used to improve the training of both\npathologists and AI systems that might support human experts. All experiments\nwere preregistered at https://osf.io/hj9a7, and the complete dataset along with\nanalysis code is available at https://go.osu.edu/pathogaze.\n","authors":["Veronica Thai","Rui Li","Meng Ling","Shuning Jiang","Jeremy Wolfe","Raghu Machiraju","Yan Hu","Zaibo Li","Anil Parwani","Jian Chen"],"pdf_url":"https://arxiv.org/pdf/2510.24653v1.pdf","comment":"16 pages, 9 figures, submitted to Nature Scientific Data"},{"id":"http://arxiv.org/abs/2510.24640v1","updated":"2025-10-28T17:06:40Z","published":"2025-10-28T17:06:40Z","title":"A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries","summary":"  The rapid advancement of generative AI has enabled the creation of highly\nrealistic forged facial images, posing significant threats to AI security,\ndigital media integrity, and public trust. Face forgery techniques, ranging\nfrom face swapping and attribute editing to powerful diffusion-based image\nsynthesis, are increasingly being used for malicious purposes such as\nmisinformation, identity fraud, and defamation. This growing challenge\nunderscores the urgent need for robust and generalizable face forgery detection\nmethods as a critical component of AI security infrastructure. In this work, we\npropose a novel dual-branch convolutional neural network for face forgery\ndetection that leverages complementary cues from both spatial and frequency\ndomains. The RGB branch captures semantic information, while the frequency\nbranch focuses on high-frequency artifacts that are difficult for generative\nmodels to suppress. A channel attention module is introduced to adaptively fuse\nthese heterogeneous features, highlighting the most informative channels for\nforgery discrimination. To guide the network's learning process, we design a\nunified loss function, FSC Loss, that combines focal loss, supervised\ncontrastive loss, and a frequency center margin loss to enhance class\nseparability and robustness. We evaluate our model on the DiFF benchmark, which\nincludes forged images generated from four representative methods:\ntext-to-image, image-to-image, face swap, and face edit. Our method achieves\nstrong performance across all categories and outperforms average human\naccuracy. These results demonstrate the model's effectiveness and its potential\ncontribution to safeguarding AI ecosystems against visual forgery attacks.\n","authors":["Xin Zhang","Yuqi Song","Fei Zuo"],"pdf_url":"https://arxiv.org/pdf/2510.24640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22693v2","updated":"2025-10-28T16:57:22Z","published":"2025-10-26T14:36:15Z","title":"VADTree: Explainable Training-Free Video Anomaly Detection via\n  Hierarchical Granularity-Aware Tree","summary":"  Video anomaly detection (VAD) focuses on identifying anomalies in videos.\nSupervised methods demand substantial in-domain training data and fail to\ndeliver clear explanations for anomalies. In contrast, training-free methods\nleverage the knowledge reserves and language interactivity of large pre-trained\nmodels to detect anomalies. However, the current fixed-length temporal window\nsampling approaches struggle to accurately capture anomalies with varying\ntemporal spans. Therefore, we propose VADTree that utilizes a Hierarchical\nGranularityaware Tree (HGTree) structure for flexible sampling in VAD. VADTree\nleverages the knowledge embedded in a pre-trained Generic Event Boundary\nDetection (GEBD) model to characterize potential anomaly event boundaries.\nSpecifically, VADTree decomposes the video into generic event nodes based on\nboundary confidence, and performs adaptive coarse-fine hierarchical structuring\nand redundancy removal to construct the HGTree. Then, the multi-dimensional\npriors are injected into the visual language models (VLMs) to enhance the\nnode-wise anomaly perception, and anomaly reasoning for generic event nodes is\nachieved via large language models (LLMs). Finally, an inter-cluster node\ncorrelation method is used to integrate the multi-granularity anomaly scores.\nExtensive experiments on three challenging datasets demonstrate that VADTree\nachieves state-of-the-art performance in training-free settings while\ndrastically reducing the number of sampled video segments. The code will be\navailable at https://github.com/wenlongli10/VADTree.\n","authors":["Wenlong Li","Yifei Xu","Yuan Rao","Zhenhua Wang","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2510.22693v2.pdf","comment":"NeurIPS 2025 poster"},{"id":"http://arxiv.org/abs/2510.24623v1","updated":"2025-10-28T16:51:50Z","published":"2025-10-28T16:51:50Z","title":"GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization","summary":"  In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline\ndesigned to localize a mobile robot in large-scale outdoor environments using\nprior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing\non the perceived ground area and utilizes the place recognition network R2D2,\nor alternatively, the non-learning approach Scale-Invariant Feature Transform\n(SIFT), to identify and select keypoints for BEV image map registration. Our\nresults demonstrate that GroundLoc outperforms state-of-the-art methods on the\nSemanticKITTI and HeLiPR datasets across various sensors. In the multi-session\nlocalization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)\nwell below 50 cm on all Ouster OS2 128 sequences while meeting online runtime\nrequirements. The system supports various sensor models, as evidenced by\nevaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,\nand Livox Avia sensors. The prior maps are stored as 2D raster image maps,\nwhich can be created from a single drive and require only 4 MB of storage per\nsquare kilometer. The source code is available at\nhttps://github.com/dcmlr/groundloc.\n","authors":["Nicolai Steinke","Daniel Goehring"],"pdf_url":"https://arxiv.org/pdf/2510.24623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.18513v2","updated":"2025-10-28T16:44:35Z","published":"2025-10-21T10:55:32Z","title":"DWaste: Greener AI for Waste Sorting using Mobile and Edge Devices","summary":"  The rise of convenience packaging has led to generation of enormous waste,\nmaking efficient waste sorting crucial for sustainable waste management. To\naddress this, we developed DWaste, a computer vision-powered platform designed\nfor real-time waste sorting on resource-constrained smartphones and edge\ndevices, including offline functionality. We benchmarked various image\nclassification models (EfficientNetV2S/M, ResNet50/101, MobileNet) and object\ndetection (YOLOv8n, YOLOv11n) including our purposed YOLOv8n-CBAM model using\nour annotated dataset designed for recycling. We found a clear trade-off\nbetween accuracy and resource consumption: the best classifier,\nEfficientNetV2S, achieved high accuracy(~ 96%) but suffered from high latency\n(~ 0.22s) and elevated carbon emissions. In contrast, lightweight object\ndetection models delivered strong performance (up to 80% mAP) with ultra-fast\ninference (~ 0.03s) and significantly smaller model sizes (< 7MB ), making them\nideal for real-time, low-power use. Model quantization further maximized\nefficiency, substantially reducing model size and VRAM usage by up to 75%. Our\nwork demonstrates the successful implementation of \"Greener AI\" models to\nsupport real-time, sustainable waste sorting on edge devices.\n","authors":["Suman Kunwar"],"pdf_url":"https://arxiv.org/pdf/2510.18513v2.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2405.07046v3","updated":"2025-10-28T16:43:19Z","published":"2024-05-11T16:22:00Z","title":"RETTA: Retrieval-Enhanced Test-Time Adaptation for Zero-Shot Video\n  Captioning","summary":"  Despite the significant progress of fully-supervised video captioning,\nzero-shot methods remain much less explored. In this paper, we propose a novel\nzero-shot video captioning framework named Retrieval-Enhanced Test-Time\nAdaptation (RETTA), which takes advantage of existing pretrained large-scale\nvision and language models to directly generate captions with test-time\nadaptation. Specifically, we bridge video and text using four key models: a\ngeneral video-text retrieval model XCLIP, a general image-text matching model\nCLIP, a text alignment model AnglE, and a text generation model GPT-2, due to\ntheir source-code availability. The main challenge is how to enable the text\ngeneration model to be sufficiently aware of the content in a given video so as\nto generate corresponding captions. To address this problem, we propose using\nlearnable tokens as a communication medium among these four frozen models\nGPT-2, XCLIP, CLIP, and AnglE. Different from the conventional way that trains\nthese tokens with training data, we propose to learn these tokens with soft\ntargets of the inference data under several carefully crafted loss functions,\nwhich enable the tokens to absorb video information catered for GPT-2. This\nprocedure can be efficiently done in just a few iterations (we use 16\niterations in the experiments) and does not require ground truth data.\nExtensive experimental results on three widely used datasets, MSR-VTT, MSVD,\nand VATEX, show absolute 5.1%-32.4% improvements in terms of the main metric\nCIDEr compared to several state-of-the-art zero-shot video captioning methods.\n","authors":["Yunchuan Ma","Laiyun Qing","Guorong Li","Yuankai Qi","Amin Beheshti","Quan Z. Sheng","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2405.07046v3.pdf","comment":"Published in Pattern Recognition"},{"id":"http://arxiv.org/abs/2510.24579v1","updated":"2025-10-28T16:13:14Z","published":"2025-10-28T16:13:14Z","title":"Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter\n  Correction in Cone-Beam CT","summary":"  Cone-beam CT (CBCT) employs a flat-panel detector to achieve\nthree-dimensional imaging with high spatial resolution. However, CBCT is\nsusceptible to scatter during data acquisition, which introduces CT value bias\nand reduced tissue contrast in the reconstructed images, ultimately degrading\ndiagnostic accuracy. To address this issue, we propose a deep learning-based\nscatter artifact correction method inspired by physical prior knowledge.\nLeveraging the fact that the observed point scatter probability density\ndistribution exhibits rotational symmetry in the projection domain. The method\nuses Gaussian Radial Basis Functions (RBF) to model the point scatter function\nand embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides\nefficient nonlinear mapping capabilities for learning high-dimensional scatter\nfeatures. By incorporating the physical characteristics of the scattered photon\ndistribution together with the complex function mapping capacity of KAN, the\nmodel improves its ability to accurately represent scatter. The effectiveness\nof the method is validated through both synthetic and real-scan experiments.\nExperimental results show that the model can effectively correct the scatter\nartifacts in the reconstructed images and is superior to the current methods in\nterms of quantitative metrics.\n","authors":["Xu Jiang","Huiying Pan","Ligen Shi","Jianing Sun","Wenfeng Xu","Xing Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.24579v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.12427v4","updated":"2025-10-28T16:06:34Z","published":"2025-02-18T01:52:41Z","title":"Frequency-Aware Vision Transformers for High-Fidelity Super-Resolution\n  of Earth System Models","summary":"  Super-resolution (SR) is crucial for enhancing the spatial fidelity of Earth\nSystem Model (ESM) outputs, allowing fine-scale structures vital to climate\nscience to be recovered from coarse simulations. However, traditional deep\nsuper-resolution methods, including convolutional and transformer-based models,\ntend to exhibit spectral bias, reconstructing low-frequency content more\nreadily than valuable high-frequency details. In this work, we introduce two\nfrequency-aware frameworks: the Vision Transformer-Tuned Sinusoidal Implicit\nRepresentation (ViSIR), combining Vision Transformers and sinusoidal\nactivations to mitigate spectral bias, and the Vision Transformer Fourier\nRepresentation Network (ViFOR), which integrates explicit Fourier-based\nfiltering for independent low- and high-frequency learning. Evaluated on the\nE3SM-HR Earth system dataset across surface temperature, shortwave, and\nlongwave fluxes, these models outperform leading CNN, GAN, and vanilla\ntransformer baselines, with ViFOR demonstrating up to 2.6~dB improvements in\nPSNR and significantly higher SSIM. Detailed ablation and scaling studies\nhighlight the benefit of full-field training, the impact of frequency\nhyperparameters, and the potential for generalization. The results establish\nViFOR as a state-of-the-art, scalable solution for climate data downscaling.\nFuture extensions will address temporal super-resolution, multimodal climate\nvariables, automated parameter selection, and integration of physical\nconservation constraints to broaden scientific applicability.\n","authors":["Ehsan Zeraatkar","Salah A Faroughi","Jelena Tešić"],"pdf_url":"https://arxiv.org/pdf/2502.12427v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06415v2","updated":"2025-10-28T16:04:58Z","published":"2025-03-09T03:17:28Z","title":"Polygonal network disorder and the turning distance","summary":"  The turning distance is a well-studied metric for measuring the similarity\nbetween two polygons. This metric is constructed by taking an $L^p$ distance\nbetween step functions which track each shape's tangent angle of a path tracing\nits boundary. In this study, we introduce \\textit{turning disorders} for\npolygonal planar networks, defined by averaging turning distances between\nnetwork faces and \"ordered\" shapes (regular polygons or circles). We derive\nclosed-form expressions of turning distances for special classes of regular\npolygons, related to the divisibility of $m$ and $n$, and also between regular\npolygons and circles. These formulas are used to show that the time for\ncomputing the 2-turning distances reduces to $O((m+n) \\log(m+n))$ when both\nshapes are regular polygons, an improvement from $O(mn\\log(mn))$ operations\nneeded to compute distances between general polygons of $n$ and $m$ sides. We\nalso apply these formulas to several examples of network microstructure with\nvarying disorder. For Archimedean lattices, a class of regular tilings, we can\nexpress turning disorders with exact expressions. We also consider turning\ndisorders applied to two examples of stochastic processes on networks: spring\nnetworks evolving under T1 moves and polygonal rupture processes. We find that\nthe two aspects of defining different turning disorders, the choice of ordered\nshape and whether to apply area-weighting, can capture different notions of\nnetwork disorder.\n","authors":["Alex Dolce","Ryan Lavelle","Bernard Scott","Ashlyn Urbanski","Joseph Klobusicky"],"pdf_url":"https://arxiv.org/pdf/2503.06415v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05177v3","updated":"2025-10-28T16:02:48Z","published":"2025-02-07T18:59:56Z","title":"Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with\n  Leading Short-Context Accuracy","summary":"  We introduce Long-VITA, a simple yet effective large multi-modal model for\nlong-context visual-language understanding tasks. It is adept at concurrently\nprocessing and analyzing modalities of image, video, and text over 4K frames or\n1M tokens while delivering advanced performances on short-context multi-modal\ntasks. We propose an effective multi-modal training schema that starts with\nlarge language models and proceeds through vision-language alignment, general\nknowledge learning, and two sequential stages of long-sequence fine-tuning. We\nfurther implement context-parallelism distributed inference and logits-masked\nlanguage modeling head to scale Long-VITA to infinitely long inputs of images\nand texts during model inference. Regarding training data, Long-VITA is built\non a mix of 17M samples from public datasets only and demonstrates\nstate-of-the-art performance on various multi-modal benchmarks, compared\nagainst recent cutting-edge models with internal data. Long-VITA is fully\nopen-source and reproducible.. By leveraging our inference designs, Long-VITA\nmodels achieve a remarkable 2x prefill speedup and 4x context length extension\nin a single node with 8 GPUs. We hope Long-VITA can serve as a competitive\nbaseline and offer valuable insights for the open-source community in advancing\nlong-context multi-modal understanding.\n","authors":["Yunhang Shen","Chaoyou Fu","Shaoqi Dong","Xiong Wang","Yi-Fan Zhang","Peixian Chen","Mengdan Zhang","Haoyu Cao","Ke Li","Shaohui Lin","Xiawu Zheng","Yan Zhang","Yiyi Zhou","Ran He","Caifeng Shan","Rongrong Ji","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2502.05177v3.pdf","comment":"https://github.com/VITA-MLLM/Long-VITA"},{"id":"http://arxiv.org/abs/2510.24563v1","updated":"2025-10-28T15:56:36Z","published":"2025-10-28T15:56:36Z","title":"OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents","summary":"  With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.\n","authors":["Hongrui Jia","Jitong Liao","Xi Zhang","Haiyang Xu","Tianbao Xie","Chaoya Jiang","Ming Yan","Si Liu","Wei Ye","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2510.24563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02293v2","updated":"2025-10-28T15:28:13Z","published":"2025-08-04T11:03:12Z","title":"Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning","summary":"  So-called unsupervised anomaly detection is better described as\nsemi-supervised, as it assumes all training data are nominal. This assumption\nsimplifies training but requires manual data curation, introducing bias and\nlimiting adaptability. We propose Confident Meta-learning (CoMet), a novel\ntraining strategy that enables deep anomaly detection models to learn from\nuncurated datasets where nominal and anomalous samples coexist, eliminating the\nneed for explicit filtering. Our approach integrates Soft Confident Learning,\nwhich assigns lower weights to low-confidence samples, and Meta-Learning, which\nstabilizes training by regularizing updates based on training validation loss\ncovariance. This prevents overfitting and enhances robustness to noisy data.\nCoMet is model-agnostic and can be applied to any anomaly detection method\ntrainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2\nwith two state-of-the-art models demonstrate the effectiveness of our approach,\nconsistently improving over the baseline methods, remaining insensitive to\nanomalies in the training set, and setting a new state-of-the-art across all\ndatasets. Code is available at https://github.com/aqeeelmirza/CoMet\n","authors":["Muhammad Aqeel","Shakiba Sharifi","Marco Cristani","Francesco Setti"],"pdf_url":"https://arxiv.org/pdf/2508.02293v2.pdf","comment":"Accepted to IEEE/CVF International Conference on Computer Vision\n  (ICCV2025)"},{"id":"http://arxiv.org/abs/2510.24514v1","updated":"2025-10-28T15:26:20Z","published":"2025-10-28T15:26:20Z","title":"Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs","summary":"  While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.\n","authors":["Huanyu Zhang","Wenshan Wu","Chengzu Li","Ning Shang","Yan Xia","Yangyu Huang","Yifan Zhang","Li Dong","Zhang Zhang","Liang Wang","Tieniu Tan","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2510.24514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17071v2","updated":"2025-10-28T15:20:36Z","published":"2025-03-21T11:54:16Z","title":"Superpowering Open-Vocabulary Object Detectors for X-ray Vision","summary":"  Open-vocabulary object detection (OvOD) is set to revolutionize security\nscreening by enabling systems to recognize any item in X-ray scans. However,\ndeveloping effective OvOD models for X-ray imaging presents unique challenges\ndue to data scarcity and the modality gap that prevents direct adoption of\nRGB-based solutions. To overcome these limitations, we propose RAXO, a\ntraining-free framework that repurposes off-the-shelf RGB OvOD detectors for\nrobust X-ray detection. RAXO builds high-quality X-ray class descriptors using\na dual-source retrieval strategy. It gathers relevant RGB images from the web\nand enriches them via a novel X-ray material transfer mechanism, eliminating\nthe need for labeled databases. These visual descriptors replace text-based\nclassification in OvOD, leveraging intra-modal feature distances for robust\ndetection. Extensive experiments demonstrate that RAXO consistently improves\nOvOD performance, providing an average mAP increase of up to 17.0 points over\nbase detectors. To further support research in this emerging field, we also\nintroduce DET-COMPASS, a new benchmark featuring bounding box annotations for\nover 300 object categories, enabling large-scale evaluation of OvOD in X-ray.\nCode and dataset available at: https://github.com/PAGF188/RAXO.\n","authors":["Pablo Garcia-Fernandez","Lorenzo Vaquero","Mingxuan Liu","Feng Xue","Daniel Cores","Nicu Sebe","Manuel Mucientes","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2503.17071v2.pdf","comment":"Accepted at ICCV 2025"},{"id":"http://arxiv.org/abs/2510.24503v1","updated":"2025-10-28T15:15:14Z","published":"2025-10-28T15:15:14Z","title":"Local Performance vs. Out-of-Distribution Generalization: An Empirical\n  Analysis of Personalized Federated Learning in Heterogeneous Data\n  Environments","summary":"  In the context of Federated Learning with heterogeneous data environments,\nlocal models tend to converge to their own local model optima during local\ntraining steps, deviating from the overall data distributions. Aggregation of\nthese local updates, e.g., with FedAvg, often does not align with the global\nmodel optimum (client drift), resulting in an update that is suboptimal for\nmost clients. Personalized Federated Learning approaches address this challenge\nby exclusively focusing on the average local performances of clients' models on\ntheir own data distribution. Generalization to out-of-distribution samples,\nwhich is a substantial benefit of FedAvg and represents a significant component\nof robustness, appears to be inadequately incorporated into the assessment and\nevaluation processes. This study involves a thorough evaluation of Federated\nLearning approaches, encompassing both their local performance and their\ngeneralization capabilities. Therefore, we examine different stages within a\nsingle communication round to enable a more nuanced understanding of the\nconsidered metrics. Furthermore, we propose and incorporate a modified approach\nof FedAvg, designated as Federated Learning with Individualized Updates (FLIU),\nextending the algorithm by a straightforward individualization step with an\nadaptive personalization factor. We evaluate and compare the approaches\nempirically using MNIST and CIFAR-10 under various distributional conditions,\nincluding benchmark IID and pathological non-IID, as well as additional novel\ntest environments with Dirichlet distribution specifically developed to stress\nthe algorithms on complex data heterogeneity.\n","authors":["Mortesa Hussaini","Jan Theiß","Anthony Stein"],"pdf_url":"https://arxiv.org/pdf/2510.24503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24486v1","updated":"2025-10-28T15:00:07Z","published":"2025-10-28T15:00:07Z","title":"Fast and accurate neural reflectance transformation imaging through\n  knowledge distillation","summary":"  Reflectance Transformation Imaging (RTI) is very popular for its ability to\nvisually analyze surfaces by enhancing surface details through interactive\nrelighting, starting from only a few tens of photographs taken with a fixed\ncamera and variable illumination. Traditional methods like Polynomial Texture\nMaps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle\nto accurately capture complex reflectance fields using few per-pixel\ncoefficients and fixed bases, leading to artifacts, especially in highly\nreflective or shadowed areas. The NeuralRTI approach, which exploits a neural\nautoencoder to learn a compact function that better approximates the local\nreflectance as a function of light directions, has been shown to produce\nsuperior quality at comparable storage cost. However, as it performs\ninteractive relighting with custom decoder networks with many parameters, the\nrendering step is computationally expensive and not feasible at full resolution\nfor large images on limited hardware. Earlier attempts to reduce costs by\ndirectly training smaller networks have failed to produce valid results. For\nthis reason, we propose to reduce its computational cost through a novel\nsolution based on Knowledge Distillation (DisK-NeuralRTI). ...\n","authors":["Tinsae G. Dulecha","Leonardo Righetto","Ruggero Pintus","Enrico Gobbetti","Andrea Giachetti"],"pdf_url":"https://arxiv.org/pdf/2510.24486v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2510.14383v2","updated":"2025-10-28T14:50:18Z","published":"2025-10-16T07:31:21Z","title":"DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with\n  Analytical Insights","summary":"  Accurate brain tumor segmentation is significant for clinical diagnosis and\ntreatment but remains challenging due to tumor heterogeneity. Mamba-based State\nSpace Models have demonstrated promising performance. However, despite their\ncomputational efficiency over other neural architectures, they incur\nconsiderable overhead for this task due to their sequential feature computation\nacross multiple spatial axes. Moreover, their robustness across diverse BraTS\ndata partitions remains largely unexplored, leaving a critical gap in reliable\nevaluation. To address this, we first propose a dual-resolution bi-directional\nMamba (DRBD-Mamba), an efficient 3D segmentation model that captures\nmulti-scale long-range dependencies with minimal computational overhead. We\nleverage a space-filling curve to preserve spatial locality during 3D-to-1D\nfeature mapping, thereby reducing reliance on computationally expensive\nmulti-axial feature scans. To enrich feature representation, we propose a gated\nfusion module that adaptively integrates forward and reverse contexts, along\nwith a quantization block that improves robustness. We further propose five\nsystematic folds on BraTS2023 for rigorous evaluation of segmentation\ntechniques under diverse conditions and present analysis of common failure\nscenarios. On the 20% test set used by recent methods, our model achieves Dice\nimprovements of 0.10% for whole tumor, 1.75% for tumor core, and 0.93% for\nenhancing tumor. Evaluations on the proposed systematic folds demonstrate that\nour model maintains competitive whole tumor accuracy while achieving clear\naverage Dice gains of 1.16% for tumor core and 1.68% for enhancing tumor over\nexisting state-of-the-art. Furthermore, our model achieves a 15x efficiency\nimprovement while maintaining high segmentation accuracy, highlighting its\nrobustness and computational advantage over existing methods.\n","authors":["Danish Ali","Ajmal Mian","Naveed Akhtar","Ghulam Mubashar Hassan"],"pdf_url":"https://arxiv.org/pdf/2510.14383v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.26386v2","updated":"2025-10-28T14:48:45Z","published":"2025-09-30T15:19:43Z","title":"PANDA: Towards Generalist Video Anomaly Detection via Agentic AI\n  Engineer","summary":"  Video anomaly detection (VAD) is a critical yet challenging task due to the\ncomplex and diverse nature of real-world scenarios. Previous methods typically\nrely on domain-specific training data and manual adjustments when applying to\nnew scenarios and unseen anomaly types, suffering from high labor costs and\nlimited generalization. Therefore, we aim to achieve generalist VAD, \\ie,\nautomatically handle any scene and any anomaly types without training data or\nhuman involvement. In this work, we propose PANDA, an agentic AI engineer based\non MLLMs. Specifically, we achieve PANDA by comprehensively devising four key\ncapabilities: (1) self-adaptive scene-aware strategy planning, (2) goal-driven\nheuristic reasoning, (3) tool-augmented self-reflection, and (4) self-improving\nchain-of-memory. Concretely, we develop a self-adaptive scene-aware RAG\nmechanism, enabling PANDA to retrieve anomaly-specific knowledge for anomaly\ndetection strategy planning. Next, we introduce a latent anomaly-guided\nheuristic prompt strategy to enhance reasoning precision. Furthermore, PANDA\nemploys a progressive reflection mechanism alongside a suite of context-aware\ntools to iteratively refine decision-making in complex scenarios. Finally, a\nchain-of-memory mechanism enables PANDA to leverage historical experiences for\ncontinual performance improvement. Extensive experiments demonstrate that PANDA\nachieves state-of-the-art performance in multi-scenario, open-set, and complex\nscenario settings without training and manual involvement, validating its\ngeneralizable and robust anomaly detection capability. Code is released at\nhttps://github.com/showlab/PANDA.\n","authors":["Zhiwei Yang","Chen Gao","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2509.26386v2.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24474v1","updated":"2025-10-28T14:43:48Z","published":"2025-10-28T14:43:48Z","title":"Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated\n  Sampling","summary":"  Denoising generative models, such as diffusion and flow-based models, produce\nhigh-quality samples but require many denoising steps due to discretization\nerror. Flow maps, which estimate the average velocity between timesteps,\nmitigate this error and enable faster sampling. However, their training\ntypically demands architectural changes that limit compatibility with\npretrained flow models. We introduce Decoupled MeanFlow, a simple decoding\nstrategy that converts flow models into flow map models without architectural\nmodifications. Our method conditions the final blocks of diffusion transformers\non the subsequent timestep, allowing pretrained flow models to be directly\nrepurposed as flow maps. Combined with enhanced training techniques, this\ndesign enables high-quality generation in as few as 1 to 4 steps. Notably, we\nfind that training flow models and subsequently converting them is more\nefficient and effective than training flow maps from scratch. On ImageNet\n256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,\nrespectively, surpassing prior art by a large margin. Furthermore, we achieve\nFID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the\nperformance of flow models while delivering over 100x faster inference.\n","authors":["Kyungmin Lee","Sihyun Yu","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2510.24474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17336v2","updated":"2025-10-28T14:31:14Z","published":"2025-09-22T03:13:58Z","title":"Mano Technical Report","summary":"  Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design.\n","authors":["Tianyu Fu","Anyang Su","Chenxu Zhao","Hanning Wang","Minghui Wu","Zhe Yu","Fei Hu","Mingjia Shi","Wei Dong","Jiayao Wang","Yuyang Chen","Ruiyang Yu","Siran Peng","Menglin Li","Nan Huang","Haitian Wei","Jiawei Yu","Yi Xin","Xilin Zhao","Kai Gu","Ping Jiang","Sifan Zhou","Shuo Wang"],"pdf_url":"https://arxiv.org/pdf/2509.17336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24464v1","updated":"2025-10-28T14:30:47Z","published":"2025-10-28T14:30:47Z","title":"Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras","summary":"  Markerless multiview motion capture is often constrained by the need for\nprecise camera calibration, limiting accessibility for non-experts and\nin-the-wild captures. Existing calibration-free approaches mitigate this\nrequirement but suffer from high computational cost and reduced reconstruction\naccuracy.\n  We present Kineo, a fully automatic, calibration-free pipeline for markerless\nmotion capture from videos captured by unsynchronized, uncalibrated,\nconsumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf\ndetectors to simultaneously calibrate cameras, including Brown-Conrady\ndistortion coefficients, and reconstruct 3D keypoints and dense scene point\nmaps at metric scale. A confidence-driven spatio-temporal keypoint sampling\nstrategy, combined with graph-based global optimization, ensures robust\ncalibration at a fixed computational cost independent of sequence length. We\nfurther introduce a pairwise reprojection consensus score to quantify 3D\nreconstruction reliability for downstream tasks.\n  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements\nover prior calibration-free methods. Compared to previous state-of-the-art\napproaches, Kineo reduces camera translation error by approximately 83-85%,\ncamera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by\n83-91%.\n  Kineo is also efficient in real-world scenarios, processing multi-view\nsequences faster than their duration in specific configuration (e.g., 36min to\nprocess 1h20min of footage). The full pipeline and evaluation code are openly\nreleased to promote reproducibility and practical adoption at\nhttps://liris-xr.github.io/kineo/.\n","authors":["Charles Javerliat","Pierre Raimbaud","Guillaume Lavoué"],"pdf_url":"https://arxiv.org/pdf/2510.24464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21724v2","updated":"2025-10-28T14:26:23Z","published":"2025-05-27T20:12:46Z","title":"OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions","summary":"  In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task designed to produce synchronized verbal and\nnon-verbal listener feedback online, based on the speaker's multimodal inputs.\nOMCRG captures natural dyadic interactions and introduces new challenges in\naligning generated audio with listeners' facial responses. To tackle these\nchallenges, we incorporate text as an intermediate modality to connect audio\nand facial responses. We propose OmniResponse, a Multimodal Large Language\nModel (MLLM) that autoregressively generates accurate multimodal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two core\ncomponents: Chrono-Text Markup, which precisely timestamps generated text\ntokens, and TempoVoice, a controllable online text-to-speech (TTS) module that\noutputs speech synchronized with facial responses. To advance OMCRG research,\nwe offer ResponseNet, a dataset of 696 detailed dyadic interactions featuring\nsynchronized split-screen videos, multichannel audio, transcripts, and\nannotated facial behaviors. Comprehensive evaluations on ResponseNet\ndemonstrate that OmniResponse outperforms baseline models in terms of semantic\nspeech content, audio-visual synchronization, and generation quality. Our\ndataset, code, and models are publicly available.\n","authors":["Cheng Luo","Jianghui Wang","Bing Li","Siyang Song","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2505.21724v2.pdf","comment":"25 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.24456v1","updated":"2025-10-28T14:24:34Z","published":"2025-10-28T14:24:34Z","title":"A Critical Study towards the Detection of Parkinsons Disease using ML\n  Technologies","summary":"  The proposed solution is Deep Learning Technique that will be able classify\nthree types of tea leaves diseases from which two diseases are caused by the\npests and one due to pathogens (infectious organisms) and environmental\nconditions and also show the area damaged by a disease in leaves. Namely Red\nRust, Helopeltis and Red spider mite respectively. In this paper we have\nevaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for\nthe object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU\nrange of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.\nWhile Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95\nand recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than\nSSD. Also used Mask R-CNN for Object Instance Segmentation where we have\nimplemented our custom method to calculate the damaged diseased portion of\nleaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red\nSpider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.\n","authors":["Vivek Chetia","Abdul Taher Khan","Rahish Gogoi","David Kapsian Khual","Purnendu Bikash","Sajal Saha"],"pdf_url":"https://arxiv.org/pdf/2510.24456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.20072v2","updated":"2025-10-28T14:22:20Z","published":"2025-08-27T17:39:11Z","title":"Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding\n  in Vision-Language-Action Policies","summary":"  Vision-Language-Action (VLA) models adapt large vision-language backbones to\nmap images and instructions into robot actions. However, prevailing VLAs either\ngenerate actions auto-regressively in a fixed left-to-right order or attach\nseparate MLP or diffusion heads outside the backbone, leading to fragmented\ninformation pathways and specialized training requirements that hinder a\nunified, scalable architecture. We present Discrete Diffusion VLA, a\nunified-transformer policy that models discretized action chunks with discrete\ndiffusion. The design retains diffusion's progressive refinement paradigm while\nremaining natively compatible with the discrete token interface of VLMs. Our\nmethod achieves an adaptive decoding order that resolves easy action elements\nbefore harder ones and uses secondary re-masking to revisit uncertain\npredictions across refinement rounds, which improves consistency and enables\nrobust error correction. This unified decoder preserves pre-trained\nvision-language priors, supports parallel decoding, breaks the autoregressive\nbottleneck, and reduces the number of function evaluations. Discrete Diffusion\nVLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on\nSimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge, improving over\nautoregressive, MLP decoder and continuous diffusion baselines. These findings\nindicate that discrete-diffusion VLA supports precise action modeling and\nconsistent training, laying groundwork for scaling VLA to larger models and\ndatasets. Our project page is https://github.com/Liang-ZX/DiscreteDiffusionVLA\n","authors":["Zhixuan Liang","Yizhuo Li","Tianshuo Yang","Chengyue Wu","Sitong Mao","Tian Nian","Liuao Pei","Shunbo Zhou","Xiaokang Yang","Jiangmiao Pang","Yao Mu","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2508.20072v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2510.22200v2","updated":"2025-10-28T14:19:57Z","published":"2025-10-25T07:41:02Z","title":"LongCat-Video Technical Report","summary":"  Video generation is a critical pathway toward world models, with efficient\nlong video inference as a key capability. Toward this end, we introduce\nLongCat-Video, a foundational video generation model with 13.6B parameters,\ndelivering strong performance across multiple video generation tasks. It\nparticularly excels in efficient and high-quality long video generation,\nrepresenting our first step toward world models. Key features include: Unified\narchitecture for multiple tasks: Built on the Diffusion Transformer (DiT)\nframework, LongCat-Video supports Text-to-Video, Image-to-Video, and\nVideo-Continuation tasks with a single model; Long video generation:\nPretraining on Video-Continuation tasks enables LongCat-Video to maintain high\nquality and temporal coherence in the generation of minutes-long videos;\nEfficient inference: LongCat-Video generates 720p, 30fps videos within minutes\nby employing a coarse-to-fine generation strategy along both the temporal and\nspatial axes. Block Sparse Attention further enhances efficiency, particularly\nat high resolutions; Strong performance with multi-reward RLHF: Multi-reward\nRLHF training enables LongCat-Video to achieve performance on par with the\nlatest closed-source and leading open-source models. Code and model weights are\npublicly available to accelerate progress in the field.\n","authors":[" Meituan LongCat Team","Xunliang Cai","Qilong Huang","Zhuoliang Kang","Hongyu Li","Shijun Liang","Liya Ma","Siyu Ren","Xiaoming Wei","Rixu Xie","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.22200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24448v1","updated":"2025-10-28T14:12:11Z","published":"2025-10-28T14:12:11Z","title":"Rethinking Visual Intelligence: Insights from Video Pretraining","summary":"  Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.\n","authors":["Pablo Acuaviva","Aram Davtyan","Mariam Hassan","Sebastian Stapf","Ahmad Rahimi","Alexandre Alahi","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2510.24448v1.pdf","comment":"Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on\n  visual intelligence. This work can be considered as v2"},{"id":"http://arxiv.org/abs/2507.14643v2","updated":"2025-10-28T14:09:59Z","published":"2025-07-19T14:38:03Z","title":"Multispectral State-Space Feature Fusion: Bridging Shared and\n  Cross-Parametric Interactions for Object Detection","summary":"  Modern multispectral feature fusion for object detection faces two critical\nlimitations: (1) Excessive preference for local complementary features over\ncross-modal shared semantics adversely affects generalization performance; and\n(2) The trade-off between the receptive field size and computational complexity\npresent critical bottlenecks for scalable feature modeling. Addressing these\nissues, a novel Multispectral State-Space Feature Fusion framework, dubbed\nMS2Fusion, is proposed based on the state space model (SSM), achieving\nefficient and effective fusion through a dual-path parametric interaction\nmechanism. More specifically, the first cross-parameter interaction branch\ninherits the advantage of cross-attention in mining complementary information\nwith cross-modal hidden state decoding in SSM. The second shared-parameter\nbranch explores cross-modal alignment with joint embedding to obtain\ncross-modal similar semantic features and structures through parameter sharing\nin SSM. Finally, these two paths are jointly optimized with SSM for fusing\nmultispectral features in a unified framework, allowing our MS2Fusion to enjoy\nboth functional complementarity and shared semantic space. In our extensive\nexperiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our\nMS2Fusion significantly outperforms other state-of-the-art multispectral object\ndetection methods, evidencing its superiority. Moreover, MS2Fusion is general\nand applicable to other multispectral perception tasks. We show that, even\nwithout specific design, MS2Fusion achieves state-of-the-art results on RGB-T\nsemantic segmentation and RGBT salient object detection, showing its\ngenerality. The source code will be available at\nhttps://github.com/61s61min/MS2Fusion.git.\n","authors":["Jifeng Shen","Haibo Zhan","Shaohua Dong","Xin Zuo","Wankou Yang","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2507.14643v2.pdf","comment":"submitted on 30/4/2025, Accepted by Information Fusion"},{"id":"http://arxiv.org/abs/2510.24446v1","updated":"2025-10-28T14:09:05Z","published":"2025-10-28T14:09:05Z","title":"SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box\n  Adversarial Paraphrasing in Text Autoencoder Latent Space","summary":"  Multimodal large language models (MLLMs) have shown impressive capabilities\nin vision-language tasks such as reasoning segmentation, where models generate\nsegmentation masks based on textual queries. While prior work has primarily\nfocused on perturbing image inputs, semantically equivalent textual\nparaphrases-crucial in real-world applications where users express the same\nintent in varied ways-remain underexplored. To address this gap, we introduce a\nnovel adversarial paraphrasing task: generating grammatically correct\nparaphrases that preserve the original query meaning while degrading\nsegmentation performance. To evaluate the quality of adversarial paraphrases,\nwe develop a comprehensive automatic evaluation protocol validated with human\nstudies. Furthermore, we introduce SPARTA-a black-box, sentence-level\noptimization method that operates in the low-dimensional semantic latent space\nof a text autoencoder, guided by reinforcement learning. SPARTA achieves\nsignificantly higher success rates, outperforming prior methods by up to 2x on\nboth the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive\nbaselines to assess the robustness of advanced reasoning segmentation models.\nWe reveal that they remain vulnerable to adversarial paraphrasing-even under\nstrict semantic and grammatical constraints. All code and data will be released\npublicly upon acceptance.\n","authors":["Viktoriia Zinkovich","Anton Antonov","Andrei Spiridonov","Denis Shepelev","Andrey Moskalenko","Daria Pugacheva","Elena Tutubalina","Andrey Kuznetsov","Vlad Shakhuro"],"pdf_url":"https://arxiv.org/pdf/2510.24446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24437v1","updated":"2025-10-28T14:04:19Z","published":"2025-10-28T14:04:19Z","title":"Deeply-Conditioned Image Compression via Self-Generated Priors","summary":"  Learned image compression (LIC) has shown great promise for achieving high\nrate-distortion performance. However, current LIC methods are often limited in\ntheir capability to model the complex correlation structures inherent in\nnatural images, particularly the entanglement of invariant global structures\nwith transient local textures within a single monolithic representation. This\nlimitation precipitates severe geometric deformation at low bitrates. To\naddress this, we introduce a framework predicated on functional decomposition,\nwhich we term Deeply-Conditioned Image Compression via self-generated priors\n(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior\nto encapsulate the image's structural backbone. This prior is subsequently\nutilized not as mere side-information, but to holistically modulate the entire\ncompression pipeline. This deep conditioning, most critically of the analysis\ntransform, liberates it to dedicate its representational capacity to the\nresidual, high-entropy details. This hierarchical, dependency-driven approach\nachieves an effective disentanglement of information streams. Our extensive\nexperiments validate this assertion; visual analysis demonstrates that our\nmethod substantially mitigates the geometric deformation artifacts that plague\nconventional codecs at low bitrates. Quantitatively, our framework establishes\nhighly competitive performance, achieving significant BD-rate reductions of\n14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,\nand Tecnick datasets.\n","authors":["Zhineng Zhao","Zhihai He","Zikun Zhou","Siwei Ma","Yaowei Wang"],"pdf_url":"https://arxiv.org/pdf/2510.24437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08423v4","updated":"2025-10-28T13:53:29Z","published":"2025-05-13T10:35:57Z","title":"DArFace: Deformation Aware Robustness for Low Quality Face Recognition","summary":"  Facial recognition systems have achieved remarkable success by leveraging\ndeep neural networks, advanced loss functions, and large-scale datasets.\nHowever, their performance often deteriorates in real-world scenarios involving\nlow-quality facial images. Such degradations, common in surveillance footage or\nstandoff imaging include low resolution, motion blur, and various distortions,\nresulting in a substantial domain gap from the high-quality data typically used\nduring training. While existing approaches attempt to address robustness by\nmodifying network architectures or modeling global spatial transformations,\nthey frequently overlook local, non-rigid deformations that are inherently\npresent in real-world settings. In this work, we introduce \\textbf{DArFace}, a\n\\textbf{D}eformation-\\textbf{A}ware \\textbf{r}obust \\textbf{Face} recognition\nframework that enhances robustness to such degradations without requiring\npaired high- and low-quality training samples. Our method adversarially\nintegrates both global transformations (e.g., rotation, translation) and local\nelastic deformations during training to simulate realistic low-quality\nconditions. Moreover, we introduce a contrastive objective to enforce identity\nconsistency across different deformed views. Extensive evaluations on\nlow-quality benchmarks including TinyFace, IJB-B, and IJB-C demonstrate that\nDArFace surpasses state-of-the-art methods, with significant gains attributed\nto the inclusion of local deformation modeling.\n","authors":["Sadaf Gulshad","Abdullah Aldahlawi"],"pdf_url":"https://arxiv.org/pdf/2505.08423v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05900v2","updated":"2025-10-28T13:30:55Z","published":"2024-10-08T10:57:33Z","title":"MTFL: Multi-Timescale Feature Learning for Weakly-Supervised Anomaly\n  Detection in Surveillance Videos","summary":"  Detection of anomaly events is relevant for public safety and requires a\ncombination of fine-grained motion information and contextual events at\nvariable time-scales. To this end, we propose a Multi-Timescale Feature\nLearning (MTFL) method to enhance the representation of anomaly features.\nShort, medium, and long temporal tubelets are employed to extract\nspatio-temporal video features using a Video Swin Transformer. Experimental\nresults demonstrate that MTFL outperforms state-of-the-art methods on the\nUCF-Crime dataset, achieving an anomaly detection performance 89.78% AUC.\nMoreover, it performs complementary to SotA with 95.32% AUC on the ShanghaiTech\nand 84.57% AP on the XD-Violence dataset. Furthermore, we generate an extended\ndataset of the UCF-Crime for development and evaluation on a wider range of\nanomalies, namely Video Anomaly Detection Dataset (VADD), involving 2,591\nvideos in 18 classes with extensive coverage of realistic anomalies.\n","authors":["Yiling Zhang","Erkut Akdag","Egor Bondarev","Peter H. N. De With"],"pdf_url":"https://arxiv.org/pdf/2410.05900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24414v1","updated":"2025-10-28T13:27:38Z","published":"2025-10-28T13:27:38Z","title":"XAI Evaluation Framework for Semantic Segmentation","summary":"  Ensuring transparency and trust in artificial intelligence (AI) models is\nessential, particularly as they are increasingly applied in safety-critical and\nhigh-stakes domains. Explainable AI (XAI) has emerged as a promising approach\nto address this challenge, yet the rigorous evaluation of XAI methods remains\ncrucial for optimizing the trade-offs between model complexity, predictive\nperformance, and interpretability. While extensive progress has been achieved\nin evaluating XAI techniques for classification tasks, evaluation strategies\ntailored to semantic segmentation remain relatively underexplored. This work\nintroduces a comprehensive and systematic evaluation framework specifically\ndesigned for assessing XAI in semantic segmentation, explicitly accounting for\nboth spatial and contextual task complexities. The framework employs\npixel-level evaluation strategies and carefully designed metrics to provide\nfine-grained interpretability insights. Simulation results using recently\nadapted class activation mapping (CAM)-based XAI schemes demonstrate the\nefficiency, robustness, and reliability of the proposed methodology. These\nfindings contribute to advancing transparent, trustworthy, and accountable\nsemantic segmentation models.\n","authors":["Reem Hammoud","Abdul karim Gizzini","Ali J. Ghandour"],"pdf_url":"https://arxiv.org/pdf/2510.24414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24413v1","updated":"2025-10-28T13:23:32Z","published":"2025-10-28T13:23:32Z","title":"50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir,\n  Lebanon","summary":"  The sustainable management of the Qaraaoun Reservoir, the largest surface\nwater body in Lebanon located in the Bekaa Plain, depends on reliable\nmonitoring of its storage volume despite frequent sensor malfunctions and\nlimited maintenance capacity. This study introduces a sensor-free approach that\nintegrates open-source satellite imagery, advanced water-extent segmentation,\nand machine learning to estimate the reservoir surface area and volume in near\nreal time. Sentinel-2 and Landsat images are processed, where surface water is\ndelineated using a newly proposed water segmentation index. A machine learning\nmodel based on Support Vector Regression (SVR) is trained on a curated dataset\nthat includes water surface area, water level, and water volume calculations\nusing a reservoir bathymetry survey. The model is then able to estimate\nreservoir volume relying solely on surface area extracted from satellite\nimagery, without the need for ground measurements. Water segmentation using the\nproposed index aligns with ground truth for more than 95 percent of the\nshoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR\nperformance with error under 1.5 percent of full reservoir capacity and\ncoefficients of determination exceeding 0.98. These results demonstrate the\nrobustness and cost-effectiveness of the method, offering a practical solution\nfor continuous, sensor-independent monitoring of reservoir storage. The\nproposed methodology can be replicated for other water bodies, and the\nresulting 50 years of time-series data is valuable for research on climate\nchange and environmental patterns.\n","authors":["Ali Ahmad Faour","Nabil Amacha","Ali J. Ghandour"],"pdf_url":"https://arxiv.org/pdf/2510.24413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24411v1","updated":"2025-10-28T13:22:39Z","published":"2025-10-28T13:22:39Z","title":"OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid\n  Validation in Realistic Workflows","summary":"  Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.\n","authors":["Qiushi Sun","Mukai Li","Zhoumianze Liu","Zhihui Xie","Fangzhi Xu","Zhangyue Yin","Kanzhi Cheng","Zehao Li","Zichen Ding","Qi Liu","Zhiyong Wu","Zhuosheng Zhang","Ben Kao","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2510.24411v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2510.24410v1","updated":"2025-10-28T13:22:24Z","published":"2025-10-28T13:22:24Z","title":"A Hybrid Approach for Visual Multi-Object Tracking","summary":"  This paper proposes a visual multi-object tracking method that jointly\nemploys stochastic and deterministic mechanisms to ensure identifier\nconsistency for unknown and time-varying target numbers under nonlinear\ndynamics. A stochastic particle filter addresses nonlinear dynamics and\nnon-Gaussian noise, with support from particle swarm optimization (PSO) to\nguide particles toward state distribution modes and mitigate divergence through\nproposed fitness measures incorporating motion consistency, appearance\nsimilarity, and social-interaction cues with neighboring targets. Deterministic\nassociation further enforces identifier consistency via a proposed cost matrix\nincorporating spatial consistency between particles and current detections,\ndetection confidences, and track penalties. Subsequently, a novel scheme is\nproposed for the smooth updating of target states while preserving their\nidentities, particularly for weak tracks during interactions with other targets\nand prolonged occlusions. Moreover, velocity regression over past states\nprovides trend-seed velocities, enhancing particle sampling and state updates.\nThe proposed tracker is designed to operate flexibly for both pre-recorded\nvideos and camera live streams, where future frames are unavailable.\nExperimental results confirm superior performance compared to state-of-the-art\ntrackers. The source-code reference implementations of both the proposed method\nand compared-trackers are provided on GitHub:\nhttps://github.com/SDU-VelKoTek/GenTrack2\n","authors":["Toan Van Nguyen","Rasmus G. K. Christiansen","Dirk Kraft","Leon Bodenhagen"],"pdf_url":"https://arxiv.org/pdf/2510.24410v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2510.24399v1","updated":"2025-10-28T13:13:20Z","published":"2025-10-28T13:13:20Z","title":"GenTrack: A New Generation of Multi-Object Tracking","summary":"  This paper introduces a novel multi-object tracking (MOT) method, dubbed\nGenTrack, whose main contributions include: a hybrid tracking approach\nemploying both stochastic and deterministic manners to robustly handle unknown\nand time-varying numbers of targets, particularly in maintaining target\nidentity (ID) consistency and managing nonlinear dynamics, leveraging particle\nswarm optimization (PSO) with some proposed fitness measures to guide\nstochastic particles toward their target distribution modes, enabling effective\ntracking even with weak and noisy object detectors, integration of social\ninteractions among targets to enhance PSO-guided particles as well as improve\ncontinuous updates of both strong (matched) and weak (unmatched) tracks,\nthereby reducing ID switches and track loss, especially during occlusions, a\nGenTrack-based redefined visual MOT baseline incorporating a comprehensive\nstate and observation model based on space consistency, appearance, detection\nconfidence, track penalties, and social scores for systematic and efficient\ntarget updates, and the first-ever publicly available source-code reference\nimplementation with minimal dependencies, featuring three variants, including\nGenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.\nExperimental results have shown that GenTrack provides superior performance on\nstandard benchmarks and real-world scenarios compared to state-of-the-art\ntrackers, with integrated implementations of baselines for fair comparison.\nPotential directions for future work are also discussed. The source-code\nreference implementations of both the proposed method and compared-trackers are\nprovided on GitHub: https://github.com/SDU-VelKoTek/GenTrack\n","authors":["Toan Van Nguyen","Rasmus G. K. Christiansen","Dirk Kraft","Leon Bodenhagen"],"pdf_url":"https://arxiv.org/pdf/2510.24399v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2510.24398v1","updated":"2025-10-28T13:13:01Z","published":"2025-10-28T13:13:01Z","title":"Unsupervised Detection of Post-Stroke Brain Abnormalities","summary":"  Post-stroke MRI not only delineates focal lesions but also reveals secondary\nstructural changes, such as atrophy and ventricular enlargement. These\nabnormalities, increasingly recognised as imaging biomarkers of recovery and\noutcome, remain poorly captured by supervised segmentation methods. We evaluate\nREFLECT, a flow-based generative model, for unsupervised detection of both\nfocal and non-lesional abnormalities in post-stroke patients. Using dual-expert\ncentral-slice annotations on ATLAS data, performance was assessed at the object\nlevel with Free-Response ROC analysis for anomaly maps. Two models were trained\non lesion-free slices from stroke patients (ATLAS) and on healthy controls\n(IXI) to test the effect of training data. On ATLAS test subjects, the\nIXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and\nimproved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).\nTraining on fully healthy anatomy improves the modelling of normal variability,\nenabling broader and more reliable detection of structural abnormalities.\n","authors":["Youwan Mahé","Elise Bannier","Stéphanie Leplaideur","Elisa Fromont","Francesca Galassi"],"pdf_url":"https://arxiv.org/pdf/2510.24398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.19585v2","updated":"2025-10-28T13:04:38Z","published":"2025-10-22T13:37:52Z","title":"Detecting Latin in Historical Books with Large Language Models: A\n  Multimodal Benchmark","summary":"  This paper presents a novel task of extracting Latin fragments from\nmixed-language historical documents with varied layouts. We benchmark and\nevaluate the performance of large foundation models against a multimodal\ndataset of 724 annotated pages. The results demonstrate that reliable Latin\ndetection with contemporary models is achievable. Our study provides the first\ncomprehensive analysis of these models' capabilities and limits for this task.\n","authors":["Yu Wu","Ke Shu","Jonas Fischer","Lidia Pivovarova","David Rosson","Eetu Mäkelä","Mikko Tolonen"],"pdf_url":"https://arxiv.org/pdf/2510.19585v2.pdf","comment":"Under review. Both the dataset and code will be published"},{"id":"http://arxiv.org/abs/2405.03520v2","updated":"2025-10-28T13:04:23Z","published":"2024-05-06T14:37:07Z","title":"Is Sora a World Simulator? A Comprehensive Survey on General World\n  Models and Beyond","summary":"  General world models represent a crucial pathway toward achieving Artificial\nGeneral Intelligence (AGI), serving as the cornerstone for various applications\nranging from virtual environments to decision-making systems. Recently, the\nemergence of the Sora model has attained significant attention due to its\nremarkable simulation capabilities, which exhibits an incipient comprehension\nof physical laws. In this survey, we embark on a comprehensive exploration of\nthe latest advancements in world models. Our analysis navigates through the\nforefront of generative methodologies in video generation, where world models\nstand as pivotal constructs facilitating the synthesis of highly realistic\nvisual content. Additionally, we scrutinize the burgeoning field of\nautonomous-driving world models, meticulously delineating their indispensable\nrole in reshaping transportation and urban mobility. Furthermore, we delve into\nthe intricacies inherent in world models deployed within autonomous agents,\nshedding light on their profound significance in enabling intelligent\ninteractions within dynamic environmental contexts. At last, we examine\nchallenges and limitations of world models, and discuss their potential future\ndirections. We hope this survey can serve as a foundational reference for the\nresearch community and inspire continued innovation. This survey will be\nregularly updated at:\nhttps://github.com/GigaAI-research/General-World-Models-Survey.\n","authors":["Zheng Zhu","Xiaofeng Wang","Wangbo Zhao","Chen Min","Bohan Li","Nianchen Deng","Min Dou","Yuqi Wang","Botian Shi","Kai Wang","Chi Zhang","Yang You","Zhaoxiang Zhang","Dawei Zhao","Liang Xiao","Jian Zhao","Jiwen Lu","Guan Huang"],"pdf_url":"https://arxiv.org/pdf/2405.03520v2.pdf","comment":"This survey will be regularly updated at:\n  https://github.com/GigaAI-research/General-World-Models-Survey"},{"id":"http://arxiv.org/abs/2510.24385v1","updated":"2025-10-28T13:01:42Z","published":"2025-10-28T13:01:42Z","title":"When are radiology reports useful for training medical image\n  classifiers?","summary":"  Medical images used to train machine learning models are often accompanied by\nradiology reports containing rich expert annotations. However, relying on these\nreports as inputs for clinical prediction requires the timely manual work of a\ntrained radiologist. This raises a natural question: when can radiology reports\nbe leveraged during training to improve image-only classification? Prior works\nare limited to evaluating pre-trained image representations by fine-tuning them\nto predict diagnostic labels, often extracted from reports, ignoring tasks with\nlabels that are weakly associated with the text. To address this gap, we\nconduct a systematic study of how radiology reports can be used during both\npre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,\n12-month readmission), and under varying training set sizes. Our findings\nreveal that: (1) Leveraging reports during pre-training is beneficial for\ndownstream classification tasks where the label is well-represented in the\ntext; however, pre-training through explicit image-text alignment can be\ndetrimental in settings where it's not; (2) Fine-tuning with reports can lead\nto significant improvements and even have a larger impact than the pre-training\nmethod in certain settings. These results provide actionable insights into when\nand how to leverage privileged text data to train medical image classifiers\nwhile highlighting gaps in current research.\n","authors":["Herman Bergström","Zhongqi Yue","Fredrik D. Johansson"],"pdf_url":"https://arxiv.org/pdf/2510.24385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24379v1","updated":"2025-10-28T12:57:42Z","published":"2025-10-28T12:57:42Z","title":"A Luminance-Aware Multi-Scale Network for Polarization Image Fusion with\n  a Multi-Scene Dataset","summary":"  Polarization image fusion combines S0 and DOLP images to reveal surface\nroughness and material properties through complementary texture features, which\nhas important applications in camouflage recognition, tissue pathology\nanalysis, surface defect detection and other fields. To intergrate\ncoL-Splementary information from different polarized images in complex\nluminance environment, we propose a luminance-aware multi-scale network (MLSN).\nIn the encoder stage, we propose a multi-scale spatial weight matrix through a\nbrightness-branch , which dynamically weighted inject the luminance into the\nfeature maps, solving the problem of inherent contrast difference in polarized\nimages. The global-local feature fusion mechanism is designed at the bottleneck\nlayer to perform windowed self-attention computation, to balance the global\ncontext and local details through residual linking in the feature dimension\nrestructuring stage. In the decoder stage, to further improve the adaptability\nto complex lighting, we propose a Brightness-Enhancement module, establishing\nthe mapping relationship between luminance distribution and texture features,\nrealizing the nonlinear luminance correction of the fusion result. We also\npresent MSP, an 1000 pairs of polarized images that covers 17 types of indoor\nand outdoor complex lighting scenes. MSP provides four-direction polarization\nraw maps, solving the scarcity of high-quality datasets in polarization image\nfusion. Extensive experiment on MSP, PIF and GAND datasets verify that the\nproposed MLSN outperms the state-of-the-art methods in subjective and objective\nevaluations, and the MS-SSIM and SD metircs are higher than the average values\nof other methods by 8.57%, 60.64%, 10.26%, 63.53%, 22.21%, and 54.31%,\nrespectively. The source code and dataset is avalable at\nhttps://github.com/1hzf/MLS-UNet.\n","authors":["Zhuangfan Huang","Xiaosong Li","Gao Wang","Tao Ye","Haishu Tan","Huafeng Li"],"pdf_url":"https://arxiv.org/pdf/2510.24379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24378v1","updated":"2025-10-28T12:56:48Z","published":"2025-10-28T12:56:48Z","title":"Stroke Lesion Segmentation in Clinical Workflows: A Modular,\n  Lightweight, and Deployment-Ready Tool","summary":"  Deep learning frameworks such as nnU-Net achieve state-of-the-art performance\nin brain lesion segmentation but remain difficult to deploy clinically due to\nheavy dependencies and monolithic design. We introduce \\textit{StrokeSeg}, a\nmodular and lightweight framework that translates research-grade stroke lesion\nsegmentation models into deployable applications. Preprocessing, inference, and\npostprocessing are decoupled: preprocessing relies on the Anima toolbox with\nBIDS-compliant outputs, and inference uses ONNX Runtime with \\texttt{Float16}\nquantisation, reducing model size by about 50\\%. \\textit{StrokeSeg} provides\nboth graphical and command-line interfaces and is distributed as Python scripts\nand as a standalone Windows executable. On a held-out set of 300 sub-acute and\nchronic stroke subjects, segmentation performance was equivalent to the\noriginal PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that\nhigh-performing research pipelines can be transformed into portable, clinically\nusable tools.\n","authors":["Yann Kerverdo","Florent Leray","Youwan Mahé","Stéphanie Leplaideur","Francesca Galassi"],"pdf_url":"https://arxiv.org/pdf/2510.24378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24374v1","updated":"2025-10-28T12:51:53Z","published":"2025-10-28T12:51:53Z","title":"Decoupling What to Count and Where to See for Referring Expression\n  Counting","summary":"  Referring Expression Counting (REC) extends class-level object counting to\nthe fine-grained subclass-level, aiming to enumerate objects matching a textual\nexpression that specifies both the class and distinguishing attribute. A\nfundamental challenge, however, has been overlooked: annotation points are\ntypically placed on class-representative locations (e.g., heads), forcing\nmodels to focus on class-level features while neglecting attribute information\nfrom other visual regions (e.g., legs for \"walking\"). To address this, we\npropose W2-Net, a novel framework that explicitly decouples the problem into\n\"what to count\" and \"where to see\" via a dual-query mechanism. Specifically,\nalongside the standard what-to-count (w2c) queries that localize the object, we\nintroduce dedicated where-to-see (w2s) queries. The w2s queries are guided to\nseek and extract features from attribute-specific visual regions, enabling\nprecise subclass discrimination. Furthermore, we introduce Subclass Separable\nMatching (SSM), a novel matching strategy that incorporates a repulsive force\nto enhance inter-subclass separability during label assignment. W2-Net\nsignificantly outperforms the state-of-the-art on the REC-8K dataset, reducing\ncounting error by 22.5% (validation) and 18.0% (test), and improving\nlocalization F1 by 7% and 8%, respectively. Code will be available.\n","authors":["Yuda Zou","Zijian Zhang","Yongchao Xu"],"pdf_url":"https://arxiv.org/pdf/2510.24374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11842v3","updated":"2025-10-28T12:44:07Z","published":"2025-05-17T05:06:38Z","title":"Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs","summary":"  The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies.\n","authors":["Xuannan Liu","Zekun Li","Zheqi He","Peipei Li","Shuhan Xia","Xing Cui","Huaibo Huang","Xi Yang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2505.11842v3.pdf","comment":"Accepted by NeurIPS 2025 Dataset and Benchmark Track, Project page:\n  https://liuxuannan.github.io/Video-SafetyBench.github.io/"},{"id":"http://arxiv.org/abs/2510.24366v1","updated":"2025-10-28T12:42:33Z","published":"2025-10-28T12:42:33Z","title":"Adaptive Knowledge Transferring with Switching Dual-Student Framework\n  for Semi-Supervised Medical Image Segmentation","summary":"  Teacher-student frameworks have emerged as a leading approach in\nsemi-supervised medical image segmentation, demonstrating strong performance\nacross various tasks. However, the learning effects are still limited by the\nstrong correlation and unreliable knowledge transfer process between teacher\nand student networks. To overcome this limitation, we introduce a novel\nswitching Dual-Student architecture that strategically selects the most\nreliable student at each iteration to enhance dual-student collaboration and\nprevent error reinforcement. We also introduce a strategy of Loss-Aware\nExponential Moving Average to dynamically ensure that the teacher absorbs\nmeaningful information from students, improving the quality of pseudo-labels.\nOur plug-and-play framework is extensively evaluated on 3D medical image\nsegmentation datasets, where it outperforms state-of-the-art semi-supervised\nmethods, demonstrating its effectiveness in improving segmentation accuracy\nunder limited supervision.\n","authors":["Thanh-Huy Nguyen","Hoang-Thien Nguyen","Ba-Thinh Lam","Vi Vu","Bach X. Nguyen","Jianhua Xing","Tianyang Wang","Xingjian Li","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2510.24366v1.pdf","comment":"The paper is under review at Pattern Recognition Journal"},{"id":"http://arxiv.org/abs/2505.24424v2","updated":"2025-10-28T12:08:40Z","published":"2025-05-30T10:04:00Z","title":"Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning","summary":"  Vision-language models like CLIP have demonstrated remarkable zero-shot\ncapabilities in classification and retrieval. However, these models often\nstruggle with compositional reasoning - the ability to understand the\nrelationships between concepts. A recent benchmark, SugarCrepe++, reveals that\nprevious works on improving compositionality have mainly improved lexical\nsensitivity but neglected semantic understanding. In addition, downstream\nretrieval performance often deteriorates, although one would expect that\nimproving compositionality should enhance retrieval. In this work, we introduce\nCLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a\nnovel training technique combining multiple images and their associated\ncaptions. CLIC improves compositionality across architectures as well as\ndifferently pre-trained CLIP models, both in terms of lexical and semantic\nunderstanding, and achieves consistent gains in retrieval performance. This\neven applies to the recent CLIPS, which achieves SOTA retrieval performance.\nNevertheless, the short fine-tuning with CLIC leads to an improvement in\nretrieval and to the best compositional CLIP model on SugarCrepe++. All our\nmodels and code are available at https://clic-compositional-clip.github.io\n","authors":["Amit Peleg","Naman Deep Singh","Matthias Hein"],"pdf_url":"https://arxiv.org/pdf/2505.24424v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24335v1","updated":"2025-10-28T11:57:33Z","published":"2025-10-28T11:57:33Z","title":"NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation","summary":"  We present NVSim, a framework that automatically constructs large-scale,\nnavigable indoor simulators from only common image sequences, overcoming the\ncost and scalability limitations of traditional 3D scanning. Our approach\nadapts 3D Gaussian Splatting to address visual artifacts on sparsely observed\nfloors a common issue in robotic traversal data. We introduce Floor-Aware\nGaussian Splatting to ensure a clean, navigable ground plane, and a novel\nmesh-free traversability checking algorithm that constructs a topological graph\nby directly analyzing rendered views. We demonstrate our system's ability to\ngenerate valid, large-scale navigation graphs from real-world data. A video\ndemonstration is avilable at https://youtu.be/tTiIQt6nXC8\n","authors":["Mingyu Jeong","Eunsung Kim","Sehun Park","Andrew Jaeyong Choi"],"pdf_url":"https://arxiv.org/pdf/2510.24335v1.pdf","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2510.24332v1","updated":"2025-10-28T11:55:45Z","published":"2025-10-28T11:55:45Z","title":"Sound Source Localization for Spatial Mapping of Surgical Actions in\n  Dynamic Scenes","summary":"  Purpose: Surgical scene understanding is key to advancing computer-aided and\nintelligent surgical systems. Current approaches predominantly rely on visual\ndata or end-to-end learning, which limits fine-grained contextual modeling.\nThis work aims to enhance surgical scene representations by integrating 3D\nacoustic information, enabling temporally and spatially aware multimodal\nunderstanding of surgical environments.\n  Methods: We propose a novel framework for generating 4D audio-visual\nrepresentations of surgical scenes by projecting acoustic localization\ninformation from a phased microphone array onto dynamic point clouds from an\nRGB-D camera. A transformer-based acoustic event detection module identifies\nrelevant temporal segments containing tool-tissue interactions which are\nspatially localized in the audio-visual scene representation. The system was\nexperimentally evaluated in a realistic operating room setup during simulated\nsurgical procedures performed by experts.\n  Results: The proposed method successfully localizes surgical acoustic events\nin 3D space and associates them with visual scene elements. Experimental\nevaluation demonstrates accurate spatial sound localization and robust fusion\nof multimodal data, providing a comprehensive, dynamic representation of\nsurgical activity.\n  Conclusion: This work introduces the first approach for spatial sound\nlocalization in dynamic surgical scenes, marking a significant advancement\ntoward multimodal surgical scene representations. By integrating acoustic and\nvisual data, the proposed framework enables richer contextual understanding and\nprovides a foundation for future intelligent and autonomous surgical systems.\n","authors":["Jonas Hein","Lazaros Vlachopoulos","Maurits Geert Laurent Olthof","Bastian Sigrist","Philipp Fürnstahl","Matthias Seibold"],"pdf_url":"https://arxiv.org/pdf/2510.24332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24331v1","updated":"2025-10-28T11:55:24Z","published":"2025-10-28T11:55:24Z","title":"What do vision-language models see in the context? Investigating\n  multimodal in-context learning","summary":"  In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks\nfrom demonstration examples without parameter updates. Although it has been\nextensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)\nremains underexplored. In this work, we present a systematic study of ICL in\nVLMs, evaluating seven models spanning four architectures on three image\ncaptioning benchmarks. We analyze how prompt design, architectural choices, and\ntraining strategies influence multimodal ICL. To our knowledge, we are the\nfirst to analyze how attention patterns in VLMs vary with an increasing number\nof in-context demonstrations. Our results reveal that training on imag-text\ninterleaved data enhances ICL performance but does not imply effective\nintegration of visual and textual information from demonstration examples. In\ncontrast, instruction tuning improves instruction-following but can reduce\nreliance on in-context demonstrations, suggesting a trade-off between\ninstruction alignment and in-context adaptation. Attention analyses further\nshow that current VLMs primarily focus on textual cues and fail to leverage\nvisual information, suggesting a limited capacity for multimodal integration.\nThese findings highlight key limitations in the ICL abilities of current VLMs\nand provide insights for enhancing their ability to learn from multimodal\nin-context examples.\n","authors":["Gabriel O. dos Santos","Esther Colombini","Sandra Avila"],"pdf_url":"https://arxiv.org/pdf/2510.24331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24321v1","updated":"2025-10-28T11:39:22Z","published":"2025-10-28T11:39:22Z","title":"Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt\n  Learning","summary":"  Remote sensing applications increasingly rely on deep learning for scene\nclassification. However, their performance is often constrained by the scarcity\nof labeled data and the high cost of annotation across diverse geographic and\nsensor domains. While recent vision-language models like CLIP have shown\npromise by learning transferable representations at scale by aligning visual\nand textual modalities, their direct application to remote sensing remains\nsuboptimal due to significant domain gaps and the need for task-specific\nsemantic adaptation. To address this critical challenge, we systematically\nexplore prompt learning as a lightweight and efficient adaptation strategy for\nfew-shot remote sensing image scene classification. We evaluate several\nrepresentative methods, including Context Optimization, Conditional Context\nOptimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating\nConstraints. These approaches reflect complementary design philosophies: from\nstatic context optimization to conditional prompts for enhanced generalization,\nmulti-modal prompts for joint vision-language adaptation, and semantically\nregularized prompts for stable learning without forgetting. We benchmark these\nprompt-learning methods against two standard baselines: zero-shot CLIP with\nhand-crafted prompts and a linear probe trained on frozen CLIP features.\nThrough extensive experiments on multiple benchmark remote sensing datasets,\nincluding cross-dataset generalization tests, we demonstrate that prompt\nlearning consistently outperforms both baselines in few-shot scenarios.\nNotably, Prompting with Self-Regulating Constraints achieves the most robust\ncross-domain performance. Our findings underscore prompt learning as a scalable\nand efficient solution for bridging the domain gap in satellite and aerial\nimagery, providing a strong foundation for future research in this field.\n","authors":["Ivica Dimitrovski","Vlatko Spasev","Ivan Kitanovski"],"pdf_url":"https://arxiv.org/pdf/2510.24321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.20234v3","updated":"2025-10-28T11:26:53Z","published":"2025-09-24T15:24:43Z","title":"ImageNet-trained CNNs are not biased towards texture: Revisiting feature\n  reliance through controlled suppression","summary":"  The hypothesis that Convolutional Neural Networks (CNNs) are inherently\ntexture-biased has shaped much of the discourse on feature use in deep\nlearning. We revisit this hypothesis by examining limitations in the\ncue-conflict experiment by Geirhos et al. To address these limitations, we\npropose a domain-agnostic framework that quantifies feature reliance through\nsystematic suppression of shape, texture, and color cues, avoiding the\nconfounds of forced-choice conflicts. By evaluating humans and neural networks\nunder controlled suppression conditions, we find that CNNs are not inherently\ntexture-biased but predominantly rely on local shape features. Nonetheless,\nthis reliance can be substantially mitigated through modern training strategies\nor architectures (ConvNeXt, ViTs). We further extend the analysis across\ncomputer vision, medical imaging, and remote sensing, revealing that reliance\npatterns differ systematically: computer vision models prioritize shape,\nmedical imaging models emphasize color, and remote sensing models exhibit a\nstronger reliance on texture. Code is available at\nhttps://github.com/tomburgert/feature-reliance.\n","authors":["Tom Burgert","Oliver Stoll","Paolo Rota","Begüm Demir"],"pdf_url":"https://arxiv.org/pdf/2509.20234v3.pdf","comment":"Accepted at NeurIPS 2025 (oral)"},{"id":"http://arxiv.org/abs/2510.23497v2","updated":"2025-10-28T11:09:37Z","published":"2025-10-27T16:32:12Z","title":"VOLD: Reasoning Transfer from LLMs to Vision-Language Models via\n  On-Policy Distillation","summary":"  Training vision-language models (VLMs) for complex reasoning remains a\nchallenging task, i.a. due to the scarcity of high-quality image-text reasoning\ndata. Conversely, text-based reasoning resources are abundant and scalable, but\nit is still an open question how to leveraging them for VLM reasoning. To\naddress this problem, we propose VOLD, a framework to transfer reasoning\ncapabilities from text-only teacher models to VLM student models. To this end,\nVOLD combines reinforcement learning via Group Relative Policy Optimization\n(GRPO) with on-policy distillation, which allows the student reasoning traces\nto be guided by the teacher model, resulting in a significant gain over using\nGRPO alone. We further show that a cold-start alignment is essential for an\neffective transfer during the online training phase in this scenario and that\nwithout sufficient distributional alignment between teacher and student,\non-policy distillation fails to provide meaningful guidance. We evaluate VOLD\nacross diverse benchmarks including MMMU-Pro, MathVision, MathVista, and\nLogicVista, showing that VOLD outperforms the baseline model significantly and\nimproves over the state of the art by a margin. Our ablation shows the\nimportance of a cold-start alignment via SFT for on-policy distillation with a\ntext-only teacher.\n","authors":["Walid Bousselham","Hilde Kuehne","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2510.23497v2.pdf","comment":"www.walidbousselham.com/VOLD/"},{"id":"http://arxiv.org/abs/2510.23444v2","updated":"2025-10-28T10:58:40Z","published":"2025-10-27T15:46:07Z","title":"FRBNet: Revisiting Low-Light Vision through Frequency-Domain Radial\n  Basis Network","summary":"  Low-light vision remains a fundamental challenge in computer vision due to\nsevere illumination degradation, which significantly affects the performance of\ndownstream tasks such as detection and segmentation. While recent\nstate-of-the-art methods have improved performance through invariant feature\nlearning modules, they still fall short due to incomplete modeling of low-light\nconditions. Therefore, we revisit low-light image formation and extend the\nclassical Lambertian model to better characterize low-light conditions. By\nshifting our analysis to the frequency domain, we theoretically prove that the\nfrequency-domain channel ratio can be leveraged to extract\nillumination-invariant features via a structured filtering process. We then\npropose a novel and end-to-end trainable module named \\textbf{F}requency-domain\n\\textbf{R}adial \\textbf{B}asis \\textbf{Net}work (\\textbf{FRBNet}), which\nintegrates the frequency-domain channel ratio operation with a learnable\nfrequency domain filter for the overall illumination-invariant feature\nenhancement. As a plug-and-play module, FRBNet can be integrated into existing\nnetworks for low-light downstream tasks without modifying loss functions.\nExtensive experiments across various downstream tasks demonstrate that FRBNet\nachieves superior performance, including +2.2 mAP for dark object detection and\n+2.9 mIoU for nighttime segmentation. Code is available at:\nhttps://github.com/Sing-Forevet/FRBNet.\n","authors":["Fangtong Sun","Congyu Li","Ke Yang","Yuchen Pan","Hanwen Yu","Xichuan Zhang","Yiying Li"],"pdf_url":"https://arxiv.org/pdf/2510.23444v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00129v2","updated":"2025-10-28T10:56:55Z","published":"2025-05-30T18:05:33Z","title":"Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware\n  Sign Language Translation","summary":"  Recent progress in Sign Language Translation (SLT) has focussed primarily on\nimproving the representational capacity of large language models to incorporate\nSign Language features. This work explores an alternative direction: enhancing\nthe geometric properties of skeletal representations themselves. We propose\nGeo-Sign, a method that leverages the properties of hyperbolic geometry to\nmodel the hierarchical structure inherent in sign language kinematics. By\nprojecting skeletal features derived from Spatio-Temporal Graph Convolutional\nNetworks (ST-GCNs) into the Poincar\\'e ball model, we aim to create more\ndiscriminative embeddings, particularly for fine-grained motions like finger\narticulations. We introduce a hyperbolic projection layer, a weighted Fr\\'echet\nmean aggregation scheme, and a geometric contrastive loss operating directly in\nhyperbolic space. These components are integrated into an end-to-end\ntranslation framework as a regularisation function, to enhance the\nrepresentations within the language model. This work demonstrates the potential\nof hyperbolic geometry to improve skeletal representations for Sign Language\nTranslation, improving on SOTA RGB methods while preserving privacy and\nimproving computational efficiency. Code available here:\nhttps://github.com/ed-fish/geo-sign.\n","authors":["Edward Fish","Richard Bowden"],"pdf_url":"https://arxiv.org/pdf/2506.00129v2.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24285v1","updated":"2025-10-28T10:42:57Z","published":"2025-10-28T10:42:57Z","title":"ViPER: Empowering the Self-Evolution of Visual Perception Abilities in\n  Vision-Language Model","summary":"  The limited capacity for fine-grained visual perception presents a critical\nbottleneck for Vision-Language Models (VLMs) in real-world applications.\nAddressing this is challenging due to the scarcity of high-quality data and the\nlimitations of existing methods: supervised fine-tuning (SFT) often compromises\ngeneral capabilities, while reinforcement fine-tuning (RFT) prioritizes textual\nreasoning over visual perception. To bridge this gap, we propose a novel\ntwo-stage task that structures visual perception learning as a coarse-to-fine\nprogressive process. Based on this task formulation, we develop ViPER, a\nself-bootstrapping framework specifically designed to enable iterative\nevolution through self-critiquing and self-prediction. By synergistically\nintegrating image-level and instance-level reconstruction with a two-stage\nreinforcement learning strategy, ViPER establishes a closed-loop training\nparadigm, where internally synthesized data directly fuel the enhancement of\nperceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the\nQwen-Viper series. With an average gain of 1.7% on seven comprehensive\nbenchmarks spanning various tasks and up to 6.0% on fine-grained perception,\nQwen-Viper consistently demonstrates superior performance across different\nvision-language scenarios while maintaining generalizability. Beyond enabling\nself-improvement in perceptual capabilities, ViPER provides concrete evidence\nfor the reciprocal relationship between generation and understanding, a\nbreakthrough to developing more autonomous and capable VLMs.\n","authors":["Juntian Zhang","Song Jin","Chuanqi Cheng","Yuhan Liu","Yankai Lin","Xun Zhang","Yufei Zhang","Fei Jiang","Guojun Yin","Wei Lin","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2510.24285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24278v1","updated":"2025-10-28T10:39:04Z","published":"2025-10-28T10:39:04Z","title":"Training-free Source Attribution of AI-generated Images via Resynthesis","summary":"  Synthetic image source attribution is a challenging task, especially in data\nscarcity conditions requiring few-shot or zero-shot classification\ncapabilities. We present a new training-free one-shot attribution method based\non image resynthesis. A prompt describing the image under analysis is\ngenerated, then it is used to resynthesize the image with all the candidate\nsources. The image is attributed to the model which produced the resynthesis\nclosest to the original image in a proper feature space. We also introduce a\nnew dataset for synthetic image attribution consisting of face images from\ncommercial and open-source text-to-image generators. The dataset provides a\nchallenging attribution framework, useful for developing new attribution models\nand testing their capabilities on different generative architectures. The\ndataset structure allows to test approaches based on resynthesis and to compare\nthem to few-shot methods. Results from state-of-the-art few-shot approaches and\nother baselines show that the proposed resynthesis method outperforms existing\ntechniques when only a few samples are available for training or fine-tuning.\nThe experiments also demonstrate that the new dataset is a challenging one and\nrepresents a valuable benchmark for developing and evaluating future few-shot\nand zero-shot methods.\n","authors":["Pietro Bongini","Valentina Molinari","Andrea Costanzo","Benedetta Tondi","Mauro Barni"],"pdf_url":"https://arxiv.org/pdf/2510.24278v1.pdf","comment":"14 pages, 4 figures, 1 table, accepted at \"The 17th IEEE\n  INTERNATIONAL WORKSHOP ON INFORMATION FORENSICS AND SECURITY (WIFS2025)\",\n  Perth, Australia"},{"id":"http://arxiv.org/abs/2412.18833v2","updated":"2025-10-28T10:38:37Z","published":"2024-12-25T08:40:03Z","title":"Federated Learning with Partially Labeled Data: A Conditional\n  Distillation Approach","summary":"  In medical imaging, developing generalized segmentation models that can\nhandle multiple organs and lesions is crucial. However, the scarcity of fully\nannotated datasets and strict privacy regulations present significant barriers\nto data sharing. Federated Learning (FL) allows decentralized model training,\nbut existing FL methods often struggle with partial labeling, leading to model\ndivergence and catastrophic forgetting. We propose ConDistFL, a novel FL\nframework incorporating conditional distillation to address these challenges.\nConDistFL enables effective learning from partially labeled datasets,\nsignificantly improving segmentation accuracy across distributed and\nnon-uniform datasets. In addition to its superior segmentation performance,\nConDistFL maintains computational and communication efficiency, ensuring its\nscalability for real-world applications. Furthermore, ConDistFL demonstrates\nremarkable generalizability, significantly outperforming existing FL methods in\nout-of-federation tests, even adapting to unseen contrast phases (e.g.,\nnon-contrast CT images) in our experiments. Extensive evaluations on 3D CT and\n2D chest X-ray datasets show that ConDistFL is an efficient, adaptable solution\nfor collaborative medical image segmentation in privacy-constrained settings.\n","authors":["Pochuan Wang","Chen Shen","Masahiro Oda","Chiou-Shann Fuh","Kensaku Mori","Weichung Wang","Holger R. Roth"],"pdf_url":"https://arxiv.org/pdf/2412.18833v2.pdf","comment":"This manuscript was submitted to IEEE JBHI and is currently under\n  peer review"},{"id":"http://arxiv.org/abs/2510.23225v2","updated":"2025-10-28T10:25:00Z","published":"2025-10-27T11:23:04Z","title":"Through the Lens: Benchmarking Deepfake Detectors Against\n  Moiré-Induced Distortions","summary":"  Deepfake detection remains a pressing challenge, particularly in real-world\nsettings where smartphone-captured media from digital screens often introduces\nMoir\\'e artifacts that can distort detection outcomes. This study\nsystematically evaluates state-of-the-art (SOTA) deepfake detectors on\nMoir\\'e-affected videos, an issue that has received little attention. We\ncollected a dataset of 12,832 videos, spanning 35.64 hours, from the Celeb-DF,\nDFD, DFDC, UADFV, and FF++ datasets, capturing footage under diverse real-world\nconditions, including varying screens, smartphones, lighting setups, and camera\nangles. To further examine the influence of Moir\\'e patterns on deepfake\ndetection, we conducted additional experiments using our DeepMoir\\'eFake,\nreferred to as (DMF) dataset and two synthetic Moir\\'e generation techniques.\nAcross 15 top-performing detectors, our results show that Moir\\'e artifacts\ndegrade performance by as much as 25.4%, while synthetically generated Moir\\'e\npatterns lead to a 21.4% drop in accuracy. Surprisingly, demoir\\'eing methods,\nintended as a mitigation approach, instead worsened the problem, reducing\naccuracy by up to 17.2%. These findings underscore the urgent need for\ndetection models that can robustly handle Moir\\'e distortions alongside other\nrealworld challenges, such as compression, sharpening, and blurring. By\nintroducing the DMF dataset, we aim to drive future research toward closing the\ngap between controlled experiments and practical deepfake detection.\n","authors":["Razaib Tariq","Minji Heo","Simon S. Woo","Shahroz Tariq"],"pdf_url":"https://arxiv.org/pdf/2510.23225v2.pdf","comment":"48 Pages, 29 Figures, 15 Tables"},{"id":"http://arxiv.org/abs/2510.24261v1","updated":"2025-10-28T10:17:11Z","published":"2025-10-28T10:17:11Z","title":"DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic\n  Manipulation","summary":"  Learning generalizable robotic manipulation policies remains a key challenge\ndue to the scarcity of diverse real-world training data. While recent\napproaches have attempted to mitigate this through self-supervised\nrepresentation learning, most either rely on 2D vision pretraining paradigms\nsuch as masked image modeling, which primarily focus on static semantics or\nscene geometry, or utilize large-scale video prediction models that emphasize\n2D dynamics, thus failing to jointly learn the geometry, semantics, and\ndynamics required for effective manipulation. In this paper, we present\nDynaRend, a representation learning framework that learns 3D-aware and\ndynamics-informed triplane features via masked reconstruction and future\nprediction using differentiable volumetric rendering. By pretraining on\nmulti-view RGB-D video data, DynaRend jointly captures spatial geometry, future\ndynamics, and task semantics in a unified triplane representation. The learned\nrepresentations can be effectively transferred to downstream robotic\nmanipulation tasks via action value map prediction. We evaluate DynaRend on two\nchallenging benchmarks, RLBench and Colosseum, as well as in real-world robotic\nexperiments, demonstrating substantial improvements in policy success rate,\ngeneralization to environmental perturbations, and real-world applicability\nacross diverse manipulation tasks.\n","authors":["Jingyi Tian","Le Wang","Sanping Zhou","Sen Wang","Jiayi Li","Gang Hua"],"pdf_url":"https://arxiv.org/pdf/2510.24261v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24262v1","updated":"2025-10-28T10:17:11Z","published":"2025-10-28T10:17:11Z","title":"UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level\n  Task Adaptation","summary":"  Data augmentation using generative models has emerged as a powerful paradigm\nfor enhancing performance in computer vision tasks. However, most existing\naugmentation approaches primarily focus on optimizing intrinsic data attributes\n-- such as fidelity and diversity -- to generate visually high-quality\nsynthetic data, while often neglecting task-specific requirements. Yet, it is\nessential for data generators to account for the needs of downstream tasks, as\ntraining data requirements can vary significantly across different tasks and\nnetwork architectures. To address these limitations, we propose UtilGen, a\nnovel utility-centric data augmentation framework that adaptively optimizes the\ndata generation process to produce task-specific, high-utility training data\nvia downstream task feedback. Specifically, we first introduce a weight\nallocation network to evaluate the task-specific utility of each synthetic\nsample. Guided by these evaluations, UtilGen iteratively refines the data\ngeneration process using a dual-level optimization strategy to maximize the\nsynthetic data utility: (1) model-level optimization tailors the generative\nmodel to the downstream task, and (2) instance-level optimization adjusts\ngeneration policies -- such as prompt embeddings and initial noise -- at each\ngeneration round. Extensive experiments on eight benchmark datasets of varying\ncomplexity and granularity demonstrate that UtilGen consistently achieves\nsuperior performance, with an average accuracy improvement of 3.87% over\nprevious SOTA. Further analysis of data influence and distribution reveals that\nUtilGen produces more impactful and task-relevant synthetic data, validating\nthe effectiveness of the paradigm shift from visual characteristics-centric to\ntask utility-centric data augmentation.\n","authors":["Jiyu Guo","Shuo Yang","Yiming Huang","Yancheng Long","Xiaobo Xia","Xiu Su","Bo Zhao","Zeke Xie","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2510.24262v1.pdf","comment":"39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)"},{"id":"http://arxiv.org/abs/2510.24260v1","updated":"2025-10-28T10:14:23Z","published":"2025-10-28T10:14:23Z","title":"DeshadowMamba: Deshadowing as 1D Sequential Similarity","summary":"  Recent deep models for image shadow removal often rely on attention-based\narchitectures to capture long-range dependencies. However, their fixed\nattention patterns tend to mix illumination cues from irrelevant regions,\nleading to distorted structures and inconsistent colors. In this work, we\nrevisit shadow removal from a sequence modeling perspective and explore the use\nof Mamba, a selective state space model that propagates global context through\ndirectional state transitions. These transitions yield an efficient global\nreceptive field while preserving positional continuity. Despite its potential,\ndirectly applying Mamba to image data is suboptimal, since it lacks awareness\nof shadow-non-shadow semantics and remains susceptible to color interference\nfrom nearby regions. To address these limitations, we propose CrossGate, a\ndirectional modulation mechanism that injects shadow-aware similarity into\nMamba's input gate, allowing selective integration of relevant context along\ntransition axes. To further ensure appearance fidelity, we introduce ColorShift\nregularization, a contrastive learning objective driven by global color\nstatistics. By synthesizing structured informative negatives, it guides the\nmodel to suppress color contamination and achieve robust color restoration.\nTogether, these components adapt sequence modeling to the structural integrity\nand chromatic consistency required for shadow removal. Extensive experiments on\npublic benchmarks demonstrate that DeshadowMamba achieves state-of-the-art\nvisual quality and strong quantitative performance.\n","authors":["Zhaotong Yang","Yi Chen","Yanying Li","Shengfeng He","Yangyang Xu","Junyu Dong","Jian Yang","Yong Du"],"pdf_url":"https://arxiv.org/pdf/2510.24260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.00037v3","updated":"2025-10-28T09:55:21Z","published":"2025-09-26T14:42:23Z","title":"On Robustness of Vision-Language-Action Model against Multi-Modal\n  Perturbations","summary":"  In Vision-Language-Action (VLA) models, robustness to real-world\nperturbations is critical for deployment. Existing methods target simple visual\ndisturbances, overlooking the broader multi-modal perturbations that arise in\nactions, instructions, environments, and observations. Here, we first evaluate\nthe robustness of mainstream VLAs under 17 perturbations across four\nmodalities. We find (1) actions as the most fragile modality, (2) Existing\nvisual-robust VLA do not gain robustness in other modality, and (3) pi0\ndemonstrates superior robustness with a diffusion-based action head. To build\nmulti-modal robust VLAs, we propose RobustVLA against perturbations in VLA\ninputs and outputs. For output robustness, we perform offline robust\noptimization against worst-case action noise that maximizes mismatch in flow\nmatching objective. This can be seen as adversarial training, label smoothing,\nand outlier penalization. For input robustness, we enforce consistent actions\nacross input variations that preserve task semantics. To account for multiple\nperturbations, we formulate robustness as a multi-armed bandit problem and\napply an upper confidence bound algorithm to automatically identify the most\nharmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers\nabsolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the\nOpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference\nthan existing visual-robust VLAs, and a 10.4% gain under mixed perturbations.\nOur RobustVLA is particularly effective on real-world FR5 robot with limited\ndemonstrations, showing absolute gains by 65.6% under perturbations of four\nmodalities.\n","authors":["Jianing Guo","Zhenhong Wu","Chang Tu","Yiyao Ma","Xiangqi Kong","Zhiqian Liu","Jiaming Ji","Shuning Zhang","Yuanpei Chen","Kai Chen","Qi Dou","Yaodong Yang","Xianglong Liu","Huijie Zhao","Weifeng Lv","Simin Li"],"pdf_url":"https://arxiv.org/pdf/2510.00037v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24232v1","updated":"2025-10-28T09:41:42Z","published":"2025-10-28T09:41:42Z","title":"Delving into Cascaded Instability: A Lipschitz Continuity View on Image\n  Restoration and Object Detection Synergy","summary":"  To improve detection robustness in adverse conditions (e.g., haze and low\nlight), image restoration is commonly applied as a pre-processing step to\nenhance image quality for the detector. However, the functional mismatch\nbetween restoration and detection networks can introduce instability and hinder\neffective integration -- an issue that remains underexplored. We revisit this\nlimitation through the lens of Lipschitz continuity, analyzing the functional\ndifferences between restoration and detection networks in both the input space\nand the parameter space. Our analysis shows that restoration networks perform\nsmooth, continuous transformations, while object detectors operate with\ndiscontinuous decision boundaries, making them highly sensitive to minor\nperturbations. This mismatch introduces instability in traditional cascade\nframeworks, where even imperceptible noise from restoration is amplified during\ndetection, disrupting gradient flow and hindering optimization. To address\nthis, we propose Lipschitz-regularized object detection (LROD), a simple yet\neffective framework that integrates image restoration directly into the\ndetector's feature learning, harmonizing the Lipschitz continuity of both tasks\nduring training. We implement this framework as Lipschitz-regularized YOLO\n(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive\nexperiments on haze and low-light benchmarks demonstrate that LR-YOLO\nconsistently improves detection stability, optimization smoothness, and overall\naccuracy.\n","authors":["Qing Zhao","Weijian Deng","Pengxu Wei","ZiYi Dong","Hannan Lu","Xiangyang Ji","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2510.24232v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24231v1","updated":"2025-10-28T09:41:30Z","published":"2025-10-28T09:41:30Z","title":"Benchmarking Microsaccade Recognition with Event Cameras: A Novel\n  Dataset and Evaluation","summary":"  Microsaccades are small, involuntary eye movements vital for visual\nperception and neural processing. Traditional microsaccade studies typically\nuse eye trackers or frame-based analysis, which, while precise, are costly and\nlimited in scalability and temporal resolution. Event-based sensing offers a\nhigh-speed, low-latency alternative by capturing fine-grained spatiotemporal\nchanges efficiently. This work introduces a pioneering event-based microsaccade\ndataset to support research on small eye movement dynamics in cognitive\ncomputing. Using Blender, we render high-fidelity eye movement scenarios and\nsimulate microsaccades with angular displacements from 0.5 to 2.0 degrees,\ndivided into seven distinct classes. These are converted to event streams using\nv2e, preserving the natural temporal dynamics of microsaccades, with durations\nranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,\nSpiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an\noptical-flow-enhanced variant implemented in SpikingJelly. The models achieve\naround 90 percent average accuracy, successfully classifying microsaccades by\nangular displacement, independent of event count or duration. These results\ndemonstrate the potential of spiking neural networks for fine motion\nrecognition and establish a benchmark for event-based vision research. The\ndataset, code, and trained models will be publicly available at\nhttps://waseemshariff126.github.io/microsaccades/ .\n","authors":["Waseem Shariff","Timothy Hanley","Maciej Stec","Hossein Javidnia","Peter Corcoran"],"pdf_url":"https://arxiv.org/pdf/2510.24231v1.pdf","comment":"Accepted in British Machine Vision Conference (BMVC) 2025, Main\n  Conference"},{"id":"http://arxiv.org/abs/2505.20510v2","updated":"2025-10-28T09:39:55Z","published":"2025-05-26T20:22:19Z","title":"CPathAgent: An Agent-based Foundation Model for Interpretable\n  High-Resolution Pathology Image Analysis Mimicking Pathologists' Diagnostic\n  Logic","summary":"  Recent advances in computational pathology have led to the emergence of\nnumerous foundation models. These models typically rely on general-purpose\nencoders with multi-instance learning for whole slide image (WSI)\nclassification or apply multimodal approaches to generate reports directly from\nimages. However, these models cannot emulate the diagnostic approach of\npathologists, who systematically examine slides at low magnification to obtain\nan overview before progressively zooming in on suspicious regions to formulate\ncomprehensive diagnoses. Instead, existing models directly output final\ndiagnoses without revealing the underlying reasoning process. To address this\ngap, we introduce CPathAgent, an innovative agent-based approach that mimics\npathologists' diagnostic workflow by autonomously navigating across WSI based\non observed visual features, thereby generating substantially more transparent\nand interpretable diagnostic summaries. To achieve this, we develop a\nmulti-stage training strategy that unifies patch-level, region-level, and\nWSI-level capabilities within a single model, which is essential for\nreplicating how pathologists understand and reason across diverse image scales.\nAdditionally, we construct PathMMU-HR2, the first expert-validated benchmark\nfor large region analysis. This represents a critical intermediate scale\nbetween patches and whole slides, reflecting a key clinical reality where\npathologists typically examine several key large regions rather than entire\nslides at once. Extensive experiments demonstrate that CPathAgent consistently\noutperforms existing approaches across benchmarks at three different image\nscales, validating the effectiveness of our agent-based diagnostic approach and\nhighlighting a promising direction for computational pathology.\n","authors":["Yuxuan Sun","Yixuan Si","Chenglu Zhu","Kai Zhang","Zhongyi Shui","Bowen Ding","Tao Lin","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2505.20510v2.pdf","comment":"52 pages, 34 figures"},{"id":"http://arxiv.org/abs/2509.17550v3","updated":"2025-10-28T09:32:18Z","published":"2025-09-22T09:09:13Z","title":"Is It Certainly a Deepfake? Reliability Analysis in Detection &\n  Generation Ecosystem","summary":"  As generative models are advancing in quality and quantity for creating\nsynthetic content, deepfakes begin to cause online mistrust. Deepfake detectors\nare proposed to counter this effect, however, misuse of detectors claiming fake\ncontent as real or vice versa further fuels this misinformation problem. We\npresent the first comprehensive uncertainty analysis of deepfake detectors,\nsystematically investigating how generative artifacts influence prediction\nconfidence. As reflected in detectors' responses, deepfake generators also\ncontribute to this uncertainty as their generative residues vary, so we cross\nthe uncertainty analysis of deepfake detectors and generators. Based on our\nobservations, the uncertainty manifold holds enough consistent information to\nleverage uncertainty for deepfake source detection. Our approach leverages\nBayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and\nepistemic uncertainties across diverse detector architectures. We evaluate\nuncertainty on two datasets with nine generators, with four blind and two\nbiological detectors, compare different uncertainty methods, explore region-\nand pixel-based uncertainty, and conduct ablation studies. We conduct and\nanalyze binary real/fake, multi-class real/fake, source detection, and\nleave-one-out experiments between the generator/detector combinations to share\ntheir generalization capability, model calibration, uncertainty, and robustness\nagainst adversarial attacks. We further introduce uncertainty maps that\nlocalize prediction confidence at the pixel level, revealing distinct patterns\ncorrelated with generator-specific artifacts. Our analysis provides critical\ninsights for deploying reliable deepfake detection systems and establishes\nuncertainty quantification as a fundamental requirement for trustworthy\nsynthetic media detection.\n","authors":["Neslihan Kose","Anthony Rhodes","Umur Aybars Ciftci","Ilke Demir"],"pdf_url":"https://arxiv.org/pdf/2509.17550v3.pdf","comment":"Accepted for publication at the ICCV 2025 workshop - STREAM"},{"id":"http://arxiv.org/abs/2510.24214v1","updated":"2025-10-28T09:29:37Z","published":"2025-10-28T09:29:37Z","title":"SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel\n  LLMs","summary":"  Multimodal Large Language Models (MLLMs) typically process a large number of\nvisual tokens, leading to considerable computational overhead, even though many\nof these tokens are redundant. Existing visual token pruning methods primarily\nfocus on selecting the most salient tokens based on attention scores, resulting\nin the semantic incompleteness of the selected tokens. In this paper, we\npropose a novel visual token pruning strategy, called\n\\textbf{S}aliency-\\textbf{C}overage \\textbf{O}riented token \\textbf{P}runing\nfor \\textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and\ncoverage of the selected visual tokens to better preserve semantic\ncompleteness. Specifically, we introduce a set-coverage for a given set of\nselected tokens, computed based on the token relationships. We then define a\ntoken-coverage gain for each unselected token, quantifying how much additional\ncoverage would be obtained by including it. By integrating the saliency score\ninto the token-coverage gain, we propose our SCOPE score and iteratively select\nthe token with the highest SCOPE score. We conduct extensive experiments on\nmultiple vision-language understanding benchmarks using the LLaVA-1.5 and\nLLaVA-Next models. Experimental results demonstrate that our method\nconsistently outperforms prior approaches. Our code is available at\n\\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.\n","authors":["Jinhong Deng","Wen Li","Joey Tianyi Zhou","Yang He"],"pdf_url":"https://arxiv.org/pdf/2510.24214v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24213v1","updated":"2025-10-28T09:28:12Z","published":"2025-10-28T09:28:12Z","title":"Beyond Inference Intervention: Identity-Decoupled Diffusion for Face\n  Anonymization","summary":"  Face anonymization aims to conceal identity information while preserving\nnon-identity attributes. Mainstream diffusion models rely on inference-time\ninterventions such as negative guidance or energy-based optimization, which are\napplied post-training to suppress identity features. These interventions often\nintroduce distribution shifts and entangle identity with non-identity\nattributes, degrading visual fidelity and data utility. To address this, we\npropose \\textbf{ID\\textsuperscript{2}Face}, a training-centric anonymization\nframework that removes the need for inference-time optimization. The rationale\nof our method is to learn a structured latent space where identity and\nnon-identity information are explicitly disentangled, enabling direct and\ncontrollable anonymization at inference. To this end, we design a conditional\ndiffusion model with an identity-masked learning scheme. An Identity-Decoupled\nLatent Recomposer uses an Identity Variational Autoencoder to model identity\nfeatures, while non-identity attributes are extracted from same-identity pairs\nand aligned through bidirectional latent alignment. An Identity-Guided Latent\nHarmonizer then fuses these representations via soft-gating conditioned on\nnoisy feature prediction. The model is trained with a recomposition-based\nreconstruction loss to enforce disentanglement. At inference, anonymization is\nachieved by sampling a random identity vector from the learned identity space.\nTo further suppress identity leakage, we introduce an Orthogonal Identity\nMapping strategy that enforces orthogonality between sampled and source\nidentity vectors. Experiments demonstrate that ID\\textsuperscript{2}Face\noutperforms existing methods in visual quality, identity suppression, and\nutility preservation.\n","authors":["Haoxin Yang","Yihong Lin","Jingdan Kang","Xuemiao Xu","Yue Li","Cheng Xu","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2510.24213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24211v1","updated":"2025-10-28T09:26:27Z","published":"2025-10-28T09:26:27Z","title":"MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive\n  Visual Generation Acceleration","summary":"  While autoregressive (AR) modeling has recently emerged as a new paradigm in\nvisual generation, its practical adoption is severely constrained by the slow\ninference speed of per-token generation, which often requires thousands of\nsteps to produce a single sample. To address this challenge, we propose MC-SJD,\na training-free, lossless parallel decoding framework designed to accelerate AR\nvisual generation by extending the recently introduced Speculative Jacobi\nDecoding (SJD). Although SJD shows strong potential for accelerating AR\ngeneration, we demonstrate that token instability across iterations\nsignificantly reduces the acceptance rate, a limitation that primarily arises\nfrom the independent sampling process used during draft token generation. To\novercome this, we introduce MC-SJD, an information-theoretic approach based on\ncoupling, which substantially accelerates standard SJD by maximizing the\nprobability of sampling identical draft tokens across consecutive iterations,\nall while preserving its lossless property. Remarkably, this method requires\nonly a single-line modification to the existing algorithm, yet achieves\nsubstantial performance gains, delivering up to a ~4.2x acceleration in image\ngeneration and ~13.3x acceleration in video generation compared to standard AR\ndecoding, without any degradation in output quality.\n","authors":["Junhyuk So","Hyunho Kook","Chaeyeon Jang","Eunhyeok Park"],"pdf_url":"https://arxiv.org/pdf/2510.24211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24202v1","updated":"2025-10-28T09:06:27Z","published":"2025-10-28T09:06:27Z","title":"CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and\n  Uncertainty Reduction in Medical Image Segmentation","summary":"  Accurate polyp and cardiac segmentation for early detection and treatment is\nessential for the diagnosis and treatment planning of cancer-like diseases.\nTraditional convolutional neural network (CNN) based models have represented\nlimited generalizability, robustness, and inability to handle uncertainty,\nwhich affects the segmentation performance. To solve these problems, this paper\nintroduces CLFSeg, an encoder-decoder based framework that aggregates the\nFuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy\nlogic. This module enhances the segmentation performance by identifying local\nand global features while minimizing the uncertainty, noise, and ambiguity in\nboundary regions, ensuring computing efficiency. In order to handle class\nimbalance problem while focusing on the areas of interest with tiny and\nboundary regions, binary cross-entropy (BCE) with dice loss is incorporated.\nOur proposed model exhibits exceptional performance on four publicly available\ndatasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC.\nExtensive experiments and visual studies show CLFSeg surpasses the existing\nSOTA performance and focuses on relevant regions of interest in anatomical\nstructures. The proposed CLFSeg improves performance while ensuring computing\nefficiency, which makes it a potential solution for real-world medical\ndiagnostic scenarios. Project page is available at\nhttps://visdomlab.github.io/CLFSeg/\n","authors":["Anshul Kaushal","Kunal Jangid","Vinod K. Kurmi"],"pdf_url":"https://arxiv.org/pdf/2510.24202v1.pdf","comment":"The 36th British Machine Vision Conference (BMVC) 2025"},{"id":"http://arxiv.org/abs/2510.24195v1","updated":"2025-10-28T08:59:11Z","published":"2025-10-28T08:59:11Z","title":"Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for\n  SAM2","summary":"  Recent studies reveal the vulnerability of the image segmentation foundation\nmodel SAM to adversarial examples. Its successor, SAM2, has attracted\nsignificant attention due to its strong generalization capability in video\nsegmentation. However, its robustness remains unexplored, and it is unclear\nwhether existing attacks on SAM can be directly transferred to SAM2. In this\npaper, we first analyze the performance gap of existing attacks between SAM and\nSAM2 and highlight two key challenges arising from their architectural\ndifferences: directional guidance from the prompt and semantic entanglement\nacross consecutive frames. To address these issues, we propose UAP-SAM2, the\nfirst cross-prompt universal adversarial attack against SAM2 driven by dual\nsemantic deviation. For cross-prompt transferability, we begin by designing a\ntarget-scanning strategy that divides each frame into k regions, each randomly\nassigned a prompt, to reduce prompt dependency during optimization. For\neffectiveness, we design a dual semantic deviation framework that optimizes a\nUAP by distorting the semantics within the current frame and disrupting the\nsemantic consistency across consecutive frames. Extensive experiments on six\ndatasets across two segmentation tasks demonstrate the effectiveness of the\nproposed method for SAM2. The comparative results show that UAP-SAM2\nsignificantly outperforms state-of-the-art (SOTA) attacks by a large margin.\n","authors":["Ziqi Zhou","Yifan Hu","Yufei Song","Zijing Li","Shengshan Hu","Leo Yu Zhang","Dezhong Yao","Long Zheng","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2510.24195v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2507.12841v2","updated":"2025-10-28T08:46:18Z","published":"2025-07-17T07:04:05Z","title":"AnyCap Project: A Unified Framework, Dataset, and Benchmark for\n  Controllable Omni-modal Captioning","summary":"  Controllable captioning is essential for precise multimodal alignment and\ninstruction following, yet existing models often lack fine-grained control and\nreliable evaluation protocols. To address this gap, we present the AnyCap\nProject, an integrated solution spanning model, dataset, and evaluation. We\nintroduce AnyCapModel (ACM), a lightweight plug-and-play framework that\nenhances the controllability of existing foundation models for omni-modal\ncaptioning without retraining the base model. ACM reuses the original captions\nfrom base models while incorporating user instructions and modality features to\ngenerate improved captions. To remedy the data scarcity in controllable\nmultimodal captioning, we build AnyCapDataset (ACD), covering three modalities,\n28 user-instruction types, and 300\\,k high-quality data entries. We further\npropose AnyCapEval, a new benchmark that provides more reliable evaluation\nmetrics for controllable captioning by decoupling content accuracy and\nstylistic fidelity. ACM markedly improves caption quality across a diverse set\nof base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores\nby 45\\% and style scores by 12\\%, and it also achieves substantial gains on\nwidely used benchmarks such as MIA-Bench and VidCapBench.\n","authors":["Yiming Ren","Zhiqiang Lin","Yu Li","Gao Meng","Weiyun Wang","Junjie Wang","Zicheng Lin","Jifeng Dai","Yujiu Yang","Wenhai Wang","Ruihang Chu"],"pdf_url":"https://arxiv.org/pdf/2507.12841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22672v2","updated":"2025-10-28T08:39:14Z","published":"2025-10-26T13:27:59Z","title":"Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and\n  Exocentric Views","summary":"  We introduce Look and Tell, a multimodal dataset for studying referential\ncommunication across egocentric and exocentric perspectives. Using Meta Project\nAria smart glasses and stationary cameras, we recorded synchronized gaze,\nspeech, and video as 25 participants instructed a partner to identify\ningredients in a kitchen. Combined with 3D scene reconstructions, this setup\nprovides a benchmark for evaluating how different spatial representations (2D\nvs. 3D; ego vs. exo) affect multimodal grounding. The dataset contains 3.67\nhours of recordings, including 2,707 richly annotated referential expressions,\nand is designed to advance the development of embodied agents that can\nunderstand and engage in situated dialogue.\n","authors":["Anna Deichler","Jonas Beskow"],"pdf_url":"https://arxiv.org/pdf/2510.22672v2.pdf","comment":"10 pages, 6 figures, 2 tables. Accepted to the NeurIPS 2025 Workshop\n  on SPACE in Vision, Language, and Embodied AI (SpaVLE). Dataset:\n  https://huggingface.co/datasets/annadeichler/KTH-ARIA-referential"},{"id":"http://arxiv.org/abs/2505.13043v2","updated":"2025-10-28T08:36:12Z","published":"2025-05-19T12:33:52Z","title":"A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation","summary":"  Aiming to generalize the well-trained gaze estimation model to new target\ndomains, Cross-domain Gaze Estimation (CDGE) is developed for real-world\napplication scenarios. Existing CDGE methods typically extract the\ndomain-invariant features to mitigate domain shift in feature space, which is\nproved insufficient by Generalized Label Shift (GLS) theory. In this paper, we\nintroduce a novel GLS perspective to CDGE and modelize the cross-domain problem\nby label and conditional shift problem. A GLS correction framework is presented\nand a feasible realization is proposed, in which a importance reweighting\nstrategy based on truncated Gaussian distribution is introduced to overcome the\ncontinuity challenges in label shift correction. To embed the reweighted source\ndistribution to conditional invariant learning, we further derive a\nprobability-aware estimation of conditional operator discrepancy. Extensive\nexperiments on standard CDGE tasks with different backbone models validate the\nsuperior generalization capability across domain and applicability on various\nmodels of proposed method.\n","authors":["Hao-Ran Yang","Xiaohui Chen","Chuan-Xian Ren"],"pdf_url":"https://arxiv.org/pdf/2505.13043v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2503.08930v2","updated":"2025-10-28T08:30:20Z","published":"2025-03-11T22:18:57Z","title":"Acoustic Neural 3D Reconstruction Under Pose Drift","summary":"  We consider the problem of optimizing neural implicit surfaces for 3D\nreconstruction using acoustic images collected with drifting sensor poses. The\naccuracy of current state-of-the-art 3D acoustic modeling algorithms is highly\ndependent on accurate pose estimation; small errors in sensor pose can lead to\nsevere reconstruction artifacts. In this paper, we propose an algorithm that\njointly optimizes the neural scene representation and sonar poses. Our\nalgorithm does so by parameterizing the 6DoF poses as learnable parameters and\nbackpropagating gradients through the neural renderer and implicit\nrepresentation. We validated our algorithm on both real and simulated datasets.\nIt produces high-fidelity 3D reconstructions even under significant pose drift.\n","authors":["Tianxiang Lin","Mohamad Qadri","Kevin Zhang","Adithya Pediredla","Christopher A. Metzler","Michael Kaess"],"pdf_url":"https://arxiv.org/pdf/2503.08930v2.pdf","comment":"8 pages, 8 figures. This paper is accepted by 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2503.09336v3","updated":"2025-10-28T08:24:48Z","published":"2025-03-12T12:30:59Z","title":"Stealthy Patch-Wise Backdoor Attack in 3D Point Cloud via Curvature\n  Awareness","summary":"  Backdoor attacks pose a severe threat to deep neural networks (DNNs) by\nimplanting hidden backdoors that can be activated with predefined triggers to\nmanipulate model behaviors maliciously. Existing 3D point cloud backdoor\nattacks primarily rely on sample-wise global modifications, which suffer from\nlow imperceptibility. Although optimization can improve stealthiness,\noptimizing sample-wise triggers significantly increases computational cost. To\naddress these limitations, we propose the Stealthy Patch-Wise Backdoor Attack\n(SPBA), the first patch-wise backdoor attack framework for 3D point clouds.\nSpecifically, SPBA decomposes point clouds into local patches and employs a\ncurvature-based imperceptibility score to guide trigger injection into visually\nless sensitive patches. By optimizing a unified patch-wise trigger that\nperturbs spectral features of selected patches, SPBA significantly enhances\noptimization efficiency while maintaining high stealthiness. Extensive\nexperiments on ModelNet40 and ShapeNetPart further demonstrate that SPBA\nsurpasses prior state-of-the-art backdoor attacks in both attack effectiveness\nand resistance to defense methods. The code is available at\nhttps://github.com/HazardFY/SPBA.\n","authors":["Yu Feng","Dingxin Zhang","Runkai Zhao","Yong Xia","Heng Huang","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2503.09336v3.pdf","comment":"13 pages, 6 figures, 11 tables"},{"id":"http://arxiv.org/abs/2508.15256v2","updated":"2025-10-28T08:08:14Z","published":"2025-08-21T05:40:23Z","title":"Normal and Abnormal Pathology Knowledge-Augmented Vision-Language Model\n  for Anomaly Detection in Pathology Images","summary":"  Anomaly detection in computational pathology aims to identify rare and scarce\nanomalies where disease-related data are often limited or missing. Existing\nanomaly detection methods, primarily designed for industrial settings, face\nlimitations in pathology due to computational constraints, diverse tissue\nstructures, and lack of interpretability. To address these challenges, we\npropose Ano-NAViLa, a Normal and Abnormal pathology knowledge-augmented\nVision-Language model for Anomaly detection in pathology images. Ano-NAViLa is\nbuilt on a pre-trained vision-language model with a lightweight trainable MLP.\nBy incorporating both normal and abnormal pathology knowledge, Ano-NAViLa\nenhances accuracy and robustness to variability in pathology images and\nprovides interpretability through image-text associations. Evaluated on two\nlymph node datasets from different organs, Ano-NAViLa achieves the\nstate-of-the-art performance in anomaly detection and localization,\noutperforming competing models.\n","authors":["Jinsol Song","Jiamu Wang","Anh Tien Nguyen","Keunho Byeon","Sangjeong Ahn","Sung Hak Lee","Jin Tae Kwak"],"pdf_url":"https://arxiv.org/pdf/2508.15256v2.pdf","comment":"Accepted to ICCV 2025. Code is available at:\n  https://github.com/QuIIL/ICCV2025_Ano-NAViLa"},{"id":"http://arxiv.org/abs/2505.05229v2","updated":"2025-10-28T08:05:02Z","published":"2025-05-08T13:21:10Z","title":"Does CLIP perceive art the same way we do?","summary":"  CLIP has emerged as a powerful multimodal model capable of connecting images\nand text through joint embeddings, but to what extent does it 'see' the same\nway humans do - especially when interpreting artworks? In this paper, we\ninvestigate CLIP's ability to extract high-level semantic and stylistic\ninformation from paintings, including both human-created and AI-generated\nimagery. We evaluate its perception across multiple dimensions: content, scene\nunderstanding, artistic style, historical period, and the presence of visual\ndeformations or artifacts. By designing targeted probing tasks and comparing\nCLIP's responses to human annotations and expert benchmarks, we explore its\nalignment with human perceptual and contextual understanding. Our findings\nreveal both strengths and limitations in CLIP's visual representations,\nparticularly in relation to aesthetic cues and artistic intent. We further\ndiscuss the implications of these insights for using CLIP as a guidance\nmechanism during generative processes, such as style transfer or prompt-based\nimage synthesis. Our work highlights the need for deeper interpretability in\nmultimodal systems, especially when applied to creative domains where nuance\nand subjectivity play a central role.\n","authors":["Andrea Asperti","Leonardo Dessì","Maria Chiara Tonetti","Nico Wu"],"pdf_url":"https://arxiv.org/pdf/2505.05229v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22035v2","updated":"2025-10-28T07:52:08Z","published":"2025-10-24T21:41:32Z","title":"Caption-Driven Explainability: Probing CNNs for Bias via CLIP","summary":"  Robustness has become one of the most critical problems in machine learning\n(ML). The science of interpreting ML models to understand their behavior and\nimprove their robustness is referred to as explainable artificial intelligence\n(XAI). One of the state-of-the-art XAI methods for computer vision problems is\nto generate saliency maps. A saliency map highlights the pixel space of an\nimage that excites the ML model the most. However, this property could be\nmisleading if spurious and salient features are present in overlapping pixel\nspaces. In this paper, we propose a caption-based XAI method, which integrates\na standalone model to be explained into the contrastive language-image\npre-training (CLIP) model using a novel network surgery approach. The resulting\ncaption-based XAI model identifies the dominant concept that contributes the\nmost to the models prediction. This explanation minimizes the risk of the\nstandalone model falling for a covariate shift and contributes significantly\ntowards developing robust ML models. Our code is available at\n<https://github.com/patch0816/caption-driven-xai>.\n","authors":["Patrick Koller","Amil V. Dravid","Guido M. Schuster","Aggelos K. Katsaggelos"],"pdf_url":"https://arxiv.org/pdf/2510.22035v2.pdf","comment":"Accepted and presented at the IEEE ICIP 2025 Satellite Workshop\n  \"Generative AI for World Simulations and Communications & Celebrating 40\n  Years of Excellence in Education: Honoring Professor Aggelos Katsaggelos\",\n  Anchorage, Alaska, USA, September 14, 2025. Camera-ready preprint; the\n  official IEEE Xplore publication will follow. Code is available at\n  <https://github.com/patch0816/caption-driven-xai>"},{"id":"http://arxiv.org/abs/2401.09962v3","updated":"2025-10-28T07:47:22Z","published":"2024-01-18T13:23:51Z","title":"CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects","summary":"  Customized text-to-video generation aims to generate high-quality videos\nguided by text prompts and subject references. Current approaches for\npersonalizing text-to-video generation suffer from tackling multiple subjects,\nwhich is a more challenging and practical scenario. In this work, our aim is to\npromote multi-subject guided text-to-video customization. We propose\nCustomVideo, a novel framework that can generate identity-preserving videos\nwith the guidance of multiple subjects. To be specific, firstly, we encourage\nthe co-occurrence of multiple subjects via composing them in a single image.\nFurther, upon a basic text-to-video diffusion model, we design a simple yet\neffective attention control strategy to disentangle different subjects in the\nlatent space of diffusion model. Moreover, to help the model focus on the\nspecific area of the object, we segment the object from given reference images\nand provide a corresponding object mask for attention learning. Also, we\ncollect a multi-subject text-to-video generation dataset as a comprehensive\nbenchmark. Extensive qualitative, quantitative, and user study results\ndemonstrate the superiority of our method compared to previous state-of-the-art\napproaches. The project page is https://kyfafyd.wang/projects/customvideo.\n","authors":["Zhao Wang","Aoxue Li","Lingting Zhu","Yong Guo","Qi Dou","Zhenguo Li"],"pdf_url":"https://arxiv.org/pdf/2401.09962v3.pdf","comment":"IEEE TMM 2025"},{"id":"http://arxiv.org/abs/2510.24152v1","updated":"2025-10-28T07:43:30Z","published":"2025-10-28T07:43:30Z","title":"Enhancing Vision-Language Models for Autonomous Driving through\n  Task-Specific Prompting and Spatial Reasoning","summary":"  This technical report presents our solution for the RoboSense Challenge at\nIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving\nscene understanding across perception, prediction, planning, and corruption\ndetection tasks. We propose a systematic framework built on four core\ncomponents. First, a Mixture-of-Prompts router classifies questions and\ndispatches them to task-specific expert prompts, eliminating interference\nacross diverse question types. Second, task-specific prompts embed explicit\ncoordinate systems, spatial reasoning rules, role-playing,\nChain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to\neach task. Third, a visual assembly module composes multi-view images with\nobject crops, magenta markers, and adaptive historical frames based on question\nrequirements. Fourth, we configure model inference parameters (temperature,\ntop-p, message roles) per task to optimize output quality. Implemented on\nQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean\ndata) and 72.85% on Phase-2 (corrupted data), demonstrating that structured\nprompting and spatial grounding substantially enhance VLM performance on\nsafety-critical autonomous driving tasks. Code and prompt are available at\nhttps://github.com/wuaodi/UCAS-CSU-phase2.\n","authors":["Aodi Wu","Xubo Luo"],"pdf_url":"https://arxiv.org/pdf/2510.24152v1.pdf","comment":"RoboSense Challenge with IROS 2025"},{"id":"http://arxiv.org/abs/2503.18384v2","updated":"2025-10-28T07:24:00Z","published":"2025-03-24T06:51:38Z","title":"LiDAR Remote Sensing Meets Weak Supervision: Concepts, Methods, and\n  Perspectives","summary":"  Light detection and ranging (LiDAR) remote sensing encompasses two major\ndirections: data interpretation and parameter inversion. However, both\ndirections rely heavily on costly and labor-intensive labeled data and field\nmeasurements, which constrains their scalability and spatiotemporal\nadaptability. Weakly Supervised Learning (WSL) provides a unified framework to\naddress these limitations. This paper departs from the traditional view that\ntreats interpretation and inversion as separate tasks and offers a systematic\nreview of recent advances in LiDAR remote sensing from a unified WSL\nperspective. We cover typical WSL settings including incomplete\nsupervision(e.g., sparse point labels), inexact supervision (e.g., scene-level\ntags), inaccurate supervision (e.g., noisy labels), and cross-domain\nsupervision (e.g., domain adaptation/generalization) and corresponding\ntechniques such as pseudo-labeling, consistency regularization, self-training,\nand label refinement, which collectively enable robust learning from limited\nand weak annotations.We further analyze LiDAR-specific challenges (e.g.,\nirregular geometry, data sparsity, domain heterogeneity) that require tailored\nweak supervision, and examine how sparse LiDAR observations can guide joint\nlearning with other remote-sensing data for continuous surface-parameter\nretrieval. Finally, we highlight future directions where WSL acts as a bridge\nbetween LiDAR and foundation models to leverage large-scale multimodal datasets\nand reduce labeling costs, while also enabling broader WSL-driven advances in\ngeneralization, open-world adaptation, and scalable LiDAR remote sensing.\n","authors":["Yuan Gao","Shaobo Xia","Pu Wang","Xiaohuan Xi","Sheng Nie","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2503.18384v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24136v1","updated":"2025-10-28T07:22:34Z","published":"2025-10-28T07:22:34Z","title":"MSRANetV2: An Explainable Deep Learning Architecture for Multi-class\n  Classification of Colorectal Histopathological Images","summary":"  Colorectal cancer (CRC) is a leading worldwide cause of cancer-related\nmortality, and the role of prompt precise detection is of paramount interest in\nimproving patient outcomes. Conventional diagnostic methods such as colonoscopy\nand histological examination routinely exhibit subjectivity, are extremely\ntime-consuming, and are susceptible to variation. Through the development of\ndigital pathology, deep learning algorithms have become a powerful approach in\nenhancing diagnostic precision and efficiency. In our work, we proposed a\nconvolutional neural network architecture named MSRANetV2, specially optimized\nfor the classification of colorectal tissue images. The model employs a\nResNet50V2 backbone, extended with residual attention mechanisms and\nsqueeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained\nspatial features. With channel alignment and upsampling operations, MSRANetV2\neffectively fuses multi-scale representations, thereby enhancing the robustness\nof the classification. We evaluated our model on a five-fold stratified\ncross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and\nNCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,\nrecall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900\nplus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and\n0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were\n0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,\n0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM\nvisualizations were incorporated to enhance model interpretability by\nhighlighting tissue areas that are medically relevant. These findings validate\nthat MSRANetV2 is a reliable, interpretable, and high-performing architectural\nmodel for classifying CRC tissues.\n","authors":["Ovi Sarkar","Md Shafiuzzaman","Md. Faysal Ahamed","Golam Mahmud","Muhammad E. H. Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2510.24136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24134v1","updated":"2025-10-28T07:19:01Z","published":"2025-10-28T07:19:01Z","title":"VC4VG: Optimizing Video Captions for Text-to-Video Generation","summary":"  Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels.We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/qyr0403/VC4VG to support further\nresearch.\n","authors":["Yang Du","Zhuoran Lin","Kaiqiang Song","Biao Wang","Zhicheng Zheng","Tiezheng Ge","Bo Zheng","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2510.24134v1.pdf","comment":"Accepted by EMNLP 2025"},{"id":"http://arxiv.org/abs/2510.14431v2","updated":"2025-10-28T07:17:14Z","published":"2025-10-16T08:31:44Z","title":"Real-Time Neural Video Compression with Unified Intra and Inter Coding","summary":"  Neural video compression (NVC) technologies have advanced rapidly in recent\nyears, yielding state-of-the-art schemes such as DCVC-RT that offer superior\ncompression efficiency to H.266/VVC and real-time encoding/decoding\ncapabilities. Nonetheless, existing NVC schemes have several limitations,\nincluding inefficiency in dealing with disocclusion and new content, interframe\nerror propagation and accumulation, among others. To eliminate these\nlimitations, we borrow the idea from classic video coding schemes, which allow\nintra coding within inter-coded frames. With the intra coding tool enabled,\ndisocclusion and new content are properly handled, and interframe error\npropagation is naturally intercepted without the need for manual refresh\nmechanisms. We present an NVC framework with unified intra and inter coding,\nwhere every frame is processed by a single model that is trained to perform\nintra/inter coding adaptively. Moreover, we propose a simultaneous two-frame\ncompression design to exploit interframe redundancy not only forwardly but also\nbackwardly. Experimental results show that our scheme outperforms DCVC-RT by an\naverage of 10.7\\% BD-rate reduction, delivers more stable bitrate and quality\nper frame, and retains real-time encoding/decoding performances. Code and\nmodels will be released.\n","authors":["Hui Xiang","Yifan Bian","Li Li","Jingran Wu","Xianguo Zhang","Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.14431v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2510.24133v1","updated":"2025-10-28T07:16:21Z","published":"2025-10-28T07:16:21Z","title":"Compositional Image Synthesis with Inference-Time Scaling","summary":"  Despite their impressive realism, modern text-to-image models still struggle\nwith compositionality, often failing to render accurate object counts,\nattributes, and spatial relations. To address this challenge, we present a\ntraining-free framework that combines an object-centric approach with\nself-refinement to improve layout faithfulness while preserving aesthetic\nquality. Specifically, we leverage large language models (LLMs) to synthesize\nexplicit layouts from input prompts, and we inject these layouts into the image\ngeneration process, where a object-centric vision-language model (VLM) judge\nreranks multiple candidates to select the most prompt-aligned outcome\niteratively. By unifying explicit layout-grounding with self-refine-based\ninference-time scaling, our framework achieves stronger scene alignment with\nprompts compared to recent text-to-image models. The code are available at\nhttps://github.com/gcl-inha/ReFocus.\n","authors":["Minsuk Ji","Sanghyeok Lee","Namhyuk Ahn"],"pdf_url":"https://arxiv.org/pdf/2510.24133v1.pdf","comment":"projcet page: https://github.com/gcl-inha/ReFocus"},{"id":"http://arxiv.org/abs/2510.24129v1","updated":"2025-10-28T07:08:09Z","published":"2025-10-28T07:08:09Z","title":"ETC: training-free diffusion models acceleration with Error-aware Trend\n  Consistency","summary":"  Diffusion models have achieved remarkable generative quality but remain\nbottlenecked by costly iterative sampling. Recent training-free methods\naccelerate diffusion process by reusing model outputs. However, these methods\nignore denoising trends and lack error control for model-specific tolerance,\nleading to trajectory deviations under multi-step reuse and exacerbating\ninconsistencies in the generated results. To address these issues, we introduce\nError-aware Trend Consistency (ETC), a framework that (1) introduces a\nconsistent trend predictor that leverages the smooth continuity of diffusion\ntrajectories, projecting historical denoising patterns into stable future\ndirections and progressively distributing them across multiple approximation\nsteps to achieve acceleration without deviating; (2) proposes a model-specific\nerror tolerance search mechanism that derives corrective thresholds by\nidentifying transition points from volatile semantic planning to stable quality\nrefinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX\nwith negligible (-0.074 SSIM score) degradation of consistency.\n","authors":["Jiajian Xie","Hubery Yin","Chen Li","Zhou Zhao","Shengyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24129v1.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2402.02096v2","updated":"2025-10-28T06:51:19Z","published":"2024-02-03T09:27:33Z","title":"UMCFuse: A Unified Multiple Complex Scenes Infrared and Visible Image\n  Fusion Framework","summary":"  Infrared and visible image fusion has emerged as a prominent research area in\ncomputer vision. However, little attention has been paid to the fusion task in\ncomplex scenes, leading to sub-optimal results under interference. To fill this\ngap, we propose a unified framework for infrared and visible images fusion in\ncomplex scenes, termed UMCFuse. Specifically, we classify the pixels of visible\nimages from the degree of scattering of light transmission, allowing us to\nseparate fine details from overall intensity. Maintaining a balance between\ninterference removal and detail preservation is essential for the\ngeneralization capacity of the proposed method. Therefore, we propose an\nadaptive denoising strategy for the fusion of detail layers. Meanwhile, we fuse\nthe energy features from different modalities by analyzing them from multiple\ndirections. Extensive fusion experiments on real and synthetic complex scenes\ndatasets cover adverse weather conditions, noise, blur, overexposure, fire, as\nwell as downstream tasks including semantic segmentation, object detection,\nsalient object detection, and depth estimation, consistently indicate the\nsuperiority of the proposed method compared with the recent representative\nmethods. Our code is available at https://github.com/ixilai/UMCFuse.\n","authors":["Xilai Li","Xiaosong Li","Tianshu Tan","Huafeng Li","Tao Ye"],"pdf_url":"https://arxiv.org/pdf/2402.02096v2.pdf","comment":"Published in IEEE-TIP 2025"},{"id":"http://arxiv.org/abs/2510.24117v1","updated":"2025-10-28T06:41:49Z","published":"2025-10-28T06:41:49Z","title":"DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion\n  Recovery","summary":"  We present DogMo, a large-scale multi-view RGB-D video dataset capturing\ndiverse canine movements for the task of motion recovery from images. DogMo\ncomprises 1.2k motion sequences collected from 10 unique dogs, offering rich\nvariation in both motion and breed. It addresses key limitations of existing\ndog motion datasets, including the lack of multi-view and real 3D data, as well\nas limited scale and diversity. Leveraging DogMo, we establish four motion\nrecovery benchmark settings that support systematic evaluation across monocular\nand multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,\nwe further introduce a three-stage, instance-specific optimization pipeline\nthat fits the SMAL model to the motion sequences. Our method progressively\nrefines body shape and pose through coarse alignment, dense correspondence\nsupervision, and temporal regularization. Our dataset and method provide a\nprincipled foundation for advancing research in dog motion recovery and open up\nnew directions at the intersection of computer vision, computer graphics, and\nanimal behavior modeling.\n","authors":["Zan Wang","Siyu Chen","Luya Mo","Xinfeng Gao","Yuxin Shen","Lebin Ding","Wei Liang"],"pdf_url":"https://arxiv.org/pdf/2510.24117v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2510.24116v1","updated":"2025-10-28T06:41:43Z","published":"2025-10-28T06:41:43Z","title":"UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via\n  Frequency-Domain Representations","summary":"  Knowledge distillation (KD) is an effective model compression technique that\ntransfers knowledge from a high-performance teacher to a lightweight student,\nreducing cost while maintaining accuracy. In visual applications, where\nlarge-scale image models are widely used, KD enables efficient deployment.\nHowever, architectural diversity introduces semantic discrepancies that hinder\nthe use of intermediate representations. Most existing KD methods are designed\nfor homogeneous models and degrade in heterogeneous scenarios, especially when\nintermediate features are involved. Prior studies mainly focus on the logits\nspace, making limited use of the semantic information in intermediate layers.\nTo address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)\nis proposed as a framework that leverages intermediate features in the\nfrequency domain for cross-architecture transfer. Fourier transform is applied\nto capture global feature information, alleviating representational\ndiscrepancies between heterogeneous teacher-student pairs. A Feature\nTransformation Module (FTM) produces compact frequency-domain representations\nof teacher features, while a learnable Feature Alignment Module (FAM) projects\nstudent features and aligns them via multi-level matching. Training is guided\nby a joint objective combining mean squared error on intermediate features with\nKullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K\ndemonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD\nas an effective approach for unifying heterogeneous representations and\nenabling efficient utilization of visual knowledge\n","authors":["Fengming Yu","Haiwei Pan","Kejia Zhang","Jian Guan","Haiying Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.24116v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.17939v2","updated":"2025-10-28T06:37:24Z","published":"2025-06-22T08:09:58Z","title":"GEMeX-RMCoT: An Enhanced Med-VQA Dataset for Region-Aware Multimodal\n  Chain-of-Thought Reasoning","summary":"  Medical visual question answering aims to support clinical decision-making by\nenabling models to answer natural language questions based on medical images.\nWhile recent advances in multi-modal learning have significantly improved\nperformance, current methods still suffer from limited answer reliability and\npoor interpretability, impairing the ability of clinicians and patients to\nunderstand and trust model outputs. To address these limitations, this work\nfirst proposes a Region-Aware Multimodal Chain-of-Thought (RMCoT) dataset, in\nwhich the process of producing an answer is preceded by a sequence of\nintermediate reasoning steps that explicitly ground relevant visual regions of\nthe medical image, thereby providing fine-grained explainability. Furthermore,\nwe introduce a novel verifiable reward mechanism for reinforcement learning to\nguide post-training, improving the alignment between the model's reasoning\nprocess and its final answer. Remarkably, our method achieves comparable\nperformance using only one-eighth of the training data, demonstrating the\nefficiency and effectiveness of the proposal. The dataset is available at\nhttps://www.med-vqa.com/GEMeX/.\n","authors":["Bo Liu","Xiangyu Zhao","Along He","Yidi Chen","Huazhu Fu","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2506.17939v2.pdf","comment":"Accepted at ACM MM 2025 (also known as GEMeX-ThinkVG)"},{"id":"http://arxiv.org/abs/2505.12758v4","updated":"2025-10-28T06:27:55Z","published":"2025-05-19T06:35:11Z","title":"Global urban visual perception varies across demographics and\n  personalities","summary":"  Understanding people's preferences is crucial for urban planning, yet current\napproaches often combine responses from multi-cultural populations, obscuring\ndemographic differences and risking amplifying biases. We conducted a\nlargescale urban visual perception survey of streetscapes worldwide using\nstreet view imagery, examining how demographics -- including gender, age,\nincome, education, race and ethnicity, and personality traits -- shape\nperceptions among 1,000 participants with balanced demographics from five\ncountries and 45 nationalities. This dataset, Street Perception Evaluation\nConsidering Socioeconomics (SPECS), reveals demographic- and personality-based\ndifferences across six traditional indicators -- safe, lively, wealthy,\nbeautiful, boring, depressing -- and four new ones -- live nearby, walk, cycle,\ngreen. Location-based sentiments further shape these preferences. Machine\nlearning models trained on existing global datasets tend to overestimate\npositive indicators and underestimate negative ones compared to human\nresponses, underscoring the need for local context. Our study aspires to\nrectify the myopic treatment of street perception, which rarely considers\ndemographics or personality traits.\n","authors":["Matias Quintana","Youlong Gu","Xiucheng Liang","Yujun Hou","Koichi Ito","Yihan Zhu","Mahmoud Abdelrahman","Filip Biljecki"],"pdf_url":"https://arxiv.org/pdf/2505.12758v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24108v1","updated":"2025-10-28T06:26:36Z","published":"2025-10-28T06:26:36Z","title":"ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory\n  Scoring","summary":"  End-to-end autonomous driving maps raw sensor inputs directly into\nego-vehicle trajectories to avoid cascading errors from perception modules and\nto leverage rich semantic cues. Existing frameworks largely rely on Imitation\nLearning (IL), which can be limited by sub-optimal expert demonstrations and\ncovariate shift during deployment. On the other hand, Reinforcement Learning\n(RL) has recently shown potential in scaling up with simulations, but is\ntypically confined to low-dimensional symbolic inputs (e.g. 3D objects and\nmaps), falling short of full end-to-end learning from raw sensor data. We\nintroduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory\nScoring), a framework that combines the strengths of both worlds: sensor inputs\nwithout losing information and RL training for robust planning. To the best of\nour knowledge, ZTRS is the first framework that eliminates IL entirely by only\nlearning from rewards while operating directly on high-dimensional sensor data.\nZTRS utilizes offline reinforcement learning with our proposed Exhaustive\nPolicy Optimization (EPO), a variant of policy gradient tailored for enumerable\nactions and rewards. ZTRS demonstrates strong performance across three\nbenchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop\nplanning in challenging real-world and synthetic scenarios), and HUGSIM\n(simulated closed-loop driving). Specifically, ZTRS achieves the\nstate-of-the-art result on Navhard and outperforms IL-based baselines on\nHUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.\n","authors":["Zhenxin Li","Wenhao Yao","Zi Wang","Xinglong Sun","Jingde Chen","Nadine Chang","Maying Shen","Jingyu Song","Zuxuan Wu","Shiyi Lan","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2510.24108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24105v1","updated":"2025-10-28T06:21:06Z","published":"2025-10-28T06:21:06Z","title":"Enhancing Pre-trained Representation Classifiability can Boost its\n  Interpretability","summary":"  The visual representation of a pre-trained model prioritizes the\nclassifiability on downstream tasks, while the widespread applications for\npre-trained visual models have posed new requirements for representation\ninterpretability. However, it remains unclear whether the pre-trained\nrepresentations can achieve high interpretability and classifiability\nsimultaneously. To answer this question, we quantify the representation\ninterpretability by leveraging its correlation with the ratio of interpretable\nsemantics within the representations. Given the pre-trained representations,\nonly the interpretable semantics can be captured by interpretations, whereas\nthe uninterpretable part leads to information loss. Based on this fact, we\npropose the Inherent Interpretability Score (IIS) that evaluates the\ninformation loss, measures the ratio of interpretable semantics, and quantifies\nthe representation interpretability. In the evaluation of the representation\ninterpretability with different classifiability, we surprisingly discover that\nthe interpretability and classifiability are positively correlated, i.e.,\nrepresentations with higher classifiability provide more interpretable\nsemantics that can be captured in the interpretations. This observation further\nsupports two benefits to the pre-trained representations. First, the\nclassifiability of representations can be further improved by fine-tuning with\ninterpretability maximization. Second, with the classifiability improvement for\nthe representations, we obtain predictions based on their interpretations with\nless accuracy degradation. The discovered positive correlation and\ncorresponding applications show that practitioners can unify the improvements\nin interpretability and classifiability for pre-trained vision models. Codes\nare available at https://github.com/ssfgunner/IIS.\n","authors":["Shufan Shen","Zhaobo Qi","Junshu Sun","Qingming Huang","Qi Tian","Shuhui Wang"],"pdf_url":"https://arxiv.org/pdf/2510.24105v1.pdf","comment":"ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.23118v2","updated":"2025-10-28T06:10:07Z","published":"2025-10-27T08:38:52Z","title":"Task-Agnostic Fusion of Time Series and Imagery for Earth Observation","summary":"  We propose a task-agnostic framework for multimodal fusion of time series and\nsingle timestamp images, enabling cross-modal generation and robust downstream\nperformance. Our approach explores deterministic and learned strategies for\ntime series quantization and then leverages a masked correlation learning\nobjective, aligning discrete image and time series tokens in a unified\nrepresentation space. Instantiated in the Earth observation domain, the\npretrained model generates consistent global temperature profiles from\nsatellite imagery and is validated through counterfactual experiments. Across\ndownstream tasks, our task-agnostic pretraining outperforms task-specific\nfusion by 6% in R^2 and 2% in RMSE on average, and exceeds baseline methods by\n50\\% in R$^2$ and 12\\% in RMSE. Finally, we analyze gradient sensitivity across\nmodalities, providing insights into model robustness. Code, data, and weights\nwill be released under a permissive license.\n","authors":["Gianfranco Basile","Johannes Jakubik","Benedikt Blumenstiel","Thomas Brunschwiler","Juan Bernabe Moreno"],"pdf_url":"https://arxiv.org/pdf/2510.23118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24093v1","updated":"2025-10-28T06:06:52Z","published":"2025-10-28T06:06:52Z","title":"OmniText: A Training-Free Generalist for Controllable Text-Image\n  Manipulation","summary":"  Recent advancements in diffusion-based text synthesis have demonstrated\nsignificant performance in inserting and editing text within images via\ninpainting. However, despite the potential of text inpainting methods, three\nkey limitations hinder their applicability to broader Text Image Manipulation\n(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over\nthe style of rendered text, and (iii) a tendency to generate duplicated\nletters. To address these challenges, we propose OmniText, a training-free\ngeneralist capable of performing a wide range of TIM tasks. Specifically, we\ninvestigate two key properties of cross- and self-attention mechanisms to\nenable text removal and to provide control over both text styles and content.\nOur findings reveal that text removal can be achieved by applying\nself-attention inversion, which mitigates the model's tendency to focus on\nsurrounding text, thus reducing text hallucinations. Additionally, we\nredistribute cross-attention, as increasing the probability of certain text\ntokens reduces text hallucination. For controllable inpainting, we introduce\nnovel loss functions in a latent optimization framework: a cross-attention\ncontent loss to improve text rendering accuracy and a self-attention style loss\nto facilitate style customization. Furthermore, we present OmniText-Bench, a\nbenchmark dataset for evaluating diverse TIM tasks. It includes input images,\ntarget text with masks, and style references, covering diverse applications\nsuch as text removal, rescaling, repositioning, and insertion and editing with\nvarious styles. Our OmniText framework is the first generalist method capable\nof performing diverse TIM tasks. It achieves state-of-the-art performance\nacross multiple tasks and metrics compared to other text inpainting methods and\nis comparable with specialist methods.\n","authors":["Agus Gunawan","Samuel Teodoro","Yun Chen","Soo Ye Kim","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2510.24093v1.pdf","comment":"The first two authors contributed equally to this work. The last two\n  authors are co-corresponding authors"},{"id":"http://arxiv.org/abs/2508.17102v2","updated":"2025-10-28T06:06:16Z","published":"2025-08-23T18:05:06Z","title":"GRASP: Geospatial pixel Reasoning viA Structured Policy learning","summary":"  Geospatial pixel reasoning aims to generate segmentation masks in remote\nsensing imagery directly from natural-language instructions. Most existing\napproaches follow a paradigm that fine-tunes multimodal large language models\nunder supervision with dense pixel-level masks as ground truth. While effective\nwithin the training data distribution, this design suffers from two main\ndrawbacks: (1) the high cost of large-scale dense mask annotation, and (2) the\nlimited generalization capability of supervised fine-tuning in out-of-domain\nscenarios. To address these issues, we propose GRASP, a structured\npolicy-learning framework that integrates a multimodal large language model\nwith a pretrained segmentation model in a cascaded manner. To enhance\ngeneralization, we introduce PRIME, a training paradigm that replaces\nsupervised fine-tuning with reinforcement learning to better align reasoning\nand grounding behaviors with task objectives. To reduce annotation costs, we\ndesign BoP-Rewards, which substitutes dense mask labels with bounding box and\npositive points. It further verifies outputs through two complementary signals:\nformat, which constrains the reasoning and grounding structure to remain\nsyntactically parsable, and accuracy, which evaluates the quality of predicted\nboxes and points. For evaluation, we train our method and all baselines on\nEarthReason and GeoPixInstruct, constructing an in-domain benchmark by merging\ntheir test sets. We further release GRASP-1k, a fully out-of-domain benchmark\nwith reasoning-intensive queries, reasoning traces, and fine-grained masks.\nExperimental results demonstrate state-of-the-art (SOTA) in-domain performance\nand up to 54\\% improvement in out-of-domain scenarios, confirming that\nreinforcement learning with cost-aware rewards provides a robust and scalable\nparadigm for geospatial pixel reasoning. All code and datasets will be released\npublicly.\n","authors":["Chengjie Jiang","Yunqi Zhou","Jiafeng Yan","Jing Li","Jiayang Li","Yue Zhou","Hongjie He","Jonathan Li"],"pdf_url":"https://arxiv.org/pdf/2508.17102v2.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.22943v2","updated":"2025-10-28T05:57:57Z","published":"2025-10-27T02:56:17Z","title":"Switchable Token-Specific Codebook Quantization For Face Image\n  Compression","summary":"  With the ever-increasing volume of visual data, the efficient and lossless\ntransmission, along with its subsequent interpretation and understanding, has\nbecome a critical bottleneck in modern information systems. The emerged\ncodebook-based solution utilize a globally shared codebook to quantize and\ndequantize each token, controlling the bpp by adjusting the number of tokens or\nthe codebook size. However, for facial images, which are rich in attributes,\nsuch global codebook strategies overlook both the category-specific\ncorrelations within images and the semantic differences among tokens, resulting\nin suboptimal performance, especially at low bpp. Motivated by these\nobservations, we propose a Switchable Token-Specific Codebook Quantization for\nface image compression, which learns distinct codebook groups for different\nimage categories and assigns an independent codebook to each token. By\nrecording the codebook group to which each token belongs with a small number of\nbits, our method can reduce the loss incurred when decreasing the size of each\ncodebook group. This enables a larger total number of codebooks under a lower\noverall bpp, thereby enhancing the expressive capability and improving\nreconstruction performance. Owing to its generalizable design, our method can\nbe integrated into any existing codebook-based representation learning approach\nand has demonstrated its effectiveness on face recognition datasets, achieving\nan average accuracy of 93.51% for reconstructed images at 0.05 bpp.\n","authors":["Yongbo Wang","Haonan Wang","Guodong Mu","Ruixin Zhang","Jiaqi Chen","Jingyun Zhang","Jun Wang","Yuan Xie","Zhizhong Zhang","Shouhong Ding"],"pdf_url":"https://arxiv.org/pdf/2510.22943v2.pdf","comment":"NeurIPS 2025 accepted"},{"id":"http://arxiv.org/abs/2505.12702v2","updated":"2025-10-28T05:41:49Z","published":"2025-05-19T04:52:31Z","title":"Long-RVOS: A Comprehensive Benchmark for Long-term Referring Video\n  Object Segmentation","summary":"  Referring video object segmentation (RVOS) aims to identify, track and\nsegment the objects in a video based on language descriptions, which has\nreceived great attention in recent years. However, existing datasets remain\nfocus on short video clips within several seconds, with salient objects visible\nin most frames. To advance the task towards more practical scenarios, we\nintroduce \\textbf{Long-RVOS}, a large-scale benchmark for long-term referring\nvideo object segmentation. Long-RVOS contains 2,000+ videos of an average\nduration exceeding 60 seconds, covering a variety of objects that undergo\nocclusion, disappearance-reappearance and shot changing. The objects are\nmanually annotated with three different types of descriptions to individually\nevaluate the understanding of static attributes, motion patterns and\nspatiotemporal relationships. Moreover, unlike previous benchmarks that rely\nsolely on the per-frame spatial evaluation, we introduce two new metrics to\nassess the temporal and spatiotemporal consistency. We benchmark 6\nstate-of-the-art methods on Long-RVOS. The results show that current approaches\nstruggle severely with the long-video challenges. To address this, we further\npropose ReferMo, a promising baseline method that integrates motion information\nto expand the temporal receptive field, and employs a local-to-global\narchitecture to capture both short-term dynamics and long-term dependencies.\nDespite simplicity, ReferMo achieves significant improvements over current\nmethods in long-term scenarios. We hope that Long-RVOS and our baseline can\ndrive future RVOS research towards tackling more realistic and long-form\nvideos.\n","authors":["Tianming Liang","Haichao Jiang","Yuting Yang","Chaolei Tan","Shuai Li","Wei-Shi Zheng","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2505.12702v2.pdf","comment":"Project Page: \\url{https://isee-laboratory.github.io/Long-RVOS}"},{"id":"http://arxiv.org/abs/2510.24078v1","updated":"2025-10-28T05:40:14Z","published":"2025-10-28T05:40:14Z","title":"Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained\n  Classification","summary":"  Text-to-image (T2I) models are increasingly used for synthetic dataset\ngeneration, but generating effective synthetic training data for classification\nremains challenging. Fine-tuning a T2I model with a few real examples can help\nimprove the quality of synthetic training data; however, it may also cause\noverfitting and reduce diversity in the generated samples. We propose a\nfine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for\nfine-grained classification. Given a small set of real examples, we first\nextract class-agnostic attributes such as scene background and object pose. We\nthen explicitly condition on these attributes during fine-tuning of the T2I\nmodel and marginalize them out during generation. This design mitigates\noverfitting, preserves the T2I model's generative prior, reduces estimation\nerrors, and further minimizes unintended inter-class associations. Extensive\nexperiments across multiple T2I models, backbones, and datasets show that our\nmethod achieves state-of-the-art performance in low-shot fine-grained\nclassification when augmented with synthetic data. Concretely, BOB outperforms\nDataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning\na CLIP classifier with five real images augmented with 100 synthetic images).\nIn three of the four benchmarks, fine-tuning downstream models with 5 real\nimages augmented with BOB achieves better performance than fine-tuning with 10\nreal images. Collectively, BOB outperforms prior art in 18 of 24 experimental\nsettings, with 2+% accuracy improvements in 14 of these settings.\n","authors":["William Yang","Xindi Wu","Zhiwei Deng","Esin Tureci","Olga Russakovsky"],"pdf_url":"https://arxiv.org/pdf/2510.24078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06517v2","updated":"2025-10-28T05:40:02Z","published":"2025-06-06T20:28:37Z","title":"GS4: Generalizable Sparse Splatting Semantic SLAM","summary":"  Traditional SLAM algorithms excel at camera tracking, but typically produce\nincomplete and low-resolution maps that are not tightly integrated with\nsemantics prediction. Recent work integrates Gaussian Splatting (GS) into SLAM\nto enable dense, photorealistic 3D mapping, yet existing GS-based SLAM methods\nrequire per-scene optimization that is slow and consumes an excessive number of\nGaussians. We present GS4, the first generalizable GS-based semantic SLAM\nsystem. Compared with prior approaches, GS4 runs 10x faster, uses 10x fewer\nGaussians, and achieves state-of-the-art performance across color, depth,\nsemantic mapping and camera tracking. From an RGB-D video stream, GS4\nincrementally builds and updates a set of 3D Gaussians using a feed-forward\nnetwork. First, the Gaussian Prediction Model estimates a sparse set of\nGaussian parameters from input frame, which integrates both color and semantic\nprediction with the same backbone. Then, the Gaussian Refinement Network merges\nnew Gaussians with the existing set while avoiding redundancy. Finally, we\npropose to optimize GS for only 1-5 iterations that corrects drift and floaters\nwhen significant pose changes are detected. Experiments on the real-world\nScanNet and ScanNet++ benchmarks demonstrate state-of-the-art semantic SLAM\nperformance, with strong generalization capability shown through zero-shot\ntransfer to the NYUv2 and TUM RGB-D datasets.\n","authors":["Mingqi Jiang","Chanho Kim","Chen Ziwen","Li Fuxin"],"pdf_url":"https://arxiv.org/pdf/2506.06517v2.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2510.21412v2","updated":"2025-10-28T05:32:23Z","published":"2025-10-24T12:54:13Z","title":"Bridging the gap to real-world language-grounded visual concept learning","summary":"  Human intelligence effortlessly interprets visual scenes along a rich\nspectrum of semantic dimensions. However, existing approaches to\nlanguage-grounded visual concept learning are limited to a few predefined\nprimitive axes, such as color and shape, and are typically explored in\nsynthetic datasets. In this work, we propose a scalable framework that\nadaptively identifies image-related concept axes and grounds visual concepts\nalong these axes in real-world scenes. Leveraging a pretrained vision-language\nmodel and our universal prompting strategy, our framework identifies a diverse\nimage-related axes without any prior knowledge. Our universal concept encoder\nadaptively binds visual features to the discovered axes without introducing\nadditional model parameters for each concept. To ground visual concepts along\nthe discovered axes, we optimize a compositional anchoring objective, which\nensures that each axis can be independently manipulated without affecting\nothers. We demonstrate the effectiveness of our framework on subsets of\nImageNet, CelebA-HQ, and AFHQ, showcasing superior editing capabilities across\ndiverse real-world concepts that are too varied to be manually predefined. Our\nmethod also exhibits strong compositional generalization, outperforming\nexisting visual concept learning and text-based editing methods. The code is\navailable at https://github.com/whieya/Language-grounded-VCL.\n","authors":["Whie Jung","Semin Kim","Junee Kim","Seunghoon Hong"],"pdf_url":"https://arxiv.org/pdf/2510.21412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18672v8","updated":"2025-10-28T05:22:48Z","published":"2025-03-24T13:44:12Z","title":"CalFuse: Multi-Modal Continual Learning via Feature Calibration and\n  Parameter Fusion","summary":"  With the proliferation of multi-modal data in large-scale visual recognition\nsystems, enabling models to continuously acquire knowledge from evolving data\nstreams while preserving prior information has become increasingly critical.\nClass-Continual Learning (CCL) addresses this challenge by incrementally\nincorporating new class knowledge without revisiting historical data, making it\nessential for real-world big data applications. While traditional CCL methods\nrely solely on visual features, recent advances in Vision-Language Models\n(VLMs) such as CLIP demonstrate significant potential for CCL by leveraging\npre-trained multi-modal knowledge. However, existing approaches face challenges\nin mitigating catastrophic forgetting while maintaining the cross-modal\ngeneralization capabilities of VLMs. To address these limitations, we propose\nCalFuse, a framework that synergizes feature Calibration with parameter Fusion\nto enable effective multi-modal knowledge integration in continual learning\nscenarios. CalFuse introduces a dynamic feature calibration mechanism that\nadaptively balances original CLIP visual representations with task-specific\nfeatures, preserving the model's intrinsic cross-modal generalization while\nadapting to new classes. Concurrently, a QR decomposition-based parameter\nfusion strategy progressively integrates newly acquired knowledge with\nhistorical task parameters, maintaining equilibrium between learning new class\nrepresentations and retaining prior knowledge across sequential tasks.\nExtensive experiments on benchmark datasets validate the effectiveness of our\napproach in large-scale multi-modal continual learning settings, demonstrating\nsuperior performance over state-of-the-art methods in both average accuracy and\nfinal task retention.\n","authors":["Juncen Guo","Siao Liu","Xiaoguang Zhu","Lianlong Sun","Liangyu Teng","Jingyi Wu","Di Li","Linxiao Gong","Weiwei Jiang","Wei Zhou","Liang Song"],"pdf_url":"https://arxiv.org/pdf/2503.18672v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09282v4","updated":"2025-10-28T04:40:41Z","published":"2025-02-13T12:54:13Z","title":"MsEdF: A Multi-stream Encoder-decoder Framework for Remote Sensing Image\n  Captioning","summary":"  Remote sensing images contain complex spatial patterns and semantic\nstructures, which makes the captioning model difficult to accurately describe.\nEncoder-decoder architectures have become the widely used approach for RSIC by\ntranslating visual content into descriptive text. However, many existing\nmethods rely on a single-stream architecture, which weakens the model to\naccurately describe the image. Such single-stream architectures typically\nstruggle to extract diverse spatial features or capture complex semantic\nrelationships, limiting their effectiveness in scenes with high intraclass\nsimilarity or contextual ambiguity. In this work, we propose a novel\nMulti-stream Encoder-decoder Framework (MsEdF) which improves the performance\nof RSIC by optimizing both the spatial representation and language generation\nof encoder-decoder architecture. The encoder fuses information from two\ncomplementary image encoders, thereby promoting feature diversity through the\nintegration of multiscale and structurally distinct cues. To improve the\ncapture of context-aware descriptions, we refine the input sequence's semantic\nmodeling on the decoder side using a stacked GRU architecture with an\nelement-wise aggregation scheme. Experiments on three benchmark RSIC datasets\nshow that MsEdF outperforms several baseline models.\n","authors":["Swadhin Das","Raksha Sharma"],"pdf_url":"https://arxiv.org/pdf/2502.09282v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02885v2","updated":"2025-10-28T04:18:29Z","published":"2025-01-06T09:55:55Z","title":"MDP3: A Training-free Approach for List-wise Frame Selection in\n  Video-LLMs","summary":"  Video large language models (Video-LLMs) have made significant progress in\nunderstanding videos. However, processing multiple frames leads to lengthy\nvisual token sequences, presenting challenges such as the limited context\nlength cannot accommodate the entire video, and the inclusion of irrelevant\nframes hinders visual perception. Hence, effective frame selection is crucial.\nThis paper emphasizes that frame selection should follow three key principles:\nquery relevance, list-wise diversity, and sequentiality. Existing methods, such\nas uniform frame sampling and query-frame matching, do not capture all of these\nprinciples. Thus, we propose Markov decision determinantal point process with\ndynamic programming (MDP3) for frame selection, a training-free and\nmodel-agnostic method that can be seamlessly integrated into existing\nVideo-LLMs. Our method first estimates frame similarities conditioned on the\nquery using a conditional Gaussian kernel within the reproducing kernel Hilbert\nspace~(RKHS). We then apply the determinantal point process~(DPP) to the\nsimilarity matrix to capture both query relevance and list-wise diversity. To\nincorporate sequentiality, we segment the video and apply DPP within each\nsegment, conditioned on the preceding segment selection, modeled as a Markov\ndecision process~(MDP) for allocating selection sizes across segments.\nTheoretically, MDP3 provides a \\((1 - 1/e)\\)-approximate solution to the\nNP-hard list-wise frame selection problem with pseudo-polynomial time\ncomplexity, demonstrating its efficiency. Empirically, MDP3 significantly\noutperforms existing methods, verifying its effectiveness and robustness.\n","authors":["Hui Sun","Shiyin Lu","Huanyu Wang","Qing-Guo Chen","Zhao Xu","Weihua Luo","Kaifu Zhang","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2501.02885v2.pdf","comment":"26 pages, 14 figures"},{"id":"http://arxiv.org/abs/2510.22706v2","updated":"2025-10-28T04:16:45Z","published":"2025-10-26T14:57:44Z","title":"IGGT: Instance-Grounded Geometry Transformer for Semantic 3D\n  Reconstruction","summary":"  Humans naturally perceive the geometric structure and semantic content of a\n3D world as intertwined dimensions, enabling coherent and accurate\nunderstanding of complex scenes. However, most prior approaches prioritize\ntraining large geometry models for low-level 3D reconstruction and treat\nhigh-level spatial understanding in isolation, overlooking the crucial\ninterplay between these two fundamental aspects of 3D-scene analysis, thereby\nlimiting generalization and leading to poor performance in downstream 3D\nunderstanding tasks. Recent attempts have mitigated this issue by simply\naligning 3D models with specific language models, thus restricting perception\nto the aligned model's capacity and limiting adaptability to downstream tasks.\nIn this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an\nend-to-end large unified transformer to unify the knowledge for both spatial\nreconstruction and instance-level contextual understanding. Specifically, we\ndesign a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode\na unified representation with geometric structures and instance-grounded\nclustering through only 2D visual inputs. This representation supports\nconsistent lifting of 2D visual inputs into a coherent 3D scene with explicitly\ndistinct object instances. To facilitate this task, we further construct\nInsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth\nmaps, and 3D-consistent instance-level mask annotations with a novel data\ncuration pipeline.\n","authors":["Hao Li","Zhengyu Zou","Fangfu Liu","Xuanyang Zhang","Fangzhou Hong","Yukang Cao","Yushi Lan","Manyuan Zhang","Gang Yu","Dingwen Zhang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2510.22706v2.pdf","comment":"https://github.com/lifuguan/IGGT_official"},{"id":"http://arxiv.org/abs/2505.13389v5","updated":"2025-10-28T04:13:18Z","published":"2025-05-19T17:30:13Z","title":"VSA: Faster Video Diffusion with Trainable Sparse Attention","summary":"  Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models. Code will be available at\nhttps://github.com/hao-ai-lab/FastVideo.\n","authors":["Peiyuan Zhang","Yongqi Chen","Haofeng Huang","Will Lin","Zhengzhong Liu","Ion Stoica","Eric Xing","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.13389v5.pdf","comment":"Accepted by Neurips 2025"},{"id":"http://arxiv.org/abs/2506.22802v2","updated":"2025-10-28T03:55:35Z","published":"2025-06-28T08:08:16Z","title":"Riemannian-Geometric Fingerprints of Generative Models","summary":"  Recent breakthroughs and rapid integration of generative models (GMs) have\nsparked interest in the problem of model attribution and their fingerprints.\nFor instance, service providers need reliable methods of authenticating their\nmodels to protect their IP, while users and law enforcement seek to verify the\nsource of generated content for accountability and trust. In addition, a\ngrowing threat of model collapse is arising, as more model-generated data are\nbeing fed back into sources (e.g., YouTube) that are often harvested for\ntraining (\"regurgitative training\"), heightening the need to differentiate\nsynthetic from human data. Yet, a gap still exists in understanding generative\nmodels' fingerprints, we believe, stemming from the lack of a formal framework\nthat can define, represent, and analyze the fingerprints in a principled way.\nTo address this gap, we take a geometric approach and propose a new definition\nof artifact and fingerprint of GMs using Riemannian geometry, which allows us\nto leverage the rich theory of differential geometry. Our new definition\ngeneralizes previous work (Song et al., 2024) to non-Euclidean manifolds by\nlearning Riemannian metrics from data and replacing the Euclidean distances and\nnearest-neighbor search with geodesic distances and kNN-based Riemannian center\nof mass. We apply our theory to a new gradient-based algorithm for computing\nthe fingerprints in practice. Results show that it is more effective in\ndistinguishing a large array of GMs, spanning across 4 different datasets in 2\ndifferent resolutions (64 by 64, 256 by 256), 27 model architectures, and 2\nmodalities (Vision, Vision-Language). Using our proposed definition\nsignificantly improves the performance on model attribution, as well as a\ngeneralization to unseen datasets, model types, and modalities, suggesting its\npractical efficacy.\n","authors":["Hae Jin Song","Laurent Itti"],"pdf_url":"https://arxiv.org/pdf/2506.22802v2.pdf","comment":"ICCV 2025 Highlight paper"},{"id":"http://arxiv.org/abs/2510.24038v1","updated":"2025-10-28T03:47:44Z","published":"2025-10-28T03:47:44Z","title":"Enhancing CLIP Robustness via Cross-Modality Alignment","summary":"  Vision-language models (VLMs) such as CLIP demonstrate strong generalization\nin zero-shot classification but remain highly vulnerable to adversarial\nperturbations. Existing methods primarily focus on adversarial fine-tuning or\nprompt optimization; they often overlook the gaps in CLIP's encoded features,\nwhich is shown as the text and image features lie far apart from each other.\nThis misalignment is significantly amplified under adversarial perturbations,\nleading to severe degradation in classification performance. To address this\nproblem, we propose Cross-modality Alignment, dubbed COLA, an optimal\ntransport-based framework that explicitly addresses adversarial misalignment by\nrestoring both global image-text alignment and local structural consistency in\nthe feature space. (1) COLA first projects adversarial image embeddings onto a\nsubspace spanned by class text features, effectively filtering out non-semantic\ndistortions while preserving discriminative information. (2) It then models\nimages and texts as discrete distributions over multiple augmented views and\nrefines their alignment via OT, with the subspace projection seamlessly\nintegrated into the cost computation. This design ensures stable cross-modal\nalignment even under adversarial conditions. COLA is training-free and\ncompatible with existing fine-tuned models. Extensive evaluations across 14\nzero-shot classification benchmarks demonstrate the effectiveness of COLA,\nespecially with an average improvement of 6.7% on ImageNet and its variants\nunder PGD adversarial attacks, while maintaining high accuracy on clean\nsamples.\n","authors":["Xingyu Zhu","Beier Zhu","Shuo Wang","Kesen Zhao","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24038v1.pdf","comment":"NeurIPS 2025 Spotlight"},{"id":"http://arxiv.org/abs/2510.24037v1","updated":"2025-10-28T03:39:18Z","published":"2025-10-28T03:39:18Z","title":"Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for\n  Vision Models","summary":"  Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision\nmodels to downstream tasks. Among PEFT paradigms, sparse tuning achieves\nremarkable performance by adjusting only the weights most relevant to\ndownstream tasks, rather than densely tuning the entire weight matrix. Current\nmethods follow a two-stage paradigm. First, it locates task-relevant weights by\ngradient information, which overlooks the parameter adjustments during\nfine-tuning and limits the performance. Second, it updates only the located\nweights by applying a sparse mask to the gradient of the weight matrix, which\nresults in high memory usage due to the storage of all weight matrices in the\noptimizer. In this paper, we propose a one-stage method named SNELLA to\novercome the above limitations. For memory usage, SNELLA selectively updates\nthe weight matrix by adding it to another sparse matrix that is merged by two\nlow-rank learnable matrices. We extend the low-rank decomposition by\nintroducing nonlinear kernel functions, thereby increasing the rank of the\nresulting merged matrix to prevent the interdependency among weight updates,\nenabling better adaptation to downstream tasks. For locating task-relevant\nweights, we propose an adaptive bi-level sparsity allocation mechanism that\nencourages weights to compete across and inside layers based on their\nimportance scores in an end-to-end manner. Extensive experiments are conducted\non classification, segmentation, and generation tasks using different\npre-trained vision models. The results show that SNELLA achieves SOTA\nperformance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.\n90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.\nCompared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%\nacross models with parameter scales from 86M to 632M. Our source codes are\navailable at https://github.com/ssfgunner/SNELL.\n","authors":["Shufan Shen","Junshu Sun","Shuhui Wang","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2510.24037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.16691v2","updated":"2025-10-28T03:37:32Z","published":"2025-09-20T13:37:37Z","title":"InstanceAssemble: Layout-Aware Image Generation via Instance Assembling\n  Attention","summary":"  Diffusion models have demonstrated remarkable capabilities in generating\nhigh-quality images. Recent advancements in Layout-to-Image (L2I) generation\nhave leveraged positional conditions and textual descriptions to facilitate\nprecise and controllable image synthesis. Despite overall progress, current L2I\nmethods still exhibit suboptimal performance. Therefore, we propose\nInstanceAssemble, a novel architecture that incorporates layout conditions via\ninstance-assembling attention, enabling position control with bounding boxes\n(bbox) and multimodal content control including texts and additional visual\ncontent. Our method achieves flexible adaption to existing DiT-based T2I models\nthrough light-weighted LoRA modules. Additionally, we propose a Layout-to-Image\nbenchmark, Denselayout, a comprehensive benchmark for layout-to-image\ngeneration, containing 5k images with 90k instances in total. We further\nintroduce Layout Grounding Score (LGS), an interpretable evaluation metric to\nmore precisely assess the accuracy of L2I generation. Experiments demonstrate\nthat our InstanceAssemble method achieves state-of-the-art performance under\ncomplex layout conditions, while exhibiting strong compatibility with diverse\nstyle LoRA modules. The code and pretrained models are publicly available at\nhttps://github.com/FireRedTeam/InstanceAssemble.\n","authors":["Qiang Xiang","Shuang Sun","Binglei Li","Dejia Song","Huaxia Li","Nemo Chen","Xu Tang","Yao Hu","Junping Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.16691v2.pdf","comment":"Accepted in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24036v1","updated":"2025-10-28T03:36:15Z","published":"2025-10-28T03:36:15Z","title":"ResNet: Enabling Deep Convolutional Neural Networks through Residual\n  Learning","summary":"  Convolutional Neural Networks (CNNs) has revolutionized computer vision, but\ntraining very deep networks has been challenging due to the vanishing gradient\nproblem. This paper explores Residual Networks (ResNet), introduced by He et\nal. (2015), which overcomes this limitation by using skip connections. ResNet\nenables the training of networks with hundreds of layers by allowing gradients\nto flow directly through shortcut connections that bypass intermediate layers.\nIn our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%\naccuracy compared to 84.1% for a traditional deep CNN of similar depth, while\nalso converging faster and training more stably.\n","authors":["Xingyu Liu","Kun Ming Goh"],"pdf_url":"https://arxiv.org/pdf/2510.24036v1.pdf","comment":"3 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2505.20744v2","updated":"2025-10-28T03:32:37Z","published":"2025-05-27T05:34:56Z","title":"MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity\n  Recognition","summary":"  Human Activity Recognition (HAR) with wearable sensors is challenged by\nlimited interpretability, which significantly impacts cross-dataset\ngeneralization. To address this challenge, we propose Motion-Primitive\nTransformer (MoPFormer), a novel self-supervised framework that enhances\ninterpretability by tokenizing inertial measurement unit signals into\nsemantically meaningful motion primitives and leverages a Transformer\narchitecture to learn rich temporal representations. MoPFormer comprises two\nstages. The first stage is to partition multi-channel sensor streams into short\nsegments and quantize them into discrete ``motion primitive'' codewords, while\nthe second stage enriches those tokenized sequences through a context-aware\nembedding module and then processes them with a Transformer encoder. The\nproposed MoPFormer can be pre-trained using a masked motion-modeling objective\nthat reconstructs missing primitives, enabling it to develop robust\nrepresentations across diverse sensor configurations. Experiments on six HAR\nbenchmarks demonstrate that MoPFormer not only outperforms state-of-the-art\nmethods but also successfully generalizes across multiple datasets. More\nimportantly, the learned motion primitives significantly enhance both\ninterpretability and cross-dataset performance by capturing fundamental\nmovement patterns that remain consistent across similar activities, regardless\nof dataset origin.\n","authors":["Hao Zhang","Zhan Zhuang","Xuehao Wang","Xiaodong Yang","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.20744v2.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24034v1","updated":"2025-10-28T03:32:14Z","published":"2025-10-28T03:32:14Z","title":"AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven\n  Adversarial Prompts","summary":"  Despite rapid advancements in text-to-image (T2I) models, their safety\nmechanisms are vulnerable to adversarial prompts, which maliciously generate\nunsafe images. Current red-teaming methods for proactively assessing such\nvulnerabilities usually require white-box access to T2I models, and rely on\ninefficient per-prompt optimization, as well as inevitably generate\nsemantically meaningless prompts easily blocked by filters. In this paper, we\npropose APT (AutoPrompT), a black-box framework that leverages large language\nmodels (LLMs) to automatically generate human-readable adversarial suffixes for\nbenign prompts. We first introduce an alternating optimization-finetuning\npipeline between adversarial suffix optimization and fine-tuning the LLM\nutilizing the optimized suffix. Furthermore, we integrates a dual-evasion\nstrategy in optimization phase, enabling the bypass of both perplexity-based\nfilter and blacklist word filter: (1) we constrain the LLM generating\nhuman-readable prompts through an auxiliary LLM perplexity scoring, which\nstarkly contrasts with prior token-level gibberish, and (2) we also introduce\nbanned-token penalties to suppress the explicit generation of banned-tokens in\nblacklist. Extensive experiments demonstrate the excellent red-teaming\nperformance of our human-readable, filter-resistant adversarial prompts, as\nwell as superior zero-shot transferability which enables instant adaptation to\nunseen prompts and exposes critical vulnerabilities even in commercial APIs\n(e.g., Leonardo.Ai.).\n","authors":["Yufan Liu","Wanqian Zhang","Huashan Chen","Lin Wang","Xiaojun Jia","Zheng Lin","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2510.24034v1.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2508.05186v3","updated":"2025-10-28T03:21:38Z","published":"2025-08-07T09:21:20Z","title":"Learning to See and Act: Task-Aware View Planning for Robotic\n  Manipulation","summary":"  Recent vision-language-action (VLA) models for multi-task robotic\nmanipulation commonly rely on static viewpoints and shared visual encoders,\nwhich limit 3D perception and cause task interference, hindering robustness and\ngeneralization. In this work, we propose Task-Aware View Planning (TAVP), a\nframework designed to overcome these challenges by integrating active view\nplanning with task-specific representation learning. TAVP employs an efficient\nexploration policy, accelerated by a novel pseudo-environment, to actively\nacquire informative views. Furthermore, we introduce a Mixture-of-Experts (MoE)\nvisual encoder to disentangle features across different tasks, boosting both\nrepresentation fidelity and task generalization. By learning to see the world\nin a task-aware way, TAVP generates more complete and discriminative visual\nrepresentations, demonstrating significantly enhanced action prediction across\na wide array of manipulation challenges. Extensive experiments on RLBench tasks\nshow that our proposed TAVP model achieves superior performance over\nstate-of-the-art fixed-view approaches. Visual results and code are provided\nat: https://hcplab-sysu.github.io/TAVP.\n","authors":["Yongjie Bai","Zhouxia Wang","Yang Liu","Weixing Chen","Ziliang Chen","Mingtong Dai","Yongsen Zheng","Lingbo Liu","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2508.05186v3.pdf","comment":"14 pages, 8 figures, project page: https://hcplab-sysu.github.io/TAVP"},{"id":"http://arxiv.org/abs/2412.02542v3","updated":"2025-10-28T03:07:50Z","published":"2024-12-03T16:34:49Z","title":"Unveiling Concept Attribution in Diffusion Models","summary":"  Diffusion models have shown remarkable abilities in generating realistic and\nhigh-quality images from text prompts. However, a trained model remains largely\nblack-box; little do we know about the roles of its components in exhibiting a\nconcept such as objects or styles. Recent works employ causal tracing to\nlocalize knowledge-storing layers in generative models without showing how\nother layers contribute to the target concept. In this work, we approach\ndiffusion models' interpretability problem from a more general perspective and\npose a question: \\textit{``How do model components work jointly to demonstrate\nknowledge?''}. To answer this question, we decompose diffusion models using\ncomponent attribution, systematically unveiling the importance of each\ncomponent (specifically the model parameter) in generating a concept. The\nproposed framework, called \\textbf{C}omponent \\textbf{A}ttribution for\n\\textbf{D}iffusion Model (CAD), discovers the localization of concept-inducing\n(positive) components, while interestingly uncovers another type of components\nthat contribute negatively to generating a concept, which is missing in the\nprevious knowledge localization work. Based on this holistic understanding of\ndiffusion models, we introduce two fast, inference-time model editing\nalgorithms, CAD-Erase and CAD-Amplify; in particular, CAD-Erase enables erasure\nand CAD-Amplify allows amplification of a generated concept by ablating the\npositive and negative components, respectively, while retaining knowledge of\nother concepts. Extensive experimental results validate the significance of\nboth positive and negative components pinpointed by our framework,\ndemonstrating the potential of providing a complete view of interpreting\ngenerative models. Our code is available\n\\href{https://github.com/mail-research/CAD-attribution4diffusion}{here}.\n","authors":["Quang H. Nguyen","Hoang Phan","Khoa D. Doan"],"pdf_url":"https://arxiv.org/pdf/2412.02542v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24024v1","updated":"2025-10-28T03:06:28Z","published":"2025-10-28T03:06:28Z","title":"Listening without Looking: Modality Bias in Audio-Visual Captioning","summary":"  Audio-visual captioning aims to generate holistic scene descriptions by\njointly modeling sound and vision. While recent methods have improved\nperformance through sophisticated modality fusion, it remains unclear to what\nextent the two modalities are complementary in current audio-visual captioning\nmodels and how robust these models are when one modality is degraded. We\naddress these questions by conducting systematic modality robustness tests on\nLAVCap, a state-of-the-art audio-visual captioning model, in which we\nselectively suppress or corrupt the audio or visual streams to quantify\nsensitivity and complementarity. The analysis reveals a pronounced bias toward\nthe audio stream in LAVCap. To evaluate how balanced audio-visual captioning\nmodels are in their use of both modalities, we augment AudioCaps with textual\nannotations that jointly describe the audio and visual streams, yielding the\nAudioVisualCaps dataset. In our experiments, we report LAVCap baseline results\non AudioVisualCaps. We also evaluate the model under modality robustness tests\non AudioVisualCaps and the results indicate that LAVCap trained on\nAudioVisualCaps exhibits less modality bias than when trained on AudioCaps.\n","authors":["Yuchi Ishikawa","Toranosuke Manabe","Tatsuya Komatsu","Yoshimitsu Aoki"],"pdf_url":"https://arxiv.org/pdf/2510.24024v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2510.22379v2","updated":"2025-10-28T03:06:09Z","published":"2025-10-25T17:48:46Z","title":"TraceTrans: Translation and Spatial Tracing for Surgical Prediction","summary":"  Image-to-image translation models have achieved notable success in converting\nimages across visual domains and are increasingly used for medical tasks such\nas predicting post-operative outcomes and modeling disease progression.\nHowever, most existing methods primarily aim to match the target distribution\nand often neglect spatial correspondences between the source and translated\nimages. This limitation can lead to structural inconsistencies and\nhallucinations, undermining the reliability and interpretability of the\npredictions. These challenges are accentuated in clinical applications by the\nstringent requirement for anatomical accuracy. In this work, we present\nTraceTrans, a novel deformable image translation model designed for\npost-operative prediction that generates images aligned with the target\ndistribution while explicitly revealing spatial correspondences with the\npre-operative input. The framework employs an encoder for feature extraction\nand dual decoders for predicting spatial deformations and synthesizing the\ntranslated image. The predicted deformation field imposes spatial constraints\non the generated output, ensuring anatomical consistency with the source.\nExtensive experiments on medical cosmetology and brain MRI datasets demonstrate\nthat TraceTrans delivers accurate and interpretable post-operative predictions,\nhighlighting its potential for reliable clinical deployment.\n","authors":["Xiyu Luo","Haodong Li","Xinxing Cheng","He Zhao","Yang Hu","Xuan Song","Tianyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.22379v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04897v3","updated":"2025-10-28T02:59:19Z","published":"2025-06-05T11:28:02Z","title":"From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual\n  Grounding in 3D Scenes","summary":"  3D visual grounding has made notable progress in localizing objects within\ncomplex 3D scenes. However, grounding referring expressions beyond objects in\n3D scenes remains unexplored. In this paper, we introduce Anywhere3D-Bench, a\nholistic 3D visual grounding benchmark consisting of 2,886 referring\nexpression-3D bounding box pairs spanning four different grounding levels:\nhuman-activity areas, unoccupied space beyond objects, individual objects in\nthe scene, and fine-grained object parts. We assess a range of state-of-the-art\n3D visual grounding methods alongside large language models (LLMs) and\nmultimodal LLMs (MLLMs) on Anywhere3D-Bench. Experimental results reveal that\nspace-level and part-level visual grounding pose the greatest challenges:\nspace-level tasks require a more comprehensive spatial reasoning ability, for\nexample, modeling distances and spatial relations within 3D space, while\npart-level tasks demand fine-grained perception of object composition. Even the\nbest-performing models, Google Gemini-2.5-Pro and OpenAI o3, achieve just\naround 30% accuracy on space-level tasks and around 40% on part-level tasks,\nsignificantly lower than its performance on area-level and object-level tasks.\nThese findings underscore a critical gap in current models' capacity to\nunderstand and reason about 3D scenes beyond object-level semantics.\n","authors":["Tianxu Wang","Zhuofan Zhang","Ziyu Zhu","Yue Fan","Jing Xiong","Pengxiang Li","Xiaojian Ma","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2506.04897v3.pdf","comment":"Update v3 of the NeurIPS 2025 Datasets and Benchmarks paper (v2),\n  including additional evaluations of state-of-the-art multimodal large\n  language models. Project page: https://anywhere-3d.github.io/"},{"id":"http://arxiv.org/abs/2406.09782v3","updated":"2025-10-28T02:46:13Z","published":"2024-06-14T07:31:20Z","title":"Unsupervised Monocular Depth Estimation Based on Hierarchical\n  Feature-Guided Diffusion","summary":"  Unsupervised monocular depth estimation has received widespread attention\nbecause of its capability to train without ground truth. In real-world\nscenarios, the images may be blurry or noisy due to the influence of weather\nconditions and inherent limitations of the camera. Therefore, it is\nparticularly important to develop a robust depth estimation model. Benefiting\nfrom the training strategies of generative networks, generative-based methods\noften exhibit enhanced robustness. In light of this, we employ a\nwell-converging diffusion model among generative networks for unsupervised\nmonocular depth estimation. Additionally, we propose a hierarchical\nfeature-guided denoising module. This model significantly enriches the model's\ncapacity for learning and interpreting depth distribution by fully leveraging\nimage features to guide the denoising process. Furthermore, we explore the\nimplicit depth within reprojection and design an implicit depth consistency\nloss. This loss function serves to enhance the performance of the model and\nensure the scale consistency of depth within a video sequence. We conduct\nexperiments on the KITTI, Make3D, and our self-collected SIMIT datasets. The\nresults indicate that our approach stands out among generative-based models,\nwhile also showcasing remarkable robustness.\n","authors":["Runze Liu","Dongchen Zhu","Guanghui Zhang","Yue Xu","Wenjun Shi","Xiaolin Zhang","Lei Wang","Jiamao Li"],"pdf_url":"https://arxiv.org/pdf/2406.09782v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02787v2","updated":"2025-10-28T02:45:04Z","published":"2024-09-18T02:29:00Z","title":"Navigation with VLM framework: Towards Going to Any Language","summary":"  Navigating towards fully open language goals and exploring open scenes in an\nintelligent way have always raised significant challenges. Recently, Vision\nLanguage Models (VLMs) have demonstrated remarkable capabilities to reason with\nboth language and visual data. Although many works have focused on leveraging\nVLMs for navigation in open scenes, they often require high computational cost,\nrely on object-centric approaches, or depend on environmental priors in\ndetailed human instructions. We introduce Navigation with VLM (NavVLM), a\ntraining-free framework that harnesses open-source VLMs to enable robots to\nnavigate effectively, even for human-friendly language goal such as abstract\nplaces, actions, or specific objects in open scenes. NavVLM leverages the VLM\nas its cognitive core to perceive environmental information and constantly\nprovides exploration guidance achieving intelligent navigation with only a neat\ntarget rather than a detailed instruction with environment prior. We evaluated\nand validated NavVLM in both simulation and real-world experiments. In\nsimulation, our framework achieves state-of-the-art performance in Success\nweighted by Path Length (SPL) on object-specifc tasks in richly detailed\nenvironments from Matterport 3D (MP3D), Habitat Matterport 3D (HM3D) and\nGibson. With navigation episode reported, NavVLM demonstrates the capabilities\nto navigate towards any open-set languages. In real-world validation, we\nvalidated our framework's effectiveness in real-world robot at indoor scene.\n","authors":["Zecheng Yin","Chonghao Cheng","and Yao Guo","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2410.02787v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2510.24010v1","updated":"2025-10-28T02:34:08Z","published":"2025-10-28T02:34:08Z","title":"Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars\n  Science Tasks","summary":"  Foundation models have enabled rapid progress across many specialized domains\nby leveraging large-scale pre-training on unlabeled data, demonstrating strong\ngeneralization to a variety of downstream tasks. While such models have gained\nsignificant attention in fields like Earth Observation, their application to\nMars science remains limited. A key enabler of progress in other domains has\nbeen the availability of standardized benchmarks that support systematic\nevaluation. In contrast, Mars science lacks such benchmarks and standardized\nevaluation frameworks, which have limited progress toward developing foundation\nmodels for Martian tasks. To address this gap, we introduce Mars-Bench, the\nfirst benchmark designed to systematically evaluate models across a broad range\nof Mars-related tasks using both orbital and surface imagery. Mars-Bench\ncomprises 20 datasets spanning classification, segmentation, and object\ndetection, focused on key geologic features such as craters, cones, boulders,\nand frost. We provide standardized, ready-to-use datasets and baseline\nevaluations using models pre-trained on natural images, Earth satellite data,\nand state-of-the-art vision-language models. Results from all analyses suggest\nthat Mars-specific foundation models may offer advantages over general-domain\ncounterparts, motivating further exploration of domain-adapted pre-training.\nMars-Bench aims to establish a standardized foundation for developing and\ncomparing machine learning models for Mars science. Our data, models, and code\nare available at: https://mars-bench.github.io/.\n","authors":["Mirali Purohit","Bimal Gajera","Vatsal Malaviya","Irish Mehta","Kunal Kasodekar","Jacob Adler","Steven Lu","Umaa Rebbapragada","Hannah Kerner"],"pdf_url":"https://arxiv.org/pdf/2510.24010v1.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24009v1","updated":"2025-10-28T02:33:45Z","published":"2025-10-28T02:33:45Z","title":"Towards the Automatic Segmentation, Modeling and Meshing of the Aortic\n  Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023\n  Segmentation of the Aorta Challenge","summary":"  The automated analysis of the aortic vessel tree (AVT) from computed\ntomography angiography (CTA) holds immense clinical potential, but its\ndevelopment has been impeded by a lack of shared, high-quality data. We\nlaunched the SEG.A. challenge to catalyze progress in this field by introducing\na large, publicly available, multi-institutional dataset for AVT segmentation.\nThe challenge benchmarked automated algorithms on a hidden test set, with\nsubsequent optional tasks in surface meshing for computational simulations. Our\nfindings reveal a clear convergence on deep learning methodologies, with 3D\nU-Net architectures dominating the top submissions. A key result was that an\nensemble of the highest-ranking algorithms significantly outperformed\nindividual models, highlighting the benefits of model fusion. Performance was\nstrongly linked to algorithmic design, particularly the use of customized\npost-processing steps, and the characteristics of the training data. This\ninitiative not only establishes a new performance benchmark but also provides a\nlasting resource to drive future innovation toward robust, clinically\ntranslatable tools.\n","authors":["Yuan Jin","Antonio Pepe","Gian Marco Melito","Yuxuan Chen","Yunsu Byeon","Hyeseong Kim","Kyungwon Kim","Doohyun Park","Euijoon Choi","Dosik Hwang","Andriy Myronenko","Dong Yang","Yufan He","Daguang Xu","Ayman El-Ghotni","Mohamed Nabil","Hossam El-Kady","Ahmed Ayyad","Amr Nasr","Marek Wodzinski","Henning Müller","Hyeongyu Kim","Yejee Shin","Abbas Khan","Muhammad Asad","Alexander Zolotarev","Caroline Roney","Anthony Mathur","Martin Benning","Gregory Slabaugh","Theodoros Panagiotis Vagenas","Konstantinos Georgas","George K. Matsopoulos","Jihan Zhang","Zhen Zhang","Liqin Huang","Christian Mayer","Heinrich Mächler","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2510.24009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00034v2","updated":"2025-10-28T02:15:21Z","published":"2025-05-27T01:43:02Z","title":"GaussianFusion: Gaussian-Based Multi-Sensor Fusion for End-to-End\n  Autonomous Driving","summary":"  Multi-sensor fusion is crucial for improving the performance and robustness\nof end-to-end autonomous driving systems. Existing methods predominantly adopt\neither attention-based flatten fusion or bird's eye view fusion through\ngeometric transformations. However, these approaches often suffer from limited\ninterpretability or dense computational overhead. In this paper, we introduce\nGaussianFusion, a Gaussian-based multi-sensor fusion framework for end-to-end\nautonomous driving. Our method employs intuitive and compact Gaussian\nrepresentations as intermediate carriers to aggregate information from diverse\nsensors. Specifically, we initialize a set of 2D Gaussians uniformly across the\ndriving scene, where each Gaussian is parameterized by physical attributes and\nequipped with explicit and implicit features. These Gaussians are progressively\nrefined by integrating multi-modal features. The explicit features capture rich\nsemantic and spatial information about the traffic scene, while the implicit\nfeatures provide complementary cues beneficial for trajectory planning. To\nfully exploit rich spatial and semantic information in Gaussians, we design a\ncascade planning head that iteratively refines trajectory predictions through\ninteractions with Gaussians. Extensive experiments on the NAVSIM and\nBench2Drive benchmarks demonstrate the effectiveness and robustness of the\nproposed GaussianFusion framework. The source code will be released at\nhttps://github.com/Say2L/GaussianFusion.\n","authors":["Shuai Liu","Quanmin Liang","Zefeng Li","Boyang Li","Kai Huang"],"pdf_url":"https://arxiv.org/pdf/2506.00034v2.pdf","comment":"Accepted at NeurIPS2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.24000v1","updated":"2025-10-28T02:10:54Z","published":"2025-10-28T02:10:54Z","title":"AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification\n  and Cross-Domain Generalization","summary":"  Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet\nearly and accurate detection can significantly improve treatment outcomes.\nWhile numerous Deep learning (DL) models have been developed to predict DR from\nfundus images, many face challenges in maintaining robustness due to\ndistributional variations caused by differences in acquisition devices,\ndemographic disparities, and imaging conditions. This paper addresses this\ncritical limitation by proposing a novel DR classification approach, a method\ncalled AdvBlur. Our method integrates adversarial blurred images into the\ndataset and employs a dual-loss function framework to address domain\ngeneralization. This approach effectively mitigates the impact of unseen\ndistributional variations, as evidenced by comprehensive evaluations across\nmultiple datasets. Additionally, we conduct extensive experiments to explore\nthe effects of factors such as camera type, low-quality images, and dataset\nsize. Furthermore, we perform ablation studies on blurred images and the loss\nfunction to ensure the validity of our choices. The experimental results\ndemonstrate the effectiveness of our proposed method, achieving competitive\nperformance compared to state-of-the-art domain generalization DR models on\nunseen external datasets.\n","authors":["Heethanjan Kanagalingam","Thenukan Pathmanathan","Mokeeshan Vathanakumar","Tharmakulasingam Mukunthan"],"pdf_url":"https://arxiv.org/pdf/2510.24000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23594v2","updated":"2025-10-28T02:07:50Z","published":"2025-10-27T17:57:52Z","title":"PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error\n  Detection","summary":"  Multimodal large language models (MLLMs) have achieved remarkable progress on\nvision-language tasks, yet their reasoning processes remain sometimes\nunreliable. We introduce PRISM-Bench, a benchmark of puzzle-based visual\nchallenges designed to evaluate not only whether models can solve problems, but\nhow their reasoning unfolds. Unlike prior evaluations that measure only\nfinal-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual\npuzzle and a step-by-step chain-of-thought (CoT) containing exactly one error,\nmodels must identify the first incorrect step. This setting enables\nfine-grained assessment of logical consistency, error detection, and visual\nreasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric,\nand analogical reasoning, resisting shortcuts based on superficial pattern\nmatching. Evaluations across state-of-the-art MLLMs reveal a persistent gap\nbetween fluent generation and faithful reasoning: models that produce plausible\nCoTs often fail to locate simple logical faults. By disentangling answer\ngeneration from reasoning verification, PRISM-Bench offers a sharper lens on\nmultimodal reasoning competence and underscores the need for diagnostic\nevaluation protocols in the development of trustworthy MLLMs.\n","authors":["Yusu Qian","Cheng Wan","Chao Jia","Yinfei Yang","Qingyu Zhao","Zhe Gan"],"pdf_url":"https://arxiv.org/pdf/2510.23594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.18443v2","updated":"2025-10-28T01:59:34Z","published":"2025-06-23T09:27:22Z","title":"Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation","summary":"  Achieving reliable ego motion estimation for agile robots, e.g., aerobatic\naircraft, remains challenging because most robot sensors fail to respond timely\nand clearly to highly dynamic robot motions, often resulting in measurement\nblurring, distortion, and delays. In this paper, we propose an IMU-free and\nfeature-association-free framework to achieve aggressive ego-motion velocity\nestimation of a robot platform in highly dynamic scenarios by combining two\ntypes of exteroceptive sensors, an event camera and a millimeter wave radar,\nFirst, we used instantaneous raw events and Doppler measurements to derive\nrotational and translational velocities directly. Without a sophisticated\nassociation process between measurement frames, the proposed method is more\nrobust in texture-less and structureless environments and is more\ncomputationally efficient for edge computing devices. Then, in the back-end, we\npropose a continuous-time state-space model to fuse the hybrid time-based and\nevent-based measurements to estimate the ego-motion velocity in a fixed-lagged\nsmoother fashion. In the end, we validate our velometer framework extensively\nin self-collected experiment datasets. The results indicate that our IMU-free\nand association-free ego motion estimation framework can achieve reliable and\nefficient velocity output in challenging environments. The source code,\nillustrative video and dataset are available at\nhttps://github.com/ZzhYgwh/TwistEstimator.\n","authors":["Yang Lyu","Zhenghao Zou","Yanfeng Li","Xiaohu Guo","Chunhui Zhao","Quan Pan"],"pdf_url":"https://arxiv.org/pdf/2506.18443v2.pdf","comment":"2025.10.28 version v2 for TwistEstimator"},{"id":"http://arxiv.org/abs/2503.23502v3","updated":"2025-10-28T01:42:48Z","published":"2025-03-30T16:24:22Z","title":"Boosting Omnidirectional Stereo Matching with a Pre-trained Depth\n  Foundation Model","summary":"  Omnidirectional depth perception is essential for mobile robotics\napplications that require scene understanding across a full 360{\\deg} field of\nview. Camera-based setups offer a cost-effective option by using stereo depth\nestimation to generate dense, high-resolution depth maps without relying on\nexpensive active sensing. However, existing omnidirectional stereo matching\napproaches achieve only limited depth accuracy across diverse environments,\ndepth ranges, and lighting conditions, due to the scarcity of real-world data.\nWe present DFI-OmniStereo, a novel omnidirectional stereo matching method that\nleverages a large-scale pre-trained foundation model for relative monocular\ndepth estimation within an iterative optimization-based stereo matching\narchitecture. We introduce a dedicated two-stage training strategy to utilize\nthe relative monocular depth features for our omnidirectional stereo matching\nbefore scale-invariant fine-tuning. DFI-OmniStereo achieves state-of-the-art\nresults on the real-world Helvipad dataset, reducing disparity MAE by\napproximately 16% compared to the previous best omnidirectional stereo method.\n","authors":["Jannik Endres","Oliver Hahn","Charles Corbière","Simone Schaub-Meyer","Stefan Roth","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2503.23502v3.pdf","comment":"Accepted at IROS 2025. Project page:\n  https://vita-epfl.github.io/DFI-OmniStereo-website/"},{"id":"http://arxiv.org/abs/2503.04852v2","updated":"2025-10-28T01:41:35Z","published":"2025-03-06T03:40:01Z","title":"CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data","summary":"  True intelligence hinges on the ability to uncover and leverage hidden causal\nrelations. Despite significant progress in AI and computer vision (CV), there\nremains a lack of benchmarks for assessing models' abilities to infer latent\ncausality from complex visual data. In this paper, we introduce\n\\textsc{\\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates\nstructured data (tables) with corresponding visual representations (images) to\nevaluate causal reasoning. Designed within a systematic framework, Causal3D\ncomprises 19 3D-scene datasets capturing diverse causal relations, views, and\nbackgrounds, enabling evaluations across scenes of varying complexity. We\nassess multiple state-of-the-art methods, including classical causal discovery,\ncausal representation learning, and large/vision-language models (LLMs/VLMs).\nOur experiments show that as causal structures grow more complex without prior\nknowledge, performance declines significantly, highlighting the challenges even\nadvanced methods face in complex causal scenarios. Causal3D serves as a vital\nresource for advancing causal reasoning in CV and fostering trustworthy AI in\ncritical domains.\n","authors":["Disheng Liu","Yiran Qiao","Wuche Liu","Yiren Lu","Yunlai Zhou","Tuo Liang","Yu Yin","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2503.04852v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23981v1","updated":"2025-10-28T01:24:24Z","published":"2025-10-28T01:24:24Z","title":"TeleEgo: Benchmarking Egocentric AI Assistants in the Wild","summary":"  Egocentric AI assistants in real-world settings must process multi-modal\ninputs (video, audio, text), respond in real time, and retain evolving\nlong-term memory. However, existing benchmarks typically evaluate these\nabilities in isolation, lack realistic streaming scenarios, or support only\nshort-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming,\nomni-modal benchmark for evaluating egocentric AI assistants in realistic daily\ncontexts. The dataset features over 14 hours per participant of synchronized\negocentric video, audio, and text across four domains: work \\& study, lifestyle\n\\& routines, social activities, and outings \\& culture. All data is aligned on\na unified global timeline and includes high-quality visual narrations and\nspeech transcripts, curated through human refinement.TeleEgo defines 12\ndiagnostic subtasks across three core capabilities: Memory (recalling past\nevents), Understanding (interpreting the current moment), and Cross-Memory\nReasoning (linking distant events). It contains 3,291 human-verified QA items\nspanning multiple question formats (single-choice, binary, multi-choice, and\nopen-ended), evaluated strictly in a streaming setting. We propose two key\nmetrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess\ncorrectness, temporal responsiveness, and long-term retention. TeleEgo provides\na realistic and comprehensive evaluation to advance the development of\npractical AI assistants.\n","authors":["Jiaqi Yan","Ruilong Ren","Jingren Liu","Shuning Xu","Ling Wang","Yiheng Wang","Yun Wang","Long Zhang","Xiangyu Chen","Changzhi Sun","Jixiang Luo","Dell Zhang","Hao Sun","Chi Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2510.23981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23978v1","updated":"2025-10-28T01:19:54Z","published":"2025-10-28T01:19:54Z","title":"Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution\n  with Fourier Constraints","summary":"  Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is\ncrucial. Existing methods predict Fourier components one by one using a\nrecurrent neural network. However, this approach leads to performance\ndegradation and inefficiency due to independent prediction. This paper proposes\npredicting multiple components jointly to improve both quality and efficiency.\n","authors":["Kazutoshi Akita","Norimichi Ukita"],"pdf_url":"https://arxiv.org/pdf/2510.23978v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2510.23977v1","updated":"2025-10-28T01:18:00Z","published":"2025-10-28T01:18:00Z","title":"Synergistic Neural Forecasting of Air Pollution with Stochastic Sampling","summary":"  Air pollution remains a leading global health and environmental risk,\nparticularly in regions vulnerable to episodic air pollution spikes due to\nwildfires, urban haze and dust storms. Accurate forecasting of particulate\nmatter (PM) concentrations is essential to enable timely public health warnings\nand interventions, yet existing models often underestimate rare but hazardous\npollution events. Here, we present SynCast, a high-resolution neural\nforecasting model that integrates meteorological and air composition data to\nimprove predictions of both average and extreme pollution levels. Built on a\nregionally adapted transformer backbone and enhanced with a diffusion-based\nstochastic refinement module, SynCast captures the nonlinear dynamics driving\nPM spikes more accurately than existing approaches. Leveraging on harmonized\nERA5 and CAMS datasets, our model shows substantial gains in forecasting\nfidelity across multiple PM variables (PM$_1$, PM$_{2.5}$, PM$_{10}$),\nespecially under extreme conditions. We demonstrate that conventional loss\nfunctions underrepresent distributional tails (rare pollution events) and show\nthat SynCast, guided by domain-aware objectives and extreme value theory,\nsignificantly enhances performance in highly impacted regions without\ncompromising global accuracy. This approach provides a scalable foundation for\nnext-generation air quality early warning systems and supports climate-health\nrisk mitigation in vulnerable regions.\n","authors":["Yohan Abeysinghe","Muhammad Akhtar Munir","Sanoojan Baliah","Ron Sarafian","Fahad Shahbaz Khan","Yinon Rudich","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2510.23977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22118v2","updated":"2025-10-28T00:53:28Z","published":"2025-10-25T02:07:23Z","title":"GRAID: Enhancing Spatial Reasoning of VLMs Through High-Fidelity Data\n  Generation","summary":"  Vision Language Models (VLMs) achieve strong performance on many\nvision-language tasks but often struggle with spatial\nreasoning$\\unicode{x2014}$a prerequisite for many applications. Empirically, we\nfind that a dataset produced by a current training data generation pipeline has\na 57.6% human validation rate. These rates stem from current limitations:\nsingle-image 3D reconstruction introduces cascading modeling errors and\nrequires wide answer tolerances, while caption-based methods require\nhyper-detailed annotations and suffer from generative hallucinations. We\npresent GRAID, built on the key insight that qualitative spatial relationships\ncan be reliably determined from 2D geometric primitives alone. By operating\nexclusively on 2D bounding boxes from standard object detectors, GRAID avoids\nboth 3D reconstruction errors and generative hallucinations, resulting in\ndatasets that are of higher quality than existing tools that produce similar\ndatasets as validated by human evaluations. We apply our framework to the\nBDD100k, NuImages, and Waymo datasets, generating over 8.5 million high-quality\nVQA pairs creating questions spanning spatial relations, counting, ranking, and\nsize comparisons. We evaluate one of the datasets and find it achieves 91.16%\nhuman-validated accuracy$\\unicode{x2014}$compared to 57.6% on a dataset\ngenerated by recent work. Critically, we demonstrate that when trained on GRAID\ndata, models learn spatial reasoning concepts that generalize: models\nfine-tuned on 6 question types improve on over 10 held-out types, with accuracy\ngains of 47.5% on BDD and 37.9% on NuImages for Llama 3.2B 11B, and when\ntrained on all questions types, achieve improvements on several existing\nbenchmarks such as BLINK. The GRAID framework, datasets, and additional\ninformation can be found $\\href{this https URL}{here}$.\n","authors":["Karim Elmaaroufi","Liheng Lai","Justin Svegliato","Yutong Bai","Sanjit A. Seshia","Matei Zaharia"],"pdf_url":"https://arxiv.org/pdf/2510.22118v2.pdf","comment":"22 pages, 3 figures, 3 tables, project page:\n  https://ke7.github.io/graid/"},{"id":"http://arxiv.org/abs/2510.23968v1","updated":"2025-10-28T00:48:00Z","published":"2025-10-28T00:48:00Z","title":"Reasoning Visual Language Model for Chest X-Ray Analysis","summary":"  Vision-language models (VLMs) have shown strong promise for medical image\nanalysis, but most remain opaque, offering predictions without the transparent,\nstepwise reasoning clinicians rely on. We present a framework that brings\nchain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by\nreasoning-first training paradigms, our approach is designed to learn how\nexperts reason, not just what they conclude, by aligning intermediate steps\nwith observable image evidence and radiology workflow. Beyond accuracy, the\nexplicit reasoning traces support clinical auditability: they reveal why a\nconclusion was reached, which alternatives were considered, and where\nuncertainty remains, enabling quality assurance, error analysis, and safer\nhuman-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training\nrecipe: a reasoning-style supervised fine-tuning (SFT) followed by\nreinforcement learning (RL) that uses verifiable rewards over a list of X-ray\nabnormalities. The model outputs reasoning that mirrors radiologists systematic\nthought process, uncertainty, and differential diagnosis. In\nout-of-distribution evaluation, the approach achieves competitive multi-label\nclassification while improving interpretability. In a reader study with expert\nradiologists, full reasoning traces increased confidence, supported error\nauditing, and reduced time to finalize reports. We release code and the model\nNV-Reason-CXR-3B to support community progress toward trustworthy, explainable\nAI in chest radiography and other medical imaging tasks where reasoning quality\nis as critical as prediction quality.\n","authors":["Andriy Myronenko","Dong Yang","Baris Turkbey","Mariam Aboian","Sena Azamat","Esra Akcicek","Hongxu Yin","Pavlo Molchanov","Marc Edgar","Yufan He","Pengfei Guo","Yucheng Tang","Daguang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.23968v1.pdf","comment":"NV-Reason-CXR-3B"},{"id":"http://arxiv.org/abs/2510.23960v1","updated":"2025-10-28T00:35:59Z","published":"2025-10-28T00:35:59Z","title":"SafeVision: Efficient Image Guardrail with Robust Policy Adherence and\n  Explainability","summary":"  With the rapid proliferation of digital media, the need for efficient and\ntransparent safeguards against unsafe content is more critical than ever.\nTraditional image guardrail models, constrained by predefined categories, often\nmisclassify content due to their pure feature-based learning without semantic\nreasoning. Moreover, these models struggle to adapt to emerging threats,\nrequiring costly retraining for new threats. To address these limitations, we\nintroduce SafeVision, a novel image guardrail that integrates human-like\nreasoning to enhance adaptability and transparency. Our approach incorporates\nan effective data collection and generation framework, a policy-following\ntraining pipeline, and a customized loss function. We also propose a diverse QA\ngeneration and training strategy to enhance learning effectiveness. SafeVision\ndynamically aligns with evolving safety policies at inference time, eliminating\nthe need for retraining while ensuring precise risk assessments and\nexplanations. Recognizing the limitations of existing unsafe image benchmarks,\nwhich either lack granularity or cover limited risks, we introduce VisionHarm,\na high-quality dataset comprising two subsets: VisionHarm Third-party\n(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse\nharmful categories. Through extensive experiments, we show that SafeVision\nachieves state-of-the-art performance on different benchmarks. SafeVision\noutperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while\nbeing over 16x faster. SafeVision sets a comprehensive, policy-following, and\nexplainable image guardrail with dynamic adaptation to emerging threats.\n","authors":["Peiyang Xu","Minzhou Pan","Zhaorun Chen","Shuang Yang","Chaowei Xiao","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2510.23960v1.pdf","comment":"42 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.23956v1","updated":"2025-10-28T00:19:42Z","published":"2025-10-28T00:19:42Z","title":"Neural USD: An object-centric framework for iterative editing and\n  control","summary":"  Amazing progress has been made in controllable generative modeling,\nespecially over the last few years. However, some challenges remain. One of\nthem is precise and iterative object editing. In many of the current methods,\ntrying to edit the generated image (for example, changing the color of a\nparticular object in the scene or changing the background while keeping other\nelements unchanged) by changing the conditioning signals often leads to\nunintended global changes in the scene. In this work, we take the first steps\nto address the above challenges. Taking inspiration from the Universal Scene\nDescriptor (USD) standard developed in the computer graphics community, we\nintroduce the \"Neural Universal Scene Descriptor\" or Neural USD. In this\nframework, we represent scenes and objects in a structured, hierarchical\nmanner. This accommodates diverse signals, minimizes model-specific\nconstraints, and enables per-object control over appearance, geometry, and\npose. We further apply a fine-tuning approach which ensures that the above\ncontrol signals are disentangled from one another. We evaluate several design\nconsiderations for our framework, demonstrating how Neural USD enables\niterative and incremental workflows. More information at:\nhttps://escontrela.me/neural_usd .\n","authors":["Alejandro Escontrela","Shrinu Kushagra","Sjoerd van Steenkiste","Yulia Rubanova","Aleksander Holynski","Kelsey Allen","Kevin Murphy","Thomas Kipf"],"pdf_url":"https://arxiv.org/pdf/2510.23956v1.pdf","comment":"22 pages, 16 figures, 1 table"},{"id":"http://arxiv.org/abs/2510.03786v2","updated":"2025-10-28T23:43:17Z","published":"2025-10-04T11:25:10Z","title":"MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based\n  Fusion for Medical Image Segmentation","summary":"  In recent years, deep learning has shown near-expert performance in\nsegmenting complex medical tissues and tumors. However, existing models are\noften task-specific, with performance varying across modalities and anatomical\nregions. Balancing model complexity and performance remains challenging,\nparticularly in clinical settings where both accuracy and efficiency are\ncritical. To address these issues, we propose a hybrid segmentation\narchitecture featuring a three-branch encoder that integrates CNNs,\nTransformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture\nlocal, global, and long-range dependencies. A multi-scale attention-based CNN\ndecoder reconstructs fine-grained segmentation maps while preserving contextual\nconsistency. Additionally, a co-attention gate enhances feature selection by\nemphasizing relevant spatial and semantic information across scales during both\nencoding and decoding, improving feature interaction and cross-scale\ncommunication. Extensive experiments on multiple benchmark datasets show that\nour approach outperforms state-of-the-art methods in accuracy and\ngeneralization, while maintaining comparable computational complexity. By\neffectively balancing efficiency and effectiveness, our architecture offers a\npractical and scalable solution for diverse medical imaging tasks. Source code\nand trained models will be publicly released upon acceptance to support\nreproducibility and further research.\n","authors":["T-Mai Bui","Fares Bougourzi","Fadi Dornaika","Vinh Truong Hoang"],"pdf_url":"https://arxiv.org/pdf/2510.03786v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25032v1","updated":"2025-10-28T23:21:00Z","published":"2025-10-28T23:21:00Z","title":"Efficient License Plate Recognition via Pseudo-Labeled Supervision with\n  Grounding DINO and YOLOv8","summary":"  Developing a highly accurate automatic license plate recognition system\n(ALPR) is challenging due to environmental factors such as lighting, rain, and\ndust. Additional difficulties include high vehicle speeds, varying camera\nangles, and low-quality or low-resolution images. ALPR is vital in traffic\ncontrol, parking, vehicle tracking, toll collection, and law enforcement\napplications. This paper proposes a deep learning strategy using YOLOv8 for\nlicense plate detection and recognition tasks. This method seeks to enhance the\nperformance of the model using datasets from Ontario, Quebec, California, and\nNew York State. It achieved an impressive recall rate of 94% on the dataset\nfrom the Center for Pattern Recognition and Machine Intelligence (CENPARMI) and\n91% on the UFPR-ALPR dataset. In addition, our method follows a semi-supervised\nlearning framework, combining a small set of manually labeled data with\npseudo-labels generated by Grounding DINO to train our detection model.\nGrounding DINO, a powerful vision-language model, automatically annotates many\nimages with bounding boxes for license plates, thereby minimizing the reliance\non labor-intensive manual labeling. By integrating human-verified and\nmodel-generated annotations, we can scale our dataset efficiently while\nmaintaining label quality, which significantly enhances the training process\nand overall model performance. Furthermore, it reports character error rates\nfor both datasets, providing additional insight into system performance.\n","authors":["Zahra Ebrahimi Vargoorani","Amir Mohammad Ghoreyshi","Ching Yee Suen"],"pdf_url":"https://arxiv.org/pdf/2510.25032v1.pdf","comment":"6 pages, 8 figures. Presented at 2025 IEEE International Workshop on\n  Machine Learning for Signal Processing (MLSP), August 31 - September 3, 2025,\n  Istanbul, Turkey"},{"id":"http://arxiv.org/abs/2412.08619v3","updated":"2025-10-28T22:43:29Z","published":"2024-12-11T18:40:16Z","title":"Physics Context Builders: A Modular Framework for Physical Reasoning in\n  Vision-Language Models","summary":"  Physical reasoning remains a significant challenge for Vision-Language Models\n(VLMs). This limitation arises from an inability to translate learned knowledge\ninto predictions about physical behavior. Although continual fine-tuning can\nmitigate this issue, it is expensive for large models and impractical to\nperform repeatedly for every task. This necessitates the creation of modular\nand scalable ways to teach VLMs about physical reasoning. To that end, we\nintroduce Physics Context Builders (PCBs), a modular framework where\nspecialized smaller VLMs are fine-tuned to generate detailed physical scene\ndescriptions. These can be used as physical contexts to enhance the reasoning\ncapabilities of larger VLMs. PCBs enable the separation of visual perception\nfrom reasoning, allowing us to analyze their relative contributions to physical\nunderstanding. We perform experiments on CLEVRER and on Falling Tower, a\nstability detection dataset with both simulated and real-world scenes, to\ndemonstrate that PCBs provide substantial performance improvements, increasing\naverage accuracy by up to 13.8% on complex physical reasoning tasks. Notably,\nPCBs also show strong Sim2Real transfer, successfully generalizing from\nsimulated training data to real-world scenes.\n","authors":["Vahid Balazadeh","Mohammadmehdi Ataei","Hyunmin Cheong","Amir Hosein Khasahmadi","Rahul G. Krishnan"],"pdf_url":"https://arxiv.org/pdf/2412.08619v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.10266v2","updated":"2025-10-28T22:32:18Z","published":"2025-09-12T14:08:06Z","title":"SignMouth: Leveraging Mouthing Cues for Sign Language Translation by\n  Multimodal Contrastive Fusion","summary":"  Sign language translation (SLT) aims to translate natural language from sign\nlanguage videos, serving as a vital bridge for inclusive communication. While\nrecent advances leverage powerful visual backbones and large language models,\nmost approaches mainly focus on manual signals (hand gestures) and tend to\noverlook non-manual cues like mouthing. In fact, mouthing conveys essential\nlinguistic information in sign languages and plays a crucial role in\ndisambiguating visually similar signs. In this paper, we propose SignClip, a\nnovel framework to improve the accuracy of sign language translation. It fuses\nmanual and non-manual cues, specifically spatial gesture and lip movement\nfeatures. Besides, SignClip introduces a hierarchical contrastive learning\nframework with multi-level alignment objectives, ensuring semantic consistency\nacross sign-lip and visual-text modalities. Extensive experiments on two\nbenchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our\napproach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip\nsurpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from\n24.32 to 24.71, and ROUGE from 46.57 to 48.38.\n","authors":["Wenfang Wu","Tingting Yuan","Yupeng Li","Daling Wang","Xiaoming Fu"],"pdf_url":"https://arxiv.org/pdf/2509.10266v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25002v1","updated":"2025-10-28T22:02:36Z","published":"2025-10-28T22:02:36Z","title":"Resi-VidTok: An Efficient and Decomposed Progressive Tokenization\n  Framework for Ultra-Low-Rate and Lightweight Video Transmission","summary":"  Real-time transmission of video over wireless networks remains highly\nchallenging, even with advanced deep models, particularly under severe channel\nconditions such as limited bandwidth and weak connectivity. In this paper, we\npropose Resi-VidTok, a Resilient Tokenization-Enabled framework designed for\nultra-low-rate and lightweight video transmission that delivers strong\nrobustness while preserving perceptual and semantic fidelity on commodity\ndigital hardware. By reorganizing spatio--temporal content into a discrete,\nimportance-ordered token stream composed of key tokens and refinement tokens,\nResi-VidTok enables progressive encoding, prefix-decodable reconstruction, and\ngraceful quality degradation under constrained channels. A key contribution is\na resilient 1D tokenization pipeline for video that integrates differential\ntemporal token coding, explicitly supporting reliable recovery from incomplete\ntoken sets using a single shared framewise decoder--without auxiliary temporal\nextractors or heavy generative models. Furthermore, stride-controlled frame\nsparsification combined with a lightweight decoder-side interpolator reduces\ntransmission load while maintaining motion continuity. Finally, a\nchannel-adaptive source--channel coding and modulation scheme dynamically\nallocates rate and protection according to token importance and channel\ncondition, yielding stable quality across adverse SNRs. Evaluation results\nindicate robust visual and semantic consistency at channel bandwidth ratios\n(CBR) as low as 0.0004 and real-time reconstruction at over 30 fps,\ndemonstrating the practicality of Resi-VidTok for energy-efficient,\nlatency-sensitive, and reliability-critical wireless applications.\n","authors":["Zhenyu Liu","Yi Ma","Rahim Tafazolli","Zhi Ding"],"pdf_url":"https://arxiv.org/pdf/2510.25002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22918v4","updated":"2025-10-28T21:55:57Z","published":"2025-05-28T22:39:12Z","title":"Re-ttention: Ultra Sparse Visual Generation via Attention Statistical\n  Reshape","summary":"  Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. Experimental results\non T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate that\nRe-ttention requires as few as 3.1% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference.\n","authors":["Ruichen Chen","Keith G. Mills","Liyao Jiang","Chao Gao","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2505.22918v4.pdf","comment":"author comment: This version was previously removed by arXiv\n  administrators as the submitter did not have the rights to agree to the\n  license at the time of submission. The authors have now obtained the\n  necessary permissions, and the paper is resubmitted accordingly"},{"id":"http://arxiv.org/abs/2510.24980v1","updated":"2025-10-28T21:23:32Z","published":"2025-10-28T21:23:32Z","title":"FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for\n  Pressure Ulcer Severity Classification with Reasoning","summary":"  Pressure ulcers (PUs) are a serious and prevalent healthcare concern.\nAccurate classification of PU severity (Stages I-IV) is essential for proper\ntreatment but remains challenging due to subtle visual distinctions and\nsubjective interpretation, leading to variability among clinicians. Prior\nAI-based approaches using Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViTs) achieved promising accuracy but offered limited\ninterpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal\nmodel), a fine-tuned multimodal large language model (MLLM) with an agentic\nself-reflection mechanism for pressure ulcer severity classification. Inspired\nby clinician-style diagnostic reassessment, FT-ARM iteratively refines its\npredictions by reasoning over visual features and encoded clinical knowledge\nfrom text, enhancing both accuracy and consistency. On the publicly available\nPressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B,\nachieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based\nmodels by +4%. Unlike earlier CNN/ViT studies that relied solely on offline\nevaluations, FT-ARM is designed and tested for live inference, reflecting\nreal-time deployment conditions. Furthermore, it produces clinically grounded\nnatural-language explanations, improving interpretability and trust. By\nintegrating fine-tuning and reflective reasoning across multimodal inputs,\nFT-ARM advances the reliability, transparency, and clinical applicability of\nautomated wound assessment systems, addressing the critical need for consistent\nand explainable PU staging to support improved patient care.\n","authors":["Reza Saadati Fard","Emmanuel Agu","Palawat Busaranuvong","Deepak Kumar","Shefalika Gautam","Bengisu Tulu","Diane Strong","Lorraine Loretz"],"pdf_url":"https://arxiv.org/pdf/2510.24980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09066v5","updated":"2025-10-28T21:13:11Z","published":"2024-03-14T03:13:01Z","title":"Hyperparameters in Continual Learning: A Reality Check","summary":"  Continual learning (CL) aims to train a model on a sequence of tasks (i.e., a\nCL scenario) while balancing the trade-off between plasticity (learning new\ntasks) and stability (retaining prior knowledge). The dominantly adopted\nconventional evaluation protocol for CL algorithms selects the best\nhyperparameters (e.g., learning rate, mini-batch size, regularization\nstrengths, etc.) within a given scenario and then evaluates the algorithms\nusing these hyperparameters in the same scenario. However, this protocol has\nsignificant shortcomings: it overestimates the CL capacity of algorithms and\nrelies on unrealistic hyperparameter tuning, which is not feasible for\nreal-world applications. From the fundamental principles of evaluation in\nmachine learning, we argue that the evaluation of CL algorithms should focus on\nassessing the generalizability of their CL capacity to unseen scenarios. Based\non this, we propose the Generalizable Two-phase Evaluation Protocol (GTEP)\nconsisting of hyperparameter tuning and evaluation phases. Both phases share\nthe same scenario configuration (e.g., number of tasks) but are generated from\ndifferent datasets. Hyperparameters of CL algorithms are tuned in the first\nphase and applied in the second phase to evaluate the algorithms. We apply this\nprotocol to class-incremental learning, both with and without pretrained\nmodels. Across more than 8,000 experiments, our results show that most\nstate-of-the-art algorithms fail to replicate their reported performance,\nhighlighting that their CL capacity has been significantly overestimated in the\nconventional evaluation protocol. Our implementation can be found in\nhttps://github.com/csm9493/GTEP.\n","authors":["Sungmin Cha","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2403.09066v5.pdf","comment":"TMLR 2025 camera ready version"},{"id":"http://arxiv.org/abs/2507.22017v2","updated":"2025-10-28T20:48:51Z","published":"2025-07-29T17:06:22Z","title":"Cyst-X: A Federated AI System Outperforms Clinical Guidelines to Detect\n  Pancreatic Cancer Precursors and Reduce Unnecessary Surgery","summary":"  Pancreatic cancer is projected to be the second-deadliest cancer by 2030,\nmaking early detection critical. Intraductal papillary mucinous neoplasms\n(IPMNs), key cancer precursors, present a clinical dilemma, as current\nguidelines struggle to stratify malignancy risk, leading to unnecessary\nsurgeries or missed diagnoses. Here, we developed Cyst-X, an AI framework for\nIPMN risk prediction trained on a unique, multi-center dataset of 1,461 MRI\nscans from 764 patients. Cyst-X achieves significantly higher accuracy (AUC =\n0.82) than both the established Kyoto guidelines (AUC = 0.75) and expert\nradiologists, particularly in correct identification of high-risk lesions.\nClinically, this translates to a 20% increase in cancer detection sensitivity\n(87.8% vs. 64.1%) for high-risk lesions. We demonstrate that this performance\nis maintained in a federated learning setting, allowing for collaborative model\ntraining without compromising patient privacy. To accelerate research in early\npancreatic cancer detection, we publicly release the Cyst-X dataset and models,\nproviding the first large-scale, multi-center MRI resource for pancreatic cyst\nanalysis.\n","authors":["Hongyi Pan","Gorkem Durak","Elif Keles","Deniz Seyithanoglu","Zheyuan Zhang","Alpay Medetalibeyoglu","Halil Ertugrul Aktas","Andrea Mia Bejar","Ziliang Hong","Yavuz Taktak","Gulbiz Dagoglu Kartal","Mehmet Sukru Erturk","Timurhan Cebeci","Maria Jaramillo Gonzalez","Yury Velichko","Lili Zhao","Emil Agarunov","Federica Proietto Salanitri","Concetto Spampinato","Pallavi Tiwari","Ziyue Xu","Sachin Jambawalikar","Ivo G. Schoots","Marco J. Bruno","Chenchang Huang","Candice W. Bolan","Tamas Gonda","Frank H. Miller","Rajesh N. Keswani","Michael B. Wallace","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2507.22017v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24949v1","updated":"2025-10-28T20:31:19Z","published":"2025-10-28T20:31:19Z","title":"SCOUT: A Lightweight Framework for Scenario Coverage Assessment in\n  Autonomous Driving","summary":"  Assessing scenario coverage is crucial for evaluating the robustness of\nautonomous agents, yet existing methods rely on expensive human annotations or\ncomputationally intensive Large Vision-Language Models (LVLMs). These\napproaches are impractical for large-scale deployment due to cost and\nefficiency constraints. To address these shortcomings, we propose SCOUT\n(Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate\nmodel designed to predict scenario coverage labels directly from an agent's\nlatent sensor representations. SCOUT is trained through a distillation process,\nlearning to approximate LVLM-generated coverage labels while eliminating the\nneed for continuous LVLM inference or human annotation. By leveraging\nprecomputed perception features, SCOUT avoids redundant computations and\nenables fast, scalable scenario coverage estimation. We evaluate our method\nacross a large dataset of real-life autonomous navigation scenarios,\ndemonstrating that it maintains high accuracy while significantly reducing\ncomputational cost. Our results show that SCOUT provides an effective and\npractical alternative for large-scale coverage analysis. While its performance\ndepends on the quality of LVLM-generated training labels, SCOUT represents a\nmajor step toward efficient scenario coverage oversight in autonomous systems.\n","authors":["Anil Yildiz","Sarah M. Thornton","Carl Hildebrandt","Sreeja Roy-Singh","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2510.24949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22589v2","updated":"2025-10-28T20:08:24Z","published":"2025-10-26T09:09:52Z","title":"PSScreen V2: Partially Supervised Multiple Retinal Disease Screening","summary":"  In this work, we propose PSScreen V2, a partially supervised self-training\nframework for multiple retinal disease screening. Unlike previous methods that\nrely on fully labelled or single-domain datasets, PSScreen V2 is designed to\nlearn from multiple partially labelled datasets with different distributions,\naddressing both label absence and domain shift challenges. To this end,\nPSScreen V2 adopts a three-branch architecture with one teacher and two student\nnetworks. The teacher branch generates pseudo labels from weakly augmented\nimages to address missing labels, while the two student branches introduce\nnovel feature augmentation strategies: Low-Frequency Dropout (LF-Dropout),\nwhich enhances domain robustness by randomly discarding domain-related\nlow-frequency components, and Low-Frequency Uncertainty (LF-Uncert), which\nestimates uncertain domain variability via adversarially learned Gaussian\nperturbations of low-frequency statistics. Extensive experiments on multiple\nin-domain and out-of-domain fundus datasets demonstrate that PSScreen V2\nachieves state-of-the-art performance and superior domain generalization\nability. Furthermore, compatibility tests with diverse backbones, including the\nvision foundation model DINOv2, as well as evaluations on chest X-ray datasets,\nhighlight the universality and adaptability of the proposed framework. The\ncodes are available at https://github.com/boyiZheng99/PSScreen_V2.\n","authors":["Boyi Zheng","Yalin Zheng","Hrvoje Bogunović","Qing Liu"],"pdf_url":"https://arxiv.org/pdf/2510.22589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24936v1","updated":"2025-10-28T20:06:08Z","published":"2025-10-28T20:06:08Z","title":"IBIS: A Powerful Hybrid Architecture for Human Activity Recognition","summary":"  The increasing interest in Wi-Fi sensing stems from its potential to capture\nenvironmental data in a low-cost, non-intrusive way, making it ideal for\napplications like healthcare, space occupancy analysis, and gesture-based IoT\ncontrol. However, a major limitation in this field is the common problem of\noverfitting, where models perform well on training data but fail to generalize\nto new data. To overcome this, we introduce a novel hybrid architecture that\nintegrates Inception-BiLSTM with a Support Vector Machine (SVM), which we refer\nto as IBIS. Our IBIS approach is uniquely engineered to improve model\ngeneralization and create more robust classification boundaries. By applying\nthis method to Doppler-derived data, we achieve a movement recognition accuracy\nof nearly 99%. Comprehensive performance metrics and confusion matrices confirm\nthe significant effectiveness of our proposed solution.\n","authors":["Alison M. Fernandes","Hermes I. Del Monego","Bruno S. Chang","Anelise Munaretto","Hélder M. Fontes","Rui L. Campos"],"pdf_url":"https://arxiv.org/pdf/2510.24936v1.pdf","comment":"8 pages. 8 figures. Wireless Days Conference, December 2025"},{"id":"http://arxiv.org/abs/2510.24919v1","updated":"2025-10-28T19:44:20Z","published":"2025-10-28T19:44:20Z","title":"Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient\n  Modulation for Harmonized Multimodal Learning","summary":"  In multimodal learning, dominant modalities often overshadow others, limiting\ngeneralization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM),\na model-agnostic framework that applies to many modalities and supports early\nand late fusion scenarios. In every iteration, M-SAM in three steps optimizes\nlearning. \\textbf{First, it identifies the dominant modality} based on\nmodalities' contribution in the accuracy using Shapley. \\textbf{Second, it\ndecomposes the loss landscape}, or in another language, it modulates the loss\nto prioritize the robustness of the model in favor of the dominant modality,\nand \\textbf{third, M-SAM updates the weights} by backpropagation of modulated\ngradients. This ensures robust learning for the dominant modality while\nenhancing contributions from others, allowing the model to explore and exploit\ncomplementary features that strengthen overall performance. Extensive\nexperiments on four diverse datasets show that M-SAM outperforms the latest\nstate-of-the-art optimization and gradient manipulation methods and\nsignificantly balances and improves multimodal learning.\n","authors":["Hossein R. Nowdeh","Jie Ji","Xiaolong Ma","Fatemeh Afghah"],"pdf_url":"https://arxiv.org/pdf/2510.24919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24907v1","updated":"2025-10-28T19:19:35Z","published":"2025-10-28T19:19:35Z","title":"Understanding Multi-View Transformers","summary":"  Multi-view transformers such as DUSt3R are revolutionizing 3D vision by\nsolving 3D tasks in a feed-forward manner. However, contrary to previous\noptimization-based pipelines, the inner mechanisms of multi-view transformers\nare unclear. Their black-box nature makes further improvements beyond data\nscaling challenging and complicates usage in safety- and reliability-critical\napplications. Here, we present an approach for probing and visualizing 3D\nrepresentations from the residual connections of the multi-view transformers'\nlayers. In this manner, we investigate a variant of the DUSt3R model, shedding\nlight on the development of its latent state across blocks, the role of the\nindividual layers, and suggest how it differs from methods with stronger\ninductive biases of explicit global pose. Finally, we show that the\ninvestigated variant of DUSt3R estimates correspondences that are refined with\nreconstructed geometry. The code used for the analysis is available at\nhttps://github.com/JulienGaubil/und3rstand .\n","authors":["Michal Stary","Julien Gaubil","Ayush Tewari","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2510.24907v1.pdf","comment":"Presented at the ICCV 2025 E2E3D Workshop"},{"id":"http://arxiv.org/abs/2506.03089v2","updated":"2025-10-28T19:19:25Z","published":"2025-06-03T17:13:51Z","title":"Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End\n  Improves CNN Robustness","summary":"  Convolutional neural networks (CNNs) trained on object recognition achieve\nhigh task performance but continue to exhibit vulnerability under a range of\nvisual perturbations and out-of-domain images, when compared with biological\nvision. Prior work has demonstrated that coupling a standard CNN with a\nfront-end (VOneBlock) that mimics the primate primary visual cortex (V1) can\nimprove overall model robustness. Expanding on this, we introduce Early Vision\nNetworks (EVNets), a new class of hybrid CNNs that combine the VOneBlock with a\nnovel SubcorticalBlock, whose architecture draws from computational models in\nneuroscience and is parameterized to maximize alignment with subcortical\nresponses reported across multiple experimental studies. Without being\noptimized to do so, the assembly of the SubcorticalBlock with the VOneBlock\nimproved V1 alignment across most standard V1 benchmarks, and better modeled\nextra-classical receptive field phenomena. In addition, EVNets exhibit stronger\nemergent shape bias and outperform the base CNN architecture by 9.3% on an\naggregate benchmark of robustness evaluations, including adversarial\nperturbations, common corruptions, and domain shifts. Finally, we show that\nEVNets can be further improved when paired with a state-of-the-art data\naugmentation technique, surpassing the performance of the isolated data\naugmentation approach by 6.2% on our robustness benchmark. This result reveals\ncomplementary benefits between changes in architecture to better mimic biology\nand training-based machine learning approaches.\n","authors":["Lucas Piper","Arlindo L. Oliveira","Tiago Marques"],"pdf_url":"https://arxiv.org/pdf/2506.03089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24904v1","updated":"2025-10-28T19:12:22Z","published":"2025-10-28T19:12:22Z","title":"VividCam: Learning Unconventional Camera Motions from Virtual Synthetic\n  Videos","summary":"  Although recent text-to-video generative models are getting more capable of\nfollowing external camera controls, imposed by either text descriptions or\ncamera trajectories, they still struggle to generalize to unconventional camera\nmotions, which is crucial in creating truly original and artistic videos. The\nchallenge lies in the difficulty of finding sufficient training videos with the\nintended uncommon camera motions. To address this challenge, we propose\nVividCam, a training paradigm that enables diffusion models to learn complex\ncamera motions from synthetic videos, releasing the reliance on collecting\nrealistic training videos. VividCam incorporates multiple disentanglement\nstrategies that isolates camera motion learning from synthetic appearance\nartifacts, ensuring more robust motion representation and mitigating domain\nshift. We demonstrate that our design synthesizes a wide range of precisely\ncontrolled and complex camera motions using surprisingly simple synthetic data.\nNotably, this synthetic data often consists of basic geometries within a\nlow-poly 3D scene and can be efficiently rendered by engines like Unity. Our\nvideo results can be found in https://wuqiuche.github.io/VividCamDemoPage/ .\n","authors":["Qiucheng Wu","Handong Zhao","Zhixin Shu","Jing Shi","Yang Zhang","Shiyu Chang"],"pdf_url":"https://arxiv.org/pdf/2510.24904v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.24902v1","updated":"2025-10-28T19:04:53Z","published":"2025-10-28T19:04:53Z","title":"Pixels to Signals: A Real-Time Framework for Traffic Demand Estimation","summary":"  Traffic congestion is becoming a challenge in the rapidly growing urban\ncities, resulting in increasing delays and inefficiencies within urban\ntransportation systems. To address this issue a comprehensive methodology is\ndesigned to optimize traffic flow and minimize delays. The framework is\nstructured with three primary components: (a) vehicle detection, (b) traffic\nprediction, and (c) traffic signal optimization. This paper presents the first\ncomponent, vehicle detection. The methodology involves analyzing multiple\nsequential frames from a camera feed to compute the background, i.e. the\nunderlying roadway, by averaging pixel values over time. The computed\nbackground is then utilized to extract the foreground, where the Density-Based\nSpatial Clustering of Applications with Noise (DBSCAN) algorithm is applied to\ndetect vehicles. With its computational efficiency and minimal infrastructure\nmodification requirements, the proposed methodology offers a practical and\nscalable solution for real-world deployment.\n","authors":["H Mhatre","M Vyas","A Mittal"],"pdf_url":"https://arxiv.org/pdf/2510.24902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.21609v3","updated":"2025-10-28T18:57:29Z","published":"2025-09-25T21:21:00Z","title":"VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster\n  Assessment","summary":"  Immediate damage assessment is essential after natural catastrophes; yet,\nconventional hand evaluation techniques are sluggish and perilous. Although\nsatellite and unmanned aerial vehicle (UAV) photos offer extensive perspectives\nof impacted regions, current computer vision methodologies generally yield just\nclassification labels or segmentation masks, so constraining their capacity to\ndeliver a thorough situational comprehension. We introduce the Vision Language\nCaption Enhancer (VLCE), a multimodal system designed to produce comprehensive,\ncontextually-informed explanations of disaster imagery. VLCE employs a\ndual-architecture approach: a CNN-LSTM model with a ResNet50 backbone\npretrained on EuroSat satellite imagery for the xBD dataset, and a Vision\nTransformer (ViT) model pretrained on UAV pictures for the RescueNet dataset.\nBoth systems utilize external semantic knowledge from ConceptNet and WordNet to\nexpand vocabulary coverage and improve description accuracy. We assess VLCE in\ncomparison to leading vision-language models (LLaVA and QwenVL) utilizing\nCLIPScore for semantic alignment and InfoMetIC for caption informativeness.\nExperimental findings indicate that VLCE markedly surpasses baseline models,\nattaining a maximum of 95.33% on InfoMetIC while preserving competitive\nsemantic alignment. Our dual-architecture system demonstrates significant\npotential for improving disaster damage assessment by automating the production\nof actionable, information-dense descriptions from satellite and drone photos.\n","authors":["Md. Mahfuzur Rahman","Kishor Datta Gupta","Marufa Kamal","Fahad Rahman","Sunzida Siddique","Ahmed Rafi Hasan","Mohd Ariful Haque","Roy George"],"pdf_url":"https://arxiv.org/pdf/2509.21609v3.pdf","comment":"29 pages, 40 figures, 3 algorithms"},{"id":"http://arxiv.org/abs/2510.24887v1","updated":"2025-10-28T18:43:17Z","published":"2025-10-28T18:43:17Z","title":"Proper Body Landmark Subset Enables More Accurate and 5X Faster\n  Recognition of Isolated Signs in LIBRAS","summary":"  This paper investigates the feasibility of using lightweight body landmark\ndetection for the recognition of isolated signs in Brazilian Sign Language\n(LIBRAS). Although the skeleton-based approach by Alves et al. (2024) enabled\nsubstantial improvements in recognition performance, the use of OpenPose for\nlandmark extraction hindered time performance. In a preliminary investigation,\nwe observed that simply replacing OpenPose with the lightweight MediaPipe,\nwhile improving processing speed, significantly reduced accuracy. To overcome\nthis limitation, we explored landmark subset selection strategies aimed at\noptimizing recognition performance. Experimental results showed that a proper\nlandmark subset achieves comparable or superior performance to state-of-the-art\nmethods while reducing processing time by more than 5X compared to Alves et al.\n(2024). As an additional contribution, we demonstrated that spline-based\nimputation effectively mitigates missing landmark issues, leading to\nsubstantial accuracy gains. These findings highlight that careful landmark\nselection, combined with simple imputation techniques, enables efficient and\naccurate isolated sign recognition, paving the way for scalable Sign Language\nRecognition systems.\n","authors":["Daniele L. V. dos Santos","Thiago B. Pereira","Carlos Eduardo G. R. Alves","Richard J. M. G. Tello","Francisco de A. Boldt","Thiago M. Paixão"],"pdf_url":"https://arxiv.org/pdf/2510.24887v1.pdf","comment":"Submitted to Int. Conf. on Computer Vision Theory and Applications\n  (VISAPP 2026)"},{"id":"http://arxiv.org/abs/2510.24885v1","updated":"2025-10-28T18:39:03Z","published":"2025-10-28T18:39:03Z","title":"FruitProm: Probabilistic Maturity Estimation and Detection of Fruits and\n  Vegetables","summary":"  Maturity estimation of fruits and vegetables is a critical task for\nagricultural automation, directly impacting yield prediction and robotic\nharvesting. Current deep learning approaches predominantly treat maturity as a\ndiscrete classification problem (e.g., unripe, ripe, overripe). This rigid\nformulation, however, fundamentally conflicts with the continuous nature of the\nbiological ripening process, leading to information loss and ambiguous class\nboundaries. In this paper, we challenge this paradigm by reframing maturity\nestimation as a continuous, probabilistic learning task. We propose a novel\narchitectural modification to the state-of-the-art, real-time object detector,\nRT-DETRv2, by introducing a dedicated probabilistic head. This head enables the\nmodel to predict a continuous distribution over the maturity spectrum for each\ndetected object, simultaneously learning the mean maturity state and its\nassociated uncertainty. This uncertainty measure is crucial for downstream\ndecision-making in robotics, providing a confidence score for tasks like\nselective harvesting. Our model not only provides a far richer and more\nbiologically plausible representation of plant maturity but also maintains\nexceptional detection performance, achieving a mean Average Precision (mAP) of\n85.6\\% on a challenging, large-scale fruit dataset. We demonstrate through\nextensive experiments that our probabilistic approach offers more granular and\naccurate maturity assessments than its classification-based counterparts,\npaving the way for more intelligent, uncertainty-aware automated systems in\nmodern agriculture\n","authors":["Sidharth Rai","Rahul Harsha Cheppally","Benjamin Vail","Keziban Yalçın Dokumacı","Ajay Sharda"],"pdf_url":"https://arxiv.org/pdf/2510.24885v1.pdf","comment":"Sidharth Rai, Rahul Harsha Cheppally contributed equally to this work"},{"id":"http://arxiv.org/abs/2510.24870v1","updated":"2025-10-28T18:21:19Z","published":"2025-10-28T18:21:19Z","title":"Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented\n  Generation","summary":"  We introduce MiRAGE, an evaluation framework for retrieval-augmented\ngeneration (RAG) from multimodal sources. As audiovisual media becomes a\nprevalent source of information online, it is essential for RAG systems to\nintegrate information from these sources into generation. However, existing\nevaluations for RAG are text-centric, limiting their applicability to\nmultimodal, reasoning intensive settings because they don't verify information\nagainst sources. MiRAGE is a claim-centric approach to multimodal RAG\nevaluation, consisting of InfoF1, evaluating factuality and information\ncoverage, and CiteF1, measuring citation support and completeness. We show that\nMiRAGE, when applied by humans, strongly aligns with extrinsic quality\njudgments. We additionally introduce automatic variants of MiRAGE and three\nprominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the\nlimitations of text-centric work and laying the groundwork for automatic\nevaluation. We release open-source implementations and outline how to assess\nmultimodal RAG.\n","authors":["Alexander Martin","William Walden","Reno Kriz","Dengjia Zhang","Kate Sanders","Eugene Yang","Chihsheng Jin","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2510.24870v1.pdf","comment":"https://github.com/alexmartin1722/mirage"},{"id":"http://arxiv.org/abs/2510.05034v5","updated":"2025-10-28T18:02:26Z","published":"2025-10-06T17:10:44Z","title":"Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models","summary":"  Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training\n","authors":["Yolo Yunlong Tang","Jing Bi","Pinxin Liu","Zhenyu Pan","Zhangyun Tan","Qianxiang Shen","Jiani Liu","Hang Hua","Junjia Guo","Yunzhong Xiao","Chao Huang","Zhiyuan Wang","Susan Liang","Xinyi Liu","Yizhi Song","Junhua Huang","Jia-Xing Zhong","Bozheng Li","Daiqing Qi","Ziyun Zeng","Ali Vosoughi","Luchuan Song","Zeliang Zhang","Daiki Shimada","Han Liu","Jiebo Luo","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.05034v5.pdf","comment":"Version v1.1"},{"id":"http://arxiv.org/abs/2510.24830v1","updated":"2025-10-28T16:42:53Z","published":"2025-10-28T16:42:53Z","title":"The Generation Phases of Flow Matching: a Denoising Perspective","summary":"  Flow matching has achieved remarkable success, yet the factors influencing\nthe quality of its generation process remain poorly understood. In this work,\nwe adopt a denoising perspective and design a framework to empirically probe\nthe generation process. Laying down the formal connections between flow\nmatching models and denoisers, we provide a common ground to compare their\nperformances on generation and denoising. This enables the design of principled\nand controlled perturbations to influence sample generation: noise and drift.\nThis leads to new insights on the distinct dynamical phases of the generative\nprocess, enabling us to precisely characterize at which stage of the generative\nprocess denoisers succeed or fail and why this matters.\n","authors":["Anne Gagneux","Ségolène Martin","Rémi Gribonval","Mathurin Massias"],"pdf_url":"https://arxiv.org/pdf/2510.24830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24827v1","updated":"2025-10-28T16:04:03Z","published":"2025-10-28T16:04:03Z","title":"MCIHN: A Hybrid Network Model Based on Multi-path Cross-modal\n  Interaction for Multimodal Emotion Recognition","summary":"  Multimodal emotion recognition is crucial for future human-computer\ninteraction. However, accurate emotion recognition still faces significant\nchallenges due to differences between different modalities and the difficulty\nof characterizing unimodal emotional information. To solve these problems, a\nhybrid network model based on multipath cross-modal interaction (MCIHN) is\nproposed. First, adversarial autoencoders (AAE) are constructed separately for\neach modality. The AAE learns discriminative emotion features and reconstructs\nthe features through a decoder to obtain more discriminative information about\nthe emotion classes. Then, the latent codes from the AAE of different\nmodalities are fed into a predefined Cross-modal Gate Mechanism model (CGMM) to\nreduce the discrepancy between modalities, establish the emotional relationship\nbetween interacting modalities, and generate the interaction features between\ndifferent modalities. Multimodal fusion using the Feature Fusion module (FFM)\nfor better emotion recognition. Experiments were conducted on publicly\navailable SIMS and MOSI datasets, demonstrating that MCIHN achieves superior\nperformance.\n","authors":["Haoyang Zhang","Zhou Yang","Ke Sun","Yucai Pang","Guoliang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.24827v1.pdf","comment":"The paper will be published in the MMAsia2025 conference proceedings"},{"id":"http://arxiv.org/abs/2510.24821v1","updated":"2025-10-28T15:24:13Z","published":"2025-10-28T15:24:13Z","title":"Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal\n  Perception and Generation","summary":"  We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a\nsparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion\ntotal parameters, of which only 6.1 billion are active per token. This\narchitecture enables highly efficient scaling (dramatically improving\ncomputational efficiency while significantly expanding model capacity) and\nempowers stronger unified multimodal intelligence across vision, speech, and\nlanguage, representing a key step toward Artificial General Intelligence (AGI).\nCompared to its predecessor, the upgraded version exhibits substantial\nimprovements across multimodal understanding and generation. We significantly\nadvance speech recognition capabilities, achieving state-of-the-art performance\nin contextual ASR and highly competitive results in dialect-aware ASR. In image\ngeneration, Ming-Flash-Omni introduces high-fidelity text rendering and\ndemonstrates marked gains in scene consistency and identity preservation during\nimage editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,\na capability that not only achieves strong standalone segmentation performance\nbut also enhances spatial control in image generation and improves editing\nconsistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in\ntext-to-image generation and generative segmentation, and sets new records on\nall 12 contextual ASR benchmarks, all within a single unified architecture.\n","authors":["Inclusion AI"," :","Bowen Ma","Cheng Zou","Canxiang Yan","Chunxiang Jin","Chunjie Shen","Dandan Zheng","Fudong Wang","Furong Xu","GuangMing Yao","Jun Zhou","Jingdong Chen","Jianing Li","Jianxin Sun","Jiajia Liu","Jianjiang Zhu","Jianping Jiang","Jun Peng","Kaixiang Ji","Kaimeng Ren","Libin Wang","Lixiang Ru","Longhua Tan","Lan Wang","Mochen Bai","Ning Gao","Qingpei Guo","Qinglong Zhang","Qiang Xu","Rui Liu","Ruijie Xiong","Ruobing Zheng","Sirui Gao","Tianqi Li","Tinghao Liu","Weilong Chai","Xinyu Xiao","Xiaomei Wang","Xiaolong Wang","Xiao Lu","Xiaoyu Li","Xingning Dong","Xuzheng Yu","Yi Yuan","Yuting Gao","Yuting Xiao","Yunxiao Sun","Yipeng Chen","Yifan Mao","Yifei Wu","Yongjie Lyu","Ziping Ma","Zhiqiang Fang","Zhihao Qiu","Ziyuan Huang","Zizheng Yang","Zhengyu He"],"pdf_url":"https://arxiv.org/pdf/2510.24821v1.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.24820v1","updated":"2025-10-28T15:12:15Z","published":"2025-10-28T15:12:15Z","title":"SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing","summary":"  With the rapid advancement of text-to-image (T2I) models, ensuring their\nsafety has become increasingly critical. Existing safety approaches can be\ncategorized into training-time and inference-time methods. While inference-time\nmethods are widely adopted due to their cost-effectiveness, they often suffer\nfrom limitations such as over-refusal and imbalance between safety and utility.\nTo address these challenges, we propose a multi-round safety editing framework\nthat functions as a model-agnostic, plug-and-play module, enabling efficient\nsafety alignment for any text-to-image model. Central to this framework is\nMR-SafeEdit, a multi-round image-text interleaved dataset specifically\nconstructed for safety editing in text-to-image generation. We introduce a\npost-hoc safety editing paradigm that mirrors the human cognitive process of\nidentifying and refining unsafe content. To instantiate this paradigm, we\ndevelop SafeEditor, a unified MLLM capable of multi-round safety editing on\ngenerated images. Experimental results show that SafeEditor surpasses prior\nsafety approaches by reducing over-refusal while achieving a more favorable\nsafety-utility balance.\n","authors":["Ruiyang Zhang","Jiahao Luo","Xiaoru Feng","Qiufan Pang","Yaodong Yang","Juntao Dai"],"pdf_url":"https://arxiv.org/pdf/2510.24820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24816v1","updated":"2025-10-28T10:04:13Z","published":"2025-10-28T10:04:13Z","title":"Perception, Understanding and Reasoning, A Multimodal Benchmark for\n  Video Fake News Detection","summary":"  The advent of multi-modal large language models (MLLMs) has greatly advanced\nresearch into applications for Video fake news detection (VFND) tasks.\nTraditional video-based FND benchmarks typically focus on the accuracy of the\nfinal decision, often failing to provide fine-grained assessments for the\nentire detection process, making the detection process a black box. Therefore,\nwe introduce the MVFNDB (Multi-modal Video Fake News Detection Benchmark) based\non the empirical analysis, which provides foundation for tasks definition. The\nbenchmark comprises 10 tasks and is meticulously crafted to probe MLLMs'\nperception, understanding, and reasoning capacities during detection, featuring\n9730 human-annotated video-related questions based on a carefully constructed\ntaxonomy ability of VFND. To validate the impact of combining multiple features\non the final results, we design a novel framework named MVFND-CoT, which\nincorporates both creator-added content and original shooting footage\nreasoning. Building upon the benchmark, we conduct an in-depth analysis of the\ndeeper factors influencing accuracy, including video processing strategies and\nthe alignment between video features and model capabilities. We believe this\nbenchmark will lay a solid foundation for future evaluations and advancements\nof MLLMs in the domain of video fake news detection.\n","authors":["Cui Yakun","Fushuo Huo","Weijie Shi","Juntao Dai","Hang Du","Zhenghao Zhu","Sirui Han","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2510.24816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24814v1","updated":"2025-10-28T09:02:10Z","published":"2025-10-28T09:02:10Z","title":"Deep Feature Optimization for Enhanced Fish Freshness Assessment","summary":"  Assessing fish freshness is vital for ensuring food safety and minimizing\neconomic losses in the seafood industry. However, traditional sensory\nevaluation remains subjective, time-consuming, and inconsistent. Although\nrecent advances in deep learning have automated visual freshness prediction,\nchallenges related to accuracy and feature transparency persist. This study\nintroduces a unified three-stage framework that refines and leverages deep\nvisual representations for reliable fish freshness assessment. First, five\nstate-of-the-art vision architectures - ResNet-50, DenseNet-121,\nEfficientNet-B0, ConvNeXt-Base, and Swin-Tiny - are fine-tuned to establish a\nstrong baseline. Next, multi-level deep features extracted from these backbones\nare used to train seven classical machine learning classifiers, integrating\ndeep and traditional decision mechanisms. Finally, feature selection methods\nbased on Light Gradient Boosting Machine (LGBM), Random Forest, and Lasso\nidentify a compact and informative subset of features. Experiments on the\nFreshness of the Fish Eyes (FFE) dataset demonstrate that the best\nconfiguration combining Swin-Tiny features, an Extra Trees classifier, and\nLGBM-based feature selection achieves an accuracy of 85.99%, outperforming\nrecent studies on the same dataset by 8.69-22.78%. These findings confirm the\neffectiveness and generalizability of the proposed framework for visual quality\nevaluation tasks.\n","authors":["Phi-Hung Hoang","Nam-Thuan Trinh","Van-Manh Tran","Thi-Thu-Hong Phan"],"pdf_url":"https://arxiv.org/pdf/2510.24814v1.pdf","comment":"39 pages; 10 tables; 9 figures"},{"id":"http://arxiv.org/abs/2510.24813v1","updated":"2025-10-28T08:29:02Z","published":"2025-10-28T08:29:02Z","title":"DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with\n  Similar Scenes Visual Prompts","summary":"  Recent lightweight retrieval-augmented image caption models often utilize\nretrieved data solely as text prompts, thereby creating a semantic gap by\nleaving the original visual features unenhanced, particularly for object\ndetails or complex scenes. To address this limitation, we propose $DualCap$, a\nnovel approach that enriches the visual representation by generating a visual\nprompt from retrieved similar images. Our model employs a dual retrieval\nmechanism, using standard image-to-text retrieval for text prompts and a novel\nimage-to-image retrieval to source visually analogous scenes. Specifically,\nsalient keywords and phrases are derived from the captions of visually similar\nscenes to capture key objects and similar details. These textual features are\nthen encoded and integrated with the original image features through a\nlightweight, trainable feature fusion network. Extensive experiments\ndemonstrate that our method achieves competitive performance while requiring\nfewer trainable parameters compared to previous visual-prompting captioning\napproaches.\n","authors":["Binbin Li","Guimiao Yang","Zisen Qi","Haiping Wang","Yu Ding"],"pdf_url":"https://arxiv.org/pdf/2510.24813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24805v1","updated":"2025-10-28T01:18:35Z","published":"2025-10-28T01:18:35Z","title":"CT-Less Attenuation Correction Using Multiview Ensemble Conditional\n  Diffusion Model on High-Resolution Uncorrected PET Images","summary":"  Accurate quantification in positron emission tomography (PET) is essential\nfor accurate diagnostic results and effective treatment tracking. A major issue\nencountered in PET imaging is attenuation. Attenuation refers to the diminution\nof photon detected as they traverse biological tissues before reaching\ndetectors. When such corrections are absent or inadequate, this signal\ndegradation can introduce inaccurate quantification, making it difficult to\ndifferentiate benign from malignant conditions, and can potentially lead to\nmisdiagnosis. Typically, this correction is done with co-computed Computed\nTomography (CT) imaging to obtain structural data for calculating photon\nattenuation across the body. However, this methodology subjects patients to\nextra ionizing radiation exposure, suffers from potential spatial\nmisregistration between PET/CT imaging sequences, and demands costly equipment\ninfrastructure. Emerging advances in neural network architectures present an\nalternative approach via synthetic CT image synthesis. Our investigation\nreveals that Conditional Denoising Diffusion Probabilistic Models (DDPMs) can\ngenerate high quality CT images from non attenuation corrected PET images in\norder to correct attenuation. By utilizing all three orthogonal views from\nnon-attenuation-corrected PET images, the DDPM approach combined with ensemble\nvoting generates higher quality pseudo-CT images with reduced artifacts and\nimproved slice-to-slice consistency. Results from a study of 159 head scans\nacquired with the Siemens Biograph Vision PET/CT scanner demonstrate both\nqualitative and quantitative improvements in pseudo-CT generation. The method\nachieved a mean absolute error of 32 $\\pm$ 10.4 HU on the CT images and an\naverage error of (1.48 $\\pm$ 0.68)\\% across all regions of interest when\ncomparing PET images reconstructed using the attenuation map of the generated\npseudo-CT versus the true CT.\n","authors":["Alexandre St-Georges","Gabriel Richard","Maxime Toussaint","Christian Thibaudeau","Etienne Auger","Étienne Croteau","Stephen Cunnane","Roger Lecomte","Jean-Baptiste Michaud"],"pdf_url":"https://arxiv.org/pdf/2510.24805v1.pdf","comment":"This is a preprint and not the final version of this paper"},{"id":"http://arxiv.org/abs/2510.24804v1","updated":"2025-10-28T01:05:32Z","published":"2025-10-28T01:05:32Z","title":"Conflict Adaptation in Vision-Language Models","summary":"  A signature of human cognitive control is conflict adaptation: improved\nperformance on a high-conflict trial following another high-conflict trial.\nThis phenomenon offers an account for how cognitive control, a scarce resource,\nis recruited. Using a sequential Stroop task, we find that 12 of 13\nvision-language models (VLMs) tested exhibit behavior consistent with conflict\nadaptation, with the lone exception likely reflecting a ceiling effect. To\nunderstand the representational basis of this behavior, we use sparse\nautoencoders (SAEs) to identify task-relevant supernodes in InternVL 3.5 4B.\nPartially overlapping supernodes emerge for text and color in both early and\nlate layers, and their relative sizes mirror the automaticity asymmetry between\nreading and color naming in humans. We further isolate a conflict-modulated\nsupernode in layers 24-25 whose ablation significantly increases Stroop errors\nwhile minimally affecting congruent trials.\n","authors":["Xiaoyang Hu"],"pdf_url":"https://arxiv.org/pdf/2510.24804v1.pdf","comment":"Workshop on Interpreting Cognition in Deep Learning Models at NeurIPS\n  2025"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2510.24718v1","updated":"2025-10-28T17:59:58Z","published":"2025-10-28T17:59:58Z","title":"Generative View Stitching","summary":"  Autoregressive video diffusion models are capable of long rollouts that are\nstable and consistent with history, but they are unable to guide the current\ngeneration with conditioning from the future. In camera-guided video generation\nwith a predefined camera trajectory, this limitation leads to collisions with\nthe generated scene, after which autoregression quickly collapses. To address\nthis, we propose Generative View Stitching (GVS), which samples the entire\nsequence in parallel such that the generated scene is faithful to every part of\nthe predefined camera trajectory. Our main contribution is a sampling algorithm\nthat extends prior work on diffusion stitching for robot planning to video\ngeneration. While such stitching methods usually require a specially trained\nmodel, GVS is compatible with any off-the-shelf video model trained with\nDiffusion Forcing, a prevalent sequence diffusion framework that we show\nalready provides the affordances necessary for stitching. We then introduce\nOmni Guidance, a technique that enhances the temporal consistency in stitching\nby conditioning on both the past and future, and that enables our proposed\nloop-closing mechanism for delivering long-range coherence. Overall, GVS\nachieves camera-guided video generation that is stable, collision-free,\nframe-to-frame consistent, and closes loops for a variety of predefined camera\npaths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best\nviewed as videos at https://andrewsonga.github.io/gvs.\n","authors":["Chonghyuk Song","Michal Stary","Boyuan Chen","George Kopanas","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2510.24718v1.pdf","comment":"Project website: https://andrewsonga.github.io/gvs"},{"id":"http://arxiv.org/abs/2501.08428v3","updated":"2025-10-28T17:58:31Z","published":"2025-01-14T20:38:30Z","title":"Physics-Informed Latent Neural Operator for Real-time Predictions of\n  time-dependent parametric PDEs","summary":"  Deep operator network (DeepONet) has shown significant promise as surrogate\nmodels for systems governed by partial differential equations (PDEs), enabling\naccurate mappings between infinite-dimensional function spaces. However, when\napplied to systems with high-dimensional input-output mappings arising from\nlarge numbers of spatial and temporal collocation points, these models often\nrequire heavily overparameterized networks, leading to long training times.\nLatent DeepONet addresses some of these challenges by introducing a two-step\napproach: first learning a reduced latent space using a separate model,\nfollowed by operator learning within this latent space. While efficient, this\nmethod is inherently data-driven and lacks mechanisms for incorporating\nphysical laws, limiting its robustness and generalizability in data-scarce\nsettings. In this work, we propose PI-Latent-NO, a physics-informed latent\nneural operator framework that integrates governing physics directly into the\nlearning process. Our architecture features two coupled DeepONets trained\nend-to-end: a Latent-DeepONet that learns a low-dimensional representation of\nthe solution, and a Reconstruction-DeepONet that maps this latent\nrepresentation back to the physical space. By embedding PDE constraints into\nthe training via automatic differentiation, our method eliminates the need for\nlabeled training data and ensures physics-consistent predictions. The proposed\nframework is both memory and compute-efficient, exhibiting near-constant\nscaling with problem size and demonstrating significant speedups over\ntraditional physics-informed operator models. We validate our approach on a\nrange of parametric PDEs, showcasing its accuracy, scalability, and suitability\nfor real-time prediction in complex physical systems.\n","authors":["Sharmila Karumuri","Lori Graham-Brady","Somdatta Goswami"],"pdf_url":"https://arxiv.org/pdf/2501.08428v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24710v1","updated":"2025-10-28T17:58:17Z","published":"2025-10-28T17:58:17Z","title":"A Single-Loop First-Order Algorithm for Linearly Constrained Bilevel\n  Optimization","summary":"  We study bilevel optimization problems where the lower-level problems are\nstrongly convex and have coupled linear constraints. To overcome the potential\nnon-smoothness of the hyper-objective and the computational challenges\nassociated with the Hessian matrix, we utilize penalty and augmented Lagrangian\nmethods to reformulate the original problem as a single-level one. Especially,\nwe establish a strong theoretical connection between the reformulated function\nand the original hyper-objective by characterizing the closeness of their\nvalues and derivatives. Based on this reformulation, we propose a single-loop,\nfirst-order algorithm for linearly constrained bilevel optimization (SFLCB). We\nprovide rigorous analyses of its non-asymptotic convergence rates, showing an\nimprovement over prior double-loop algorithms -- form\n$O(\\epsilon^{-3}\\log(\\epsilon^{-1}))$ to $O(\\epsilon^{-3})$. The experiments\ncorroborate our theoretical findings and demonstrate the practical efficiency\nof the proposed SFLCB algorithm. Simulation code is provided at\nhttps://github.com/ShenGroup/SFLCB.\n","authors":["Wei Shen","Jiawei Zhang","Minhui Huang","Cong Shen"],"pdf_url":"https://arxiv.org/pdf/2510.24710v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24709v1","updated":"2025-10-28T17:57:05Z","published":"2025-10-28T17:57:05Z","title":"Does Object Binding Naturally Emerge in Large Pretrained Vision\n  Transformers?","summary":"  Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.\n","authors":["Yihao Li","Saeed Salehi","Lyle Ungar","Konrad P. Kording"],"pdf_url":"https://arxiv.org/pdf/2510.24709v1.pdf","comment":"Accepted as a Spotlight at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2406.09795v2","updated":"2025-10-28T17:56:59Z","published":"2024-06-14T07:45:07Z","title":"DeltaPhi: Physical States Residual Learning for Neural Operators in\n  Data-Limited PDE Solving","summary":"  The limited availability of high-quality training data poses a major obstacle\nin data-driven PDE solving, where expensive data collection and resolution\nconstraints severely impact the ability of neural operator networks to learn\nand generalize the underlying physical system. To address this challenge, we\npropose DeltaPhi, a novel learning framework that transforms the PDE solving\ntask from learning direct input-output mappings to learning the residuals\nbetween similar physical states, a fundamentally different approach to neural\noperator learning. This reformulation provides implicit data augmentation by\nexploiting the inherent stability of physical systems where closer initial\nstates lead to closer evolution trajectories. DeltaPhi is architecture-agnostic\nand can be seamlessly integrated with existing neural operators to enhance\ntheir performance. Extensive experiments demonstrate consistent and significant\nimprovements across diverse physical systems including regular and irregular\ndomains, different neural architectures, multiple training data amount, and\ncross-resolution scenarios, confirming its effectiveness as a general\nenhancement for neural operators in data-limited PDE solving.\n","authors":["Xihang Yue","Yi Yang","Linchao Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.09795v2.pdf","comment":"Neurips 2025"},{"id":"http://arxiv.org/abs/2306.08848v4","updated":"2025-10-28T17:53:16Z","published":"2023-06-15T04:24:13Z","title":"Datasheets for Machine Learning Sensors","summary":"  Machine learning (ML) is becoming prevalent in embedded AI sensing systems.\nThese \"ML sensors\" enable context-sensitive, real-time data collection and\ndecision-making across diverse applications ranging from anomaly detection in\nindustrial settings to wildlife tracking for conservation efforts. As such,\nthere is a need to provide transparency in the operation of such ML-enabled\nsensing systems through comprehensive documentation. This is needed to enable\ntheir reproducibility, to address new compliance and auditing regimes mandated\nin regulation and industry-specific policy, and to verify and validate the\nresponsible nature of their operation. To address this gap, we introduce the\ndatasheet for ML sensors framework. We provide a comprehensive template,\ncollaboratively developed in academia-industry partnerships, that captures the\ndistinct attributes of ML sensors, including hardware specifications, ML model\nand dataset characteristics, end-to-end performance metrics, and environmental\nimpacts. Our framework addresses the continuous streaming nature of sensor\ndata, real-time processing requirements, and embeds benchmarking methodologies\nthat reflect real-world deployment conditions, ensuring practical viability.\nAligned with the FAIR principles (Findability, Accessibility, Interoperability,\nand Reusability), our approach enhances the transparency and reusability of ML\nsensor documentation across academic, industrial, and regulatory domains. To\nshow the application of our approach, we present two datasheets: the first for\nan open-source ML sensor designed in-house and the second for a commercial ML\nsensor developed by industry collaborators, both performing computer\nvision-based person detection.\n","authors":["Matthew Stewart","Yuke Zhang","Pete Warden","Yasmine Omri","Shvetank Prakash","Jacob Huckelberry","Joao Henrique Santos","Shawn Hymel","Benjamin Yeager Brown","Jim MacArthur","Nat Jeffries","Emanuel Moss","Mona Sloane","Brian Plancher","Vijay Janapa Reddi"],"pdf_url":"https://arxiv.org/pdf/2306.08848v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24701v1","updated":"2025-10-28T17:53:02Z","published":"2025-10-28T17:53:02Z","title":"Tongyi DeepResearch Technical Report","summary":"  We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.\n","authors":[" Tongyi DeepResearch Team","Baixuan Li","Bo Zhang","Dingchu Zhang","Fei Huang","Guangyu Li","Guoxin Chen","Huifeng Yin","Jialong Wu","Jingren Zhou","Kuan Li","Liangcai Su","Litu Ou","Liwen Zhang","Pengjun Xie","Rui Ye","Wenbiao Yin","Xinmiao Yu","Xinyu Wang","Xixi Wu","Xuanzhong Chen","Yida Zhao","Zhen Zhang","Zhengwei Tao","Zhongwang Zhang","Zile Qiao","Chenxi Wang","Donglei Yu","Gang Fu","Haiyang Shen","Jiayin Yang","Jun Lin","Junkai Zhang","Kui Zeng","Li Yang","Hailong Yin","Maojia Song","Ming Yan","Peng Xia","Qian Xiao","Rui Min","Ruixue Ding","Runnan Fang","Shaowei Chen","Shen Huang","Shihang Wang","Shihao Cai","Weizhou Shen","Xiaobin Wang","Xin Guan","Xinyu Geng","Yingcheng Shi","Yuning Wu","Zhuo Chen","Zijian Li","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.24701v1.pdf","comment":"https://tongyi-agent.github.io/blog"},{"id":"http://arxiv.org/abs/2510.24700v1","updated":"2025-10-28T17:52:08Z","published":"2025-10-28T17:52:08Z","title":"Greedy Sampling Is Provably Efficient for RLHF","summary":"  Reinforcement Learning from Human Feedback (RLHF) has emerged as a key\ntechnique for post-training large language models. Despite its empirical\nsuccess, the theoretical understanding of RLHF is still limited, as learning\nthe KL-regularized target with only preference feedback poses additional\nchallenges compared with canonical RL. Existing works mostly study the\nreward-based Bradley-Terry (BT) preference model, and extend classical designs\nutilizing optimism or pessimism. This work, instead, considers the general\npreference model (whose practical relevance has been observed recently) and\nobtains performance guarantees with major, order-wise improvements over\nexisting ones. Surprisingly, these results are derived from algorithms that\ndirectly use the empirical estimates (i.e., greedy sampling), as opposed to\nconstructing optimistic or pessimistic estimates in previous works. This\ninsight has a deep root in the unique structural property of the optimal policy\nclass under the KL-regularized target, and we further specialize it to the BT\nmodel, highlighting the surprising sufficiency of greedy sampling in RLHF.\n","authors":["Di Wu","Chengshuai Shi","Jing Yang","Cong Shen"],"pdf_url":"https://arxiv.org/pdf/2510.24700v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24699v1","updated":"2025-10-28T17:51:50Z","published":"2025-10-28T17:51:50Z","title":"AgentFold: Long-Horizon Web Agents with Proactive Context Management","summary":"  LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.\n","authors":["Rui Ye","Zhongwang Zhang","Kuan Li","Huifeng Yin","Zhengwei Tao","Yida Zhao","Liangcai Su","Liwen Zhang","Zile Qiao","Xinyu Wang","Pengjun Xie","Fei Huang","Siheng Chen","Jingren Zhou","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.24699v1.pdf","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.24674v1","updated":"2025-10-28T17:40:04Z","published":"2025-10-28T17:40:04Z","title":"Learning to Drive Safely with Hybrid Options","summary":"  Out of the many deep reinforcement learning approaches for autonomous\ndriving, only few make use of the options (or skills) framework. That is\nsurprising, as this framework is naturally suited for hierarchical control\napplications in general, and autonomous driving tasks in specific. Therefore,\nin this work the options framework is applied and tailored to autonomous\ndriving tasks on highways. More specifically, we define dedicated options for\nlongitudinal and lateral manoeuvres with embedded safety and comfort\nconstraints. This way, prior domain knowledge can be incorporated into the\nlearning process and the learned driving behaviour can be constrained more\neasily. We propose several setups for hierarchical control with options and\nderive practical algorithms following state-of-the-art reinforcement learning\ntechniques. By separately selecting actions for longitudinal and lateral\ncontrol, the introduced policies over combined and hybrid options obtain the\nsame expressiveness and flexibility that human drivers have, while being easier\nto interpret than classical policies over continuous actions. Of all the\ninvestigated approaches, these flexible policies over hybrid options perform\nthe best under varying traffic conditions, outperforming the baseline policies\nover actions.\n","authors":["Bram De Cooman","Johan Suykens"],"pdf_url":"https://arxiv.org/pdf/2510.24674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24672v1","updated":"2025-10-28T17:37:12Z","published":"2025-10-28T17:37:12Z","title":"Eigenfunction Extraction for Ordered Representation Learning","summary":"  Recent advances in representation learning reveal that widely used\nobjectives, such as contrastive and non-contrastive, implicitly perform\nspectral decomposition of a contextual kernel, induced by the relationship\nbetween inputs and their contexts. Yet, these methods recover only the linear\nspan of top eigenfunctions of the kernel, whereas exact spectral decomposition\nis essential for understanding feature ordering and importance. In this work,\nwe propose a general framework to extract ordered and identifiable\neigenfunctions, based on modular building blocks designed to satisfy key\ndesiderata, including compatibility with the contextual kernel and scalability\nto modern settings. We then show how two main methodological paradigms,\nlow-rank approximation and Rayleigh quotient optimization, align with this\nframework for eigenfunction extraction. Finally, we validate our approach on\nsynthetic kernels and demonstrate on real-world image datasets that the\nrecovered eigenvalues act as effective importance scores for feature selection,\nenabling principled efficiency-accuracy tradeoffs via adaptive-dimensional\nrepresentations.\n","authors":["Burak Varıcı","Che-Ping Tsai","Ritabrata Ray","Nicholas M. Boffi","Pradeep Ravikumar"],"pdf_url":"https://arxiv.org/pdf/2510.24672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07862v2","updated":"2025-10-28T17:37:03Z","published":"2025-02-11T17:19:44Z","title":"ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise\n  and Compute Resources","summary":"  Multimodal deep learning systems are deployed in dynamic scenarios due to the\nrobustness afforded by multiple sensing modalities. Nevertheless, they struggle\nwith varying compute resource availability (due to multi-tenancy, device\nheterogeneity, etc.) and fluctuating quality of inputs (from sensor feed\ncorruption, environmental noise, etc.). Statically provisioned multimodal\nsystems cannot adapt when compute resources change over time, while existing\ndynamic networks struggle with strict compute budgets. Additionally, both\nsystems often neglect the impact of variations in modality quality.\nConsequently, modalities suffering substantial corruption may needlessly\nconsume resources better allocated towards other modalities. We propose ADMN, a\nlayer-wise Adaptive Depth Multimodal Network capable of tackling both\nchallenges: it adjusts the total number of active layers across all modalities\nto meet strict compute resource constraints and continually reallocates layers\nacross input modalities according to their modality quality. Our evaluations\nshowcase ADMN can match the accuracy of state-of-the-art networks while\nreducing up to 75% of their floating-point operations.\n","authors":["Jason Wu","Yuyang Yuan","Kang Yang","Lance Kaplan","Mani Srivastava"],"pdf_url":"https://arxiv.org/pdf/2502.07862v2.pdf","comment":"Accepted to Neurips 2025"},{"id":"http://arxiv.org/abs/2510.24670v1","updated":"2025-10-28T17:36:51Z","published":"2025-10-28T17:36:51Z","title":"Pearl: A Foundation Model for Placing Every Atom in the Right Location","summary":"  Accurately predicting the three-dimensional structures of protein-ligand\ncomplexes remains a fundamental challenge in computational drug discovery that\nlimits the pace and success of therapeutic design. Deep learning methods have\nrecently shown strong potential as structural prediction tools, achieving\npromising accuracy across diverse biomolecular systems. However, their\nperformance and utility are constrained by scarce experimental data,\ninefficient architectures, physically invalid poses, and the limited ability to\nexploit auxiliary information available at inference. To address these issues,\nwe introduce Pearl (Placing Every Atom in the Right Location), a foundation\nmodel for protein-ligand cofolding at scale. Pearl addresses these challenges\nwith three key innovations: (1) training recipes that include large-scale\nsynthetic data to overcome data scarcity; (2) architectures that incorporate an\nSO(3)-equivariant diffusion module to inherently respect 3D rotational\nsymmetries, improving generalization and sample efficiency, and (3)\ncontrollable inference, including a generalized multi-chain templating system\nsupporting both protein and non-polymeric components as well as dual\nunconditional/conditional modes. Pearl establishes a new state-of-the-art\nperformance in protein-ligand cofolding. On the key metric of generating\naccurate (RMSD < 2 \\r{A}) and physically valid poses, Pearl surpasses AlphaFold\n3 and other open source baselines on the public Runs N' Poses and PoseBusters\nbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over the\nnext best model. In the pocket-conditional cofolding regime, Pearl delivers\n$3.6\\times$ improvement on a proprietary set of challenging, real-world drug\ntargets at the more rigorous RMSD < 1 \\r{A} threshold. Finally, we demonstrate\nthat model performance correlates directly with synthetic dataset size used in\ntraining.\n","authors":[" Genesis Research Team","Alejandro Dobles","Nina Jovic","Kenneth Leidal","Pranav Murugan","David C. Williams","Drausin Wulsin","Nate Gruver","Christina X. Ji","Korrawat Pruegsanusak","Gianluca Scarpellini","Ansh Sharma","Wojciech Swiderski","Andrea Bootsma","Richard Strong Bowen","Charlotte Chen","Jamin Chen","Marc André Dämgen","Roy Tal Dew","Benjamin DiFrancesco","J. D. Fishman","Alla Ivanova","Zach Kagin","David Li-Bland","Zuli Liu","Igor Morozov","Jeffrey Ouyang-Zhang","Frank C. Pickard IV","Kushal S. Shah","Ben Shor","Gabriel Monteiro da Silva","Maxx Tessmer","Carl Tilbury","Cyr Vetcher","Daniel Zeng","Maruan Al-Shedivat","Aleksandra Faust","Evan N. Feinberg","Michael V. LeVine","Matteus Pan"],"pdf_url":"https://arxiv.org/pdf/2510.24670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23455v2","updated":"2025-10-28T17:15:50Z","published":"2025-10-27T15:56:19Z","title":"SGFusion: Stochastic Geographic Gradient Fusion in Federated Learning","summary":"  This paper proposes Stochastic Geographic Gradient Fusion (SGFusion), a novel\ntraining algorithm to leverage the geographic information of mobile users in\nFederated Learning (FL). SGFusion maps the data collected by mobile devices\nonto geographical zones and trains one FL model per zone, which adapts well to\nthe data and behaviors of users in that zone. SGFusion models the local\ndata-based correlation among geographical zones as a hierarchical random graph\n(HRG) optimized by Markov Chain Monte Carlo sampling. At each training step,\nevery zone fuses its local gradient with gradients derived from a small set of\nother zones sampled from the HRG. This approach enables knowledge fusion and\nsharing among geographical zones in a probabilistic and stochastic gradient\nfusion process with self-attention weights, such that \"more similar\" zones have\n\"higher probabilities\" of sharing gradients with \"larger attention weights.\"\nSGFusion remarkably improves model utility without introducing undue\ncomputational cost. Extensive theoretical and empirical results using a\nheart-rate prediction dataset collected across 6 countries show that models\ntrained with SGFusion converge with upper-bounded expected errors and\nsignificantly improve utility in all countries compared to existing approaches\nwithout notable cost in system scalability.\n","authors":["Khoa Nguyen","Khang Tran","NhatHai Phan","Cristian Borcea","Rouming Jin","Issa Khalil"],"pdf_url":"https://arxiv.org/pdf/2510.23455v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24643v1","updated":"2025-10-28T17:09:43Z","published":"2025-10-28T17:09:43Z","title":"The Cost of Robustness: Tighter Bounds on Parameter Complexity for\n  Robust Memorization in ReLU Nets","summary":"  We study the parameter complexity of robust memorization for $\\mathrm{ReLU}$\nnetworks: the number of parameters required to interpolate any given dataset\nwith $\\epsilon$-separation between differently labeled points, while ensuring\npredictions remain consistent within a $\\mu$-ball around each training sample.\nWe establish upper and lower bounds on the parameter count as a function of the\nrobustness ratio $\\rho = \\mu / \\epsilon$. Unlike prior work, we provide a\nfine-grained analysis across the entire range $\\rho \\in (0,1)$ and obtain\ntighter upper and lower bounds that improve upon existing results. Our findings\nreveal that the parameter complexity of robust memorization matches that of\nnon-robust memorization when $\\rho$ is small, but grows with increasing $\\rho$.\n","authors":["Yujun Kim","Chaewon Moon","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2510.24643v1.pdf","comment":"Accepted to NeurIPS 2025, 72 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.24639v1","updated":"2025-10-28T17:06:15Z","published":"2025-10-28T17:06:15Z","title":"Causal Ordering for Structure Learning From Time Series","summary":"  Predicting causal structure from time series data is crucial for\nunderstanding complex phenomena in physiology, brain connectivity, climate\ndynamics, and socio-economic behaviour. Causal discovery in time series is\nhindered by the combinatorial complexity of identifying true causal\nrelationships, especially as the number of variables and time points grow. A\ncommon approach to simplify the task is the so-called ordering-based methods.\nTraditional ordering methods inherently limit the representational capacity of\nthe resulting model. In this work, we fix this issue by leveraging multiple\nvalid causal orderings, instead of a single one as standard practice. We\npropose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based\ncausal discovery for temporal data. By integrating multiple orderings, DOTS\neffectively recovers the transitive closure of the underlying directed acyclic\ngraph, mitigating spurious artifacts inherent in single-ordering approaches. We\nformalise the problem under standard assumptions such as stationarity and the\nadditive noise model, and leverage score matching with diffusion processes to\nenable efficient Hessian estimation. Extensive experiments validate the\napproach. Empirical evaluations on synthetic and real-world datasets\ndemonstrate that DOTS outperforms state-of-the-art baselines, offering a\nscalable and robust approach to temporal causal discovery. On synthetic\nbenchmarks ($d{=}\\!3-\\!6$ variables, $T{=}200\\!-\\!5{,}000$ samples), DOTS\nimproves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the\nCausalTime real-world benchmark ($d{=}20\\!-\\!36$), while baselines remain the\nbest on individual datasets, DOTS attains the highest average summary-graph\n$F1$ while halving runtime relative to graph-optimisation methods. These\nresults establish DOTS as a scalable and accurate solution for temporal causal\ndiscovery.\n","authors":["Pedro P. Sanchez","Damian Machlanski","Steven McDonagh","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2510.24639v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2504.13883v3","updated":"2025-10-28T17:05:46Z","published":"2025-04-03T17:54:59Z","title":"Hybrid Deep Learning Model to Estimate Cognitive Effort from fNIRS\n  Signals","summary":"  This study estimates cognitive effort based on functional near-infrared\nspectroscopy data and performance scores using a hybrid DeepNet model. The\nestimation of cognitive effort enables educators to modify material to enhance\nlearning effectiveness and student engagement. In this study, we collected\noxygenated hemoglobin using functional near-infrared spectroscopy during an\neducational quiz game. Participants (n=16) responded to 16 questions in a\nUnity-based educational game, each within a 30-second response time limit. We\nused DeepNet models to predict the performance score from the oxygenated\nhemoglobin, and compared traditional machine learning and DeepNet models to\ndetermine which approach provides better accuracy in predicting performance\nscores. The result shows that the proposed CNN-GRU gives better performance\nwith 73% than other models. After the prediction, we used the predicted score\nand the oxygenated hemoglobin to observe cognitive effort by calculating\nrelative neural efficiency and involvement in our test cases. Our result shows\nthat even with moderate accuracy, the predicted cognitive effort closely follow\nthe actual trends. This findings can be helpful in designing and improving\nlearning environments and provide valuable insights into learning materials.\n","authors":["Shayla Sharmin","Roghayeh Leila Barmaki"],"pdf_url":"https://arxiv.org/pdf/2504.13883v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24633v1","updated":"2025-10-28T17:01:38Z","published":"2025-10-28T17:01:38Z","title":"Symbolic Snapshot Ensembles","summary":"  Inductive logic programming (ILP) is a form of logical machine learning. Most\nILP algorithms learn a single hypothesis from a single training run. Ensemble\nmethods train an ILP algorithm multiple times to learn multiple hypotheses. In\nthis paper, we train an ILP algorithm only once and save intermediate\nhypotheses. We then combine the hypotheses using a minimum description length\nweighting scheme. Our experiments on multiple benchmarks, including game\nplaying and visual reasoning, show that our approach improves predictive\naccuracy by 4% with less than 1% computational overhead.\n","authors":["Mingyue Liu","Andrew Cropper"],"pdf_url":"https://arxiv.org/pdf/2510.24633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24621v1","updated":"2025-10-28T16:49:03Z","published":"2025-10-28T16:49:03Z","title":"Coreset for Robust Geometric Median: Eliminating Size Dependency on\n  Outliers","summary":"  We study the robust geometric median problem in Euclidean space\n$\\mathbb{R}^d$, with a focus on coreset construction.A coreset is a compact\nsummary of a dataset $P$ of size $n$ that approximates the robust cost for all\ncenters $c$ within a multiplicative error $\\varepsilon$. Given an outlier count\n$m$, we construct a coreset of size $\\tilde{O}(\\varepsilon^{-2} \\cdot\n\\min\\{\\varepsilon^{-2}, d\\})$ when $n \\geq 4m$, eliminating the $O(m)$\ndependency present in prior work [Huang et al., 2022 & 2023]. For the special\ncase of $d = 1$, we achieve an optimal coreset size of\n$\\tilde{\\Theta}(\\varepsilon^{-1/2} + \\frac{m}{n} \\varepsilon^{-1})$, revealing\na clear separation from the vanilla case studied in [Huang et al., 2023;\nAfshani and Chris, 2024]. Our results further extend to robust\n$(k,z)$-clustering in various metric spaces, eliminating the $m$-dependence\nunder mild data assumptions. The key technical contribution is a novel\nnon-component-wise error analysis, enabling substantial reduction of outlier\ninfluence, unlike prior methods that retain them.Empirically, our algorithms\nconsistently outperform existing baselines in terms of size-accuracy tradeoffs\nand runtime, even when data assumptions are violated across a wide range of\ndatasets.\n","authors":["Ziyi Fang","Lingxiao Huang","Runkai Yang"],"pdf_url":"https://arxiv.org/pdf/2510.24621v1.pdf","comment":"This paper has been accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24619v1","updated":"2025-10-28T16:48:03Z","published":"2025-10-28T16:48:03Z","title":"Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation","summary":"  With the release of new large language models (LLMs) like Llama and Mistral,\nzero-shot cross-lingual transfer has become increasingly feasible due to their\nmultilingual pretraining and strong generalization capabilities. However,\nadapting these decoder-only LLMs to new tasks across languages remains\nchallenging. While parameter-efficient fine-tuning (PeFT) techniques like\nLow-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as\nsoft prompt tuning, prefix tuning, and Llama Adapter are less explored,\nespecially for zero-shot transfer in decoder-only models. We present a\ncomprehensive study of three prefix-based methods for zero-shot cross-lingual\ntransfer from English to 35+ high- and low-resource languages. Our analysis\nfurther explores transfer across linguistic families and scripts, as well as\nthe impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix\nmethods outperform LoRA-baselines by up to 6% on the Belebele benchmark.\nSimilar improvements were observed with Mistral v0.3 7B as well. Despite using\nonly 1.23M learning parameters with prefix tuning, we achieve consistent\nimprovements across diverse benchmarks. These findings highlight the potential\nof prefix-based techniques as an effective and scalable alternative to LoRA,\nparticularly in low-resource multilingual settings.\n","authors":["Snegha A","Sayambhu Sen","Piyush Singh Pasi","Abhishek Singhania","Preethi Jyothi"],"pdf_url":"https://arxiv.org/pdf/2510.24619v1.pdf","comment":"12 Pages"},{"id":"http://arxiv.org/abs/2410.16893v2","updated":"2025-10-28T16:44:42Z","published":"2024-10-22T10:56:52Z","title":"Global Optimization of Gaussian Process Acquisition Functions Using a\n  Piecewise-Linear Kernel Approximation","summary":"  Bayesian optimization relies on iteratively constructing and optimizing an\nacquisition function. The latter turns out to be a challenging, non-convex\noptimization problem itself. Despite the relative importance of this step, most\nalgorithms employ sampling- or gradient-based methods, which do not provably\nconverge to global optima. This work investigates mixed-integer programming\n(MIP) as a paradigm for global acquisition function optimization. Specifically,\nour Piecewise-linear Kernel Mixed Integer Quadratic Programming (PK-MIQP)\nformulation introduces a piecewise-linear approximation for Gaussian process\nkernels and admits a corresponding MIQP representation for acquisition\nfunctions. The proposed method is applicable to uncertainty-based acquisition\nfunctions for any stationary or dot-product kernel. We analyze the theoretical\nregret bounds of the proposed approximation, and empirically demonstrate the\nframework on synthetic functions, constrained benchmarks, and a hyperparameter\ntuning task.\n","authors":["Yilin Xie","Shiqiang Zhang","Joel A. Paulson","Calvin Tsay"],"pdf_url":"https://arxiv.org/pdf/2410.16893v2.pdf","comment":"18 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2510.24616v1","updated":"2025-10-28T16:44:34Z","published":"2025-10-28T16:44:34Z","title":"Statistical physics of deep learning: Optimal learning of a multi-layer\n  perceptron near interpolation","summary":"  For three decades statistical physics has been providing a framework to\nanalyse neural networks. A long-standing question remained on its capacity to\ntackle deep learning models capturing rich feature learning effects, thus going\nbeyond the narrow networks or kernel methods analysed until now. We positively\nanswer through the study of the supervised learning of a multi-layer\nperceptron. Importantly, (i) its width scales as the input dimension, making it\nmore prone to feature learning than ultra wide networks, and more expressive\nthan narrow ones or with fixed embedding layers; and (ii) we focus on the\nchallenging interpolation regime where the number of trainable parameters and\ndata are comparable, which forces the model to adapt to the task. We consider\nthe matched teacher-student setting. It provides the fundamental limits of\nlearning random deep neural network targets and helps in identifying the\nsufficient statistics describing what is learnt by an optimally trained network\nas the data budget increases. A rich phenomenology emerges with various\nlearning transitions. With enough data optimal performance is attained through\nmodel's \"specialisation\" towards the target, but it can be hard to reach for\ntraining algorithms which get attracted by sub-optimal solutions predicted by\nthe theory. Specialisation occurs inhomogeneously across layers, propagating\nfrom shallow towards deep ones, but also across neurons in each layer.\nFurthermore, deeper targets are harder to learn. Despite its simplicity, the\nBayesian-optimal setting provides insights on how the depth, non-linearity and\nfinite (proportional) width influence neural networks in the feature learning\nregime that are potentially relevant way beyond it.\n","authors":["Jean Barbier","Francesco Camilli","Minh-Toan Nguyen","Mauro Pastore","Rudy Skerk"],"pdf_url":"https://arxiv.org/pdf/2510.24616v1.pdf","comment":"30 pages, 19 figures + appendix. This submission supersedes both\n  arXiv:2505.24849 and arXiv:2501.18530"},{"id":"http://arxiv.org/abs/2510.24614v1","updated":"2025-10-28T16:44:11Z","published":"2025-10-28T16:44:11Z","title":"Semi-supervised and unsupervised learning for health indicator\n  extraction from guided waves in aerospace composite structures","summary":"  Health indicators (HIs) are central to diagnosing and prognosing the\ncondition of aerospace composite structures, enabling efficient maintenance and\noperational safety. However, extracting reliable HIs remains challenging due to\nvariability in material properties, stochastic damage evolution, and diverse\ndamage modes. Manufacturing defects (e.g., disbonds) and in-service incidents\n(e.g., bird strikes) further complicate this process. This study presents a\ncomprehensive data-driven framework that learns HIs via two learning approaches\nintegrated with multi-domain signal processing. Because ground-truth HIs are\nunavailable, a semi-supervised and an unsupervised approach are proposed: (i) a\ndiversity deep semi-supervised anomaly detection (Diversity-DeepSAD) approach\naugmented with continuous auxiliary labels used as hypothetical damage proxies,\nwhich overcomes the limitation of prior binary labels that only distinguish\nhealthy and failed states while neglecting intermediate degradation, and (ii) a\ndegradation-trend-constrained variational autoencoder (DTC-VAE), in which the\nmonotonicity criterion is embedded via an explicit trend constraint. Guided\nwaves with multiple excitation frequencies are used to monitor single-stiffener\ncomposite structures under fatigue loading. Time, frequency, and time-frequency\nrepresentations are explored, and per-frequency HIs are fused via unsupervised\nensemble learning to mitigate frequency dependence and reduce variance. Using\nfast Fourier transform features, the augmented Diversity-DeepSAD model achieved\n81.6% performance, while DTC-VAE delivered the most consistent HIs with 92.3%\nperformance, outperforming existing baselines.\n","authors":["James Josep Perry","Pablo Garcia-Conde Ortiz","George Konstantinou","Cornelie Vergouwen","Edlyn Santha Kumaran","Morteza Moradi"],"pdf_url":"https://arxiv.org/pdf/2510.24614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11390v3","updated":"2025-10-28T16:44:02Z","published":"2024-09-17T17:50:15Z","title":"Says Who? Effective Zero-Shot Annotation of Focalization","summary":"  Focalization describes the way in which access to narrative information is\nrestricted or controlled based on the knowledge available to knowledge of the\nnarrator. It is encoded via a wide range of lexico-grammatical features and is\nsubject to reader interpretation. Even trained annotators frequently disagree\non correct labels, suggesting this task is both qualitatively and\ncomputationally challenging. In this work, we test how well five contemporary\nlarge language model (LLM) families and two baselines perform when annotating\nshort literary excerpts for focalization. Despite the challenging nature of the\ntask, we find that LLMs show comparable performance to trained human\nannotators, with GPT-4o achieving an average F1 of 84.79%. Further, we\ndemonstrate that the log probabilities output by GPT-family models frequently\nreflect the difficulty of annotating particular excerpts. Finally, we provide a\ncase study analyzing sixteen Stephen King novels, demonstrating the usefulness\nof this approach for computational literary studies and the insights gleaned\nfrom examining focalization at scale.\n","authors":["Rebecca M. M. Hicke","Yuri Bizzoni","Pascale Feldkamp","Ross Deans Kristensen-McLachlan"],"pdf_url":"https://arxiv.org/pdf/2409.11390v3.pdf","comment":"Accepted at CHR 2025"},{"id":"http://arxiv.org/abs/2510.24601v1","updated":"2025-10-28T16:28:42Z","published":"2025-10-28T16:28:42Z","title":"Comparison of generalised additive models and neural networks in\n  applications: A systematic review","summary":"  Neural networks have become a popular tool in predictive modelling, more\ncommonly associated with machine learning and artificial intelligence than with\nstatistics. Generalised Additive Models (GAMs) are flexible non-linear\nstatistical models that retain interpretability. Both are state-of-the-art in\ntheir own right, with their respective advantages and disadvantages. This paper\nanalyses how these two model classes have performed on real-world tabular data.\nFollowing PRISMA guidelines, we conducted a systematic review of papers that\nperformed empirical comparisons of GAMs and neural networks. Eligible papers\nwere identified, yielding 143 papers, with 430 datasets. Key attributes at both\npaper and dataset levels were extracted and reported. Beyond summarising\ncomparisons, we analyse reported performance metrics using mixed-effects\nmodelling to investigate potential characteristics that can explain and\nquantify observed differences, including application area, study year, sample\nsize, number of predictors, and neural network complexity. Across datasets, no\nconsistent evidence of superiority was found for either GAMs or neural networks\nwhen considering the most frequently reported metrics (RMSE, $R^2$, and AUC).\nNeural networks tended to outperform in larger datasets and in those with more\npredictors, but this advantage narrowed over time. Conversely, GAMs remained\ncompetitive, particularly in smaller data settings, while retaining\ninterpretability. Reporting of dataset characteristics and neural network\ncomplexity was incomplete in much of the literature, limiting transparency and\nreproducibility. This review highlights that GAMs and neural networks should be\nviewed as complementary approaches rather than competitors. For many tabular\napplications, the performance trade-off is modest, and interpretability may\nfavour GAMs.\n","authors":["Jessica Doohan","Lucas Kook","Kevin Burke"],"pdf_url":"https://arxiv.org/pdf/2510.24601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24598v1","updated":"2025-10-28T16:27:10Z","published":"2025-10-28T16:27:10Z","title":"A Novel XAI-Enhanced Quantum Adversarial Networks for Velocity\n  Dispersion Modeling in MaNGA Galaxies","summary":"  Current quantum machine learning approaches often face challenges balancing\npredictive accuracy, robustness, and interpretability. To address this, we\npropose a novel quantum adversarial framework that integrates a hybrid quantum\nneural network (QNN) with classical deep learning layers, guided by an\nevaluator model with LIME-based interpretability, and extended through quantum\nGAN and self-supervised variants. In the proposed model, an adversarial\nevaluator concurrently guides the QNN by computing feedback loss, thereby\noptimizing both prediction accuracy and model explainability. Empirical\nevaluations show that the Vanilla model achieves RMSE = 0.27, MSE = 0.071, MAE\n= 0.21, and R^2 = 0.59, delivering the most consistent performance across\nregression metrics compared to adversarial counterparts. These results\ndemonstrate the potential of combining quantum-inspired methods with classical\narchitectures to develop lightweight, high-performance, and interpretable\npredictive models, advancing the applicability of QML beyond current\nlimitations.\n","authors":["Sathwik Narkedimilli","N V Saran Kumar","Aswath Babu H","Manjunath K Vanahalli","Manish M","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2510.24598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15737v4","updated":"2025-10-28T16:23:53Z","published":"2024-11-24T07:02:32Z","title":"TableTime: Reformulating Time Series Classification as Training-Free\n  Table Understanding with Large Language Models","summary":"  Large language models (LLMs) have demonstrated their effectiveness in\nmultivariate time series classification (MTSC). Effective adaptation of LLMs\nfor MTSC necessitates informative data representations. Existing LLM-based\nmethods directly encode embeddings for time series within the latent space of\nLLMs from scratch to align with semantic space of LLMs. Despite their\neffectiveness, we reveal that these methods conceal three inherent bottlenecks:\n(1) they struggle to encode temporal and channel-specific information in a\nlossless manner, both of which are critical components of multivariate time\nseries; (2) it is much difficult to align the learned representation space with\nthe semantic space of the LLMs; (3) they require task-specific retraining,\nwhich is both computationally expensive and labor-intensive. To bridge these\ngaps, we propose TableTime, which reformulates MTSC as a table understanding\ntask. Specifically, TableTime introduces the following strategies: (1) convert\nmultivariate time series into a tabular form, thus minimizing information loss\nto the greatest extent; (2) represent tabular time series in text format to\nachieve natural alignment with the semantic space of LLMs; (3) design a\nreasoning framework that integrates contextual text information, neighborhood\nassistance, multi-path inference and problem decomposition to enhance the\nreasoning ability of LLMs and realize zero-shot classification. Extensive\nexperiments performed on 10 publicly representative datasets from UEA archive\nverify the superiorities of the TableTime.\n","authors":["Jiahao Wang","Mingyue Cheng","Qingyang Mao","Yitong Zhou","Daoyu Wang","Qi Liu","Feiyang Xu","Xin Li"],"pdf_url":"https://arxiv.org/pdf/2411.15737v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19000v2","updated":"2025-10-28T16:16:00Z","published":"2024-05-29T11:28:06Z","title":"FedMAP: Personalised Federated Learning for Real Large-Scale Healthcare\n  Systems","summary":"  Federated learning (FL) promises to enable collaborative machine learning\nacross healthcare sites whilst preserving data privacy. Practical deployment\nremains limited by statistical heterogeneity arising from differences in\npatient demographics, treatments, and outcomes, and infrastructure constraints.\nWe introduce FedMAP, a personalised FL (PFL) framework that addresses\nheterogeneity through local Maximum a Posteriori (MAP) estimation with Input\nConvex Neural Network priors. These priors represent global knowledge gathered\nfrom other sites that guides the model while adapting to local data, and we\nprovide a formal proof of convergence. Unlike many PFL methods that rely on\nfixed regularisation, FedMAP's prior adaptively learns patterns that capture\ncomplex inter-site relationships. We demonstrate improved performance compared\nto local training, FedAvg, and several PFL methods across three large-scale\nclinical datasets: 10-year cardiovascular risk prediction (CPRD, 387 general\npractitioner practices, 258,688 patients), iron deficiency detection (INTERVAL,\n4 donor centres, 31,949 blood donors), and mortality prediction (eICU, 150\nhospitals, 44,842 patients). FedMAP incorporates a three-tier design that\nenables participation across healthcare sites with varying infrastructure and\ntechnical capabilities, from full federated training to inference-only\ndeployment. Geographical analysis reveals substantial equity improvements, with\nunderperforming regions achieving up to 14.3% performance gains. This framework\nprovides the first practical pathway for large-scale healthcare FL deployment,\nwhich ensures clinical sites at all scales can benefit, equity is enhanced, and\nprivacy is retained.\n","authors":["Fan Zhang","Daniel Kreuter","Carlos Esteve-Yagüe","Sören Dittmer","Javier Fernandez-Marques","Samantha Ip","BloodCounts! Consortium","Norbert C. J. de Wit","Angela Wood","James HF Rudd","Nicholas Lane","Nicholas S Gleadall","Carola-Bibiane Schönlieb","Michael Roberts"],"pdf_url":"https://arxiv.org/pdf/2405.19000v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24577v1","updated":"2025-10-28T16:11:16Z","published":"2025-10-28T16:11:16Z","title":"Physics-Informed Extreme Learning Machine (PIELM): Opportunities and\n  Challenges","summary":"  We are very delighted to see the fast development of physics-informed extreme\nlearning machine (PIELM) in recent years for higher computation efficiency and\naccuracy in physics-informed machine learning. As a summary or review on PIELM\nis currently not available, we would like to take this opportunity to show our\nperspective and experience for this promising research direction. We can see\nmany efforts are made to solve PDEs with sharp gradients, nonlinearities,\nhigh-frequency behavior, hard constraints, uncertainty, multiphysics coupling.\nDespite the success, many urgent challenges remain to be tackled, which also\nprovides us opportunities to develop more robust, interpretable, and\ngeneralizable PIELM frameworks with applications in science and engineering.\n","authors":["He Yang","Fei Ren","Hai-Sui Yu","Xiaohui Chen","Pei-Zhi Zhuang"],"pdf_url":"https://arxiv.org/pdf/2510.24577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24574v1","updated":"2025-10-28T16:09:59Z","published":"2025-10-28T16:09:59Z","title":"DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein\n  Alignment","summary":"  Training time-series forecast models requires aligning the conditional\ndistribution of model forecasts with that of the label sequence. The standard\ndirect forecast (DF) approach resorts to minimize the conditional negative\nlog-likelihood of the label sequence, typically estimated using the mean\nsquared error. However, this estimation proves to be biased in the presence of\nlabel autocorrelation. In this paper, we propose DistDF, which achieves\nalignment by alternatively minimizing a discrepancy between the conditional\nforecast and label distributions. Because conditional discrepancies are\ndifficult to estimate from finite time-series observations, we introduce a\nnewly proposed joint-distribution Wasserstein discrepancy for time-series\nforecasting, which provably upper bounds the conditional discrepancy of\ninterest. This discrepancy admits tractable, differentiable estimation from\nempirical samples and integrates seamlessly with gradient-based training.\nExtensive experiments show that DistDF improves the performance diverse\nforecast models and achieves the state-of-the-art forecasting performance. Code\nis available at https://anonymous.4open.science/r/DistDF-F66B.\n","authors":["Hao Wang","Licheng Pan","Yuan Lu","Zhixuan Chu","Xiaoxi Li","Shuting He","Zhichao Chen","Haoxuan Li","Qingsong Wen","Zhouchen Lin"],"pdf_url":"https://arxiv.org/pdf/2510.24574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05295v2","updated":"2025-10-28T16:01:40Z","published":"2025-02-07T19:56:01Z","title":"GST-UNet: A Neural Framework for Spatiotemporal Causal Inference with\n  Time-Varying Confounding","summary":"  Estimating causal effects from spatiotemporal observational data is essential\nin public health, environmental science, and policy evaluation, where\nrandomized experiments are often infeasible. Existing approaches, however,\neither rely on strong structural assumptions or fail to handle key challenges\nsuch as interference, spatial confounding, temporal carryover, and time-varying\nconfounding -- where covariates are influenced by past treatments and, in turn,\naffect future ones. We introduce GST-UNet (G-computation Spatio-Temporal UNet),\na theoretically grounded neural framework that combines a U-Net-based\nspatiotemporal encoder with regression-based iterative G-computation to\nestimate location-specific potential outcomes under complex intervention\nsequences. GST-UNet explicitly adjusts for time-varying confounders and\ncaptures non-linear spatial and temporal dependencies, enabling valid causal\ninference from a single observed trajectory in data-scarce settings. We\nvalidate its effectiveness in synthetic experiments and in a real-world\nanalysis of wildfire smoke exposure and respiratory hospitalizations during the\n2018 California Camp Fire. Together, these results position GST-UNet as a\nprincipled and ready-to-use framework for spatiotemporal causal inference,\nadvancing reliable estimation in policy-relevant and scientific domains.\n","authors":["Miruna Oprescu","David K. Park","Xihaier Luo","Shinjae Yoo","Nathan Kallus"],"pdf_url":"https://arxiv.org/pdf/2502.05295v2.pdf","comment":"29 pages, 6 figures, 6 tables, NeurIPS 2025"},{"id":"http://arxiv.org/abs/2409.11529v3","updated":"2025-10-28T15:59:49Z","published":"2024-09-17T19:59:57Z","title":"Adaptive Anomaly Detection in Network Flows with Low-Rank Tensor\n  Decompositions and Deep Unrolling","summary":"  Anomaly detection (AD) is increasingly recognized as a key component for\nensuring the resilience of future communication systems. While deep learning\nhas shown state-of-the-art AD performance, its application in critical systems\nis hindered by concerns regarding training data efficiency, domain adaptation\nand interpretability. This work considers AD in network flows using incomplete\nmeasurements, leveraging a robust tensor decomposition approach and deep\nunrolling techniques to address these challenges. We first propose a novel\nblock-successive convex approximation algorithm based on a regularized\nmodel-fitting objective where the normal flows are modeled as low-rank tensors\nand anomalies as sparse. An augmentation of the objective is introduced to\ndecrease the computational cost. We apply deep unrolling to derive a novel deep\nnetwork architecture based on our proposed algorithm, treating the\nregularization parameters as learnable weights. Inspired by Bayesian\napproaches, we extend the model architecture to perform online adaptation to\nper-flow and per-time-step statistics, improving AD performance while\nmaintaining a low parameter count and preserving the problem's permutation\nequivariances. To optimize the deep network weights for detection performance,\nwe employ a homotopy optimization approach based on an efficient approximation\nof the area under the receiver operating characteristic curve. Extensive\nexperiments on synthetic and real-world data demonstrate that our proposed deep\nnetwork architecture exhibits a high training data efficiency, outperforms\nreference methods, and adapts seamlessly to varying network topologies.\n","authors":["Lukas Schynol","Marius Pesavento"],"pdf_url":"https://arxiv.org/pdf/2409.11529v3.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2510.24561v1","updated":"2025-10-28T15:55:36Z","published":"2025-10-28T15:55:36Z","title":"LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via\n  Asymptotic Analysis","summary":"  With the widespread adoption of LLMs, LoRA has become a dominant method for\nPEFT, and its initialization methods have attracted increasing attention.\nHowever, existing methods have notable limitations: many methods do not\nincorporate target-domain data, while gradient-based methods exploit data only\nat a shallow level by relying on one-step gradient decomposition, which remains\nunsatisfactory due to the weak empirical performance of the one-step\nfine-tuning model that serves as their basis, as well as the fact that these\nmethods either lack a rigorous theoretical foundation or depend heavily on\nrestrictive isotropic assumptions. In this paper, we establish a theoretical\nframework for data-aware LoRA initialization based on asymptotic analysis.\nStarting from a general optimization objective that minimizes the expectation\nof the parameter discrepancy between the fine-tuned and target models, we\nderive an optimization problem with two components: a bias term, which is\nrelated to the parameter distance between the fine-tuned and target models, and\nis approximated using a Fisher-gradient formulation to preserve anisotropy; and\na variance term, which accounts for the uncertainty introduced by sampling\nstochasticity through the Fisher information. By solving this problem, we\nobtain an optimal initialization strategy for LoRA. Building on this\ntheoretical framework, we develop an efficient algorithm, LoRA-DA, which\nestimates the terms in the optimization problem from a small set of target\ndomain samples and obtains the optimal LoRA initialization. Empirical results\nacross multiple benchmarks demonstrate that LoRA-DA consistently improves final\naccuracy over existing initialization methods. Additional studies show faster,\nmore stable convergence, robustness across ranks, and only a small\ninitialization overhead for LoRA-DA. The source code will be released upon\npublication.\n","authors":["Qingyue Zhang","Chang Chu","Tianren Peng","Qi Li","Xiangyang Luo","Zhihao Jiang","Shao-Lun Huang"],"pdf_url":"https://arxiv.org/pdf/2510.24561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24557v1","updated":"2025-10-28T15:51:48Z","published":"2025-10-28T15:51:48Z","title":"Enforcing boundary conditions for physics-informed neural operators","summary":"  Machine-learning based methods like physics-informed neural networks and\nphysics-informed neural operators are becoming increasingly adept at solving\neven complex systems of partial differential equations. Boundary conditions can\nbe enforced either weakly by penalizing deviations in the loss function or\nstrongly by training a solution structure that inherently matches the\nprescribed values and derivatives. The former approach is easy to implement but\nthe latter can provide benefits with respect to accuracy and training times.\nHowever, previous approaches to strongly enforcing Neumann or Robin boundary\nconditions require a domain with a fully $C^1$ boundary and, as we demonstrate,\ncan lead to instability if those boundary conditions are posed on a segment of\nthe boundary that is piecewise $C^1$ but only $C^0$ globally. We introduce a\ngeneralization of the approach by Sukumar \\& Srivastava (doi:\n10.1016/j.cma.2021.114333), and a new approach based on orthogonal projections\nthat overcome this limitation. The performance of these new techniques is\ncompared against weakly and semi-weakly enforced boundary conditions for the\nscalar Darcy flow equation and the stationary Navier-Stokes equations.\n","authors":["Niklas Göschel","Sebastian Götschel","Daniel Ruprecht"],"pdf_url":"https://arxiv.org/pdf/2510.24557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22931v2","updated":"2025-10-28T15:51:13Z","published":"2025-10-27T02:15:51Z","title":"Robust Uncertainty Quantification for Self-Evolving Large Language\n  Models via Continual Domain Pretraining","summary":"  Continual Learning (CL) is essential for enabling self-evolving large\nlanguage models (LLMs) to adapt and remain effective amid rapid knowledge\ngrowth. Yet, despite its importance, little attention has been given to\nestablishing statistical reliability guarantees for LLMs under CL, particularly\nin the setting of continual domain pretraining (CDP). Conformal Prediction (CP)\nhas shown promise in offering correctness guarantees for LLMs, but it faces\nmajor challenges in CDP: testing data often stems from unknown or shifting\ndomain distributions, under which CP may no longer provide valid guarantees.\nMoreover, when high coverage is required, CP can yield excessively large\nprediction sets for unanswerable queries, reducing informativeness. To address\nthese challenges, we introduce an adaptive rejection and non-exchangeable CP\nframework. Our method first estimates the distribution of questions across\ndomains in the test set using transformer-based clustering, then reweights or\nresamples the calibration data accordingly. Building on this, adaptive\nrejection CP allows the LLM to selectively abstain from answering when its\nconfidence or competence shifts significantly. Extensive experiments\ndemonstrate that our framework enhances both the effectiveness and reliability\nof CP under CDP scenarios. Our code is available at:\nhttps://anonymous.4open.science/r/CPCL-8C12/\n","authors":["Xiaofan Zhou","Lu Cheng"],"pdf_url":"https://arxiv.org/pdf/2510.22931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18976v3","updated":"2025-10-28T15:46:13Z","published":"2025-05-25T04:58:57Z","title":"GraSS: Scalable Data Attribution with Gradient Sparsification and Sparse\n  Projection","summary":"  Gradient-based data attribution methods, such as influence functions, are\ncritical for understanding the impact of individual training samples without\nrequiring repeated model retraining. However, their scalability is often\nlimited by the high computational and memory costs associated with per-sample\ngradient computation. In this work, we propose GraSS, a novel gradient\ncompression algorithm and its variants FactGraSS for linear layers\nspecifically, that explicitly leverage the inherent sparsity of per-sample\ngradients to achieve sub-linear space and time complexity. Extensive\nexperiments demonstrate the effectiveness of our approach, achieving\nsubstantial speedups while preserving data influence fidelity. In particular,\nFactGraSS achieves up to 165% faster throughput on billion-scale models\ncompared to the previous state-of-the-art baselines. Our code is publicly\navailable at https://github.com/TRAIS-Lab/GraSS.\n","authors":["Pingbang Hu","Joseph Melkonian","Weijing Tang","Han Zhao","Jiaqi W. Ma"],"pdf_url":"https://arxiv.org/pdf/2505.18976v3.pdf","comment":"Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.24546v1","updated":"2025-10-28T15:45:15Z","published":"2025-10-28T15:45:15Z","title":"Dual-Mind World Models: A General Framework for Learning in Dynamic\n  Wireless Networks","summary":"  Despite the popularity of reinforcement learning (RL) in wireless networks,\nexisting approaches that rely on model-free RL (MFRL) and model-based RL (MBRL)\nare data inefficient and short-sighted. Such RL-based solutions cannot\ngeneralize to novel network states since they capture only statistical patterns\nrather than the underlying physics and logic from wireless data. These\nlimitations become particularly challenging in complex wireless networks with\nhigh dynamics and long-term planning requirements. To address these\nlimitations, in this paper, a novel dual-mind world model-based learning\nframework is proposed with the goal of optimizing completeness-weighted age of\ninformation (CAoI) in a challenging mmWave V2X scenario. Inspired by cognitive\npsychology, the proposed dual-mind world model encompasses a pattern-driven\nSystem 1 component and a logic-driven System 2 component to learn dynamics and\nlogic of the wireless network, and to provide long-term link scheduling over\nreliable imagined trajectories. Link scheduling is learned through end-to-end\ndifferentiable imagined trajectories with logical consistency over an extended\nhorizon rather than relying on wireless data obtained from environment\ninteractions. Moreover, through imagination rollouts, the proposed world model\ncan jointly reason network states and plan link scheduling. During intervals\nwithout observations, the proposed method remains capable of making efficient\ndecisions. Extensive experiments are conducted on a realistic simulator based\non Sionna with real-world physical channel, ray-tracing, and scene objects with\nmaterial properties. Simulation results show that the proposed world model\nachieves a significant improvement in data efficiency and achieves strong\ngeneralization and adaptation to unseen environments, compared to the\nstate-of-the-art RL baselines, and the world model approach with only System 1.\n","authors":["Lingyi Wang","Rashed Shelim","Walid Saad","Naren Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2510.24546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07530v3","updated":"2025-10-28T15:36:36Z","published":"2023-01-18T13:48:20Z","title":"Online (Non-)Convex Learning via Tempered Optimism","summary":"  Optimistic Online Learning aims to exploit experts conveying reliable\ninformation to predict the future. However, such implicit optimism may be\nchallenged when it comes to practical crafting of such experts. A fundamental\nexample consists in approximating a minimiser of the current problem and use it\nas expert. In the context of dynamic environments, such an expert only conveys\npartially relevant information as it may lead to overfitting. To tackle this\nissue, we introduce in this work the \\emph{optimistically tempered} (OT) online\nlearning framework designed to handle such imperfect experts. As a first\ncontribution, we show that tempered optimism is a fruitful paradigm for Online\nNon-Convex Learning by proposing simple, yet powerful modification of Online\nGradient and Mirror Descent. Second, we derive a second OT algorithm for convex\nlosses and third, evaluate the practical efficiency of tempered optimism on\nreal-life datasets and a toy experiment.\n","authors":["Maxime Haddouche","Olivier Wintenberger","Benjamin Guedj"],"pdf_url":"https://arxiv.org/pdf/2301.07530v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24523v1","updated":"2025-10-28T15:34:23Z","published":"2025-10-28T15:34:23Z","title":"Unsupervised Machine-Learning Pipeline for Data-Driven Defect Detection\n  and Characterisation: Application to Displacement Cascades","summary":"  Neutron irradiation produces, within a few picoseconds, displacement cascades\nthat are sequences of atomic collisions generating point and extended defects\nwhich subsequently affects the long-term evolution of materials. The diversity\nof these defects, characterized morphologically and statistically, defines what\nis called the \"primary damage\". In this work, we present a fully unsupervised\nmachine learning (ML) workflow that detects and classifies these defects\ndirectly from molecular dynamics data. Local environments are encoded by the\nSmooth Overlap of Atomic Positions (SOAP) vector, anomalous atoms are isolated\nwith autoencoder neural networks (AE), embedded with Uniform Man- ifold\nApproximation and Projection (UMAP) and clustered using Hierarchical\nDensity-Based Spatial Clustering of Applications with Noise (HDBSCAN). Applied\nto 80 keV displacement cascades in Ni, Fe70Ni10Cr20, and Zr, the AE\nsuccessfully identify the small fraction of outlier atoms that participate in\ndefect formation. HDBSCAN then partitions the UMAP latent space of AE-flagged\nSOAP de- scriptors into well defined groups representing vacancy- and\ninterstitial-dominated regions and, within each, separates small from large\naggregates, assigning 99.7 % of outliers to compact physical motifs. A signed\ncluster-identification score confirms this separation, and cluster size scales\nwith net defect counts (R2 > 0.89). Statistical cross analyses between the ML\noutlier map and several conventional detectors (centrosymmetry, dislocation\nextraction, etc.) reveal strong overlap and complementary coverage, all\nachieved without template or threshold tuning. This ML workflow thus provides\nan efficient tool for the quantitative mapping of structural anomalies in\nmaterials, particularly those arising from irradiation damage in displacement\ncascades.\n","authors":["Samuel Del Fré","Andrée de Backer","Christophe Domain","Ludovic Thuinet","Charlotte S. Becquart"],"pdf_url":"https://arxiv.org/pdf/2510.24523v1.pdf","comment":"22 pages, 1 graphical abstract, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2508.02293v2","updated":"2025-10-28T15:28:13Z","published":"2025-08-04T11:03:12Z","title":"Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning","summary":"  So-called unsupervised anomaly detection is better described as\nsemi-supervised, as it assumes all training data are nominal. This assumption\nsimplifies training but requires manual data curation, introducing bias and\nlimiting adaptability. We propose Confident Meta-learning (CoMet), a novel\ntraining strategy that enables deep anomaly detection models to learn from\nuncurated datasets where nominal and anomalous samples coexist, eliminating the\nneed for explicit filtering. Our approach integrates Soft Confident Learning,\nwhich assigns lower weights to low-confidence samples, and Meta-Learning, which\nstabilizes training by regularizing updates based on training validation loss\ncovariance. This prevents overfitting and enhances robustness to noisy data.\nCoMet is model-agnostic and can be applied to any anomaly detection method\ntrainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2\nwith two state-of-the-art models demonstrate the effectiveness of our approach,\nconsistently improving over the baseline methods, remaining insensitive to\nanomalies in the training set, and setting a new state-of-the-art across all\ndatasets. Code is available at https://github.com/aqeeelmirza/CoMet\n","authors":["Muhammad Aqeel","Shakiba Sharifi","Marco Cristani","Francesco Setti"],"pdf_url":"https://arxiv.org/pdf/2508.02293v2.pdf","comment":"Accepted to IEEE/CVF International Conference on Computer Vision\n  (ICCV2025)"},{"id":"http://arxiv.org/abs/2501.00913v2","updated":"2025-10-28T15:26:34Z","published":"2025-01-01T18:12:18Z","title":"$β$-DQN: Improving Deep Q-Learning By Evolving the Behavior","summary":"  While many sophisticated exploration methods have been proposed, their lack\nof generality and high computational cost often lead researchers to favor\nsimpler methods like $\\epsilon$-greedy. Motivated by this, we introduce\n$\\beta$-DQN, a simple and efficient exploration method that augments the\nstandard DQN with a behavior function $\\beta$. This function estimates the\nprobability that each action has been taken at each state. By leveraging\n$\\beta$, we generate a population of diverse policies that balance exploration\nbetween state-action coverage and overestimation bias correction. An adaptive\nmeta-controller is designed to select an effective policy for each episode,\nenabling flexible and explainable exploration. $\\beta$-DQN is straightforward\nto implement and adds minimal computational overhead to the standard DQN.\nExperiments on both simple and challenging exploration domains show that\n$\\beta$-DQN outperforms existing baseline methods across a wide range of tasks,\nproviding an effective solution for improving exploration in deep reinforcement\nlearning.\n","authors":["Hongming Zhang","Fengshuo Bai","Chenjun Xiao","Chao Gao","Bo Xu","Martin Müller"],"pdf_url":"https://arxiv.org/pdf/2501.00913v2.pdf","comment":"aamas 2025"},{"id":"http://arxiv.org/abs/2506.00799v3","updated":"2025-10-28T15:20:47Z","published":"2025-06-01T03:00:09Z","title":"Uni-LoRA: One Vector is All You Need","summary":"  Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient\nfine-tuning (PEFT) method for large language models (LLMs) by constraining\nweight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and\nVB-LoRA push efficiency further by introducing additional constraints to reduce\nthe trainable parameter space. In this paper, we show that the parameter space\nreduction strategies employed by these LoRA variants can be formulated within a\nunified framework, Uni-LoRA, where the LoRA parameter space, flattened as a\nhigh-dimensional vector space $R^D$, can be reconstructed through a projection\nfrom a subspace R^d, with $d \\ll D$. We demonstrate that the fundamental\ndifference among various LoRA methods lies in the choice of the projection\nmatrix, $P \\in R^{D \\times d}$.Most existing LoRA variants rely on layer-wise\nor structure-specific projections that limit cross-layer parameter sharing,\nthereby compromising parameter efficiency. In light of this, we introduce an\nefficient and theoretically grounded projection matrix that is isometric,\nenabling global parameter sharing and reducing computation overhead.\nFurthermore, under the unified view of Uni-LoRA, this design requires only a\nsingle trainable vector to reconstruct LoRA parameters for the entire LLM -\nmaking Uni-LoRA both a unified framework and a \"one-vector-only\" solution.\nExtensive experiments on GLUE, mathematical reasoning, and instruction tuning\nbenchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter\nefficiency while outperforming or matching prior approaches in predictive\nperformance. Our code is available at\nhttps://github.com/KaiyangLi1992/Uni-LoRA.\n","authors":["Kaiyang Li","Shaobo Han","Qing Su","Wei Li","Zhipeng Cai","Shihao Ji"],"pdf_url":"https://arxiv.org/pdf/2506.00799v3.pdf","comment":"NeurIPS 2025 Spotlight"},{"id":"http://arxiv.org/abs/2510.24503v1","updated":"2025-10-28T15:15:14Z","published":"2025-10-28T15:15:14Z","title":"Local Performance vs. Out-of-Distribution Generalization: An Empirical\n  Analysis of Personalized Federated Learning in Heterogeneous Data\n  Environments","summary":"  In the context of Federated Learning with heterogeneous data environments,\nlocal models tend to converge to their own local model optima during local\ntraining steps, deviating from the overall data distributions. Aggregation of\nthese local updates, e.g., with FedAvg, often does not align with the global\nmodel optimum (client drift), resulting in an update that is suboptimal for\nmost clients. Personalized Federated Learning approaches address this challenge\nby exclusively focusing on the average local performances of clients' models on\ntheir own data distribution. Generalization to out-of-distribution samples,\nwhich is a substantial benefit of FedAvg and represents a significant component\nof robustness, appears to be inadequately incorporated into the assessment and\nevaluation processes. This study involves a thorough evaluation of Federated\nLearning approaches, encompassing both their local performance and their\ngeneralization capabilities. Therefore, we examine different stages within a\nsingle communication round to enable a more nuanced understanding of the\nconsidered metrics. Furthermore, we propose and incorporate a modified approach\nof FedAvg, designated as Federated Learning with Individualized Updates (FLIU),\nextending the algorithm by a straightforward individualization step with an\nadaptive personalization factor. We evaluate and compare the approaches\nempirically using MNIST and CIFAR-10 under various distributional conditions,\nincluding benchmark IID and pathological non-IID, as well as additional novel\ntest environments with Dirichlet distribution specifically developed to stress\nthe algorithms on complex data heterogeneity.\n","authors":["Mortesa Hussaini","Jan Theiß","Anthony Stein"],"pdf_url":"https://arxiv.org/pdf/2510.24503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24500v1","updated":"2025-10-28T15:13:38Z","published":"2025-10-28T15:13:38Z","title":"MIMIC-Sepsis: A Curated Benchmark for Modeling and Learning from Sepsis\n  Trajectories in the ICU","summary":"  Sepsis is a leading cause of mortality in intensive care units (ICUs), yet\nexisting research often relies on outdated datasets, non-reproducible\npreprocessing pipelines, and limited coverage of clinical interventions. We\nintroduce MIMIC-Sepsis, a curated cohort and benchmark framework derived from\nthe MIMIC-IV database, designed to support reproducible modeling of sepsis\ntrajectories. Our cohort includes 35,239 ICU patients with time-aligned\nclinical variables and standardized treatment data, including vasopressors,\nfluids, mechanical ventilation and antibiotics. We describe a transparent\npreprocessing pipeline-based on Sepsis-3 criteria, structured imputation\nstrategies, and treatment inclusion-and release it alongside benchmark tasks\nfocused on early mortality prediction, length-of-stay estimation, and shock\nonset classification. Empirical results demonstrate that incorporating\ntreatment variables substantially improves model performance, particularly for\nTransformer-based architectures. MIMIC-Sepsis serves as a robust platform for\nevaluating predictive and sequential models in critical care research.\n","authors":["Yong Huang","Zhongqi Yang","Amir Rahmani"],"pdf_url":"https://arxiv.org/pdf/2510.24500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10978v3","updated":"2025-10-28T15:11:36Z","published":"2025-05-16T08:26:59Z","title":"Group-in-Group Policy Optimization for LLM Agent Training","summary":"  Recent advances in group-based reinforcement learning (RL) have driven\nfrontier large language models (LLMs) in single-turn tasks like mathematical\nreasoning. However, their scalability to multi-turn LLM agent training remains\nlimited. Unlike static tasks, agent-environment interactions unfold over many\nsteps and often yield sparse or delayed rewards, making credit assignment\nacross individual steps significantly more challenging. In this work, we\npropose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that\nachieves fine-grained credit assignment for LLM agents while preserving the\nappealing properties of group-based RL: critic-free, low memory, and stable\nconvergence. GiGPO introduces a two-level structure for estimating relative\nadvantage: (i) At the episode-level, GiGPO computes macro relative advantages\nbased on groups of complete trajectories; (ii) At the step-level, GiGPO\nintroduces an anchor state grouping mechanism that retroactively constructs\nstep-level groups by identifying repeated environment states across\ntrajectories. Actions stemming from the same state are grouped together,\nenabling micro relative advantage estimation. This hierarchical structure\neffectively captures both global trajectory quality and local step\neffectiveness without relying on auxiliary models or additional rollouts. We\nevaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop,\nas well as tool-integrated reasoning on search-augmented QA tasks, using\nQwen2.5-1.5B/3B/7B-Instruct. Crucially, GiGPO delivers fine-grained per-step\ncredit signals, achieves performance gains of > 12% on ALFWorld and > 9% on\nWebShop over GRPO, and obtains superior performance on QA tasks (42.1% on 3B\nand 47.2% on 7B): all while maintaining the same GPU memory overhead, identical\nLLM rollout, and incurring little to no additional time cost.\n","authors":["Lang Feng","Zhenghai Xue","Tingcong Liu","Bo An"],"pdf_url":"https://arxiv.org/pdf/2505.10978v3.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2406.04378v3","updated":"2025-10-28T15:03:25Z","published":"2024-06-05T22:18:36Z","title":"TIDMAD: Time Series Dataset for Discovering Dark Matter with AI\n  Denoising","summary":"  Dark matter makes up approximately 85% of total matter in our universe, yet\nit has never been directly observed in any laboratory on Earth. The origin of\ndark matter is one of the most important questions in contemporary physics, and\na convincing detection of dark matter would be a Nobel-Prize-level breakthrough\nin fundamental science. The ABRACADABRA experiment was specifically designed to\nsearch for dark matter. Although it has not yet made a discovery, ABRACADABRA\nhas produced several dark matter search results widely endorsed by the physics\ncommunity. The experiment generates ultra-long time-series data at a rate of 10\nmillion samples per second, where the dark matter signal would manifest itself\nas a sinusoidal oscillation mode within the ultra-long time series. In this\npaper, we present the TIDMAD -- a comprehensive data release from the\nABRACADABRA experiment including three key components: an ultra-long time\nseries dataset divided into training, validation, and science subsets; a\ncarefully-designed denoising score for direct model benchmarking; and a\ncomplete analysis framework which produces a community-standard dark matter\nsearch result suitable for publication as a physics paper. This data release\nenables core AI algorithms to extract the dark matter signal and produce real\nphysics results thereby advancing fundamental science. The data downloading and\nassociated analysis scripts are available at\nhttps://github.com/jessicafry/TIDMAD\n","authors":["J. T. Fry","Xinyi Hope Fu","Zhenghao Fu","Kaliroe M. W. Pappas","Lindley Winslow","Aobo Li"],"pdf_url":"https://arxiv.org/pdf/2406.04378v3.pdf","comment":"Accepted by NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2503.08748v4","updated":"2025-10-28T15:01:16Z","published":"2025-03-11T10:50:07Z","title":"Mirror Descent and Novel Exponentiated Gradient Algorithms Using\n  Trace-Form Entropies and Deformed Logarithms","summary":"  This paper introduces a broad class of Mirror Descent (MD) and Generalized\nExponentiated Gradient (GEG) algorithms derived from trace-form entropies\ndefined via deformed logarithms. Leveraging these generalized entropies yields\nMD \\& GEG algorithms with improved convergence behavior, robustness to\nvanishing and exploding gradients, and inherent adaptability to non-Euclidean\ngeometries through mirror maps. We establish deep connections between these\nmethods and Amari's natural gradient, revealing a unified geometric foundation\nfor additive, multiplicative, and natural gradient updates. Focusing on the\nTsallis, Kaniadakis, Sharma--Taneja--Mittal, and Kaniadakis--Lissia--Scarfone\nentropy families, we show that each entropy induces a distinct Riemannian\nmetric on the parameter space, leading to GEG algorithms that preserve the\nnatural statistical geometry. The tunable parameters of deformed logarithms\nenable adaptive geometric selection, providing enhanced robustness and\nconvergence over classical Euclidean optimization. Overall, our framework\nunifies key first-order MD optimization methods under a single\ninformation-geometric perspective based on generalized Bregman divergences,\nwhere the choice of entropy determines the underlying metric and dual geometric\nstructure.\n","authors":["Andrzej Cichocki","Toshihisa Tanaka","Frank Nielsen","Sergio Cruces"],"pdf_url":"https://arxiv.org/pdf/2503.08748v4.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.24482v1","updated":"2025-10-28T14:54:12Z","published":"2025-10-28T14:54:12Z","title":"Sample-efficient and Scalable Exploration in Continuous-Time RL","summary":"  Reinforcement learning algorithms are typically designed for discrete-time\ndynamics, even though the underlying real-world control systems are often\ncontinuous in time. In this paper, we study the problem of continuous-time\nreinforcement learning, where the unknown system dynamics are represented using\nnonlinear ordinary differential equations (ODEs). We leverage probabilistic\nmodels, such as Gaussian processes and Bayesian neural networks, to learn an\nuncertainty-aware model of the underlying ODE. Our algorithm, COMBRL, greedily\nmaximizes a weighted sum of the extrinsic reward and model epistemic\nuncertainty. This yields a scalable and sample-efficient approach to\ncontinuous-time model-based RL. We show that COMBRL achieves sublinear regret\nin the reward-driven setting, and in the unsupervised RL setting (i.e., without\nextrinsic rewards), we provide a sample complexity bound. In our experiments,\nwe evaluate COMBRL in both standard and unsupervised RL settings and\ndemonstrate that it scales better, is more sample-efficient than prior methods,\nand outperforms baselines across several deep RL tasks.\n","authors":["Klemens Iten","Lenart Treven","Bhavya Sukhija","Florian Dörfler","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2510.24482v1.pdf","comment":"26 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.21142v2","updated":"2025-10-28T14:49:07Z","published":"2025-02-28T15:24:17Z","title":"Multimodal Dreaming: A Global Workspace Approach to World Model-Based\n  Reinforcement Learning","summary":"  Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.\n","authors":["Léopold Maytié","Roland Bertin Johannet","Rufin VanRullen"],"pdf_url":"https://arxiv.org/pdf/2502.21142v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2510.03534v2","updated":"2025-10-28T14:48:21Z","published":"2025-10-03T22:08:08Z","title":"Long-Term Mapping of the Douro River Plume with Multi-Agent\n  Reinforcement Learning","summary":"  We study the problem of long-term (multiple days) mapping of a river plume\nusing multiple autonomous underwater vehicles (AUVs), focusing on the Douro\nriver representative use-case. We propose an energy - and communication -\nefficient multi-agent reinforcement learning approach in which a central\ncoordinator intermittently communicates with the AUVs, collecting measurements\nand issuing commands. Our approach integrates spatiotemporal Gaussian process\nregression (GPR) with a multi-head Q-network controller that regulates\ndirection and speed for each AUV. Simulations using the Delft3D ocean model\ndemonstrate that our method consistently outperforms both single- and\nmulti-agent benchmarks, with scaling the number of agents both improving mean\nsquared error (MSE) and operational endurance. In some instances, our algorithm\ndemonstrates that doubling the number of AUVs can more than double endurance\nwhile maintaining or improving accuracy, underscoring the benefits of\nmulti-agent coordination. Our learned policies generalize across unseen\nseasonal regimes over different months and years, demonstrating promise for\nfuture developments of data-driven long-term monitoring of dynamic plume\nenvironments.\n","authors":["Nicolò Dal Fabbro","Milad Mesbahi","Renato Mendes","João Borges de Sousa","George J. Pappas"],"pdf_url":"https://arxiv.org/pdf/2510.03534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22830v2","updated":"2025-10-28T14:43:58Z","published":"2025-10-26T20:59:22Z","title":"Exploration of Summarization by Generative Language Models for Automated\n  Scoring of Long Essays","summary":"  BERT and its variants are extensively explored for automated scoring.\nHowever, a limit of 512 tokens for these encoder-based models showed the\ndeficiency in automated scoring of long essays. Thus, this research explores\ngenerative language models for automated scoring of long essays via\nsummarization and prompting. The results revealed great improvement of scoring\naccuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab\nAutomated Essay Scoring 2.0 dataset.\n","authors":["Haowei Hua","Hong Jiao","Xinyi Wang"],"pdf_url":"https://arxiv.org/pdf/2510.22830v2.pdf","comment":"19 pages, 5 Tables 7 Figures, Presentation at Artificial Intelligence\n  in Measurement and Education Conference (AIME-Con)"},{"id":"http://arxiv.org/abs/2510.24473v1","updated":"2025-10-28T14:42:28Z","published":"2025-10-28T14:42:28Z","title":"Methodology for Comparing Machine Learning Algorithms for Survival\n  Analysis","summary":"  This study presents a comparative methodological analysis of six machine\nlearning models for survival analysis (MLSA). Using data from nearly 45,000\ncolorectal cancer patients in the Hospital-Based Cancer Registries of S\\~ao\nPaulo, we evaluated Random Survival Forest (RSF), Gradient Boosting for\nSurvival Analysis (GBSA), Survival SVM (SSVM), XGBoost-Cox (XGB-Cox),\nXGBoost-AFT (XGB-AFT), and LightGBM (LGBM), capable of predicting survival\nconsidering censored data. Hyperparameter optimization was performed with\ndifferent samplers, and model performance was assessed using the Concordance\nIndex (C-Index), C-Index IPCW, time-dependent AUC, and Integrated Brier Score\n(IBS). Survival curves produced by the models were compared with predictions\nfrom classification algorithms, and predictor interpretation was conducted\nusing SHAP and permutation importance. XGB-AFT achieved the best performance\n(C-Index = 0.7618; IPCW = 0.7532), followed by GBSA and RSF. The results\nhighlight the potential and applicability of MLSA to improve survival\nprediction and support decision making.\n","authors":["Lucas Buk Cardoso","Simone Aldrey Angelo","Yasmin Pacheco Gil Bonilha","Fernando Maia","Adeylson Guimarães Ribeiro","Maria Paula Curado","Gisele Aparecida Fernandes","Vanderlei Cunha Parro","Flávio Almeida de Magalhães Cipparrone","Alexandre Dias Porto Chiavegatto Filho","Tatiana Natasha Toporcov"],"pdf_url":"https://arxiv.org/pdf/2510.24473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.20039v3","updated":"2025-10-28T14:35:32Z","published":"2025-04-28T17:59:28Z","title":"AutoJudge: Judge Decoding Without Manual Annotation","summary":"  We introduce AutoJudge, a method that accelerates large language model (LLM)\ninference with task-specific lossy speculative decoding. Instead of matching\nthe original model output distribution token-by-token, we identify which of the\ngenerated tokens affect the downstream quality of the response, relaxing the\ndistribution match guarantee so that the \"unimportant\" tokens can be generated\nfaster. Our approach relies on a semi-greedy search algorithm to test which of\nthe mismatches between target and draft models should be corrected to preserve\nquality and which ones may be skipped. We then train a lightweight classifier\nbased on existing LLM embeddings to predict, at inference time, which\nmismatching tokens can be safely accepted without compromising the final answer\nquality. We evaluate the effectiveness of AutoJudge with multiple draft/target\nmodel pairs on mathematical reasoning and programming benchmarks, achieving\nsignificant speedups at the cost of a minor accuracy reduction. Notably, on\nGSM8k with the Llama 3.1 70B target model, our approach achieves up to\n$\\approx2\\times$ speedup over speculative decoding at the cost of $\\le 1\\%$\ndrop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge\nautomatically detects programming-specific important tokens, accepting $\\ge 25$\ntokens per speculation cycle at $2\\%$ drop in Pass@1. Our approach requires no\nhuman annotation and is easy to integrate with modern LLM inference frameworks.\n","authors":["Roman Garipov","Fedor Velikonivtsev","Ivan Ermakov","Ruslan Svirschevski","Vage Egiazarian","Max Ryabinin"],"pdf_url":"https://arxiv.org/pdf/2504.20039v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24466v1","updated":"2025-10-28T14:34:33Z","published":"2025-10-28T14:34:33Z","title":"Non-Singularity of the Gradient Descent map for Neural Networks with\n  Piecewise Analytic Activations","summary":"  The theory of training deep networks has become a central question of modern\nmachine learning and has inspired many practical advancements. In particular,\nthe gradient descent (GD) optimization algorithm has been extensively studied\nin recent years. A key assumption about GD has appeared in several recent\nworks: the \\emph{GD map is non-singular} -- it preserves sets of measure zero\nunder preimages. Crucially, this assumption has been used to prove that GD\navoids saddle points and maxima, and to establish the existence of a computable\nquantity that determines the convergence to global minima (both for GD and\nstochastic GD). However, the current literature either assumes the\nnon-singularity of the GD map or imposes restrictive assumptions, such as\nLipschitz smoothness of the loss (for example, Lipschitzness does not hold for\ndeep ReLU networks with the cross-entropy loss) and restricts the analysis to\nGD with small step-sizes. In this paper, we investigate the neural network map\nas a function on the space of weights and biases. We also prove, for the first\ntime, the non-singularity of the gradient descent (GD) map on the loss\nlandscape of realistic neural network architectures (with fully connected,\nconvolutional, or softmax attention layers) and piecewise analytic activations\n(which includes sigmoid, ReLU, leaky ReLU, etc.) for almost all step-sizes. Our\nwork significantly extends the existing results on the convergence of GD and\nSGD by guaranteeing that they apply to practical neural network settings and\nhas the potential to unlock further exploration of learning dynamics.\n","authors":["Alexandru Crăciun","Debarghya Ghoshdastidar"],"pdf_url":"https://arxiv.org/pdf/2510.24466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22911v2","updated":"2025-10-28T14:33:37Z","published":"2025-10-27T01:28:57Z","title":"Towards Personalized Treatment Plan: Geometrical Model-Agnostic Approach\n  to Counterfactual Explanations","summary":"  In our article, we describe a method for generating counterfactual\nexplanations in high-dimensional spaces using four steps that involve fitting\nour dataset to a model, finding the decision boundary, determining constraints\non the problem, and computing the closest point (counterfactual explanation)\nfrom that boundary. We propose a discretized approach where we find many\ndiscrete points on the boundary and then identify the closest feasible\ncounterfactual explanation. This method, which we later call $\\textit{Segmented\nSampling for Boundary Approximation}$ (SSBA), applies binary search to find\ndecision boundary points and then searches for the closest boundary point.\nAcross four datasets of varying dimensionality, we show that our method can\noutperform current methods for counterfactual generation with reductions in\ndistance between $5\\%$ to $50\\%$ in terms of the $L_2$ norm. Our method can\nalso handle real-world constraints by restricting changes to immutable and\ncategorical features, such as age, gender, sex, height, and other related\ncharacteristics such as the case for a health-based dataset. In terms of\nruntime, the SSBA algorithm generates decision boundary points on multiple\norders of magnitude in the same given time when we compare to a grid-based\napproach. In general, our method provides a simple and effective model-agnostic\nmethod that can compute nearest feasible (i.e. realistic with constraints)\ncounterfactual explanations. All of our results and code are available at:\nhttps://github.com/dsin85691/SSBA_For_Counterfactuals\n","authors":["Daniel Sin","Milad Toutounchian"],"pdf_url":"https://arxiv.org/pdf/2510.22911v2.pdf","comment":"This paper is 15 pages long consisting of multiple sections including\n  an abstract, introduction, related works, methodology, results, ablation\n  studies, conclusion, future works, and an appendix section. There are 10\n  figures and 5 tables in total"},{"id":"http://arxiv.org/abs/2508.20072v2","updated":"2025-10-28T14:22:20Z","published":"2025-08-27T17:39:11Z","title":"Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding\n  in Vision-Language-Action Policies","summary":"  Vision-Language-Action (VLA) models adapt large vision-language backbones to\nmap images and instructions into robot actions. However, prevailing VLAs either\ngenerate actions auto-regressively in a fixed left-to-right order or attach\nseparate MLP or diffusion heads outside the backbone, leading to fragmented\ninformation pathways and specialized training requirements that hinder a\nunified, scalable architecture. We present Discrete Diffusion VLA, a\nunified-transformer policy that models discretized action chunks with discrete\ndiffusion. The design retains diffusion's progressive refinement paradigm while\nremaining natively compatible with the discrete token interface of VLMs. Our\nmethod achieves an adaptive decoding order that resolves easy action elements\nbefore harder ones and uses secondary re-masking to revisit uncertain\npredictions across refinement rounds, which improves consistency and enables\nrobust error correction. This unified decoder preserves pre-trained\nvision-language priors, supports parallel decoding, breaks the autoregressive\nbottleneck, and reduces the number of function evaluations. Discrete Diffusion\nVLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on\nSimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge, improving over\nautoregressive, MLP decoder and continuous diffusion baselines. These findings\nindicate that discrete-diffusion VLA supports precise action modeling and\nconsistent training, laying groundwork for scaling VLA to larger models and\ndatasets. Our project page is https://github.com/Liang-ZX/DiscreteDiffusionVLA\n","authors":["Zhixuan Liang","Yizhuo Li","Tianshuo Yang","Chengyue Wu","Sitong Mao","Tian Nian","Liuao Pei","Shunbo Zhou","Xiaokang Yang","Jiangmiao Pang","Yao Mu","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2508.20072v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2504.06923v4","updated":"2025-10-28T14:21:34Z","published":"2025-04-09T14:30:30Z","title":"The Importance of Being Discrete: Measuring the Impact of Discretization\n  in End-to-End Differentially Private Synthetic Data","summary":"  Differentially Private (DP) generative marginal models are often used in the\nwild to release synthetic tabular datasets in lieu of sensitive data while\nproviding formal privacy guarantees. These models approximate low-dimensional\nmarginals or query workloads; crucially, they require the training data to be\npre-discretized, i.e., continuous values need to first be partitioned into\nbins. However, as the range of values (or their domain) is often inferred\ndirectly from the training data, with the number of bins and bin edges\ntypically defined arbitrarily, this approach can ultimately break end-to-end DP\nguarantees and may not always yield optimal utility.\n  In this paper, we present an extensive measurement study of four\ndiscretization strategies in the context of DP marginal generative models. More\nprecisely, we design DP versions of three discretizers (uniform, quantile, and\nk-means) and reimplement the PrivTree algorithm. We find that optimizing both\nthe choice of discretizer and bin count can improve utility, on average, by\nalmost 30% across six DP marginal models, compared to the default strategy and\nnumber of bins, with PrivTree being the best-performing discretizer in the\nmajority of cases. We demonstrate that, while DP generative models with\nnon-private discretization remain vulnerable to membership inference attacks,\napplying DP during discretization effectively mitigates this risk. Finally, we\nimprove on an existing approach for automatically selecting the optimal number\nof bins, and achieve high utility while reducing both privacy budget\nconsumption and computational overhead.\n","authors":["Georgi Ganev","Meenatchi Sundaram Muthu Selva Annamalai","Sofiane Mahiou","Emiliano De Cristofaro"],"pdf_url":"https://arxiv.org/pdf/2504.06923v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24452v1","updated":"2025-10-28T14:18:50Z","published":"2025-10-28T14:18:50Z","title":"ARIMA_PLUS: Large-scale, Accurate, Automatic and Interpretable\n  In-Database Time Series Forecasting and Anomaly Detection in Google BigQuery","summary":"  Time series forecasting and anomaly detection are common tasks for\npractitioners in industries such as retail, manufacturing, advertising and\nenergy. Two unique challenges stand out: (1) efficiently and accurately\nforecasting time series or detecting anomalies in large volumes automatically;\nand (2) ensuring interpretability of results to effectively incorporate\nbusiness insights. We present ARIMA_PLUS, a novel framework to overcome these\ntwo challenges by a unique combination of (a) accurate and interpretable time\nseries models and (b) scalable and fully managed system infrastructure. The\nmodel has a sequential and modular structure to handle different components of\nthe time series, including holiday effects, seasonality, trend, and anomalies,\nwhich enables high interpretability of the results. Novel enhancements are made\nto each module, and a unified framework is established to address both\nforecasting and anomaly detection tasks simultaneously. In terms of accuracy,\nits comprehensive benchmark on the 42 public datasets in the Monash forecasting\nrepository shows superior performance over not only well-established\nstatistical alternatives (such as ETS, ARIMA, TBATS, Prophet) but also newer\nneural network models (such as DeepAR, N-BEATS, PatchTST, TimeMixer). In terms\nof infrastructure, it is directly built into the query engine of BigQuery in\nGoogle Cloud. It uses a simple SQL interface and automates tedious\ntechnicalities such as data cleaning and model selection. It automatically\nscales with managed cloud computational and storage resources, making it\npossible to forecast 100 million time series using only 1.5 hours with a\nthroughput of more than 18000 time series per second. In terms of\ninterpretability, we present several case studies to demonstrate time series\ninsights it generates and customizability it offers.\n","authors":["Xi Cheng","Weijie Shen","Haoming Chen","Chaoyi Shen","Jean Ortega","Jiashang Liu","Steve Thomas","Honglin Zheng","Haoyun Wu","Yuxiang Li","Casey Lichtendahl","Jenny Ortiz","Gang Liu","Haiyang Qi","Omid Fatemieh","Chris Fry","Jing Jing Long"],"pdf_url":"https://arxiv.org/pdf/2510.24452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.16175v2","updated":"2025-10-28T14:06:41Z","published":"2025-10-17T19:35:54Z","title":"The Formalism-Implementation Gap in Reinforcement Learning Research","summary":"  The last decade has seen an upswing in interest and adoption of reinforcement\nlearning (RL) techniques, in large part due to its demonstrated capabilities at\nperforming certain tasks at \"super-human levels\". This has incentivized the\ncommunity to prioritize research that demonstrates RL agent performance, often\nat the expense of research aimed at understanding their learning dynamics.\nPerformance-focused research runs the risk of overfitting on academic\nbenchmarks -- thereby rendering them less useful -- which can make it difficult\nto transfer proposed techniques to novel problems. Further, it implicitly\ndiminishes work that does not push the performance-frontier, but aims at\nimproving our understanding of these techniques. This paper argues two points:\n(i) RL research should stop focusing solely on demonstrating agent\ncapabilities, and focus more on advancing the science and understanding of\nreinforcement learning; and (ii) we need to be more precise on how our\nbenchmarks map to the underlying mathematical formalisms. We use the popular\nArcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a\nbenchmark that, despite being increasingly considered \"saturated\", can be\neffectively used for developing this understanding, and facilitating the\ndeployment of RL techniques in impactful real-world problems.\n","authors":["Pablo Samuel Castro"],"pdf_url":"https://arxiv.org/pdf/2510.16175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24433v1","updated":"2025-10-28T14:01:51Z","published":"2025-10-28T14:01:51Z","title":"Nearest Neighbor Matching as Least Squares Density Ratio Estimation and\n  Riesz Regression","summary":"  This study proves that Nearest Neighbor (NN) matching can be interpreted as\nan instance of Riesz regression for automatic debiased machine learning. Lin et\nal. (2023) shows that NN matching is an instance of density-ratio estimation\nwith their new density-ratio estimator. Chernozhukov et al. (2024) develops\nRiesz regression for automatic debiased machine learning, which directly\nestimates the Riesz representer (or equivalently, the bias-correction term) by\nminimizing the mean squared error. In this study, we first prove that the\ndensity-ratio estimation method proposed in Lin et al. (2023) is essentially\nequivalent to Least-Squares Importance Fitting (LSIF) proposed in Kanamori et\nal. (2009) for direct density-ratio estimation. Furthermore, we derive Riesz\nregression using the LSIF framework. Based on these results, we derive NN\nmatching from Riesz regression. This study is based on our work Kato (2025a)\nand Kato (2025b).\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2510.24433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24432v1","updated":"2025-10-28T14:01:13Z","published":"2025-10-28T14:01:13Z","title":"Fill in the Blanks: Accelerating Q-Learning with a Handful of\n  Demonstrations in Sparse Reward Settings","summary":"  Reinforcement learning (RL) in sparse-reward environments remains a\nsignificant challenge due to the lack of informative feedback. We propose a\nsimple yet effective method that uses a small number of successful\ndemonstrations to initialize the value function of an RL agent. By precomputing\nvalue estimates from offline demonstrations and using them as targets for early\nlearning, our approach provides the agent with a useful prior over promising\nactions. The agent then refines these estimates through standard online\ninteraction. This hybrid offline-to-online paradigm significantly reduces the\nexploration burden and improves sample efficiency in sparse-reward settings.\nExperiments on benchmark tasks demonstrate that our method accelerates\nconvergence and outperforms standard baselines, even with minimal or suboptimal\ndemonstration data.\n","authors":["Seyed Mahdi Basiri Azad","Joschka Boedecker"],"pdf_url":"https://arxiv.org/pdf/2510.24432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17638v2","updated":"2025-10-28T13:54:07Z","published":"2025-05-23T08:58:47Z","title":"Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical\n  Regularization in Training","summary":"  Diffusion models have achieved remarkable success across a wide range of\ngenerative tasks. A key challenge is understanding the mechanisms that prevent\ntheir memorization of training data and allow generalization. In this work, we\ninvestigate the role of the training dynamics in the transition from\ngeneralization to memorization. Through extensive experiments and theoretical\nanalysis, we identify two distinct timescales: an early time\n$\\tau_\\mathrm{gen}$ at which models begin to generate high-quality samples, and\na later time $\\tau_\\mathrm{mem}$ beyond which memorization emerges. Crucially,\nwe find that $\\tau_\\mathrm{mem}$ increases linearly with the training set size\n$n$, while $\\tau_\\mathrm{gen}$ remains constant. This creates a growing window\nof training times with $n$ where models generalize effectively, despite showing\nstrong memorization if training continues beyond it. It is only when $n$\nbecomes larger than a model-dependent threshold that overfitting disappears at\ninfinite training times. These findings reveal a form of implicit dynamical\nregularization in the training dynamics, which allow to avoid memorization even\nin highly overparameterized settings. Our results are supported by numerical\nexperiments with standard U-Net architectures on realistic and synthetic\ndatasets, and by a theoretical analysis using a tractable random features model\nstudied in the high-dimensional limit.\n","authors":["Tony Bonnaire","Raphaël Urfin","Giulio Biroli","Marc Mézard"],"pdf_url":"https://arxiv.org/pdf/2505.17638v2.pdf","comment":"Accepted as an oral at Neurips 2025. 40 pages, 15 figures"},{"id":"http://arxiv.org/abs/2412.10856v4","updated":"2025-10-28T13:45:25Z","published":"2024-12-14T15:11:07Z","title":"RWKV-edge: Deeply Compressed RWKV for Resource-Constrained Devices","summary":"  To deploy LLMs on resource-contained platforms such as mobile robots and\nsmartphones, non-transformers LLMs have achieved major breakthroughs. Recently,\na novel RNN-based LLM family, Repentance Weighted Key Value (RWKV) has shown\nstrong computational efficiency; nevertheless, RWKV models still have high\nparameter counts which limited their deployment. In this paper, we propose a\nsuite of compression techniques, ranging from model architecture optimizations\nto post-training compression, tailored to the RWKV architecture. Combined, our\ntechniques reduce the memory footprint of RWKV models by 3.4x -- 5x with only\nnegligible degradation in accuracy; compared to transformer LLMs with similar\naccuracy, our models require 4x less memory footprint.\n","authors":["Wonkyo Choe","Yangfeng Ji","Felix Xiaozhu Lin"],"pdf_url":"https://arxiv.org/pdf/2412.10856v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24422v1","updated":"2025-10-28T13:43:00Z","published":"2025-10-28T13:43:00Z","title":"Attack on a PUF-based Secure Binary Neural Network","summary":"  Binarized Neural Networks (BNNs) deployed on memristive crossbar arrays\nprovide energy-efficient solutions for edge computing but are susceptible to\nphysical attacks due to memristor nonvolatility. Recently, Rajendran et al.\n(IEEE Embedded Systems Letter 2025) proposed a Physical Unclonable Function\n(PUF)-based scheme to secure BNNs against theft attacks. Specifically, the\nweight and bias matrices of the BNN layers were secured by swapping columns\nbased on device's PUF key bits.\n  In this paper, we demonstrate that this scheme to secure BNNs is vulnerable\nto PUF-key recovery attack. As a consequence of our attack, we recover the\nsecret weight and bias matrices of the BNN. Our approach is motivated by\ndifferential cryptanalysis and reconstructs the PUF key bit-by-bit by observing\nthe change in model accuracy, and eventually recovering the BNN model\nparameters. Evaluated on a BNN trained on the MNIST dataset, our attack could\nrecover 85% of the PUF key, and recover the BNN model up to 93% classification\naccuracy compared to the original model's 96% accuracy. Our attack is very\nefficient and it takes a couple of minutes to recovery the PUF key and the\nmodel parameters.\n","authors":["Bijeet Basak","Nupur Patil","Kurian Polachan","Srinivas Vivek"],"pdf_url":"https://arxiv.org/pdf/2510.24422v1.pdf","comment":"Accepted at VLSID 2026. To be published in IEEE Xplore"},{"id":"http://arxiv.org/abs/2504.07297v2","updated":"2025-10-28T13:27:06Z","published":"2025-04-09T21:40:15Z","title":"Data Fusion of Deep Learned Molecular Embeddings for Property Prediction","summary":"  Data-driven approaches such as deep learning can result in predictive models\nfor material properties with exceptional accuracy and efficiency. However, in\nmany applications, data is sparse, severely limiting their accuracy and\napplicability. To improve predictions, techniques such as transfer learning and\nmultitask learning have been used. The performance of multitask learning models\ndepends on the strength of the underlying correlations between tasks and the\ncompleteness of the data set. Standard multitask models tend to underperform\nwhen trained on sparse data sets with weakly correlated properties. To address\nthis gap, we fuse deep-learned embeddings generated by independent pretrained\nsingle-task models, resulting in a multitask model that inherits rich,\nproperty-specific representations. By reusing (rather than retraining) these\nembeddings, the resulting fused model outperforms standard multitask models and\ncan be extended with fewer trainable parameters. We demonstrate this technique\non a widely used benchmark data set of quantum chemistry data for small\nmolecules as well as a newly compiled sparse data set of experimental data\ncollected from literature and our own quantum chemistry and thermochemical\ncalculations.\n","authors":["Robert J Appleton","Brian C Barnes","Alejandro Strachan"],"pdf_url":"https://arxiv.org/pdf/2504.07297v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06328v5","updated":"2025-10-28T13:10:33Z","published":"2023-10-10T05:54:00Z","title":"UniCrossFi: A Unified Framework For Cross-Domain Wi-Fi-based Gesture\n  Recognition","summary":"  Wi-Fi sensing systems are severely hindered by cross domain problem when\ndeployed in unseen real-world environments. Existing methods typically design\nseparate frameworks for either domain adaptation or domain generalization,\noften relying on extensive labeled data. Existing methods that designed for\ndomain generalization is often relying on extensive labeled data. However,\nreal-world scenarios are far more complex, where the deployed model must be\ncapable of handling generalization under limited labeled source data. To this\nend, we propose UniCrossFi, a unified framework designed to mitigate\nperformance drop in CSI-based sensing across diverse deployment settings. Our\nframework not only extends conventional Domain Generalization (DG) to a more\npractical Semi-Supervised Domain Generalization (SSDG) setting, where only\npartially labeled source data are available, but also introduces a\nphysics-informed data augmentation strategy, Antenna Response Consistency\n(ARC). ARC mitigates the risk of learning superficial shortcuts by exploiting\nthe intrinsic spatial diversity of multi-antenna systems, treating signals from\ndifferent antennas as naturally augmented views of the same event. In addition,\nwe design a Unified Contrastive Objective to prevent conventional contrastive\nlearning from pushing apart samples from different domains that share the same\nclass. We conduct extensive experiments on the public Widar and CSIDA datasets.\nThe results demonstrate that UniCrossFi consistently establishes a new\nstate-of-the-art, significantly outperforming existing methods across all\nunsupervised domain adaptation, DG, and SSDG benchmarks. UniCrossFi provides a\nprincipled and practical solution to the domain shift challenge, advancing the\nfeasibility of robust, real-world Wi-Fi sensing systems that can operate\neffectively with limited labeled data.\n","authors":["Ke Xu","Zhiyong Zheng","Hongyuan Zhu","Lei Wang","Jiangtao Wang"],"pdf_url":"https://arxiv.org/pdf/2310.06328v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24380v1","updated":"2025-10-28T12:57:59Z","published":"2025-10-28T12:57:59Z","title":"APEX: Approximate-but-exhaustive search for ultra-large combinatorial\n  synthesis libraries","summary":"  Make-on-demand combinatorial synthesis libraries (CSLs) like Enamine REAL\nhave significantly enabled drug discovery efforts. However, their large size\npresents a challenge for virtual screening, where the goal is to identify the\ntop compounds in a library according to a computational objective (e.g.,\noptimizing docking score) subject to computational constraints under a limited\ncomputational budget. For current library sizes -- numbering in the tens of\nbillions of compounds -- and scoring functions of interest, a routine virtual\nscreening campaign may be limited to scoring fewer than 0.1% of the available\ncompounds, leaving potentially many high scoring compounds undiscovered.\nFurthermore, as constraints (and sometimes objectives) change during the course\nof a virtual screening campaign, existing virtual screening algorithms\ntypically offer little room for amortization. We propose the\napproximate-but-exhaustive search protocol for CSLs, or APEX. APEX utilizes a\nneural network surrogate that exploits the structure of CSLs in the prediction\nof objectives and constraints to make full enumeration on a consumer GPU\npossible in under a minute, allowing for exact retrieval of approximate top-$k$\nsets. To demonstrate APEX's capabilities, we develop a benchmark CSL comprised\nof more than 10 million compounds, all of which have been annotated with their\ndocking scores on five medically relevant targets along with physicohemical\nproperties measured with RDKit such that, for any objective and set of\nconstraints, the ground truth top-$k$ compounds can be identified and compared\nagainst the retrievals from any virtual screening algorithm. We show APEX's\nconsistently strong performance both in retrieval accuracy and runtime compared\nto alternative methods.\n","authors":["Aryan Pedawi","Jordi Silvestre-Ryan","Bradley Worley","Darren J Hsu","Kushal S Shah","Elias Stehle","Jingrong Zhang","Izhar Wallach"],"pdf_url":"https://arxiv.org/pdf/2510.24380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.20641v4","updated":"2025-10-28T12:57:51Z","published":"2025-06-25T17:37:21Z","title":"Telegrapher's Generative Model via Kac Flows","summary":"  We break the mold in flow-based generative modeling by proposing a new model\nbased on the damped wave equation, also known as telegrapher's equation.\nSimilar to the diffusion equation and Brownian motion, there is a Feynman-Kac\ntype relation between the telegrapher's equation and the stochastic Kac process\nin 1D. The Kac flow evolves stepwise linearly in time, so that the probability\nflow is Lipschitz continuous in the Wasserstein distance and, in contrast to\ndiffusion flows, the norm of the velocity is globally bounded. Furthermore, the\nKac model has the diffusion model as its asymptotic limit. We extend these\nconsiderations to a multi-dimensional stochastic process which consists of\nindependent 1D Kac processes in each spatial component. We show that this\nprocess gives rise to an absolutely continuous curve in the Wasserstein space\nand compute the conditional velocity field starting in a Dirac point\nanalytically. Using the framework of flow matching, we train a neural network\nthat approximates the velocity field and use it for sample generation. Our\nnumerical experiments demonstrate the scalability of our approach, and show its\nadvantages over diffusion models.\n","authors":["Richard Duong","Jannis Chemseddine","Peter K. Friz","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2506.20641v4.pdf","comment":"Update V2: We added CIFAR experiments. Update V3: The old FID scores\n  & CIFAR images of the Kac model corresponded to the schedule g(t) = t. We now\n  updated them with both schedules t and t^2. Update V4: We corrected a minor\n  implementation error and updated the CIFAR images/table"},{"id":"http://arxiv.org/abs/2502.17500v2","updated":"2025-10-28T12:53:44Z","published":"2025-02-21T11:05:04Z","title":"Generalized Exponentiated Gradient Algorithms Using the Euler\n  Two-Parameter Logarithm","summary":"  IIn this paper we propose and investigate a new class of Generalized\nExponentiated Gradient (GEG) algorithms using Mirror Descent (MD) updates, and\napplying the Bregman divergence with a two--parameter\n  deformation of the logarithm as a link function. This link function (referred\nhere to as the Euler logarithm) is associated with a relatively wide class of\ntrace--form entropies. In order to derive novel GEG/MD updates, we estimate a\ndeformed exponential function, which closely approximates the inverse of the\nEuler two--parameter deformed logarithm. The characteristic shape and\nproperties of the Euler logarithm and its inverse--deformed exponential\nfunctions, are tuned by two hyperparameters. By learning these hyperparameters,\nwe can adapt to the distribution of training data and adjust them to achieve\ndesired properties of gradient descent algorithms. In the literature, there\nexist nowadays more than fifty mathematically well-established entropic\nfunctionals and associated deformed logarithms, so it is impossible to\ninvestigate all of them in one research paper. Therefore, we focus here on a\nclass of trace-form entropies and the associated deformed two--parameters\nlogarithms.\n","authors":["Andrzej Cichocki"],"pdf_url":"https://arxiv.org/pdf/2502.17500v2.pdf","comment":"10 pages, preprint of Journal paper"},{"id":"http://arxiv.org/abs/2510.24375v1","updated":"2025-10-28T12:52:47Z","published":"2025-10-28T12:52:47Z","title":"A Comprehensive Evaluation Framework for Synthetic Trip Data Generation\n  in Public Transport","summary":"  Synthetic data offers a promising solution to the privacy and accessibility\nchallenges of using smart card data in public transport research. Despite rapid\nprogress in generative modeling, there is limited attention to comprehensive\nevaluation, leaving unclear how reliable, safe, and useful synthetic data truly\nare. Existing evaluations remain fragmented, typically limited to\npopulation-level representativeness or record-level privacy, without\nconsidering group-level variations or task-specific utility. To address this\ngap, we propose a Representativeness-Privacy-Utility (RPU) framework that\nsystematically evaluates synthetic trip data across three complementary\ndimensions and three hierarchical levels (record, group, population). The\nframework integrates a consistent set of metrics to quantify similarity,\ndisclosure risk, and practical usefulness, enabling transparent and balanced\nassessment of synthetic data quality. We apply the framework to benchmark\ntwelve representative generation methods, spanning conventional statistical\nmodels, deep generative networks, and privacy-enhanced variants. Results show\nthat synthetic data do not inherently guarantee privacy and there is no\n\"one-size-fits-all\" model, the trade-off between privacy and\nrepresentativeness/utility is obvious. Conditional Tabular generative\nadversarial network (CTGAN) provide the most balanced trade-off and is\nsuggested for practical applications. The RPU framework provides a systematic\nand reproducible basis for researchers and practitioners to compare synthetic\ndata generation techniques and select appropriate methods in public transport\napplications.\n","authors":["Yuanyuan Wu","Zhenlin Qin","Zhenliang Ma"],"pdf_url":"https://arxiv.org/pdf/2510.24375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01143v2","updated":"2025-10-28T12:49:08Z","published":"2025-06-01T19:55:31Z","title":"Linear regression with overparameterized linear neural networks: Tight\n  upper and lower bounds for implicit $\\ell^1$-regularization","summary":"  Modern machine learning models are often trained in a setting where the\nnumber of parameters exceeds the number of training samples. To understand the\nimplicit bias of gradient descent in such overparameterized models, prior work\nhas studied diagonal linear neural networks in the regression setting. These\nstudies have shown that, when initialized with small weights, gradient descent\ntends to favor solutions with minimal $\\ell^1$-norm - an effect known as\nimplicit regularization. In this paper, we investigate implicit regularization\nin diagonal linear neural networks of depth $D\\ge 2$ for overparameterized\nlinear regression problems. We focus on analyzing the approximation error\nbetween the limit point of gradient flow trajectories and the solution to the\n$\\ell^1$-minimization problem. By deriving tight upper and lower bounds on the\napproximation error, we precisely characterize how the approximation error\ndepends on the scale of initialization $\\alpha$. Our results reveal a\nqualitative difference between depths: for $D \\ge 3$, the error decreases\nlinearly with $\\alpha$, whereas for $D=2$, it decreases at rate\n$\\alpha^{1-\\varrho}$, where the parameter $\\varrho \\in [0,1)$ can be explicitly\ncharacterized. Interestingly, this parameter is closely linked to so-called\nnull space property constants studied in the sparse recovery literature. We\ndemonstrate the asymptotic tightness of our bounds through explicit examples.\nNumerical experiments corroborate our theoretical findings and suggest that\ndeeper networks, i.e., $D \\ge 3$, may lead to better generalization,\nparticularly for realistic initialization scales.\n","authors":["Hannes Matt","Dominik Stöger"],"pdf_url":"https://arxiv.org/pdf/2506.01143v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.13397v2","updated":"2025-10-28T12:46:53Z","published":"2025-10-15T10:51:17Z","title":"Assessing the robustness of heterogeneous treatment effects in survival\n  analysis under informative censoring","summary":"  Dropout is common in clinical studies, with up to half of patients leaving\nearly due to side effects or other reasons. When dropout is informative (i.e.,\ndependent on survival time), it introduces censoring bias, because of which\ntreatment effect estimates are also biased. In this paper, we propose an\nassumption-lean framework to assess the robustness of conditional average\ntreatment effect (CATE) estimates in survival analysis when facing censoring\nbias. Unlike existing works that rely on strong assumptions, such as\nnon-informative censoring, to obtain point estimation, we use partial\nidentification to derive informative bounds on the CATE. Thereby, our framework\nhelps to identify patient subgroups where treatment is effective despite\ninformative censoring. We further develop a novel meta-learner that estimates\nthe bounds using arbitrary machine learning models and with favorable\ntheoretical properties, including double robustness and quasi-oracle\nefficiency. We demonstrate the practical value of our meta-learner through\nnumerical experiments and in an application to a cancer drug trial. Together,\nour framework offers a practical tool for assessing the robustness of estimated\ntreatment effects in the presence of censoring and thus promotes the reliable\nuse of survival data for evidence generation in medicine and epidemiology.\n","authors":["Yuxin Wang","Dennis Frauen","Jonas Schweisthal","Maresa Schröder","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2510.13397v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24368v1","updated":"2025-10-28T12:45:20Z","published":"2025-10-28T12:45:20Z","title":"Filtering instances and rejecting predictions to obtain reliable models\n  in healthcare","summary":"  Machine Learning (ML) models are widely used in high-stakes domains such as\nhealthcare, where the reliability of predictions is critical. However, these\nmodels often fail to account for uncertainty, providing predictions even with\nlow confidence. This work proposes a novel two-step data-centric approach to\nenhance the performance of ML models by improving data quality and filtering\nlow-confidence predictions. The first step involves leveraging Instance\nHardness (IH) to filter problematic instances during training, thereby refining\nthe dataset. The second step introduces a confidence-based rejection mechanism\nduring inference, ensuring that only reliable predictions are retained. We\nevaluate our approach using three real-world healthcare datasets, demonstrating\nits effectiveness at improving model reliability while balancing predictive\nperformance and rejection rate. Additionally, we use alternative criteria -\ninfluence values for filtering and uncertainty for rejection - as baselines to\nevaluate the efficiency of the proposed method. The results demonstrate that\nintegrating IH filtering with confidence-based rejection effectively enhances\nmodel performance while preserving a large proportion of instances. This\napproach provides a practical method for deploying ML systems in\nsafety-critical applications.\n","authors":["Maria Gabriela Valeriano","David Kohan Marzagão","Alfredo Montelongo","Carlos Roberto Veiga Kiffer","Natan Katz","Ana Carolina Lorena"],"pdf_url":"https://arxiv.org/pdf/2510.24368v1.pdf","comment":"This paper is under review at Machine Learning (Springer)"},{"id":"http://arxiv.org/abs/2402.10028v3","updated":"2025-10-28T12:23:40Z","published":"2024-02-15T15:48:55Z","title":"Diffusion Models Meet Contextual Bandits","summary":"  Efficient online decision-making in contextual bandits is challenging, as\nmethods without informative priors often suffer from computational or\nstatistical inefficiencies. In this work, we leverage pre-trained diffusion\nmodels as expressive priors to capture complex action dependencies and develop\na practical algorithm that efficiently approximates posteriors under such\npriors, enabling both fast updates and sampling. Empirical results demonstrate\nthe effectiveness and versatility of our approach across diverse contextual\nbandit settings.\n","authors":["Imad Aouali"],"pdf_url":"https://arxiv.org/pdf/2402.10028v3.pdf","comment":"Neurips 2025"},{"id":"http://arxiv.org/abs/2510.24356v1","updated":"2025-10-28T12:19:49Z","published":"2025-10-28T12:19:49Z","title":"Perception Learning: A Formal Separation of Sensory Representation\n  Learning from Decision Learning","summary":"  We introduce Perception Learning (PeL), a paradigm that optimizes an agent's\nsensory interface $f_\\phi:\\mathcal{X}\\to\\mathcal{Z}$ using task-agnostic\nsignals, decoupled from downstream decision learning\n$g_\\theta:\\mathcal{Z}\\to\\mathcal{Y}$. PeL directly targets label-free\nperceptual properties, such as stability to nuisances, informativeness without\ncollapse, and controlled geometry, assessed via objective\nrepresentation-invariant metrics. We formalize the separation of perception and\ndecision, define perceptual properties independent of objectives or\nreparameterizations, and prove that PeL updates preserving sufficient\ninvariants are orthogonal to Bayes task-risk gradients. Additionally, we\nprovide a suite of task-agnostic evaluation metrics to certify perceptual\nquality.\n","authors":["Suman Sanyal"],"pdf_url":"https://arxiv.org/pdf/2510.24356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.24424v2","updated":"2025-10-28T12:08:40Z","published":"2025-05-30T10:04:00Z","title":"Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning","summary":"  Vision-language models like CLIP have demonstrated remarkable zero-shot\ncapabilities in classification and retrieval. However, these models often\nstruggle with compositional reasoning - the ability to understand the\nrelationships between concepts. A recent benchmark, SugarCrepe++, reveals that\nprevious works on improving compositionality have mainly improved lexical\nsensitivity but neglected semantic understanding. In addition, downstream\nretrieval performance often deteriorates, although one would expect that\nimproving compositionality should enhance retrieval. In this work, we introduce\nCLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a\nnovel training technique combining multiple images and their associated\ncaptions. CLIC improves compositionality across architectures as well as\ndifferently pre-trained CLIP models, both in terms of lexical and semantic\nunderstanding, and achieves consistent gains in retrieval performance. This\neven applies to the recent CLIPS, which achieves SOTA retrieval performance.\nNevertheless, the short fine-tuning with CLIC leads to an improvement in\nretrieval and to the best compositional CLIP model on SugarCrepe++. All our\nmodels and code are available at https://clic-compositional-clip.github.io\n","authors":["Amit Peleg","Naman Deep Singh","Matthias Hein"],"pdf_url":"https://arxiv.org/pdf/2505.24424v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2411.19477v5","updated":"2025-10-28T11:59:43Z","published":"2024-11-29T05:29:47Z","title":"Provable Scaling Laws for the Test-Time Compute of Large Language Models","summary":"  We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms.\n","authors":["Yanxi Chen","Xuchen Pan","Yaliang Li","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.19477v5.pdf","comment":"NeurIPS 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2510.20974v2","updated":"2025-10-28T11:58:54Z","published":"2025-10-23T20:06:29Z","title":"Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization","summary":"  Reinforcement Learning (RL) from raw visual input has achieved impressive\nsuccesses in recent years, yet it remains fragile to out-of-distribution\nvariations such as changes in lighting, color, and viewpoint. Point Cloud\nReinforcement Learning (PC-RL) offers a promising alternative by mitigating\nappearance-based brittleness, but its sensitivity to camera pose mismatches\ncontinues to undermine reliability in realistic settings. To address this\nchallenge, we propose PCA Point Cloud (PPC), a canonicalization framework\nspecifically tailored for downstream robotic control. PPC maps point clouds\nunder arbitrary rigid-body transformations to a unique canonical pose, aligning\nobservations to a consistent frame, thereby substantially decreasing\nviewpoint-induced inconsistencies. In our experiments, we show that PPC\nimproves robustness to unseen camera poses across challenging robotic tasks,\nproviding a principled alternative to domain randomization.\n","authors":["Michael Bezick","Vittorio Giammarino","Ahmed H. Qureshi"],"pdf_url":"https://arxiv.org/pdf/2510.20974v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24331v1","updated":"2025-10-28T11:55:24Z","published":"2025-10-28T11:55:24Z","title":"What do vision-language models see in the context? Investigating\n  multimodal in-context learning","summary":"  In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks\nfrom demonstration examples without parameter updates. Although it has been\nextensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)\nremains underexplored. In this work, we present a systematic study of ICL in\nVLMs, evaluating seven models spanning four architectures on three image\ncaptioning benchmarks. We analyze how prompt design, architectural choices, and\ntraining strategies influence multimodal ICL. To our knowledge, we are the\nfirst to analyze how attention patterns in VLMs vary with an increasing number\nof in-context demonstrations. Our results reveal that training on imag-text\ninterleaved data enhances ICL performance but does not imply effective\nintegration of visual and textual information from demonstration examples. In\ncontrast, instruction tuning improves instruction-following but can reduce\nreliance on in-context demonstrations, suggesting a trade-off between\ninstruction alignment and in-context adaptation. Attention analyses further\nshow that current VLMs primarily focus on textual cues and fail to leverage\nvisual information, suggesting a limited capacity for multimodal integration.\nThese findings highlight key limitations in the ICL abilities of current VLMs\nand provide insights for enhancing their ability to learn from multimodal\nin-context examples.\n","authors":["Gabriel O. dos Santos","Esther Colombini","Sandra Avila"],"pdf_url":"https://arxiv.org/pdf/2510.24331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11364v4","updated":"2025-10-28T11:36:51Z","published":"2025-04-15T16:30:02Z","title":"Offline Learning and Forgetting for Reasoning with Large Language Models","summary":"  Leveraging inference-time search in large language models has proven\neffective in further enhancing a trained model's capability to solve complex\nmathematical and reasoning problems. However, this approach significantly\nincreases computational costs and inference time, as the model must generate\nand evaluate multiple candidate solutions to identify a viable reasoning path.\nTo address this, we propose an effective approach that integrates search\ncapabilities directly into the model by fine-tuning it on unpaired successful\n(learning) and failed reasoning paths (forgetting) derived from diverse search\nmethods. A key challenge we identify is that naive fine-tuning can degrade the\nmodel's search capability; we show this can be mitigated with a smaller\nlearning rate. Extensive experiments on the challenging Game-of-24 and\nCountdown arithmetic puzzles show that, replacing CoT-generated data with\nsearch-generated data for offline fine-tuning improves success rates by around\n23% over inference-time search baselines, while reducing inference time by\n180$\\times$. On top of this, our learning and forgetting objective consistently\noutperforms both supervised fine-tuning and preference-based methods.\n","authors":["Tianwei Ni","Allen Nie","Sapana Chaudhary","Yao Liu","Huzefa Rangwala","Rasool Fakoor"],"pdf_url":"https://arxiv.org/pdf/2504.11364v4.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR), 2025.\n  Code: https://github.com/twni2016/llm-reasoning-uft"},{"id":"http://arxiv.org/abs/2510.24318v1","updated":"2025-10-28T11:36:31Z","published":"2025-10-28T11:36:31Z","title":"Transformers can do Bayesian Clustering","summary":"  Bayesian clustering accounts for uncertainty but is computationally demanding\nat scale. Furthermore, real-world datasets often contain missing values, and\nsimple imputation ignores the associated uncertainty, resulting in suboptimal\nresults. We present Cluster-PFN, a Transformer-based model that extends\nPrior-Data Fitted Networks (PFNs) to unsupervised Bayesian clustering. Trained\nentirely on synthetic datasets generated from a finite Gaussian Mixture Model\n(GMM) prior, Cluster-PFN learns to estimate the posterior distribution over\nboth the number of clusters and the cluster assignments. Our method estimates\nthe number of clusters more accurately than handcrafted model selection\nprocedures such as AIC, BIC and Variational Inference (VI), and achieves\nclustering quality competitive with VI while being orders of magnitude faster.\nCluster-PFN can be trained on complex priors that include missing data,\noutperforming imputation-based baselines on real-world genomic datasets, at\nhigh missingness. These results show that the Cluster-PFN can provide scalable\nand flexible Bayesian clustering.\n","authors":["Prajit Bhaskaran","Tom Viering"],"pdf_url":"https://arxiv.org/pdf/2510.24318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.23712v2","updated":"2025-10-28T11:34:23Z","published":"2025-09-28T07:53:41Z","title":"FraudTransformer: Time-Aware GPT for Transaction Fraud Detection","summary":"  Detecting payment fraud in real-world banking streams requires models that\ncan exploit both the order of events and the irregular time gaps between them.\nWe introduce FraudTransformer, a sequence model that augments a vanilla\nGPT-style architecture with (i) a dedicated time encoder that embeds either\nabsolute timestamps or inter-event values, and (ii) a learned positional\nencoder that preserves relative order. Experiments on a large industrial\ndataset -- tens of millions of transactions and auxiliary events -- show that\nFraudTransformer surpasses four strong classical baselines (Logistic\nRegression, XGBoost and LightGBM) as well as transformer ablations that omit\neither the time or positional component. On the held-out test set it delivers\nthe highest AUROC and PRAUC.\n","authors":["Gholamali Aminian","Andrew Elliott","Tiger Li","Timothy Cheuk Hin Wong","Victor Claude Dehon","Lukasz Szpruch","Carsten Maple","Christopher Read","Martin Brown","Gesine Reinert","Mo Mamouei"],"pdf_url":"https://arxiv.org/pdf/2509.23712v2.pdf","comment":"Accepted in AI-FIND ICAIF'25\n  (https://sites.google.com/view/icaif-fraud-detection-workshop/home)"},{"id":"http://arxiv.org/abs/2509.20234v3","updated":"2025-10-28T11:26:53Z","published":"2025-09-24T15:24:43Z","title":"ImageNet-trained CNNs are not biased towards texture: Revisiting feature\n  reliance through controlled suppression","summary":"  The hypothesis that Convolutional Neural Networks (CNNs) are inherently\ntexture-biased has shaped much of the discourse on feature use in deep\nlearning. We revisit this hypothesis by examining limitations in the\ncue-conflict experiment by Geirhos et al. To address these limitations, we\npropose a domain-agnostic framework that quantifies feature reliance through\nsystematic suppression of shape, texture, and color cues, avoiding the\nconfounds of forced-choice conflicts. By evaluating humans and neural networks\nunder controlled suppression conditions, we find that CNNs are not inherently\ntexture-biased but predominantly rely on local shape features. Nonetheless,\nthis reliance can be substantially mitigated through modern training strategies\nor architectures (ConvNeXt, ViTs). We further extend the analysis across\ncomputer vision, medical imaging, and remote sensing, revealing that reliance\npatterns differ systematically: computer vision models prioritize shape,\nmedical imaging models emphasize color, and remote sensing models exhibit a\nstronger reliance on texture. Code is available at\nhttps://github.com/tomburgert/feature-reliance.\n","authors":["Tom Burgert","Oliver Stoll","Paolo Rota","Begüm Demir"],"pdf_url":"https://arxiv.org/pdf/2509.20234v3.pdf","comment":"Accepted at NeurIPS 2025 (oral)"},{"id":"http://arxiv.org/abs/2510.24310v1","updated":"2025-10-28T11:20:06Z","published":"2025-10-28T11:20:06Z","title":"EDC: Equation Discovery for Classification","summary":"  Equation Discovery techniques have shown considerable success in regression\ntasks, where they are used to discover concise and interpretable models\n(\\textit{Symbolic Regression}). In this paper, we propose a new ED-based binary\nclassification framework. Our proposed method EDC finds analytical functions of\nmanageable size that specify the location and shape of the decision boundary.\nIn extensive experiments on artificial and real-life data, we demonstrate how\nEDC is able to discover both the structure of the target equation as well as\nthe value of its parameters, outperforming the current state-of-the-art\nED-based classification methods in binary classification and achieving\nperformance comparable to the state of the art in binary classification. We\nsuggest a grammar of modest complexity that appears to work well on the tested\ndatasets but argue that the exact grammar -- and thus the complexity of the\nmodels -- is configurable, and especially domain-specific expressions can be\nincluded in the pattern language, where that is required. The presented grammar\nconsists of a series of summands (additive terms) that include linear,\nquadratic and exponential terms, as well as products of two features (producing\nhyperbolic curves ideal for capturing XOR-like dependencies). The experiments\ndemonstrate that this grammar allows fairly flexible decision boundaries while\nnot so rich to cause overfitting.\n","authors":["Guus Toussaint","Arno Knobbe"],"pdf_url":"https://arxiv.org/pdf/2510.24310v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Lecture Notes in Computer Science, and is available online at\n  https://doi.org/10.1007/978-3-032-05461-6_9"},{"id":"http://arxiv.org/abs/2510.08146v3","updated":"2025-10-28T10:58:14Z","published":"2025-10-09T12:33:16Z","title":"Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM\n  Reasoning","summary":"  We introduce a simple, yet novel entropy-based framework to drive token\nefficiency in large language models during reasoning tasks. Our approach uses\nShannon entropy from token-level logprobs as a confidence signal to enable\nearly stopping, achieving 25-50% computational savings while maintaining task\naccuracy. Crucially, we demonstrate that entropy-based confidence calibration\nrepresents an emergent property of advanced post-training optimization present\nin modern reasoning models but notably absent in standard instruction-tuned and\npre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop\nreasoning varies from model to model but can be calculated easily in one shot\nusing only a few examples from existing reasoning datasets. Our results\nindicate that advanced reasoning models often know that they've gotten a\ncorrect answer early on, and that this emergent confidence awareness can be\nexploited to save tokens and reduce latency. The framework demonstrates\nconsistent performance across reasoning-optimized model families with 25-50%\ncomputational cost reduction while preserving accuracy, revealing that\nconfidence mechanisms represent a distinguishing characteristic of modern\npost-trained reasoning systems versus their predecessors.\n","authors":["Aman Sharma","Paras Chopra"],"pdf_url":"https://arxiv.org/pdf/2510.08146v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13771v2","updated":"2025-10-28T10:57:14Z","published":"2025-05-30T06:43:03Z","title":"LittleBit: Ultra Low-Bit Quantization via Latent Factorization","summary":"  Deploying large language models (LLMs) often faces challenges from\nsubstantial memory and computational costs. Quantization offers a solution, yet\nperformance degradation in the sub-1-bit regime remains particularly difficult.\nThis paper introduces LittleBit, a novel method for extreme LLM compression. It\ntargets levels like 0.1 bits per weight (BPW), achieving nearly 31$\\times$\nmemory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents\nweights in a low-rank form using latent matrix factorization, subsequently\nbinarizing these factors. To counteract information loss from this extreme\nprecision, it integrates a multi-scale compensation mechanism. This includes\nrow, column, and an additional latent dimension that learns per-rank\nimportance. Two key contributions enable effective training: Dual\nSign-Value-Independent Decomposition (Dual-SVID) for quantization-aware\ntraining (QAT) initialization, and integrated Residual Compensation to mitigate\nerrors. Extensive experiments confirm LittleBit's superiority in sub-1-bit\nquantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading\nmethod's 0.7 BPW. LittleBit establishes a new, viable size-performance\ntrade-off--unlocking a potential 11.6$\\times$ speedup over FP16 at the kernel\nlevel--and makes powerful LLMs practical for resource-constrained environments.\n","authors":["Banseok Lee","Dongkyu Kim","Youngcheon You","Youngmin Kim"],"pdf_url":"https://arxiv.org/pdf/2506.13771v2.pdf","comment":"Accepted to NeurIPS 2025. Banseok Lee and Dongkyu Kim contributed\n  equally"},{"id":"http://arxiv.org/abs/2506.00129v2","updated":"2025-10-28T10:56:55Z","published":"2025-05-30T18:05:33Z","title":"Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware\n  Sign Language Translation","summary":"  Recent progress in Sign Language Translation (SLT) has focussed primarily on\nimproving the representational capacity of large language models to incorporate\nSign Language features. This work explores an alternative direction: enhancing\nthe geometric properties of skeletal representations themselves. We propose\nGeo-Sign, a method that leverages the properties of hyperbolic geometry to\nmodel the hierarchical structure inherent in sign language kinematics. By\nprojecting skeletal features derived from Spatio-Temporal Graph Convolutional\nNetworks (ST-GCNs) into the Poincar\\'e ball model, we aim to create more\ndiscriminative embeddings, particularly for fine-grained motions like finger\narticulations. We introduce a hyperbolic projection layer, a weighted Fr\\'echet\nmean aggregation scheme, and a geometric contrastive loss operating directly in\nhyperbolic space. These components are integrated into an end-to-end\ntranslation framework as a regularisation function, to enhance the\nrepresentations within the language model. This work demonstrates the potential\nof hyperbolic geometry to improve skeletal representations for Sign Language\nTranslation, improving on SOTA RGB methods while preserving privacy and\nimproving computational efficiency. Code available here:\nhttps://github.com/ed-fish/geo-sign.\n","authors":["Edward Fish","Richard Bowden"],"pdf_url":"https://arxiv.org/pdf/2506.00129v2.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.01855v2","updated":"2025-10-28T10:53:56Z","published":"2025-06-02T16:41:49Z","title":"Trade-offs in Data Memorization via Strong Data Processing Inequalities","summary":"  Recent research demonstrated that training large language models involves\nmemorization of a significant fraction of training data. Such memorization can\nlead to privacy violations when training on sensitive user data and thus\nmotivates the study of data memorization's role in learning. In this work, we\ndevelop a general approach for proving lower bounds on excess data\nmemorization, that relies on a new connection between strong data processing\ninequalities and data memorization. We then demonstrate that several simple and\nnatural binary classification problems exhibit a trade-off between the number\nof samples available to a learning algorithm, and the amount of information\nabout the training data that a learning algorithm needs to memorize to be\naccurate. In particular, $\\Omega(d)$ bits of information about the training\ndata need to be memorized when $O(1)$ $d$-dimensional examples are available,\nwhich then decays as the number of examples grows at a problem-specific rate.\nFurther, our lower bounds are generally matched (up to logarithmic factors) by\nsimple learning algorithms. We also extend our lower bounds to more general\nmixture-of-clusters models. Our definitions and results build on the work of\nBrown et al. (2021) and address several limitations of the lower bounds in\ntheir work.\n","authors":["Vitaly Feldman","Guy Kornowski","Xin Lyu"],"pdf_url":"https://arxiv.org/pdf/2506.01855v2.pdf","comment":"Appeared in COLT 2025; this revision includes an improved upper bound\n  in Theorem 3.1, as well as several minor clarifications and modifications"},{"id":"http://arxiv.org/abs/2510.24288v1","updated":"2025-10-28T10:50:04Z","published":"2025-10-28T10:50:04Z","title":"Problem-Parameter-Free Decentralized Bilevel Optimization","summary":"  Decentralized bilevel optimization has garnered significant attention due to\nits critical role in solving large-scale machine learning problems. However,\nexisting methods often rely on prior knowledge of problem parameters-such as\nsmoothness, convexity, or communication network topologies-to determine\nappropriate stepsizes. In practice, these problem parameters are typically\nunavailable, leading to substantial manual effort for hyperparameter tuning. In\nthis paper, we propose AdaSDBO, a fully problem-parameter-free algorithm for\ndecentralized bilevel optimization with a single-loop structure. AdaSDBO\nleverages adaptive stepsizes based on cumulative gradient norms to update all\nvariables simultaneously, dynamically adjusting its progress and eliminating\nthe need for problem-specific hyperparameter tuning. Through rigorous\ntheoretical analysis, we establish that AdaSDBO achieves a convergence rate of\n$\\widetilde{\\mathcal{O}}\\left(\\frac{1}{T}\\right)$, matching the performance of\nwell-tuned state-of-the-art methods up to polylogarithmic factors. Extensive\nnumerical experiments demonstrate that AdaSDBO delivers competitive performance\ncompared to existing decentralized bilevel optimization methods while\nexhibiting remarkable robustness across diverse stepsize configurations.\n","authors":["Zhiwei Zhai","Wenjing Yan","Ying-Jun Angela Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24288v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24287v1","updated":"2025-10-28T10:49:42Z","published":"2025-10-28T10:49:42Z","title":"Towards actionable hypotension prediction- predicting catecholamine\n  therapy initiation in the intensive care unit","summary":"  Hypotension in critically ill ICU patients is common and life-threatening.\nEscalation to catecholamine therapy marks a key management step, with both\nundertreatment and overtreatment posing risks. Most machine learning (ML)\nmodels predict hypotension using fixed MAP thresholds or MAP forecasting,\noverlooking the clinical decision behind treatment escalation. Predicting\ncatecholamine initiation, the start of vasoactive or inotropic agent\nadministration offers a more clinically actionable target reflecting real\ndecision-making. Using the MIMIC-III database, we modeled catecholamine\ninitiation as a binary event within a 15-minute prediction window. Input\nfeatures included statistical descriptors from a two-hour sliding MAP context\nwindow, along with demographics, biometrics, comorbidities, and ongoing\ntreatments. An Extreme Gradient Boosting (XGBoost) model was trained and\ninterpreted via SHapley Additive exPlanations (SHAP). The model achieved an\nAUROC of 0.822 (0.813-0.830), outperforming the hypotension baseline (MAP < 65,\nAUROC 0.686 [0.675-0.699]). SHAP analysis highlighted recent MAP values, MAP\ntrends, and ongoing treatments (e.g., sedatives, electrolytes) as dominant\npredictors. Subgroup analysis showed higher performance in males, younger\npatients (<53 years), those with higher BMI (>32), and patients without\ncomorbidities or concurrent medications. Predicting catecholamine initiation\nbased on MAP dynamics, treatment context, and patient characteristics supports\nthe critical decision of when to escalate therapy, shifting focus from\nthreshold-based alarms to actionable decision support. This approach is\nfeasible across a broad ICU cohort under natural event imbalance. Future work\nshould enrich temporal and physiological context, extend label definitions to\ninclude therapy escalation, and benchmark against existing hypotension\nprediction systems.\n","authors":["Richard Koebe","Noah Saibel","Juan Miguel Lopez Alcaraz","Simon Schäfer","Nils Strodthoff"],"pdf_url":"https://arxiv.org/pdf/2510.24287v1.pdf","comment":"27 pages, 8 figures, source code under\n  https://github.com/AI4HealthUOL/actionable-hypotension"},{"id":"http://arxiv.org/abs/2510.24279v1","updated":"2025-10-28T10:39:10Z","published":"2025-10-28T10:39:10Z","title":"HergNet: a Fast Neural Surrogate Model for Sound Field Predictions via\n  Superposition of Plane Waves","summary":"  We present a novel neural network architecture for the efficient prediction\nof sound fields in two and three dimensions. The network is designed to\nautomatically satisfy the Helmholtz equation, ensuring that the outputs are\nphysically valid. Therefore, the method can effectively learn solutions to\nboundary-value problems in various wave phenomena, such as acoustics, optics,\nand electromagnetism. Numerical experiments show that the proposed strategy can\npotentially outperform state-of-the-art methods in room acoustics simulation,\nin particular in the range of mid to high frequencies.\n","authors":["Matteo Calafà","Yuanxin Xia","Cheol-Ho Jeong"],"pdf_url":"https://arxiv.org/pdf/2510.24279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24273v1","updated":"2025-10-28T10:32:52Z","published":"2025-10-28T10:32:52Z","title":"SALS: Sparse Attention in Latent Space for KV cache Compression","summary":"  Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.\n","authors":["Junlin Mu","Hantao Huang","Jihang Zhang","Minghui Yu","Tao Wang","Yidong Li"],"pdf_url":"https://arxiv.org/pdf/2510.24273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24262v1","updated":"2025-10-28T10:17:11Z","published":"2025-10-28T10:17:11Z","title":"UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level\n  Task Adaptation","summary":"  Data augmentation using generative models has emerged as a powerful paradigm\nfor enhancing performance in computer vision tasks. However, most existing\naugmentation approaches primarily focus on optimizing intrinsic data attributes\n-- such as fidelity and diversity -- to generate visually high-quality\nsynthetic data, while often neglecting task-specific requirements. Yet, it is\nessential for data generators to account for the needs of downstream tasks, as\ntraining data requirements can vary significantly across different tasks and\nnetwork architectures. To address these limitations, we propose UtilGen, a\nnovel utility-centric data augmentation framework that adaptively optimizes the\ndata generation process to produce task-specific, high-utility training data\nvia downstream task feedback. Specifically, we first introduce a weight\nallocation network to evaluate the task-specific utility of each synthetic\nsample. Guided by these evaluations, UtilGen iteratively refines the data\ngeneration process using a dual-level optimization strategy to maximize the\nsynthetic data utility: (1) model-level optimization tailors the generative\nmodel to the downstream task, and (2) instance-level optimization adjusts\ngeneration policies -- such as prompt embeddings and initial noise -- at each\ngeneration round. Extensive experiments on eight benchmark datasets of varying\ncomplexity and granularity demonstrate that UtilGen consistently achieves\nsuperior performance, with an average accuracy improvement of 3.87% over\nprevious SOTA. Further analysis of data influence and distribution reveals that\nUtilGen produces more impactful and task-relevant synthetic data, validating\nthe effectiveness of the paradigm shift from visual characteristics-centric to\ntask utility-centric data augmentation.\n","authors":["Jiyu Guo","Shuo Yang","Yiming Huang","Yancheng Long","Xiaobo Xia","Xiu Su","Bo Zhao","Zeke Xie","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2510.24262v1.pdf","comment":"39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)"},{"id":"http://arxiv.org/abs/2510.24256v1","updated":"2025-10-28T10:09:35Z","published":"2025-10-28T10:09:35Z","title":"From Memorization to Reasoning in the Spectrum of Loss Curvature","summary":"  We characterize how memorization is represented in transformer models and\nshow that it can be disentangled in the weights of both language models (LMs)\nand vision transformers (ViTs) using a decomposition based on the loss\nlandscape curvature. This insight is based on prior theoretical and empirical\nwork showing that the curvature for memorized training points is much sharper\nthan non memorized, meaning ordering weight components from high to low\ncurvature can reveal a distinction without explicit labels. This motivates a\nweight editing procedure that suppresses far more recitation of untargeted\nmemorized data more effectively than a recent unlearning method\n(BalancedSubnet), while maintaining lower perplexity. Since the basis of\ncurvature has a natural interpretation for shared structure in model weights,\nwe analyze the editing procedure extensively on its effect on downstream tasks\nin LMs, and find that fact retrieval and arithmetic are specifically and\nconsistently negatively affected, even though open book fact retrieval and\ngeneral logical reasoning is conserved. We posit these tasks rely heavily on\nspecialized directions in weight space rather than general purpose mechanisms,\nregardless of whether those individual datapoints are memorized. We support\nthis by showing a correspondence between task data's activation strength with\nlow curvature components that we edit out, and the drop in task performance\nafter the edit. Our work enhances the understanding of memorization in neural\nnetworks with practical applications towards removing it, and provides evidence\nfor idiosyncratic, narrowly-used structures involved in solving tasks like math\nand fact retrieval.\n","authors":["Jack Merullo","Srihita Vatsavaya","Lucius Bushnaq","Owen Lewis"],"pdf_url":"https://arxiv.org/pdf/2510.24256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24254v1","updated":"2025-10-28T10:05:34Z","published":"2025-10-28T10:05:34Z","title":"Forecasting precipitation in the Arctic using probabilistic machine\n  learning informed by causal climate drivers","summary":"  Understanding and forecasting precipitation events in the Arctic maritime\nenvironments, such as Bear Island and Ny-{\\AA}lesund, is crucial for assessing\nclimate risk and developing early warning systems in vulnerable marine regions.\nThis study proposes a probabilistic machine learning framework for modeling and\npredicting the dynamics and severity of precipitation. We begin by analyzing\nthe scale-dependent relationships between precipitation and key atmospheric\ndrivers (e.g., temperature, relative humidity, cloud cover, and air pressure)\nusing wavelet coherence, which captures localized dependencies across time and\nfrequency domains. To assess joint causal influences, we employ\nSynergistic-Unique-Redundant Decomposition, which quantifies the impact of\ninteraction effects among each variable on future precipitation dynamics. These\ninsights inform the development of data-driven forecasting models that\nincorporate both historical precipitation and causal climate drivers. To\naccount for uncertainty, we employ the conformal prediction method, which\nenables the generation of calibrated non-parametric prediction intervals. Our\nresults underscore the importance of utilizing a comprehensive framework that\ncombines causal analysis with probabilistic forecasting to enhance the\nreliability and interpretability of precipitation predictions in Arctic marine\nenvironments.\n","authors":["Madhurima Panja","Dhiman Das","Tanujit Chakraborty","Arnob Ray","R. Athulya","Chittaranjan Hens","Syamal K. Dana","Nuncio Murukesh","Dibakar Ghosh"],"pdf_url":"https://arxiv.org/pdf/2510.24254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18195v2","updated":"2025-10-28T10:02:13Z","published":"2025-05-20T09:05:30Z","title":"Acoustic and Machine Learning Methods for Speech-Based Suicide Risk\n  Assessment: A Systematic Review","summary":"  Suicide remains a public health challenge, necessitating improved detection\nmethods to facilitate timely intervention and treatment. This systematic review\nevaluates the role of Artificial Intelligence (AI) and Machine Learning (ML) in\nassessing suicide risk through acoustic analysis of speech. Following PRISMA\nguidelines, we analyzed 33 articles selected from PubMed, Cochrane, Scopus, and\nWeb of Science databases. The last search was conducted in February 2025. Risk\nof bias was assessed using the PROBAST tool. Studies analyzing acoustic\nfeatures between individuals at risk of suicide (RS) and those not at risk\n(NRS) were included, while studies lacking acoustic data, a suicide-related\nfocus, or sufficient methodological details were excluded. Sample sizes varied\nwidely and were reported in terms of participants or speech segments, depending\non the study. Results were synthesized narratively based on acoustic features\nand classifier performance. Findings consistently showed significant acoustic\nfeature variations between RS and NRS populations, particularly involving\njitter, fundamental frequency (F0), Mel-frequency cepstral coefficients (MFCC),\nand power spectral density (PSD). Classifier performance varied based on\nalgorithms, modalities, and speech elicitation methods, with multimodal\napproaches integrating acoustic, linguistic, and metadata features\ndemonstrating superior performance. Among the 29 classifier-based studies,\nreported AUC values ranged from 0.62 to 0.985 and accuracies from 60% to\n99.85%. Most datasets were imbalanced in favor of NRS, and performance metrics\nwere rarely reported separately by group, limiting clear identification of\ndirection of effect.\n","authors":["Ambre Marie","Marine Garnier","Thomas Bertin","Laura Machart","Guillaume Dardenne","Gwenolé Quellec","Sofian Berrouiguet"],"pdf_url":"https://arxiv.org/pdf/2505.18195v2.pdf","comment":"Preprint version of a manuscript submitted to the Journal of\n  Affective Disorders"},{"id":"http://arxiv.org/abs/2507.20362v3","updated":"2025-10-28T09:58:09Z","published":"2025-07-27T17:31:47Z","title":"MH-GIN: Multi-scale Heterogeneous Graph-based Imputation Network for AIS\n  Data (Extended Version)","summary":"  Location-tracking data from the Automatic Identification System, much of\nwhich is publicly available, plays a key role in a range of maritime safety and\nmonitoring applications. However, the data suffers from missing values that\nhamper downstream applications. Imputing the missing values is challenging\nbecause the values of different heterogeneous attributes are updated at diverse\nrates, resulting in the occurrence of multi-scale dependencies among\nattributes. Existing imputation methods that assume similar update rates across\nattributes are unable to capture and exploit such dependencies, limiting their\nimputation accuracy. We propose MH-GIN, a Multi-scale Heterogeneous Graph-based\nImputation Network that aims improve imputation accuracy by capturing\nmulti-scale dependencies. Specifically, MH-GIN first extracts multi-scale\ntemporal features for each attribute while preserving their intrinsic\nheterogeneous characteristics. Then, it constructs a multi-scale heterogeneous\ngraph to explicitly model dependencies between heterogeneous attributes to\nenable more accurate imputation of missing values through graph propagation.\nExperimental results on two real-world datasets find that MH-GIN is capable of\nan average 57% reduction in imputation errors compared to state-of-the-art\nmethods, while maintaining computational efficiency. The source code and\nimplementation details of MH-GIN are publicly available\nhttps://github.com/hyLiu1994/MH-GIN.\n","authors":["Hengyu Liu","Tianyi Li","Yuqiang He","Kristian Torp","Yushuai Li","Christian S. Jensen"],"pdf_url":"https://arxiv.org/pdf/2507.20362v3.pdf","comment":"18 pages, 4 figures; This paper is accepted by PVLDB 2026"},{"id":"http://arxiv.org/abs/2510.23216v2","updated":"2025-10-28T09:50:12Z","published":"2025-10-27T11:06:00Z","title":"Human-Like Goalkeeping in a Realistic Football Simulation: a\n  Sample-Efficient Reinforcement Learning Approach","summary":"  While several high profile video games have served as testbeds for Deep\nReinforcement Learning (DRL), this technique has rarely been employed by the\ngame industry for crafting authentic AI behaviors. Previous research focuses on\ntraining super-human agents with large models, which is impractical for game\nstudios with limited resources aiming for human-like agents. This paper\nproposes a sample-efficient DRL method tailored for training and fine-tuning\nagents in industrial settings such as the video game industry. Our method\nimproves sample efficiency of value-based DRL by leveraging pre-collected data\nand increasing network plasticity. We evaluate our method training a goalkeeper\nagent in EA SPORTS FC 25, one of the best-selling football simulations today.\nOur agent outperforms the game's built-in AI by 10% in ball saving rate.\nAblation studies show that our method trains agents 50% faster compared to\nstandard DRL methods. Finally, qualitative evaluation from domain experts\nindicates that our approach creates more human-like gameplay compared to\nhand-crafted agents. As a testimony of the impact of the approach, the method\nis intended to replace the hand-crafted counterpart in next iterations of the\nseries.\n","authors":["Alessandro Sestini","Joakim Bergdahl","Jean-Philippe Barrette-LaPierre","Florian Fuchs","Brady Chen","Michael Jones","Linus Gisslén"],"pdf_url":"https://arxiv.org/pdf/2510.23216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24242v1","updated":"2025-10-28T09:48:26Z","published":"2025-10-28T09:48:26Z","title":"Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration\n  of Large Vision-Language Models","summary":"  Large vision-language models (LVLMs) have recently demonstrated great\npotential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by\nlow Earth orbit (LEO) satellites. However, their deployment in real-world LEO\nsatellite systems remains largely unexplored, hindered by limited onboard\ncomputing resources and brief satellite-ground contacts. We propose Grace, a\nsatellite-ground collaborative system designed for near-realtime LVLM inference\nin RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime\ninference, but larger ones on ground stations (GSs) to guarantee end-to-end\nperformance. Grace is comprised of two main phases that are asynchronous\nsatellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch\nalgorithm. Firstly, we still the knowledge archive of GS RAG to satellite\narchive with tailored adaptive update algorithm during limited satellite-ground\ndata exchange period. Secondly, propose a confidence-based test algorithm that\neither processes the task onboard the satellite or offloads it to the GS.\nExtensive experiments based on real-world satellite orbital data show that\nGrace reduces the average latency by 76-95% compared to state-of-the-art\nmethods, without compromising inference accuracy.\n","authors":["Zihan Li","Jiahao Yang","Yuxin Zhang","Zhe Chen","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2510.24242v1.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2510.24240v1","updated":"2025-10-28T09:47:38Z","published":"2025-10-28T09:47:38Z","title":"Temporal Knowledge Graph Hyperedge Forecasting: Exploring\n  Entity-to-Category Link Prediction","summary":"  Temporal Knowledge Graphs have emerged as a powerful way of not only modeling\nstatic relationships between entities but also the dynamics of how relations\nevolve over time. As these informational structures can be used to store\ninformation from a real-world setting, such as a news flow, predicting future\ngraph components to a certain extent equates predicting real-world events. Most\nof the research in this field focuses on embedding-based methods, often\nleveraging convolutional neural net architectures. These solutions act as black\nboxes, limiting insight. In this paper, we explore an extension to an\nestablished rule-based framework, TLogic, that yields a high accuracy in\ncombination with explainable predictions. This offers transparency and allows\nthe end-user to critically evaluate the rules applied at the end of the\nprediction stage. The new rule format incorporates entity category as a key\ncomponent with the purpose of limiting rule application only to relevant\nentities. When categories are unknown for building the graph, we propose a\ndata-driven method to generate them with an LLM-based approach. Additionally,\nwe investigate the choice of aggregation method for scores of retrieved\nentities when performing category prediction.\n","authors":["Edward Markai","Sina Molavipour"],"pdf_url":"https://arxiv.org/pdf/2510.24240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24235v1","updated":"2025-10-28T09:43:47Z","published":"2025-10-28T09:43:47Z","title":"PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware\n  Task-Adaptive Reward Modeling","summary":"  Reward models (RMs) are central to reinforcement learning from human feedback\n(RLHF), providing the critical supervision signals that align large language\nmodels (LLMs) with human preferences. While generative reward models (GRMs)\noffer greater interpretability than traditional scalar RMs, current training\nparadigms remain limited. Pair-wise methods rely on binary good-versus-bad\nlabels, which cause mismatches for point-wise inference and necessitate complex\npairing strategies for effective application in RLHF. On the other hand,\npoint-wise methods require more elaborate absolute labeling with rubric-driven\ncriteria, resulting in poor adaptability and high annotation costs. In this\nwork, we propose the Preference-Aware Task-Adaptive Reward Model (PaTaRM), a\nunified framework that integrates a preference-aware reward (PAR) mechanism\nwith dynamic rubric adaptation. PaTaRM leverages relative preference\ninformation from pairwise data to construct robust point-wise training signals,\neliminating the need for explicit point-wise labels. Simultaneously, it employs\na task-adaptive rubric system that flexibly generates evaluation criteria for\nboth global task consistency and instance-specific fine-grained reasoning. This\ndesign enables efficient, generalizable, and interpretable reward modeling for\nRLHF. Extensive experiments show that PaTaRM achieves an average relative\nimprovement of 4.7% on RewardBench and RMBench across Qwen3-8B and Qwen3-14B\nmodels. Furthermore, PaTaRM boosts downstream RLHF performance, with an average\nimprovement of 13.6% across IFEval and InFoBench benchmarks, confirming its\neffectiveness and robustness. Our code is available at\nhttps://github.com/JaneEyre0530/PaTaRM.\n","authors":["Ai Jian","Jingqing Ruan","Xing Ma","Dailin Li","QianLin Zhou","Ke Zeng","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2510.24235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11930v2","updated":"2025-10-28T09:43:41Z","published":"2025-05-17T09:34:57Z","title":"The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product\n  Logics","summary":"  In recent years, the expressive power of various neural architectures --\nincluding graph neural networks (GNNs), transformers, and recurrent neural\nnetworks -- has been characterised using tools from logic and formal language\ntheory. As the capabilities of basic architectures are becoming well\nunderstood, increasing attention is turning to models that combine multiple\narchitectural paradigms. Among them particularly important, and challenging to\nanalyse, are temporal extensions of GNNs, which integrate both spatial\n(graph-structure) and temporal (evolution over time) dimensions. In this paper,\nwe initiate the study of logical characterisation of temporal GNNs by\nconnecting them to two-dimensional product logics. We show that the expressive\npower of temporal GNNs depends on how graph and temporal components are\ncombined. In particular, temporal GNNs that apply static GNNs recursively over\ntime can capture all properties definable in the product logic of (past)\npropositional temporal logic PTL and the modal logic K. In contrast,\narchitectures such as graph-and-time TGNNs and global TGNNs can only express\nrestricted fragments of this logic, where the interaction between temporal and\nspatial operators is syntactically constrained. These provide us with the first\nresults on the logical expressiveness of temporal GNNs.\n","authors":["Marco Sälzer","Przemysław Andrzej Wałęga","Martin Lange"],"pdf_url":"https://arxiv.org/pdf/2505.11930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24234v1","updated":"2025-10-28T09:42:15Z","published":"2025-10-28T09:42:15Z","title":"Sparse Optimistic Information Directed Sampling","summary":"  Many high-dimensional online decision-making problems can be modeled as\nstochastic sparse linear bandits. Most existing algorithms are designed to\nachieve optimal worst-case regret in either the data-rich regime, where\npolynomial depen- dence on the ambient dimension is unavoidable, or the\ndata-poor regime, where dimension-independence is possible at the cost of worse\ndependence on the num- ber of rounds. In contrast, the sparse Information\nDirected Sampling (IDS) algo- rithm satisfies a Bayesian regret bound that has\nthe optimal rate in both regimes simultaneously. In this work, we explore the\nuse of Sparse Optimistic Informa- tion Directed Sampling (SOIDS) to achieve the\nsame adaptivity in the worst-case setting, without Bayesian assumptions.\nThrough a novel analysis that enables the use of a time-dependent learning\nrate, we show that SOIDS can optimally balance information and regret. Our\nresults extend the theoretical guarantees of IDS, pro- viding the first\nalgorithm that simultaneously achieves optimal worst-case regret in both the\ndata-rich and data-poor regimes. We empirically demonstrate the good\nperformance of SOIDS.\n","authors":["Ludovic Schwartz","Hamish Flynn","Gergely Neu"],"pdf_url":"https://arxiv.org/pdf/2510.24234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24233v1","updated":"2025-10-28T09:42:03Z","published":"2025-10-28T09:42:03Z","title":"PRIVET: Privacy Metric Based on Extreme Value Theory","summary":"  Deep generative models are often trained on sensitive data, such as genetic\nsequences, health data, or more broadly, any copyrighted, licensed or protected\ncontent. This raises critical concerns around privacy-preserving synthetic\ndata, and more specifically around privacy leakage, an issue closely tied to\noverfitting. Existing methods almost exclusively rely on global criteria to\nestimate the risk of privacy failure associated to a model, offering only\nquantitative non interpretable insights. The absence of rigorous evaluation\nmethods for data privacy at the sample-level may hinder the practical\ndeployment of synthetic data in real-world applications. Using extreme value\nstatistics on nearest-neighbor distances, we propose PRIVET, a generic\nsample-based, modality-agnostic algorithm that assigns an individual privacy\nleak score to each synthetic sample. We empirically demonstrate that PRIVET\nreliably detects instances of memorization and privacy leakage across diverse\ndata modalities, including settings with very high dimensionality, limited\nsample sizes such as genetic data and even under underfitting regimes. We\ncompare our method to existing approaches under controlled settings and show\nits advantage in providing both dataset level and sample level assessments\nthrough qualitative and quantitative outputs. Additionally, our analysis\nreveals limitations in existing computer vision embeddings to yield\nperceptually meaningful distances when identifying near-duplicate samples.\n","authors":["Antoine Szatkownik","Aurélien Decelle","Beatriz Seoane","Nicolas Bereux","Léo Planche","Guillaume Charpiat","Burak Yelmen","Flora Jay","Cyril Furtlehner"],"pdf_url":"https://arxiv.org/pdf/2510.24233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16947v2","updated":"2025-10-28T09:41:22Z","published":"2025-05-22T17:32:50Z","title":"MixAT: Combining Continuous and Discrete Adversarial Training for LLMs","summary":"  Despite recent efforts in Large Language Model (LLM) safety and alignment,\ncurrent adversarial attacks on frontier LLMs can still consistently force\nharmful generations. Although adversarial training has been widely studied and\nshown to significantly improve the robustness of traditional machine learning\nmodels, its strengths and weaknesses in the context of LLMs are less\nunderstood. Specifically, while existing discrete adversarial attacks are\neffective at producing harmful content, training LLMs with concrete adversarial\nprompts is often computationally expensive, leading to reliance on continuous\nrelaxations. At the same time, despite their effectiveness and generalization\ncapabilities, training with continuous perturbations does not always capture\nthe full spectrum of vulnerabilities exploited by discrete attacks. In this\nwork, we aim to bridge this gap by introducing MixAT, a novel method that\ncombines stronger discrete and faster continuous attacks during training. We\nrigorously evaluate MixAT across a wide spectrum of state-of-the-art attacks,\nproposing the At Least One Attack Success Rate (ALO-ASR) metric to capture the\nworst-case vulnerability of models. We show MixAT achieves substantially better\nrobustness (ALO-ASR < 20%) compared to prior defenses (ALO-ASR > 50%), while\nmaintaining a runtime comparable to methods based on continuous relaxations. We\nfurther analyze MixAT in realistic deployment settings, exploring how chat\ntemplates, quantization, low-rank adapters, and temperature affect both\nadversarial training and evaluation, revealing additional blind spots in\ncurrent methodologies. Our results demonstrate that MixAT's discrete-continuous\ndefense offers a principled and superior robustness-accuracy tradeoff with\nminimal computational overhead, highlighting its promise for building safer\nLLMs. We provide our code and models at\nhttps://github.com/insait-institute/MixAT.\n","authors":["Csaba Dékány","Stefan Balauca","Robin Staab","Dimitar I. Dimitrov","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2505.16947v2.pdf","comment":"Published at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.22777v2","updated":"2025-10-28T09:39:42Z","published":"2025-10-26T18:01:32Z","title":"SeeDNorm: Self-Rescaled Dynamic Normalization","summary":"  Normalization layer constitutes an essential component in neural networks. In\ntransformers, the predominantly used RMSNorm constrains vectors to a unit\nhypersphere, followed by dimension-wise rescaling through a learnable scaling\ncoefficient $\\gamma$ to maintain the representational capacity of the model.\nHowever, RMSNorm discards the input norm information in forward pass and a\nstatic scaling factor $\\gamma$ may be insufficient to accommodate the wide\nvariability of input data and distributional shifts, thereby limiting further\nperformance improvements, particularly in zero-shot scenarios that large\nlanguage models routinely encounter. To address this limitation, we propose\nSeeDNorm, which enhances the representational capability of the model by\ndynamically adjusting the scaling coefficient based on the current input,\nthereby preserving the input norm information and enabling data-dependent,\nself-rescaled dynamic normalization. During backpropagation, SeeDNorm retains\nthe ability of RMSNorm to dynamically adjust gradient according to the input\nnorm. We provide a detailed analysis of the training optimization for SeedNorm\nand proposed corresponding solutions to address potential instability issues\nthat may arise when applying SeeDNorm. We validate the effectiveness of\nSeeDNorm across models of varying sizes in large language model pre-training as\nwell as supervised and unsupervised computer vision tasks. By introducing a\nminimal number of parameters and with neglligible impact on model efficiency,\nSeeDNorm achieves consistently superior performance compared to previously\ncommonly used normalization layers such as RMSNorm and LayerNorm, as well as\nelement-wise activation alternatives to normalization layers like DyT.\n","authors":["Wenrui Cai","Defa Zhu","Qingjie Liu","Qiyang Min"],"pdf_url":"https://arxiv.org/pdf/2510.22777v2.pdf","comment":"31 pages, 14 figures, 18 tables"},{"id":"http://arxiv.org/abs/2510.24228v1","updated":"2025-10-28T09:39:41Z","published":"2025-10-28T09:39:41Z","title":"A comparison between joint and dual UKF implementations for state\n  estimation and leak localization in water distribution networks","summary":"  The sustainability of modern cities highly depends on efficient water\ndistribution management, including effective pressure control and leak\ndetection and localization. Accurate information about the network hydraulic\nstate is therefore essential. This article presents a comparison between two\ndata-driven state estimation methods based on the Unscented Kalman Filter\n(UKF), fusing pressure, demand and flow data for head and flow estimation. One\napproach uses a joint state vector with a single estimator, while the other\nuses a dual-estimator scheme. We analyse their main characteristics, discussing\ndifferences, advantages and limitations, and compare them theoretically in\nterms of accuracy and complexity. Finally, we show several estimation results\nfor the L-TOWN benchmark, allowing to discuss their properties in a real\nimplementation.\n","authors":["Luis Romero-Ben","Paul Irofti","Florin Stoican","Vicenç Puig"],"pdf_url":"https://arxiv.org/pdf/2510.24228v1.pdf","comment":"This work has been submitted to ECC2026 for review. It has 7 pages\n  and 2 figures"},{"id":"http://arxiv.org/abs/2508.06041v3","updated":"2025-10-28T09:34:36Z","published":"2025-08-08T05:57:04Z","title":"DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision\n  Assignment","summary":"  How can we effectively handle queries for on-device large language models\n(LLMs) with varying runtime constraints, such as latency and accuracy?\nMulti-scale quantization addresses this challenge by enabling memory-efficient\nruntime model adaptation of LLMs through the overlaying of multiple model\nvariants quantized to different bitwidths. Meanwhile, an important question\nstill remains open-ended: how can models be properly configured to match a\ntarget precision or latency? While mixed-precision offers a promising solution,\nwe take this further by leveraging the key observation that the sensitivity of\neach layer dynamically changes across decoding steps. Building on this insight,\nwe introduce DP-LLM, a novel mechanism that dynamically assigns precision to\neach layer based on input values. Experimental results across multiple models\nand benchmarks demonstrate that DP-LLM achieves a superior performance-latency\ntrade-off, outperforming prior approaches.\n","authors":["Sangwoo Kwon","Seong Hoon Seo","Jae W. Lee","Yeonhong Park"],"pdf_url":"https://arxiv.org/pdf/2508.06041v3.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2310.13966v2","updated":"2025-10-28T09:32:53Z","published":"2023-10-21T10:55:31Z","title":"Minimax Optimal Transfer Learning for Kernel-based Nonparametric\n  Regression","summary":"  In recent years, transfer learning has garnered significant attention in the\nmachine learning community. Its ability to leverage knowledge from related\nstudies to improve generalization performance in a target study has made it\nhighly appealing. This paper focuses on investigating the transfer learning\nproblem within the context of nonparametric regression over a reproducing\nkernel Hilbert space. The aim is to bridge the gap between practical\neffectiveness and theoretical guarantees. We specifically consider two\nscenarios: one where the transferable sources are known and another where they\nare unknown. For the known transferable source case, we propose a two-step\nkernel-based estimator by solely using kernel ridge regression. For the unknown\ncase, we develop a novel method based on an efficient aggregation algorithm,\nwhich can automatically detect and alleviate the effects of negative sources.\nThis paper provides the statistical properties of the desired estimators and\nestablishes the minimax optimal rate. Through extensive numerical experiments\non synthetic data and real examples, we validate our theoretical findings and\ndemonstrate the effectiveness of our proposed method.\n","authors":["Chao Wang","Caixing Wang","Xin He","Xingdong Feng"],"pdf_url":"https://arxiv.org/pdf/2310.13966v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17550v3","updated":"2025-10-28T09:32:18Z","published":"2025-09-22T09:09:13Z","title":"Is It Certainly a Deepfake? Reliability Analysis in Detection &\n  Generation Ecosystem","summary":"  As generative models are advancing in quality and quantity for creating\nsynthetic content, deepfakes begin to cause online mistrust. Deepfake detectors\nare proposed to counter this effect, however, misuse of detectors claiming fake\ncontent as real or vice versa further fuels this misinformation problem. We\npresent the first comprehensive uncertainty analysis of deepfake detectors,\nsystematically investigating how generative artifacts influence prediction\nconfidence. As reflected in detectors' responses, deepfake generators also\ncontribute to this uncertainty as their generative residues vary, so we cross\nthe uncertainty analysis of deepfake detectors and generators. Based on our\nobservations, the uncertainty manifold holds enough consistent information to\nleverage uncertainty for deepfake source detection. Our approach leverages\nBayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and\nepistemic uncertainties across diverse detector architectures. We evaluate\nuncertainty on two datasets with nine generators, with four blind and two\nbiological detectors, compare different uncertainty methods, explore region-\nand pixel-based uncertainty, and conduct ablation studies. We conduct and\nanalyze binary real/fake, multi-class real/fake, source detection, and\nleave-one-out experiments between the generator/detector combinations to share\ntheir generalization capability, model calibration, uncertainty, and robustness\nagainst adversarial attacks. We further introduce uncertainty maps that\nlocalize prediction confidence at the pixel level, revealing distinct patterns\ncorrelated with generator-specific artifacts. Our analysis provides critical\ninsights for deploying reliable deepfake detection systems and establishes\nuncertainty quantification as a fundamental requirement for trustworthy\nsynthetic media detection.\n","authors":["Neslihan Kose","Anthony Rhodes","Umur Aybars Ciftci","Ilke Demir"],"pdf_url":"https://arxiv.org/pdf/2509.17550v3.pdf","comment":"Accepted for publication at the ICCV 2025 workshop - STREAM"},{"id":"http://arxiv.org/abs/2505.08256v2","updated":"2025-10-28T09:31:26Z","published":"2025-05-13T06:10:05Z","title":"Clustering-Based Low-Rank Matrix Approximation for Medical Image\n  Compression","summary":"  Medical images are inherently high-resolution and contain locally varying\nstructures crucial for diagnosis. Efficient compression must preserve\ndiagnostic fidelity while minimizing redundancy. Low-rank matrix approximation\n(LoRMA) techniques have shown strong potential for image compression by\ncapturing global correlations; however, they often fail to adapt to local\nstructural variations across regions of interest. To address this, we introduce\nan adaptive LoRMA, which partitions a medical image into overlapping patches,\ngroups structurally similar patches into clusters using k-means, and performs\nSVD within each cluster. We derive the overall compression factor accounting\nfor patch overlap and analyze how patch size influences compression efficiency\nand computational cost. While applicable to any data with high local variation,\nwe focus on medical imaging due to its pronounced local variability. We\nevaluate and compare our adaptive LoRMA against global SVD across four imaging\nmodalities: MRI, ultrasound, CT scan, and chest X-ray. Results demonstrate that\nadaptive LoRMA effectively preserves structural integrity, edge details, and\ndiagnostic relevance, measured by PSNR, SSIM, MSE, IoU, and EPI. Adaptive LoRMA\nminimizes block artifacts and residual errors, particularly in pathological\nregions, consistently outperforming global SVD in PSNR, SSIM, IoU, EPI, and\nachieving lower MSE. It prioritizes clinically salient regions while allowing\naggressive compression in non-critical regions, optimizing storage efficiency.\nAlthough adaptive LoRMA requires higher processing time, its diagnostic\nfidelity justifies the overhead for high-compression applications.\n","authors":["Sisipho Hamlomo","Marcellin Atemkeng"],"pdf_url":"https://arxiv.org/pdf/2505.08256v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24217v1","updated":"2025-10-28T09:30:52Z","published":"2025-10-28T09:30:52Z","title":"Closing Gaps: An Imputation Analysis of ICU Vital Signs","summary":"  As more Intensive Care Unit (ICU) data becomes available, the interest in\ndeveloping clinical prediction models to improve healthcare protocols\nincreases. However, the lack of data quality still hinders clinical prediction\nusing Machine Learning (ML). Many vital sign measurements, such as heart rate,\ncontain sizeable missing segments, leaving gaps in the data that could\nnegatively impact prediction performance. Previous works have introduced\nnumerous time-series imputation techniques. Nevertheless, more comprehensive\nwork is needed to compare a representative set of methods for imputing ICU\nvital signs and determine the best practice. In reality, ad-hoc imputation\ntechniques that could decrease prediction accuracy, like zero imputation, are\nstill used. In this work, we compare established imputation techniques to guide\nresearchers in improving the performance of clinical prediction models by\nselecting the most accurate imputation technique. We introduce an extensible\nand reusable benchmark with currently 15 imputation and 4 amputation methods,\ncreated for benchmarking on major ICU datasets. We hope to provide a\ncomparative basis and facilitate further ML development to bring more models\ninto clinical practice.\n","authors":["Alisher Turubayev","Anna Shopova","Fabian Lange","Mahmut Kamalak","Paul Mattes","Victoria Ayvasky","Bert Arnrich","Bjarne Pfitzner","Robin P. van de Water"],"pdf_url":"https://arxiv.org/pdf/2510.24217v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2510.24216v1","updated":"2025-10-28T09:30:35Z","published":"2025-10-28T09:30:35Z","title":"Unlocking Out-of-Distribution Generalization in Dynamics through\n  Physics-Guided Augmentation","summary":"  In dynamical system modeling, traditional numerical methods are limited by\nhigh computational costs, while modern data-driven approaches struggle with\ndata scarcity and distribution shifts. To address these fundamental\nlimitations, we first propose SPARK, a physics-guided quantitative augmentation\nplugin. Specifically, SPARK utilizes a reconstruction autoencoder to integrate\nphysical parameters into a physics-rich discrete state dictionary. This state\ndictionary then acts as a structured dictionary of physical states, enabling\nthe creation of new, physically-plausible training samples via principled\ninterpolation in the latent space. Further, for downstream prediction, these\naugmented representations are seamlessly integrated with a Fourier-enhanced\nGraph ODE, a combination designed to robustly model the enriched data\ndistribution while capturing long-term temporal dependencies. Extensive\nexperiments on diverse benchmarks demonstrate that SPARK significantly\noutperforms state-of-the-art baselines, particularly in challenging\nout-of-distribution scenarios and data-scarce regimes, proving the efficacy of\nour physics-guided augmentation paradigm.\n","authors":["Fan Xu","Hao Wu","Kun Wang","Nan Wang","Qingsong Wen","Xian Wu","Wei Gong","Xibin Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.24216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24215v1","updated":"2025-10-28T09:29:46Z","published":"2025-10-28T09:29:46Z","title":"What Can Be Recovered Under Sparse Adversarial Corruption?\n  Assumption-Free Theory for Linear Measurements","summary":"  Let \\(\\bm{A} \\in \\mathbb{R}^{m \\times n}\\) be an arbitrary, known matrix and\n\\(\\bm{e}\\) a \\(q\\)-sparse adversarial vector. Given \\(\\bm{y} = \\bm{A} x^* +\n\\bm{e}\\) and \\(q\\), we seek the smallest set containing \\(x^*\\)-hence the one\nconveying maximal information about \\(x^*\\)-that is uniformly recoverable from\n\\(\\bm{y}\\) without knowing \\(\\bm{e}\\). While exact recovery of \\(x^*\\) via\nstrong (and often impractical) structural assumptions on \\(\\bm{A}\\) or \\(x^*\\)\n(for example, restricted isometry, sparsity) is well studied, recoverability\nfor arbitrary \\(\\bm{A}\\) and \\(x^*\\) remains open. Our main result shows that\nthe best that one can hope to recover is \\(x^* + \\ker(\\bm{U})\\), where\n\\(\\bm{U}\\) is the unique projection matrix onto the intersection of rowspaces\nof all possible submatrices of \\(\\bm{A}\\) obtained by deleting \\(2q\\) rows.\nMoreover, we prove that every \\(x\\) that minimizes the \\(\\ell\\_0\\)-norm of\n\\(\\bm{y} - \\bm{A} x\\) lies in \\(x^* + \\ker(\\bm{U})\\), which then gives a\nconstructive approach to recover this set.\n","authors":["Vishal Halder","Alexandre Reiffers-Masson","Abdeldjalil Aïssa-El-Bey","Gugan Thoppe"],"pdf_url":"https://arxiv.org/pdf/2510.24215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21800v2","updated":"2025-10-28T09:27:41Z","published":"2025-10-20T16:49:22Z","title":"MARS-M: When Variance Reduction Meets Matrices","summary":"  Matrix-based preconditioned optimizers, such as Muon, have recently been\nshown to be more efficient than scalar-based optimizers for training\nlarge-scale neural networks, including large language models (LLMs). On the\nother hand, recent benchmarks on optimizers for LLM pre-training have\ndemonstrated that variance-reduction techniques such as MARS can achieve\nsubstantial speedups over standard optimizers that do not employ variance\nreduction. In this paper, to achieve the best of both worlds, we introduce\nMARS-M, a new optimizer that integrates the variance reduction technique in\nMARS with Muon. Under standard regularity conditions, we prove that Muon-M\nconverges to a first-order stationary point at a rate of\n$\\tilde{\\mathcal{O}}(T^{-1/3})$, which improves upon\n$\\tilde{\\mathcal{O}}(T^{-1/4})$ rate attained by Muon. Our empirical results on\nlanguage modeling and computer vision tasks demonstrate that MARS-M\nconsistently yields lower losses and improved performance across various\ndownstream benchmarks. The implementation of MARS-M is available at\nhttps://github.com/AGI-Arena/MARS/tree/main/MARS_M.\n","authors":["Yifeng Liu","Angela Yuan","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2510.21800v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24208v1","updated":"2025-10-28T09:25:40Z","published":"2025-10-28T09:25:40Z","title":"Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in\n  Large Language Models through Latent Semantic Alignment","summary":"  Large Language Models (LLMs) encode vast amounts of knowledge in their\nmassive parameters, which is accessible to locate, trace, and analyze. Despite\nadvances in neural interpretability, it is still not clear how to transfer\nknowledge in a fine-grained manner, namely parametric knowledge transfer (PKT).\nA key problem is enabling effective and efficient knowledge transfer across\nLLMs of different scales, which is essential for achieving greater flexibility\nand broader applicability in transferring knowledge between LLMs. Due to neural\nincompatibility, referring to the architectural and parametric differences\nbetween LLMs of varying scales, existing methods that directly reuse layer\nparameters are severely limited. In this paper, we identify the semantic\nalignment in latent space as the fundamental prerequisite for LLM cross-scale\nknowledge transfer. Instead of directly using the layer parameters, our\napproach takes activations as the medium of layer-wise knowledge transfer.\nLeveraging the semantics in latent space, our approach is simple and\noutperforms prior work, better aligning model behaviors across varying scales.\nEvaluations on four benchmarks demonstrate the efficacy of our method. Further\nanalysis reveals the key factors easing cross-scale knowledge transfer and\nprovides insights into the nature of latent semantic alignment.\n","authors":["Jian Gu","Aldeida Aleti","Chunyang Chen","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24208v1.pdf","comment":"an early-stage version"},{"id":"http://arxiv.org/abs/2510.15403v2","updated":"2025-10-28T09:19:23Z","published":"2025-10-17T07:56:15Z","title":"Geometric Mixture Models for Electrolyte Conductivity Prediction","summary":"  Accurate prediction of ionic conductivity in electrolyte systems is crucial\nfor advancing numerous scientific and technological applications. While\nsignificant progress has been made, current research faces two fundamental\nchallenges: (1) the lack of high-quality standardized benchmarks, and (2)\ninadequate modeling of geometric structure and intermolecular interactions in\nmixture systems. To address these limitations, we first reorganize and enhance\nthe CALiSol and DiffMix electrolyte datasets by incorporating geometric graph\nrepresentations of molecules. We then propose GeoMix, a novel geometry-aware\nframework that preserves Set-SE(3) equivariance-an essential but challenging\nproperty for mixture systems. At the heart of GeoMix lies the Geometric\nInteraction Network (GIN), an equivariant module specifically designed for\nintermolecular geometric message passing. Comprehensive experiments demonstrate\nthat GeoMix consistently outperforms diverse baselines (including MLPs, GNNs,\nand geometric GNNs) across both datasets, validating the importance of\ncross-molecular geometric interactions and equivariant message passing for\naccurate property prediction. This work not only establishes new benchmarks for\nelectrolyte research but also provides a general geometric learning framework\nthat advances modeling of mixture systems in energy materials, pharmaceutical\ndevelopment, and beyond.\n","authors":["Anyi Li","Jiacheng Cen","Songyou Li","Mingze Li","Yang Yu","Wenbing Huang"],"pdf_url":"https://arxiv.org/pdf/2510.15403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21758v2","updated":"2025-10-28T09:14:57Z","published":"2025-10-11T20:16:32Z","title":"Taxonomy and Trends in Reinforcement Learning for Robotics and Control\n  Systems: A Structured Review","summary":"  Reinforcement learning (RL) has become a foundational approach for enabling\nintelligent robotic behavior in dynamic and uncertain environments. This work\npresents an in-depth review of RL principles, advanced deep reinforcement\nlearning (DRL) algorithms, and their integration into robotic and control\nsystems. Beginning with the formalism of Markov Decision Processes (MDPs), the\nstudy outlines essential elements of the agent-environment interaction and\nexplores core algorithmic strategies including actor-critic methods,\nvalue-based learning, and policy gradients. Emphasis is placed on modern DRL\ntechniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving\nhigh-dimensional, continuous control tasks. A structured taxonomy is introduced\nto categorize RL applications across domains such as locomotion, manipulation,\nmulti-agent coordination, and human-robot interaction, along with training\nmethodologies and deployment readiness levels. The review synthesizes recent\nresearch efforts, highlighting technical trends, design patterns, and the\ngrowing maturity of RL in real-world robotics. Overall, this work aims to\nbridge theoretical advances with practical implementations, providing a\nconsolidated perspective on the evolving role of RL in autonomous robotic\nsystems.\n","authors":["Kumater Ter","Ore-Ofe Ajayi","Daniel Udekwe"],"pdf_url":"https://arxiv.org/pdf/2510.21758v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17734v2","updated":"2025-10-28T09:06:30Z","published":"2025-05-23T10:54:53Z","title":"URB - Urban Routing Benchmark for RL-equipped Connected Autonomous\n  Vehicles","summary":"  Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future\nurban networks, potentially by optimizing their routing decisions. Unlike for\nhuman drivers, these decisions can be made with collective, data-driven\npolicies, developed using machine learning algorithms. Reinforcement learning\n(RL) can facilitate the development of such collective routing strategies, yet\nstandardized and realistic benchmarks are missing. To that end, we present URB:\nUrban Routing Benchmark for RL-equipped Connected Autonomous Vehicles. URB is a\ncomprehensive benchmarking environment that unifies evaluation across 29\nreal-world traffic networks paired with realistic demand patterns. URB comes\nwith a catalog of predefined tasks, multi-agent RL (MARL) algorithm\nimplementations, three baseline methods, domain-specific performance metrics,\nand a modular configuration scheme. Our results show that, despite the lengthy\nand costly training, state-of-the-art MARL algorithms rarely outperformed\nhumans. The experimental results reported in this paper initiate the first\nleaderboard for MARL in large-scale urban routing optimization. They reveal\nthat current approaches struggle to scale, emphasizing the urgent need for\nadvancements in this domain.\n","authors":["Ahmet Onur Akman","Anastasia Psarou","Michał Hoffmann","Łukasz Gorczyca","Łukasz Kowalski","Paweł Gora","Grzegorz Jamróz","Rafał Kucharski"],"pdf_url":"https://arxiv.org/pdf/2505.17734v2.pdf","comment":"Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025), Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2510.24200v1","updated":"2025-10-28T09:06:19Z","published":"2025-10-28T09:06:19Z","title":"SPEAR++: Scaling Gradient Inversion via Sparsely-Used Dictionary\n  Learning","summary":"  Federated Learning has seen an increased deployment in real-world scenarios\nrecently, as it enables the distributed training of machine learning models\nwithout explicit data sharing between individual clients. Yet, the introduction\nof the so-called gradient inversion attacks has fundamentally challenged its\nprivacy-preserving properties. Unfortunately, as these attacks mostly rely on\ndirect data optimization without any formal guarantees, the vulnerability of\nreal-world systems remains in dispute and requires tedious testing for each new\nfederated deployment. To overcome these issues, recently the SPEAR attack was\nintroduced, which is based on a theoretical analysis of the gradients of linear\nlayers with ReLU activations. While SPEAR is an important theoretical\nbreakthrough, the attack's practicality was severely limited by its exponential\nruntime in the batch size b. In this work, we fill this gap by applying\nState-of-the-Art techniques from Sparsely-Used Dictionary Learning to make the\nproblem of gradient inversion on linear layers with ReLU activations tractable.\nOur experiments demonstrate that our new attack, SPEAR++, retains all desirable\nproperties of SPEAR, such as robustness to DP noise and FedAvg aggregation,\nwhile being applicable to 10x bigger batch sizes.\n","authors":["Alexander Bakarsky","Dimitar I. Dimitrov","Maximilian Baader","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2510.24200v1.pdf","comment":"Published at the Workshop on Regulatable ML at the 39th Conference on\n  Neural Information Processing Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.24194v1","updated":"2025-10-28T08:57:27Z","published":"2025-10-28T08:57:27Z","title":"Blindfolded Experts Generalize Better: Insights from Robotic\n  Manipulation and Videogames","summary":"  Behavioral cloning is a simple yet effective technique for learning\nsequential decision-making from demonstrations. Recently, it has gained\nprominence as the core of foundation models for the physical world, where\nachieving generalization requires countless demonstrations of a multitude of\ntasks. Typically, a human expert with full information on the task demonstrates\na (nearly) optimal behavior. In this paper, we propose to hide some of the\ntask's information from the demonstrator. This ``blindfolded'' expert is\ncompelled to employ non-trivial exploration to solve the task. We show that\ncloning the blindfolded expert generalizes better to unseen tasks than its\nfully-informed counterpart. We conduct experiments of real-world robot peg\ninsertion tasks with (limited) human demonstrations, alongside videogames from\nthe Procgen benchmark. Additionally, we support our findings with theoretical\nanalysis, which confirms that the generalization error scales with\n$\\sqrt{I/m}$, where $I$ measures the amount of task information available to\nthe demonstrator, and $m$ is the number of demonstrated tasks. Both theory and\npractice indicate that cloning blindfolded experts generalizes better with\nfewer demonstrated tasks. Project page with videos and code:\nhttps://sites.google.com/view/blindfoldedexperts/home\n","authors":["Ev Zisselman","Mirco Mutti","Shelly Francis-Meretzki","Elisei Shafer","Aviv Tamar"],"pdf_url":"https://arxiv.org/pdf/2510.24194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13898v3","updated":"2025-10-28T08:56:58Z","published":"2025-05-20T04:00:56Z","title":"Do Language Models Use Their Depth Efficiently?","summary":"  Modern LLMs are increasingly deep, and depth correlates with performance,\nalbeit with diminishing returns. However, do these models use their depth\nefficiently? Do they compose more features to create higher-order computations\nthat are impossible in shallow models, or do they merely spread the same kinds\nof computation out over more layers? To address these questions, we analyze the\nresidual stream of the Llama 3.1, Qwen 3, and OLMo 2 family of models. We find:\nFirst, comparing the output of the sublayers to the residual stream reveals\nthat layers in the second half contribute much less than those in the first\nhalf, with a clear phase transition between the two halves. Second, skipping\nlayers in the second half has a much smaller effect on future computations and\noutput predictions. Third, for multihop tasks, we are unable to find evidence\nthat models are using increased depth to compose subresults in examples\ninvolving many hops. Fourth, we seek to directly address whether deeper models\nare using their additional layers to perform new kinds of computation. To do\nthis, we train linear maps from the residual stream of a shallow model to a\ndeeper one. We find that layers with the same relative depth map best to each\nother, suggesting that the larger model simply spreads the same computations\nout over its many layers. All this evidence suggests that deeper models are not\nusing their depth to learn new kinds of computation, but only using the greater\ndepth to perform more fine-grained adjustments to the residual. This may help\nexplain why increasing scale leads to diminishing returns for stacked\nTransformer architectures.\n","authors":["Róbert Csordás","Christopher D. Manning","Christopher Potts"],"pdf_url":"https://arxiv.org/pdf/2505.13898v3.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2508.12730v2","updated":"2025-10-28T08:47:58Z","published":"2025-08-18T08:53:53Z","title":"Unlearning Comparator: A Visual Analytics System for Comparative\n  Evaluation of Machine Unlearning Methods","summary":"  Machine Unlearning (MU) aims to remove target training data from a trained\nmodel so that the removed data no longer influences the model's behavior,\nfulfilling \"right to be forgotten\" obligations under data privacy laws. Yet, we\nobserve that researchers in this rapidly emerging field face challenges in\nanalyzing and understanding the behavior of different MU methods, especially in\nterms of three fundamental principles in MU: accuracy, efficiency, and privacy.\nConsequently, they often rely on aggregate metrics and ad-hoc evaluations,\nmaking it difficult to accurately assess the trade-offs between methods. To\nfill this gap, we introduce a visual analytics system, Unlearning Comparator,\ndesigned to facilitate the systematic evaluation of MU methods. Our system\nsupports two important tasks in the evaluation process: model comparison and\nattack simulation. First, it allows the user to compare the behaviors of two\nmodels, such as a model generated by a certain method and a retrained baseline,\nat class-, instance-, and layer-levels to better understand the changes made\nafter unlearning. Second, our system simulates membership inference attacks\n(MIAs) to evaluate the privacy of a method, where an attacker attempts to\ndetermine whether specific data samples were part of the original training set.\nWe evaluate our system through a case study visually analyzing prominent MU\nmethods and demonstrate that it helps the user not only understand model\nbehaviors but also gain insights that can inform the improvement of MU methods.\nThe source code is publicly available at\nhttps://github.com/gnueaj/Machine-Unlearning-Comparator.\n","authors":["Jaeung Lee","Suhyeon Yu","Yurim Jang","Simon S. Woo","Jaemin Jo"],"pdf_url":"https://arxiv.org/pdf/2508.12730v2.pdf","comment":"Submitted to IEEE Transactions on Visualization and Computer Graphics\n  (TVCG), under review. 15 pages. This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2510.24187v1","updated":"2025-10-28T08:47:15Z","published":"2025-10-28T08:47:15Z","title":"Self-Concordant Perturbations for Linear Bandits","summary":"  We study the adversarial linear bandits problem and present a unified\nalgorithmic framework that bridges Follow-the-Regularized-Leader (FTRL) and\nFollow-the-Perturbed-Leader (FTPL) methods, extending the known connection\nbetween them from the full-information setting. Within this framework, we\nintroduce self-concordant perturbations, a family of probability distributions\nthat mirror the role of self-concordant barriers previously employed in the\nFTRL-based SCRiBLe algorithm. Using this idea, we design a novel FTPL-based\nalgorithm that combines self-concordant regularization with efficient\nstochastic exploration. Our approach achieves a regret of $O(d\\sqrt{n \\ln n})$\non both the $d$-dimensional hypercube and the Euclidean ball. On the Euclidean\nball, this matches the rate attained by existing self-concordant FTRL methods.\nFor the hypercube, this represents a $\\sqrt{d}$ improvement over these methods\nand matches the optimal bound up to logarithmic factors.\n","authors":["Lucas Lévy","Jean-Lou Valeau","Arya Akhavan","Patrick Rebeschini"],"pdf_url":"https://arxiv.org/pdf/2510.24187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01356v2","updated":"2025-10-28T08:42:04Z","published":"2025-06-02T06:20:09Z","title":"Two-Stage Learning of Stabilizing Neural Controllers via Zubov Sampling\n  and Iterative Domain Expansion","summary":"  Learning-based neural network (NN) control policies have shown impressive\nempirical performance. However, obtaining stability guarantees and estimates of\nthe region of attraction of these learned neural controllers is challenging due\nto the lack of stable and scalable training and verification algorithms.\nAlthough previous works in this area have achieved great success, much\nconservatism remains in their frameworks. In this work, we propose a novel\ntwo-stage training framework to jointly synthesize a controller and a Lyapunov\nfunction for continuous-time systems. By leveraging a Zubov-inspired region of\nattraction characterization to directly estimate stability boundaries, we\npropose a novel training-data sampling strategy and a domain-updating mechanism\nthat significantly reduces the conservatism in training. Moreover, unlike\nexisting works on continuous-time systems that rely on an SMT solver to\nformally verify the Lyapunov condition, we extend state-of-the-art neural\nnetwork verifier $\\alpha,\\!\\beta$-CROWN with the capability of performing\nautomatic bound propagation through the Jacobian of dynamical systems and a\nnovel verification scheme that avoids expensive bisection. To demonstrate the\neffectiveness of our approach, we conduct numerical experiments by synthesizing\nand verifying controllers on several challenging nonlinear systems across\nmultiple dimensions. We show that our training can yield region of attractions\nwith volume $5 - 1.5\\cdot 10^{5}$ times larger compared to the baselines, and\nour verification on continuous systems can be up to $40-10{,}000$ times faster\ncompared to the traditional SMT solver dReal. Our code is available at\nhttps://github.com/Verified-Intelligence/Two-Stage_Neural_Controller_Training.\n","authors":["Haoyu Li","Xiangru Zhong","Bin Hu","Huan Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.01356v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24180v1","updated":"2025-10-28T08:34:27Z","published":"2025-10-28T08:34:27Z","title":"V-SAT: Video Subtitle Annotation Tool","summary":"  The surge of audiovisual content on streaming platforms and social media has\nheightened the demand for accurate and accessible subtitles. However, existing\nsubtitle generation methods primarily speech-based transcription or OCR-based\nextraction suffer from several shortcomings, including poor synchronization,\nincorrect or harmful text, inconsistent formatting, inappropriate reading\nspeeds, and the inability to adapt to dynamic audio-visual contexts. Current\napproaches often address isolated issues, leaving post-editing as a\nlabor-intensive and time-consuming process. In this paper, we introduce V-SAT\n(Video Subtitle Annotation Tool), a unified framework that automatically\ndetects and corrects a wide range of subtitle quality issues. By combining\nLarge Language Models(LLMs), Vision-Language Models (VLMs), Image Processing,\nand Automatic Speech Recognition (ASR), V-SAT leverages contextual cues from\nboth audio and video. Subtitle quality improved, with the SUBER score reduced\nfrom 9.6 to 3.54 after resolving all language mode issues and F1-scores of\n~0.80 for image mode issues. Human-in-the-loop validation ensures high-quality\nresults, providing the first comprehensive solution for robust subtitle\nannotation.\n","authors":["Arpita Kundu","Joyita Chakraborty","Anindita Desarkar","Aritra Sen","Srushti Anil Patil","Vishwanathan Raman"],"pdf_url":"https://arxiv.org/pdf/2510.24180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03188v4","updated":"2025-10-28T08:28:52Z","published":"2025-04-04T05:32:54Z","title":"Pairwise Optimal Transports for Training All-to-All Flow-Based Condition\n  Transfer Model","summary":"  In this paper, we propose a flow-based method for learning all-to-all\ntransfer maps among conditional distributions that approximates pairwise\noptimal transport. The proposed method addresses the challenge of handling the\ncase of continuous conditions, which often involve a large set of conditions\nwith sparse empirical observations per condition. We introduce a novel cost\nfunction that enables simultaneous learning of optimal transports for all pairs\nof conditional distributions. Our method is supported by a theoretical\nguarantee that, in the limit, it converges to the pairwise optimal transports\namong infinite pairs of conditional distributions. The learned transport maps\nare subsequently used to couple data points in conditional flow matching. We\ndemonstrate the effectiveness of this method on synthetic and benchmark\ndatasets, as well as on chemical datasets in which continuous physical\nproperties are defined as conditions. The code for this project can be found at\nhttps://github.com/kotatumuri-room/A2A-FM\n","authors":["Kotaro Ikeda","Masanori Koyama","Jinzhe Zhang","Kohei Hayashi","Kenji Fukumizu"],"pdf_url":"https://arxiv.org/pdf/2504.03188v4.pdf","comment":"Accepted at NeurIPS 2025, 32 pages, 18 figures"},{"id":"http://arxiv.org/abs/2510.24173v1","updated":"2025-10-28T08:27:37Z","published":"2025-10-28T08:27:37Z","title":"EddyFormer: Accelerated Neural Simulations of Three-Dimensional\n  Turbulence at Scale","summary":"  Computationally resolving turbulence remains a central challenge in fluid\ndynamics due to its multi-scale interactions. Fully resolving large-scale\nturbulence through direct numerical simulation (DNS) is computationally\nprohibitive, motivating data-driven machine learning alternatives. In this\nwork, we propose EddyFormer, a Transformer-based spectral-element (SEM)\narchitecture for large-scale turbulence simulation that combines the accuracy\nof spectral methods with the scalability of the attention mechanism. We\nintroduce an SEM tokenization that decomposes the flow into grid-scale and\nsubgrid-scale components, enabling capture of both local and global features.\nWe create a new three-dimensional isotropic turbulence dataset and train\nEddyFormer to achieves DNS-level accuracy at 256^3 resolution, providing a 30x\nspeedup over DNS. When applied to unseen domains up to 4x larger than in\ntraining, EddyFormer preserves accuracy on physics-invariant metrics-energy\nspectra, correlation functions, and structure functions-showing domain\ngeneralization. On The Well benchmark suite of diverse turbulent flows,\nEddyFormer resolves cases where prior ML models fail to converge, accurately\nreproducing complex dynamics across a wide range of physical conditions.\n","authors":["Yiheng Du","Aditi S. Krishnapriyan"],"pdf_url":"https://arxiv.org/pdf/2510.24173v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2407.13195v6","updated":"2025-10-28T08:26:28Z","published":"2024-07-18T06:16:09Z","title":"Scalable Exploration via Ensemble++","summary":"  Thompson Sampling is a principled method for balancing exploration and\nexploitation, but its real-world adoption faces computational challenges in\nlarge-scale or non-conjugate settings. While ensemble-based approaches offer\npartial remedies, they typically require prohibitively large ensemble sizes. We\npropose Ensemble++, a scalable exploration framework using a novel\nshared-factor ensemble architecture with random linear combinations. For linear\nbandits, we provide theoretical guarantees showing that Ensemble++ achieves\nregret comparable to exact Thompson Sampling with only $\\Theta(d \\log T)$\nensemble sizes--significantly outperforming prior methods. Crucially, this\nefficiency holds across both compact and finite action sets with either\ntime-invariant or time-varying contexts without configuration changes. We\nextend this theoretical foundation to nonlinear rewards by replacing fixed\nfeatures with learnable neural representations while preserving the same\nincremental update principle, effectively bridging theory and practice for\nreal-world tasks. Comprehensive experiments across linear, quadratic, neural,\nand GPT-based contextual bandits validate our theoretical findings and\ndemonstrate Ensemble++'s superior regret-computation tradeoff versus\nstate-of-the-art methods.\n","authors":["Yingru Li","Jiawei Xu","Baoxiang Wang","Zhi-Quan Luo"],"pdf_url":"https://arxiv.org/pdf/2407.13195v6.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2502.15805v3","updated":"2025-10-28T08:12:05Z","published":"2025-02-19T07:01:00Z","title":"FragFM: Hierarchical Framework for Efficient Molecule Generation via\n  Fragment-Level Discrete Flow Matching","summary":"  We introduce FragFM, a novel hierarchical framework via fragment-level\ndiscrete flow matching for efficient molecular graph generation. FragFM\ngenerates molecules at the fragment level, leveraging a coarse-to-fine\nautoencoder to reconstruct details at the atom level. Together with a\nstochastic fragment bag strategy to effectively handle an extensive fragment\nspace, our framework enables more efficient and scalable molecular generation.\nWe demonstrate that our fragment-based approach achieves better property\ncontrol than the atom-based method and additional flexibility through\nconditioning the fragment bag. We also propose a Natural Product Generation\nbenchmark (NPGen) to evaluate modern molecular graph generative models' ability\nto generate natural product-like molecules. Since natural products are\nbiologically prevalidated and differ from typical drug-like molecules, our\nbenchmark provides a more challenging yet meaningful evaluation relevant to\ndrug discovery. We conduct a FragFM comparative study against various models on\ndiverse molecular generation benchmarks, including NPGen, demonstrating\nsuperior performance. The results highlight the potential of fragment-based\ngenerative modeling for large-scale, property-aware molecular design, paving\nthe way for more efficient exploration of chemical space.\n","authors":["Joongwon Lee","Seonghwan Kim","Seokhyun Moon","Hyunwoo Kim","Woo Youn Kim"],"pdf_url":"https://arxiv.org/pdf/2502.15805v3.pdf","comment":"49 pages, 29 figures, under review"},{"id":"http://arxiv.org/abs/2507.01271v4","updated":"2025-10-28T08:11:23Z","published":"2025-07-02T01:13:08Z","title":"PULSE: Practical Evaluation Scenarios for Large Multimodal Model\n  Unlearning","summary":"  In recent years, unlearning techniques, which are methods for inducing a\nmodel to \"forget\" previously learned information, have attracted attention as a\nway to address privacy and copyright concerns in large language models (LLMs)\nand large multimodal models (LMMs). While several unlearning benchmarks have\nbeen established for LLMs, a practical evaluation framework for unlearning in\nLMMs has been less explored. Specifically, existing unlearning benchmark for\nLMMs considers only scenarios in which the model is required to unlearn\nfine-tuned knowledge through a single unlearning operation. In this study, we\nintroduce PULSE protocol for realistic unlearning scenarios for LMMs by\nintroducing two critical perspectives: (i) Pre-trained knowledge Unlearning for\nanalyzing the effect across different knowledge acquisition phases and (ii)\nLong-term Sustainability Evaluation to address sequential requests. We then\nevaluate existing unlearning methods along these dimensions. Our results reveal\nthat, although some techniques can successfully unlearn knowledge acquired\nthrough fine-tuning, they struggle to eliminate information learned during\npre-training. Moreover, methods that effectively unlearn a batch of target data\nin a single operation exhibit substantial performance degradation when the same\ndata are split and unlearned sequentially.\n","authors":["Tatsuki Kawakami","Kazuki Egashira","Atsuyuki Miyai","Go Irie","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2507.01271v4.pdf","comment":"Accepted at NeurIPS 2025 Workshop: Evaluating the Evolving LLM\n  Lifecycle"},{"id":"http://arxiv.org/abs/2505.17637v2","updated":"2025-10-28T07:57:56Z","published":"2025-05-23T08:58:38Z","title":"Causal Spatio-Temporal Prediction: An Effective and Efficient\n  Multi-Modal Approach","summary":"  Spatio-temporal prediction plays a crucial role in intelligent\ntransportation, weather forecasting, and urban planning. While integrating\nmulti-modal data has shown potential for enhancing prediction accuracy, key\nchallenges persist: (i) inadequate fusion of multi-modal information, (ii)\nconfounding factors that obscure causal relations, and (iii) high computational\ncomplexity of prediction models. To address these challenges, we propose\nE^2-CSTP, an Effective and Efficient Causal multi-modal Spatio-Temporal\nPrediction framework. E^2-CSTP leverages cross-modal attention and gating\nmechanisms to effectively integrate multi-modal data. Building on this, we\ndesign a dual-branch causal inference approach: the primary branch focuses on\nspatio-temporal prediction, while the auxiliary branch mitigates bias by\nmodeling additional modalities and applying causal interventions to uncover\ntrue causal dependencies. To improve model efficiency, we integrate GCN with\nthe Mamba architecture for accelerated spatio-temporal encoding. Extensive\nexperiments on 4 real-world datasets show that E^2-CSTP significantly\noutperforms 9 state-of-the-art methods, achieving up to 9.66% improvements in\naccuracy as well as 17.37%-56.11% reductions in computational overhead.\n","authors":["Yuting Huang","Ziquan Fang","Zhihao Zeng","Lu Chen","Yunjun Gao"],"pdf_url":"https://arxiv.org/pdf/2505.17637v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.10101v3","updated":"2025-10-28T07:57:39Z","published":"2025-10-11T08:24:07Z","title":"Rademacher Meets Colors: More Expressivity, but at What Cost ?","summary":"  The expressive power of graph neural networks (GNNs) is typically understood\nthrough their correspondence with graph isomorphism tests such as the\nWeisfeiler-Leman (WL) hierarchy. While more expressive GNNs can distinguish a\nricher set of graphs, they are also observed to suffer from higher\ngeneralization error. This work provides a theoretical explanation for this\ntrade-off by linking expressivity and generalization through the lens of\ncoloring algorithms. Specifically, we show that the number of equivalence\nclasses induced by WL colorings directly bounds the GNNs Rademacher complexity\n-- a key data-dependent measure of generalization. Our analysis reveals that\ngreater expressivity leads to higher complexity and thus weaker generalization\nguarantees. Furthermore, we prove that the Rademacher complexity is stable\nunder perturbations in the color counts across different samples, ensuring\nrobustness to sampling variability across datasets. Importantly, our framework\nis not restricted to message-passing GNNs or 1-WL, but extends to arbitrary GNN\narchitectures and expressivity measures that partition graphs into equivalence\nclasses. These results unify the study of expressivity and generalization in\nGNNs, providing a principled understanding of why increasing expressive power\noften comes at the cost of generalization.\n","authors":["Martin Carrasco","Caio F. Deberaldini Netto","Vahan A. Martirosyan","Aneeqa Mehrab","Ehimare Okoyomon","Caterina Graziani"],"pdf_url":"https://arxiv.org/pdf/2510.10101v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24160v1","updated":"2025-10-28T07:57:14Z","published":"2025-10-28T07:57:14Z","title":"Identifiable learning of dissipative dynamics","summary":"  Complex dissipative systems appear across science and engineering, from\npolymers and active matter to learning algorithms. These systems operate far\nfrom equilibrium, where energy dissipation and time irreversibility are key to\ntheir behavior, but are difficult to quantify from data. Learning accurate and\ninterpretable models of such dynamics remains a major challenge: the models\nmust be expressive enough to describe diverse processes, yet constrained enough\nto remain physically meaningful and mathematically identifiable. Here, we\nintroduce I-OnsagerNet, a neural framework that learns dissipative stochastic\ndynamics directly from trajectories while ensuring both interpretability and\nuniqueness. I-OnsagerNet extends the Onsager principle to guarantee that the\nlearned potential is obtained from the stationary density and that the drift\ndecomposes cleanly into time-reversible and time-irreversible components, as\ndictated by the Helmholtz decomposition. Our approach enables us to calculate\nthe entropy production and to quantify irreversibility, offering a principled\nway to detect and quantify deviations from equilibrium. Applications to polymer\nstretching in elongational flow and to stochastic gradient Langevin dynamics\nreveal new insights, including super-linear scaling of barrier heights and\nsub-linear scaling of entropy production rates with the strain rate, and the\nsuppression of irreversibility with increasing batch size. I-OnsagerNet thus\nestablishes a general, data-driven framework for discovering and interpreting\nnon-equilibrium dynamics.\n","authors":["Aiqing Zhu","Beatrice W. Soh","Grigorios A. Pavliotis","Qianxiao Li"],"pdf_url":"https://arxiv.org/pdf/2510.24160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24159v1","updated":"2025-10-28T07:55:34Z","published":"2025-10-28T07:55:34Z","title":"Self-supervised Synthetic Pretraining for Inference of Stellar Mass\n  Embedded in Dense Gas","summary":"  Stellar mass is a fundamental quantity that determines the properties and\nevolution of stars. However, estimating stellar masses in star-forming regions\nis challenging because young stars are obscured by dense gas and the regions\nare highly inhomogeneous, making spherical dynamical estimates unreliable.\nSupervised machine learning could link such complex structures to stellar mass,\nbut it requires large, high-quality labeled datasets from high-resolution\nmagneto-hydrodynamical (MHD) simulations, which are computationally expensive.\nWe address this by pretraining a vision transformer on one million synthetic\nfractal images using the self-supervised framework DINOv2, and then applying\nthe frozen model to limited high-resolution MHD simulations. Our results\ndemonstrate that synthetic pretraining improves frozen-feature regression\nstellar mass predictions, with the pretrained model performing slightly better\nthan a supervised model trained on the same limited simulations. Principal\ncomponent analysis of the extracted features further reveals semantically\nmeaningful structures, suggesting that the model enables unsupervised\nsegmentation of star-forming regions without the need for labeled data or\nfine-tuning.\n","authors":["Keiya Hirashima","Shingo Nozaki","Naoto Harada"],"pdf_url":"https://arxiv.org/pdf/2510.24159v1.pdf","comment":"6 pages, 3 figures, 1 table, accepted for NeurIPS 2025 ML4PS workshop"},{"id":"http://arxiv.org/abs/2509.04415v2","updated":"2025-10-28T07:32:34Z","published":"2025-09-04T17:37:35Z","title":"Interpretable Clustering with Adaptive Heterogeneous Causal Structure\n  Learning in Mixed Observational Data","summary":"  Understanding causal heterogeneity is essential for scientific discovery in\ndomains such as biology and medicine. However, existing methods lack causal\nawareness, with insufficient modeling of heterogeneity, confounding, and\nobservational constraints, leading to poor interpretability and difficulty\ndistinguishing true causal heterogeneity from spurious associations. We propose\nan unsupervised framework, HCL (Interpretable Causal Mechanism-Aware Clustering\nwith Adaptive Heterogeneous Causal Structure Learning), that jointly infers\nlatent clusters and their associated causal structures from mixed-type\nobservational data without requiring temporal ordering, environment labels,\ninterventions or other prior knowledge. HCL relaxes the homogeneity and\nsufficiency assumptions by introducing an equivalent representation that\nencodes both structural heterogeneity and confounding. It further develops a\nbi-directional iterative strategy to alternately refine causal clustering and\nstructure learning, along with a self-supervised regularization that balance\ncross-cluster universality and specificity. Together, these components enable\nconvergence toward interpretable, heterogeneous causal patterns. Theoretically,\nwe show identifiability of heterogeneous causal structures under mild\nconditions. Empirically, HCL achieves superior performance in both clustering\nand structure learning tasks, and recovers biologically meaningful mechanisms\nin real-world single-cell perturbation data, demonstrating its utility for\ndiscovering interpretable, mechanism-level causal heterogeneity.\n","authors":["Wenrui Li","Qinghao Zhang","Xiaowo Wang"],"pdf_url":"https://arxiv.org/pdf/2509.04415v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24135v1","updated":"2025-10-28T07:20:38Z","published":"2025-10-28T07:20:38Z","title":"Fixed Point Neural Acceleration and Inverse Surrogate Model for Battery\n  Parameter Identification","summary":"  The rapid expansion of electric vehicles has intensified the need for\naccurate and efficient diagnosis of lithium-ion batteries. Parameter\nidentification of electrochemical battery models is widely recognized as a\npowerful method for battery health assessment. However, conventional\nmetaheuristic approaches suffer from high computational cost and slow\nconvergence, and recent machine learning methods are limited by their reliance\non constant current data, which may not be available in practice. To overcome\nthese challenges, we propose deep learning-based framework for parameter\nidentification of electrochemical battery models. The proposed framework\ncombines a neural surrogate model of the single particle model with electrolyte\n(NeuralSPMe) and a deep learning-based fixed-point iteration method. NeuralSPMe\nis trained on realistic EV load profiles to accurately predict lithium\nconcentration dynamics under dynamic operating conditions while a parameter\nupdate network (PUNet) performs fixed-point iterative updates to significantly\nreduce both the evaluation time per sample and the overall number of iterations\nrequired for convergence. Experimental evaluations demonstrate that the\nproposed framework accelerates the parameter identification by more than 2000\ntimes, achieves superior sample efficiency and more than 10 times higher\naccuracy compared to conventional metaheuristic algorithms, particularly under\ndynamic load scenarios encountered in practical applications.\n","authors":["Hojin Cheon","Hyeongseok Seo","Jihun Jeon","Wooju Lee","Dohyun Jeong","Hongseok Kim"],"pdf_url":"https://arxiv.org/pdf/2510.24135v1.pdf","comment":"31 pages, 11 figures, submitted to Applied Energy"},{"id":"http://arxiv.org/abs/2507.14111v8","updated":"2025-10-28T07:04:44Z","published":"2025-07-18T17:43:56Z","title":"CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement\n  Learning","summary":"  The exponential growth in demand for GPU computing resources has created an\nurgent need for automated CUDA optimization strategies. While recent advances\nin LLMs show promise for code generation, current SOTA models achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization that employs a\nnovel contrastive RL algorithm.\n  CUDA-L1 achieves significant performance improvements on the CUDA\noptimization task: trained on A100, it delivers an average speedup of x3.12\nwith a median speedup of x1.42 against default baselines over across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x120. In addition to\nthe default baseline provided by KernelBench, CUDA-L1 demonstrates x2.77 over\nTorch Compile, x2.88 over Torch Compile with reduce overhead, x2.81 over CUDA\nGraph implementations, and remarkably x7.72 over cuDNN libraries. Furthermore,\nthe model also demonstrates portability across different GPU architectures.\n  Beyond these benchmark results, CUDA-L1 demonstrates several properties: it\n1) discovers a variety of CUDA optimization techniques and learns to combine\nthem strategically to achieve optimal performance; 2) uncovers fundamental\nprinciples of CUDA optimization, such as the multiplicative nature of\noptimizations; 3) identifies non-obvious performance bottlenecks and rejects\nseemingly beneficial optimizations that actually harm performance. The\ncapabilities demonstrate that, RL can transform an initially poor-performing\nLLM into an effective CUDA optimizer through speedup-based reward signals\nalone, without human expertise or domain knowledge. This paradigm opens\npossibilities for automated optimization of CUDA operations, and holds promise\nto substantially promote GPU efficiency and alleviate the rising pressure on\nGPU computing resources.\n","authors":["Xiaoya Li","Xiaofei Sun","Albert Wang","Jiwei Li","Chris Shum"],"pdf_url":"https://arxiv.org/pdf/2507.14111v8.pdf","comment":"Project Page: https://deepreinforce-ai.github.io/cudal1_blog/"},{"id":"http://arxiv.org/abs/2510.24125v1","updated":"2025-10-28T06:57:14Z","published":"2025-10-28T06:57:14Z","title":"Causal Convolutional Neural Networks as Finite Impulse Response Filters","summary":"  This study investigates the behavior of Causal Convolutional Neural Networks\n(CNNs) with quasi-linear activation functions when applied to time-series data\ncharacterized by multimodal frequency content. We demonstrate that, once\ntrained, such networks exhibit properties analogous to Finite Impulse Response\n(FIR) filters, particularly when the convolutional kernels are of extended\nlength exceeding those typically employed in standard CNN architectures. Causal\nCNNs are shown to capture spectral features both implicitly and explicitly,\noffering enhanced interpretability for tasks involving dynamic systems.\nLeveraging the associative property of convolution, we further show that the\nentire network can be reduced to an equivalent single-layer filter resembling\nan FIR filter optimized via least-squares criteria. This equivalence yields new\ninsights into the spectral learning behavior of CNNs trained on signals with\nsparse frequency content. The approach is validated on both simulated beam\ndynamics and real-world bridge vibration datasets, underlining its relevance\nfor modeling and identifying physical systems governed by dynamic responses.\n","authors":["Kiran Bacsa","Wei Liu","Xudong Jian","Huangbin Liang","Eleni Chatzi"],"pdf_url":"https://arxiv.org/pdf/2510.24125v1.pdf","comment":"14 pages, 19 figures, Under review"},{"id":"http://arxiv.org/abs/2506.05426v3","updated":"2025-10-28T06:55:14Z","published":"2025-06-05T06:29:14Z","title":"Mixture-of-Experts Meets In-Context Reinforcement Learning","summary":"  In-context reinforcement learning (ICRL) has emerged as a promising paradigm\nfor adapting RL agents to downstream tasks through prompt conditioning.\nHowever, two notable challenges remain in fully harnessing in-context learning\nwithin RL domains: the intrinsic multi-modality of the state-action-reward data\nand the diverse, heterogeneous nature of decision tasks. To tackle these\nchallenges, we propose T2MIR (Token- and Task-wise MoE for In-context RL), an\ninnovative framework that introduces architectural advances of\nmixture-of-experts (MoE) into transformer-based decision models. T2MIR\nsubstitutes the feedforward layer with two parallel layers: a token-wise MoE\nthat captures distinct semantics of input tokens across multiple modalities,\nand a task-wise MoE that routes diverse tasks to specialized experts for\nmanaging a broad task distribution with alleviated gradient conflicts. To\nenhance task-wise routing, we introduce a contrastive learning method that\nmaximizes the mutual information between the task and its router\nrepresentation, enabling more precise capture of task-relevant information. The\noutputs of two MoE components are concatenated and fed into the next layer.\nComprehensive experiments show that T2MIR significantly facilitates in-context\nlearning capacity and outperforms various types of baselines. We bring the\npotential and promise of MoE to ICRL, offering a simple and scalable\narchitectural enhancement to advance ICRL one step closer toward achievements\nin language and vision communities. Our code is available at\nhttps://github.com/NJU-RL/T2MIR.\n","authors":["Wenhao Wu","Fuhong Liu","Haoru Li","Zican Hu","Daoyi Dong","Chunlin Chen","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2506.05426v3.pdf","comment":"28 pages, 13 figures, 17 tables"},{"id":"http://arxiv.org/abs/2510.24120v1","updated":"2025-10-28T06:47:30Z","published":"2025-10-28T06:47:30Z","title":"Graph-Guided Concept Selection for Efficient Retrieval-Augmented\n  Generation","summary":"  Graph-based RAG constructs a knowledge graph (KG) from text chunks to enhance\nretrieval in Large Language Model (LLM)-based question answering. It is\nespecially beneficial in domains such as biomedicine, law, and political\nscience, where effective retrieval often involves multi-hop reasoning over\nproprietary documents. However, these methods demand numerous LLM calls to\nextract entities and relations from text chunks, incurring prohibitive costs at\nscale. Through a carefully designed ablation study, we observe that certain\nwords (termed concepts) and their associated documents are more important.\nBased on this insight, we propose Graph-Guided Concept Selection (G2ConS). Its\ncore comprises a chunk selection method and an LLM-independent concept graph.\nThe former selects salient document chunks to reduce KG construction costs; the\nlatter closes knowledge gaps introduced by chunk selection at zero cost.\nEvaluations on multiple real-world datasets show that G2ConS outperforms all\nbaselines in construction cost, retrieval effectiveness, and answering quality.\n","authors":["Ziyu Liu","Yijing Liu","Jianfei Yuan","Minzhi Yan","Le Yue","Honghui Xiong","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2510.24120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24115v1","updated":"2025-10-28T06:38:59Z","published":"2025-10-28T06:38:59Z","title":"HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws\n  in Vision-Language Models for Histopathology","summary":"  For doctors to truly trust artificial intelligence, it can't be a black box.\nThey need to understand its reasoning, almost as if they were consulting a\ncolleague. We created HistoLens1 to be that transparent, collaborative partner.\nIt allows a pathologist to simply ask a question in plain English about a\ntissue slide--just as they would ask a trainee. Our system intelligently\ntranslates this question into a precise query for its AI engine, which then\nprovides a clear, structured report. But it doesn't stop there. If a doctor\never asks, \"Why?\", HistoLens can instantly provide a 'visual proof' for any\nfinding--a heatmap that points to the exact cells and regions the AI used for\nits analysis. We've also ensured the AI focuses only on the patient's tissue,\njust like a trained pathologist would, by teaching it to ignore distracting\nbackground noise. The result is a workflow where the pathologist remains the\nexpert in charge, using a trustworthy AI assistant to verify their insights and\nmake faster, more confident diagnoses.\n","authors":["Sandeep Vissapragada","Vikrant Sahu","Gagan Raj Gupta","Vandita Singh"],"pdf_url":"https://arxiv.org/pdf/2510.24115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21582v2","updated":"2025-10-28T06:37:16Z","published":"2025-10-24T15:50:31Z","title":"An unsupervised tour through the hidden pathways of deep neural networks","summary":"  The goal of this thesis is to improve our understanding of the internal\nmechanisms by which deep artificial neural networks create meaningful\nrepresentations and are able to generalize. We focus on the challenge of\ncharacterizing the semantic content of the hidden representations with\nunsupervised learning tools, partially developed by us and described in this\nthesis, which allow harnessing the low-dimensional structure of the data.\nChapter 2. introduces Gride, a method that allows estimating the intrinsic\ndimension of the data as an explicit function of the scale without performing\nany decimation of the data set. Our approach is based on rigorous\ndistributional results that enable the quantification of uncertainty of the\nestimates. Moreover, our method is simple and computationally efficient since\nit relies only on the distances among nearest data points. In Chapter 3, we\nstudy the evolution of the probability density across the hidden layers in some\nstate-of-the-art deep neural networks. We find that the initial layers generate\na unimodal probability density getting rid of any structure irrelevant to\nclassification. In subsequent layers, density peaks arise in a hierarchical\nfashion that mirrors the semantic hierarchy of the concepts. This process\nleaves a footprint in the probability density of the output layer, where the\ntopography of the peaks allows reconstructing the semantic relationships of the\ncategories. In Chapter 4, we study the problem of generalization in deep neural\nnetworks: adding parameters to a network that interpolates its training data\nwill typically improve its generalization performance, at odds with the\nclassical bias-variance trade-off. We show that wide neural networks learn\nredundant representations instead of overfitting to spurious correlation and\nthat redundant neurons appear only if the network is regularized and the\ntraining error is zero.\n","authors":["Diego Doimo"],"pdf_url":"https://arxiv.org/pdf/2510.21582v2.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2510.24113v1","updated":"2025-10-28T06:36:44Z","published":"2025-10-28T06:36:44Z","title":"Taming the Tail: NoI Topology Synthesis for Mixed DL Workloads on\n  Chiplet-Based Accelerators","summary":"  Heterogeneous chiplet-based systems improve scaling by disag-gregating\nCPUs/GPUs and emerging technologies (HBM/DRAM).However this on-package\ndisaggregation introduces a latency inNetwork-on-Interposer(NoI). We observe\nthat in modern large-modelinference, parameters and activations routinely move\nbackand forth from HBM/DRAM, injecting large, bursty flows into theinterposer.\nThese memory-driven transfers inflate tail latency andviolate Service Level\nAgreements (SLAs) across k-ary n-cube base-line NoI topologies. To address this\ngap we introduce an InterferenceScore (IS) that quantifies worst-case slowdown\nunder contention.We then formulate NoI synthesis as a multi-objective\noptimization(MOO) problem. We develop PARL (Partition-Aware\nReinforcementLearner), a topology generator that balances throughput,\nlatency,and power. PARL-generated topologies reduce contention at the memory\ncut, meet SLAs, and cut worst-case slowdown to 1.2 times while maintaining\ncompetitive mean throughput relative to link-rich meshes. Overall, this\nreframes NoI design for heterogeneouschiplet accelerators with workload-aware\nobjectives.\n","authors":["Arnav Shukla","Harsh Sharma","Srikant Bharadwaj","Vinayak Abrol","Sujay Deb"],"pdf_url":"https://arxiv.org/pdf/2510.24113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12758v4","updated":"2025-10-28T06:27:55Z","published":"2025-05-19T06:35:11Z","title":"Global urban visual perception varies across demographics and\n  personalities","summary":"  Understanding people's preferences is crucial for urban planning, yet current\napproaches often combine responses from multi-cultural populations, obscuring\ndemographic differences and risking amplifying biases. We conducted a\nlargescale urban visual perception survey of streetscapes worldwide using\nstreet view imagery, examining how demographics -- including gender, age,\nincome, education, race and ethnicity, and personality traits -- shape\nperceptions among 1,000 participants with balanced demographics from five\ncountries and 45 nationalities. This dataset, Street Perception Evaluation\nConsidering Socioeconomics (SPECS), reveals demographic- and personality-based\ndifferences across six traditional indicators -- safe, lively, wealthy,\nbeautiful, boring, depressing -- and four new ones -- live nearby, walk, cycle,\ngreen. Location-based sentiments further shape these preferences. Machine\nlearning models trained on existing global datasets tend to overestimate\npositive indicators and underestimate negative ones compared to human\nresponses, underscoring the need for local context. Our study aspires to\nrectify the myopic treatment of street perception, which rarely considers\ndemographics or personality traits.\n","authors":["Matias Quintana","Youlong Gu","Xiucheng Liang","Yujun Hou","Koichi Ito","Yihan Zhu","Mahmoud Abdelrahman","Filip Biljecki"],"pdf_url":"https://arxiv.org/pdf/2505.12758v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24105v1","updated":"2025-10-28T06:21:06Z","published":"2025-10-28T06:21:06Z","title":"Enhancing Pre-trained Representation Classifiability can Boost its\n  Interpretability","summary":"  The visual representation of a pre-trained model prioritizes the\nclassifiability on downstream tasks, while the widespread applications for\npre-trained visual models have posed new requirements for representation\ninterpretability. However, it remains unclear whether the pre-trained\nrepresentations can achieve high interpretability and classifiability\nsimultaneously. To answer this question, we quantify the representation\ninterpretability by leveraging its correlation with the ratio of interpretable\nsemantics within the representations. Given the pre-trained representations,\nonly the interpretable semantics can be captured by interpretations, whereas\nthe uninterpretable part leads to information loss. Based on this fact, we\npropose the Inherent Interpretability Score (IIS) that evaluates the\ninformation loss, measures the ratio of interpretable semantics, and quantifies\nthe representation interpretability. In the evaluation of the representation\ninterpretability with different classifiability, we surprisingly discover that\nthe interpretability and classifiability are positively correlated, i.e.,\nrepresentations with higher classifiability provide more interpretable\nsemantics that can be captured in the interpretations. This observation further\nsupports two benefits to the pre-trained representations. First, the\nclassifiability of representations can be further improved by fine-tuning with\ninterpretability maximization. Second, with the classifiability improvement for\nthe representations, we obtain predictions based on their interpretations with\nless accuracy degradation. The discovered positive correlation and\ncorresponding applications show that practitioners can unify the improvements\nin interpretability and classifiability for pre-trained vision models. Codes\nare available at https://github.com/ssfgunner/IIS.\n","authors":["Shufan Shen","Zhaobo Qi","Junshu Sun","Qingming Huang","Qi Tian","Shuhui Wang"],"pdf_url":"https://arxiv.org/pdf/2510.24105v1.pdf","comment":"ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2502.04242v4","updated":"2025-10-28T06:15:39Z","published":"2025-02-06T17:32:49Z","title":"A High-Dimensional Statistical Method for Optimizing Transfer Quantities\n  in Multi-Source Transfer Learning","summary":"  Multi-source transfer learning provides an effective solution to data\nscarcity in real- world supervised learning scenarios by leveraging multiple\nsource tasks. In this field, existing works typically use all available samples\nfrom sources in training, which constrains their training efficiency and may\nlead to suboptimal results. To address this, we propose a theoretical framework\nthat answers the question: what is the optimal quantity of source samples\nneeded from each source task to jointly train the target model? Specifically,\nwe introduce a generalization error measure based on K-L divergence, and\nminimize it based on high-dimensional statistical analysis to determine the\noptimal transfer quantity for each source task. Additionally, we develop an\narchitecture-agnostic and data-efficient algorithm OTQMS to implement our\ntheoretical results for target model training in multi- source transfer\nlearning. Experimental studies on diverse architectures and two real-world\nbenchmark datasets show that our proposed algorithm significantly outperforms\nstate-of-the-art approaches in both accuracy and data efficiency. The code and\nsupplementary materials are available in https://github.com/zqy0126/OTQMS.\n","authors":["Qingyue Zhang","Haohao Fu","Guanbo Huang","Yaoyuan Liang","Chang Chu","Tianren Peng","Yanru Wu","Qi Li","Yang Li","Shao-Lun Huang"],"pdf_url":"https://arxiv.org/pdf/2502.04242v4.pdf","comment":"NeurIPS 2025 Poster"},{"id":"http://arxiv.org/abs/2509.16989v2","updated":"2025-10-28T06:14:52Z","published":"2025-09-21T09:07:20Z","title":"PTQTP: Post-Training Quantization to Trit-Planes for Large Language\n  Models","summary":"  Post-training quantization (PTQ) of large language models (LLMs) to extremely\nlow bit-widths remains challenging due to the fundamental trade-off between\ncomputational efficiency and model expressiveness. While existing ultra-low-bit\nPTQ methods rely on binary approximations or complex compensation mechanisms,\nthey suffer from either limited representational capacity or computational\noverhead that undermines their efficiency gains. We introduce PTQ to\nTrit-Planes (PTQTP), the first ternary-weight PTQ framework that decomposes\nweight matrices into structured ternary {-1, 0, 1} trit-planes using 2x1.58-bit\nrepresentation. PTQTP achieves multiplication-free inference, identical to\n1-bit quantization, while maintaining superior expressiveness through its novel\nstructured decomposition. Our approach provides: (1) a theoretically grounded\nprogressive approximation algorithm ensuring global weight consistency; (2)\nmodel-agnostic deployment across diverse modern LLMs without architectural\nmodifications; and (3) uniform ternary operations that eliminate the need for\nmixed-precision or compensation schemes. Comprehensive experiments across\nLLaMA3.x and Qwen3 model families (0.6B-70B parameters) demonstrate that PTQTP\nsignificantly outperforms existing low-bit PTQ methods, achieving 82.4%\nmathematical reasoning retention versus 0% for competing approaches. PTQTP\napproaches and sometimes surpasses 1.58-bit quantization-aware training\nperformance while requiring only single-hour quantization compared to 10-14 GPU\ndays for training-based methods. These results establish PTQTP as a practical\nsolution for efficient LLM deployment in resource-constrained environments. The\ncode will be available at https://github.com/HeXiao-55/PTQTP.\n","authors":["He Xiao","Runming Yang","Qingyao Yang","Wendong Xu","Zhen Li","Yupeng Su","Zhengwu Liu","Hongxia Yang","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2509.16989v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2510.24095v1","updated":"2025-10-28T06:08:25Z","published":"2025-10-28T06:08:25Z","title":"Learning Parameterized Skills from Demonstrations","summary":"  We present DEPS, an end-to-end algorithm for discovering parameterized skills\nfrom expert demonstrations. Our method learns parameterized skill policies\njointly with a meta-policy that selects the appropriate discrete skill and\ncontinuous parameters at each timestep. Using a combination of temporal\nvariational inference and information-theoretic regularization methods, we\naddress the challenge of degeneracy common in latent variable models, ensuring\nthat the learned skills are temporally extended, semantically meaningful, and\nadaptable. We empirically show that learning parameterized skills from\nmultitask expert demonstrations significantly improves generalization to unseen\ntasks. Our method outperforms multitask as well as skill learning baselines on\nboth LIBERO and MetaWorld benchmarks. We also demonstrate that DEPS discovers\ninterpretable parameterized skills, such as an object grasping skill whose\ncontinuous arguments define the grasp location.\n","authors":["Vedant Gupta","Haotian Fu","Calvin Luo","Yiding Jiang","George Konidaris"],"pdf_url":"https://arxiv.org/pdf/2510.24095v1.pdf","comment":"Neurips 2025"},{"id":"http://arxiv.org/abs/2510.24088v1","updated":"2025-10-28T05:59:05Z","published":"2025-10-28T05:59:05Z","title":"Information-Theoretic Discrete Diffusion","summary":"  We present an information-theoretic framework for discrete diffusion models\nthat yields principled estimators of log-likelihood using score-matching\nlosses. Inspired by the I-MMSE identity for the Gaussian setup, we derive\nanalogous results for the discrete setting. Specifically, we introduce the\nInformation-Minimum Denoising Score Entropy (I-MDSE) relation, which links\nmutual information between data and its diffused version to the minimum\ndenoising score entropy (DSE) loss. We extend this theory to masked diffusion\nand establish the Information-Minimum Denoising Cross-Entropy (I-MDCE)\nrelation, connecting cross-entropy losses to mutual information in discrete\nmasked processes. These results provide a time-integral decomposition of the\nlog-likelihood of the data in terms of optimal score-based losses, showing that\ncommonly used losses such as DSE and DCE are not merely variational bounds but\ntight and principled estimators of log-likelihood. The I-MDCE decomposition\nfurther enables practical extensions, including time-free formula, conditional\nlikelihood estimation in prompt-response tasks, and coupled Monte Carlo\nestimation of likelihood ratios. Experiments on synthetic and real-world data\nconfirm the accuracy, variance stability, and utility of our estimators. The\ncode is publicly available at https://github.com/Dongjae0324/infodis.\n","authors":["Moongyu Jeon","Sangwoo Shin","Dongjae Jeon","Albert No"],"pdf_url":"https://arxiv.org/pdf/2510.24088v1.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24085v1","updated":"2025-10-28T05:54:50Z","published":"2025-10-28T05:54:50Z","title":"Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine\n  Learning Approach","summary":"  The increasing adoption of electric vehicles (EVs) necessitates an\nunderstanding of their driving behavior to enhance traffic safety and develop\nsmart driving systems. This study compares classical and machine learning\nmodels for EV car following behavior. Classical models include the Intelligent\nDriver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative\nVelocity (OVRV), and a simplified CACC model, while the machine learning\napproach employs a Random Forest Regressor. Using a real world dataset of an EV\nfollowing an internal combustion engine (ICE) vehicle under varied driving\nconditions, we calibrated classical model parameters by minimizing the RMSE\nbetween predictions and real data. The Random Forest model predicts\nacceleration using spacing, speed, and gap type as inputs. Results demonstrate\nthe Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),\n0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,\nCACC performed best, with an RMSE of 2.67 for long gaps. These findings\nhighlight the machine learning model's performance across all scenarios. Such\nmodels are valuable for simulating EV behavior and analyzing mixed autonomy\ntraffic dynamics in EV integrated environments.\n","authors":["Md. Shihab Uddin","Md Nazmus Shakib","Rahul Bhadani"],"pdf_url":"https://arxiv.org/pdf/2510.24085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14118v2","updated":"2025-10-28T05:52:26Z","published":"2025-01-23T22:20:30Z","title":"Selecting Critical Scenarios of DER Adoption in Distribution Grids Using\n  Bayesian Optimization","summary":"  We develop a new methodology to select scenarios of DER adoption most\ncritical for distribution grids. Anticipating risks of future voltage and line\nflow violations due to additional PV adopters is central for utility investment\nplanning but continues to rely on deterministic or ad hoc scenario selection.\nWe propose a highly efficient search framework based on multi-objective\nBayesian Optimization. We treat underlying grid stress metrics as\ncomputationally expensive black-box functions, approximated via Gaussian\nProcess surrogates and design an acquisition function based on probability of\nscenarios being Pareto-critical across a collection of line- and bus-based\nviolation objectives. Our approach provides a statistical guarantee and offers\nan order of magnitude speed-up relative to a conservative exhaustive search.\nCase studies on realistic feeders with 200-400 buses demonstrate the\neffectiveness and accuracy of our approach.\n","authors":["Olivier Mulkin","Miguel Heleno","Mike Ludkovski"],"pdf_url":"https://arxiv.org/pdf/2501.14118v2.pdf","comment":"12 pages, 4 tables, 12 figures"},{"id":"http://arxiv.org/abs/2509.23143v3","updated":"2025-10-28T05:44:55Z","published":"2025-09-27T06:06:36Z","title":"MathBode: Understanding LLM Reasoning with Dynamical Systems","summary":"  This paper presents MathBode, a dynamic diagnostic for mathematical reasoning\nin large language models (LLMs). Instead of one-shot accuracy, MathBode treats\neach parametric problem as a system: we drive a single parameter sinusoidally\nand fit first-harmonic responses of model outputs and exact solutions. This\nyields interpretable, frequency-resolved metrics -- gain (amplitude tracking)\nand phase (lag) -- that form Bode-style fingerprints. Across five closed-form\nfamilies (linear solve, ratio/saturation, compound interest, 2x2 linear\nsystems, similar triangles), the diagnostic surfaces systematic low-pass\nbehavior and growing phase lag that accuracy alone obscures. We compare several\nmodels against a symbolic baseline that calibrates the instrument ($G \\approx\n1$, $\\phi \\approx 0$). Results separate frontier from mid-tier models on\ndynamics, providing a compact, reproducible protocol that complements standard\nbenchmarks with actionable measurements of reasoning fidelity and consistency.\nWe open-source the dataset and code to enable further research and adoption.\n","authors":["Charles L. Wang"],"pdf_url":"https://arxiv.org/pdf/2509.23143v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23190v3","updated":"2025-10-28T05:36:40Z","published":"2025-05-29T07:28:36Z","title":"DeepRTE: Pre-trained Attention-based Neural Network for Radiative\n  Transfer","summary":"  In this paper, we propose a novel neural network approach, termed DeepRTE, to\naddress the steady-state Radiative Transfer Equation (RTE). The RTE is a\ndifferential-integral equation that governs the propagation of radiation\nthrough a participating medium, with applications spanning diverse domains such\nas neutron transport, atmospheric radiative transfer, heat transfer, and\noptical imaging. Our DeepRTE framework demonstrates superior computational\nefficiency for solving the steady-state RTE, surpassing traditional methods and\nexisting neural network approaches. This efficiency is achieved by embedding\nphysical information through derivation of the RTE and mathematically-informed\nnetwork architecture. Concurrently, DeepRTE achieves high accuracy with\nsignificantly fewer parameters, largely due to its incorporation of mechanisms\nsuch as multi-head attention. Furthermore, DeepRTE is a mesh-free neural\noperator framework with inherent zero-shot capability. This is achieved by\nincorporating Green's function theory and pre-training with delta-function\ninflow boundary conditions into both its architecture design and training data\nconstruction. The efficacy of the proposed approach is substantiated through\ncomprehensive numerical experiments.\n","authors":["Yekun Zhu","Min Tang","Zheng Ma"],"pdf_url":"https://arxiv.org/pdf/2505.23190v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24074v1","updated":"2025-10-28T05:21:55Z","published":"2025-10-28T05:21:55Z","title":"Deep Learning-Enhanced Calibration of the Heston Model: A Unified\n  Framework","summary":"  The Heston stochastic volatility model is a widely used tool in financial\nmathematics for pricing European options. However, its calibration remains\ncomputationally intensive and sensitive to local minima due to the model's\nnonlinear structure and high-dimensional parameter space. This paper introduces\na hybrid deep learning-based framework that enhances both the computational\nefficiency and the accuracy of the calibration procedure. The proposed approach\nintegrates two supervised feedforward neural networks: the Price Approximator\nNetwork (PAN), which approximates the option price surface based on strike and\nmoneyness inputs, and the Calibration Correction Network (CCN), which refines\nthe Heston model's output by correcting systematic pricing errors. Experimental\nresults on real S\\&P 500 option data demonstrate that the deep learning\napproach outperforms traditional calibration techniques across multiple error\nmetrics, achieving faster convergence and superior generalization in both\nin-sample and out-of-sample settings. This framework offers a practical and\nrobust solution for real-time financial model calibration.\n","authors":["Arman Zadgar","Somayeh Fallah","Farshid Mehrdoust"],"pdf_url":"https://arxiv.org/pdf/2510.24074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01360v2","updated":"2025-10-28T05:17:40Z","published":"2025-06-02T06:34:10Z","title":"RDB2G-Bench: A Comprehensive Benchmark for Automatic Graph Modeling of\n  Relational Databases","summary":"  Recent advances have demonstrated the effectiveness of graph-based learning\non relational databases (RDBs) for predictive tasks. Such approaches require\ntransforming RDBs into graphs, a process we refer to as RDB-to-graph modeling,\nwhere rows of tables are represented as nodes and foreign-key relationships as\nedges. Yet, effective modeling of RDBs into graphs remains challenging.\nSpecifically, there exist numerous ways to model RDBs into graphs, and\nperformance on predictive tasks varies significantly depending on the chosen\ngraph model of RDBs. In our analysis, we find that the best-performing graph\nmodel can yield up to a 10% higher performance compared to the common heuristic\nrule for graph modeling, which remains non-trivial to identify. To foster\nresearch on intelligent RDB-to-graph modeling, we introduce RDB2G-Bench, the\nfirst benchmark framework for evaluating such methods. We construct extensive\ndatasets covering 5 real-world RDBs and 12 predictive tasks, resulting in\naround 50k graph model-performance pairs for efficient and reproducible\nevaluations. Thanks to our precomputed datasets, we were able to benchmark 10\nautomatic RDB-to-graph modeling methods on the 12 tasks about 380x faster than\non-the-fly evaluation, which requires repeated GNN training. Our analysis of\nthe datasets and benchmark results reveals key structural patterns affecting\ngraph model effectiveness, along with practical implications for effective\ngraph modeling. Our datasets and code are available at\nhttps://github.com/chlehdwon/RDB2G-Bench.\n","authors":["Dongwon Choi","Sunwoo Kim","Juyeon Kim","Kyungho Kim","Geon Lee","Shinhwan Kang","Myunghwan Kim","Kijung Shin"],"pdf_url":"https://arxiv.org/pdf/2506.01360v2.pdf","comment":"Accepted at NeurIPS 2025 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2510.14137v2","updated":"2025-10-28T05:11:02Z","published":"2025-10-15T22:13:59Z","title":"Learning Wireless Interference Patterns: Decoupled GNN for Throughput\n  Prediction in Heterogeneous Multi-Hop p-CSMA Networks","summary":"  The p-persistent CSMA protocol is central to random-access MAC analysis, but\npredicting saturation throughput in heterogeneous multi-hop wireless networks\nremains a hard problem. Simplified models that assume a single, shared\ninterference domain can underestimate throughput by 48-62% in sparse\ntopologies. Exact Markov-chain analyses are accurate but scale exponentially in\ncomputation time, making them impractical for large networks. These\ncomputational barriers motivate structural machine learning approaches like\nGNNs for scalable throughput prediction in general network topologies. Yet\noff-the-shelf GNNs struggle here: a standard GCN yields 63.94% normalized mean\nabsolute error (NMAE) on heterogeneous networks because symmetric normalization\nconflates a node's direct interference with higher-order, cascading effects\nthat pertain to how interference propagates over the network graph.\n  Building on these insights, we propose the Decoupled Graph Convolutional\nNetwork (D-GCN), a novel architecture that explicitly separates processing of a\nnode's own transmission probability from neighbor interference effects. D-GCN\nreplaces mean aggregation with learnable attention, yielding interpretable,\nper-neighbor contribution weights while capturing complex multihop interference\npatterns. D-GCN attains 3.3% NMAE, outperforms strong baselines, remains\ntractable even when exact analytical methods become computationally infeasible,\nand enables gradient-based network optimization that achieves within 1% of\ntheoretical optima.\n","authors":["Faezeh Dehghan Tarzjani","Bhaskar Krishnamachari"],"pdf_url":"https://arxiv.org/pdf/2510.14137v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09060v2","updated":"2025-10-28T05:01:44Z","published":"2025-04-12T03:31:03Z","title":"Multimodal 3D Genome Pre-training","summary":"  Deep learning techniques have driven significant progress in various\nanalytical tasks within 3D genomics in computational biology. However, a\nholistic understanding of 3D genomics knowledge remains underexplored. Here, we\npropose MIX-HIC, the first multimodal foundation model of 3D genome that\nintegrates both 3D genome structure and epigenomic tracks, which obtains\nunified and comprehensive semantics. For accurate heterogeneous semantic\nfusion, we design the cross-modal interaction and mapping blocks for robust\nunified representation, yielding the accurate aggregation of 3D genome\nknowledge. Besides, we introduce the first large-scale dataset comprising over\n1 million pairwise samples of Hi-C contact maps and epigenomic tracks for\nhigh-quality pre-training, enabling the exploration of functional implications\nin 3D genomics. Extensive experiments show that MIX-HIC can significantly\nsurpass existing state-of-the-art methods in diverse downstream tasks. This\nwork provides a valuable resource for advancing 3D genomics research.\n","authors":["Minghao Yang","Pengteng Li","Yan Liang","Qianyi Cai","Zhihang Zheng","Shichen Zhang","Pengfei Zhang","Zhi-An Huang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2504.09060v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2503.23981v3","updated":"2025-10-28T04:55:22Z","published":"2025-03-31T11:50:21Z","title":"Federated Structured Sparse PCA for Anomaly Detection in IoT Networks","summary":"  Although federated learning has gained prominence as a privacy-preserving\nframework tailored for distributed Internet of Things (IoT) environments,\ncurrent federated principal component analysis (PCA) methods lack integration\nof sparsity, a critical feature for robust anomaly detection. To address this\nlimitation, we propose a novel federated structured sparse PCA (FedSSP)\napproach for anomaly detection in IoT networks. The proposed model uniquely\nintegrates double sparsity regularization: (1) row-wise sparsity governed by\n$\\ell_{2,p}$-norm with $p\\in [0,1)$ to eliminate redundant feature dimensions,\nand (2) element-wise sparsity via $\\ell_{q}$-norm with $q\\in [0,1)$ to suppress\nnoise-sensitive components. To solve this nonconvex problem in a distributed\nsetting, we devise an efficient optimization algorithm based on the proximal\nalternating minimization (PAM). Numerical experiments validate that\nincorporating structured sparsity enhances both model interpretability and\ndetection accuracy. Our code is available at\nhttps://github.com/xianchaoxiu/FedSSP.\n","authors":["Chenyi Huang","Xianchao Xiu"],"pdf_url":"https://arxiv.org/pdf/2503.23981v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03520v3","updated":"2025-10-28T04:54:54Z","published":"2024-11-05T21:54:50Z","title":"Forecasting Outside the Box: Application-Driven Optimal Pointwise\n  Forecasts for Stochastic Optimization","summary":"  We study a class of two-stage stochastic programs, namely, those with fixed\nrecourse matrix and fixed costs, and linear second stage. We show that, under\nmild assumptions, the problem can be solved with just one scenario, which we\ncall an ``optimal scenario.'' Such a scenario does not have to be unique and\nmay fall outside the support of the underlying distribution. Although finding\nan optimal scenario in general might be hard, we show that the result can be\nparticularly useful in the case of stochastic optimization problems with\ncontextual information, where the goal is to optimize the expected value of a\ncertain function given some contextual information (e.g., previous demand,\ncustomer type, etc.) that accompany the main data of interest. The contextual\ninformation allows for a better estimation of the quantity of interest via\nmachine learning methods. We focus on a class of learning methods -- sometimes\ncalled in the literature decision-focused learning -- that integrate the\nlearning and optimization procedures by means of a bilevel optimization\nformulation, which determines the parameters for pointwise forecasts. By using\nthe optimal scenario result, we prove that when such models are applied to the\nclass of contextual two-stage problems considered in this paper, the pointwise\nforecasts computed from the bilevel optimization formulation actually yield\nasymptotically the best approximation of an optimal scenario within the\nmodeler's pre-specified set of parameterized forecast functions. Numerical\nresults conducted with inventory problems from the literature (with synthetic\ndata) as well as a bike-sharing problem with real data demonstrate that the\nproposed approach performs well when compared to benchmark methods from the\nliterature.\n","authors":["Tito Homem-de-Mello","Juan Valencia","Felipe Lagos","Guido Lagos"],"pdf_url":"https://arxiv.org/pdf/2411.03520v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.24085v2","updated":"2025-10-28T04:48:14Z","published":"2025-09-28T21:43:17Z","title":"PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM","summary":"  We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a\nframework for cooperative cross-layer optimization in device-to-device (D2D)\ncommunication. Building on our previous work on single-device on-device LLMs,\nPEARL extends the paradigm by leveraging both publisher and subscriber states\nto guide Wi-Fi Aware (WA) parameter selection. A context-aware reward, which\nnormalizes latency by application tolerances and modulates energy by device\nbattery states, provides richer supervision for KL-based finetuning. We study\ntwo lightweight variants: PEARL (Head + Low-Rank Adaptation (LoRA)) achieves\nthe best overall performance, while PEARL-Lite (Head-only) delivers sub-20 ms\ninference at near-identical objective scores. Across synthetic scenarios\ngrounded in real measurements, PEARL improves objective scores over heuristic\nand compact model baselines and reduces energy by up to 16% in cooperative\nlow-battery cases. These results demonstrate that peer-aware context,\nreward-aligned training, and head-based efficiency make LLMs practical for\nalways-on, on-device cross-layer control. Code, real-world demo, and dataset\nare available at https://github.com/abman23/pearl\n","authors":["Ju-Hyung Lee","Yanqing Lu","Klaus Doppler"],"pdf_url":"https://arxiv.org/pdf/2509.24085v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24061v1","updated":"2025-10-28T04:44:49Z","published":"2025-10-28T04:44:49Z","title":"FALQON: Accelerating LoRA Fine-tuning with Low-Bit Floating-Point\n  Arithmetic","summary":"  Low-bit floating-point (FP) formats, such as FP8, provide significant\nacceleration and memory savings in model training thanks to native hardware\nsupport on modern GPUs and NPUs. However, we analyze that FP8 quantization\noffers speedup primarily for large-dimensional matrix multiplications, while\ninherent quantization overheads diminish speedup when applied to low-rank\nadaptation (LoRA), which uses small-dimensional matrices for efficient\nfine-tuning of large language models (LLMs). To address this limitation, we\npropose FALQON, a novel framework that eliminates the quantization overhead\nfrom separate LoRA computational paths by directly merging LoRA adapters into\nan FP8-quantized backbone during fine-tuning. Furthermore, we reformulate the\nforward and backward computations for merged adapters to significantly reduce\nquantization overhead, and introduce a row-wise proxy update mechanism that\nefficiently integrates substantial updates into the quantized backbone.\nExperimental evaluations demonstrate that FALQON achieves approximately a\n3$\\times$ training speedup over existing quantized LoRA methods with a similar\nlevel of accuracy, providing a practical solution for efficient large-scale\nmodel fine-tuning. Moreover, FALQON's end-to-end FP8 workflow removes the need\nfor post-training quantization, facilitating efficient deployment. Code is\navailable at https://github.com/iamkanghyunchoi/falqon.\n","authors":["Kanghyun Choi","Hyeyoon Lee","SunJong Park","Dain Kwon","Jinho Lee"],"pdf_url":"https://arxiv.org/pdf/2510.24061v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2502.09282v4","updated":"2025-10-28T04:40:41Z","published":"2025-02-13T12:54:13Z","title":"MsEdF: A Multi-stream Encoder-decoder Framework for Remote Sensing Image\n  Captioning","summary":"  Remote sensing images contain complex spatial patterns and semantic\nstructures, which makes the captioning model difficult to accurately describe.\nEncoder-decoder architectures have become the widely used approach for RSIC by\ntranslating visual content into descriptive text. However, many existing\nmethods rely on a single-stream architecture, which weakens the model to\naccurately describe the image. Such single-stream architectures typically\nstruggle to extract diverse spatial features or capture complex semantic\nrelationships, limiting their effectiveness in scenes with high intraclass\nsimilarity or contextual ambiguity. In this work, we propose a novel\nMulti-stream Encoder-decoder Framework (MsEdF) which improves the performance\nof RSIC by optimizing both the spatial representation and language generation\nof encoder-decoder architecture. The encoder fuses information from two\ncomplementary image encoders, thereby promoting feature diversity through the\nintegration of multiscale and structurally distinct cues. To improve the\ncapture of context-aware descriptions, we refine the input sequence's semantic\nmodeling on the decoder side using a stacked GRU architecture with an\nelement-wise aggregation scheme. Experiments on three benchmark RSIC datasets\nshow that MsEdF outperforms several baseline models.\n","authors":["Swadhin Das","Raksha Sharma"],"pdf_url":"https://arxiv.org/pdf/2502.09282v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24058v1","updated":"2025-10-28T04:35:05Z","published":"2025-10-28T04:35:05Z","title":"PULSE: Privileged Knowledge Transfer from Electrodermal Activity to\n  Low-Cost Sensors for Stress Monitoring","summary":"  Electrodermal activity (EDA), the primary signal for stress detection,\nrequires costly hardware often unavailable in real-world wearables. In this\npaper, we propose PULSE, a framework that utilizes EDA exclusively during\nself-supervised pretraining, while enabling inference without EDA but with more\nreadily available modalities such as ECG, BVP, ACC, and TEMP. Our approach\nseparates encoder outputs into shared and private embeddings. We align shared\nembeddings across modalities and fuse them into a modality-invariant\nrepresentation. The private embeddings carry modality-specific information to\nsupport the reconstruction objective. Pretraining is followed by knowledge\ntransfer where a frozen EDA teacher transfers sympathetic-arousal\nrepresentations into student encoders. On WESAD, our method achieves strong\nstress-detection performance, showing that representations of privileged EDA\ncan be transferred to low-cost sensors to improve accuracy while reducing\nhardware cost.\n","authors":["Zihan Zhao","Masood Mortazavi","Ning Yan"],"pdf_url":"https://arxiv.org/pdf/2510.24058v1.pdf","comment":"Accepted as a finders paper at ML4H 2025"},{"id":"http://arxiv.org/abs/2510.24056v1","updated":"2025-10-28T04:33:57Z","published":"2025-10-28T04:33:57Z","title":"Copula-Stein Discrepancy: A Generator-Based Stein Operator for\n  Archimedean Dependence","summary":"  Kernel Stein discrepancies (KSDs) have become a principal tool for\ngoodness-of-fit testing, but standard KSDs are often insensitive to\nhigher-order dependency structures, such as tail dependence, which are critical\nin many scientific and financial domains. We address this gap by introducing\nthe Copula-Stein Discrepancy (CSD), a novel class of discrepancies tailored to\nthe geometry of statistical dependence. By defining a Stein operator directly\non the copula density, CSD leverages the generative structure of dependence,\nrather than relying on the joint density's score function. For the broad class\nof Archimedean copulas, this approach yields a closed-form Stein kernel derived\nfrom the scalar generator function. We provide a comprehensive theoretical\nanalysis, proving that CSD (i) metrizes weak convergence of copula\ndistributions, ensuring it detects any mismatch in dependence; (ii) has an\nempirical estimator that converges at the minimax optimal rate of\n$O_P(n^{-1/2})$; and (iii) is provably sensitive to differences in tail\ndependence coefficients. The framework is extended to general non-Archimedean\ncopulas, including elliptical and vine copulas. Computationally, the exact CSD\nkernel evaluation scales linearly in dimension, while a novel random feature\napproximation reduces the $n$-dependence from quadratic $O(n^2)$ to near-linear\n$\\tilde{O}(n)$, making CSD a practical and theoretically principled tool for\ndependence-aware inference.\n","authors":["Agnideep Aich","Ashit Baran Aich"],"pdf_url":"https://arxiv.org/pdf/2510.24056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24055v1","updated":"2025-10-28T04:27:03Z","published":"2025-10-28T04:27:03Z","title":"Language-Conditioned Representations and Mixture-of-Experts Policy for\n  Robust Multi-Task Robotic Manipulation","summary":"  Perceptual ambiguity and task conflict limit multitask robotic manipulation\nvia imitation learning. We propose a framework combining a Language-Conditioned\nVisual Representation (LCVR) module and a Language-conditioned\nMixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual\nambiguities by grounding visual features with language instructions, enabling\ndifferentiation between visually similar tasks. To mitigate task conflict,\nLMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal\naction distributions, stabilized by gradient modulation. On real-robot\nbenchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion\nPolicy (DP) success rates by 33.75% and 25%, respectively. The full framework\nachieves a 79% average success, outperforming the advanced baseline by 21%. Our\nwork shows that combining semantic grounding and expert specialization enables\nrobust, efficient multi-task manipulation\n","authors":["Xiucheng Zhang","Yang Jiang","Hongwei Qing","Jiashuo Bai"],"pdf_url":"https://arxiv.org/pdf/2510.24055v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2510.24053v1","updated":"2025-10-28T04:24:39Z","published":"2025-10-28T04:24:39Z","title":"Low-N Protein Activity Optimization with FolDE","summary":"  Proteins are traditionally optimized through the costly construction and\nmeasurement of many mutants. Active Learning-assisted Directed Evolution (ALDE)\nalleviates that cost by predicting the best improvements and iteratively\ntesting mutants to inform predictions. However, existing ALDE methods face a\ncritical limitation: selecting the highest-predicted mutants in each round\nyields homogeneous training data insufficient for accurate prediction models in\nsubsequent rounds. Here we present FolDE, an ALDE method designed to maximize\nend-of-campaign success. In simulations across 20 protein targets, FolDE\ndiscovers 23% more top 10% mutants than the best baseline ALDE method (p=0.005)\nand is 55% more likely to find top 1% mutants. FolDE achieves this primarily\nthrough naturalness-based warm-starting, which augments limited activity\nmeasurements with protein language model outputs to improve activity\nprediction. We also introduce a constant-liar batch selector, which improves\nbatch diversity; this is important in multi-mutation campaigns but had limited\neffect in our benchmarks. The complete workflow is freely available as\nopen-source software, making efficient protein optimization accessible to any\nlaboratory.\n","authors":["Jacob B. Roberts","Catherine R. Ji","Isaac Donnell","Thomas D. Young","Allison N. Pearson","Graham A. Hudson","Leah S. Keiser","Mia Wesselkamper","Peter H. Winegar","Janik Ludwig","Sarah H. Klass","Isha V. Sheth","Ezechinyere C. Ukabiala","Maria C. T. Astolfi","Benjamin Eysenbach","Jay D. Keasling"],"pdf_url":"https://arxiv.org/pdf/2510.24053v1.pdf","comment":"18 pages, 4 figures. Preprint. Open-source software available at\n  https://github.com/JBEI/foldy"},{"id":"http://arxiv.org/abs/2501.02885v2","updated":"2025-10-28T04:18:29Z","published":"2025-01-06T09:55:55Z","title":"MDP3: A Training-free Approach for List-wise Frame Selection in\n  Video-LLMs","summary":"  Video large language models (Video-LLMs) have made significant progress in\nunderstanding videos. However, processing multiple frames leads to lengthy\nvisual token sequences, presenting challenges such as the limited context\nlength cannot accommodate the entire video, and the inclusion of irrelevant\nframes hinders visual perception. Hence, effective frame selection is crucial.\nThis paper emphasizes that frame selection should follow three key principles:\nquery relevance, list-wise diversity, and sequentiality. Existing methods, such\nas uniform frame sampling and query-frame matching, do not capture all of these\nprinciples. Thus, we propose Markov decision determinantal point process with\ndynamic programming (MDP3) for frame selection, a training-free and\nmodel-agnostic method that can be seamlessly integrated into existing\nVideo-LLMs. Our method first estimates frame similarities conditioned on the\nquery using a conditional Gaussian kernel within the reproducing kernel Hilbert\nspace~(RKHS). We then apply the determinantal point process~(DPP) to the\nsimilarity matrix to capture both query relevance and list-wise diversity. To\nincorporate sequentiality, we segment the video and apply DPP within each\nsegment, conditioned on the preceding segment selection, modeled as a Markov\ndecision process~(MDP) for allocating selection sizes across segments.\nTheoretically, MDP3 provides a \\((1 - 1/e)\\)-approximate solution to the\nNP-hard list-wise frame selection problem with pseudo-polynomial time\ncomplexity, demonstrating its efficiency. Empirically, MDP3 significantly\noutperforms existing methods, verifying its effectiveness and robustness.\n","authors":["Hui Sun","Shiyin Lu","Huanyu Wang","Qing-Guo Chen","Zhao Xu","Weihua Luo","Kaifu Zhang","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2501.02885v2.pdf","comment":"26 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.20445v5","updated":"2025-10-28T04:18:04Z","published":"2024-10-27T13:51:09Z","title":"TrajAgent: An LLM-Agent Framework for Trajectory Modeling via\n  Large-and-Small Model Collaboration","summary":"  Trajectory modeling, which includes research on trajectory data pattern\nmining and future prediction, has widespread applications in areas such as life\nservices, urban transportation, and public administration. Numerous methods\nhave been proposed to address specific problems within trajectory modeling.\nHowever, the heterogeneity of data and the diversity of trajectory tasks make\neffective and reliable trajectory modeling an important yet highly challenging\nendeavor, even for domain experts. In this paper, we propose TrajAgent, an\nagent framework powered by large language models, designed to facilitate robust\nand efficient trajectory modeling through automation modeling. This framework\nleverages and optimizes diverse specialized models to address various\ntrajectory modeling tasks across different datasets effectively. In TrajAgent,\nwe first develop UniEnv, an execution environment with a unified data and model\ninterface, to support the execution and training of various models. Building on\nUniEnv, we introduce an agentic workflow designed for automatic trajectory\nmodeling across various trajectory tasks and data. Furthermore, we introduce\ncollaborative learning schema between LLM-based agents and small speciallized\nmodels, to enhance the performance of the whole framework effectively.\nExtensive experiments on five tasks using four real-world datasets demonstrate\nthe effectiveness of TrajAgent in automated trajectory modeling, achieving a\nperformance improvement of 2.38%-69.91% over baseline methods. The codes and\ndata can be accessed via https://github.com/tsinghua-fib-lab/TrajAgent.\n","authors":["Yuwei Du","Jie Feng","Jie Zhao","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2410.20445v5.pdf","comment":"Accepted by NeurIPS 2025,\n  https://github.com/tsinghua-fib-lab/TrajAgent"},{"id":"http://arxiv.org/abs/2510.24049v1","updated":"2025-10-28T04:09:16Z","published":"2025-10-28T04:09:16Z","title":"Learning from History: A Retrieval-Augmented Framework for\n  Spatiotemporal Prediction","summary":"  Accurate and long-term spatiotemporal prediction for complex physical systems\nremains a fundamental challenge in scientific computing. While deep learning\nmodels, as powerful parametric approximators, have shown remarkable success,\nthey suffer from a critical limitation: the accumulation of errors during\nlong-term autoregressive rollouts often leads to physically implausible\nartifacts. This deficiency arises from their purely parametric nature, which\nstruggles to capture the full constraints of a system's intrinsic dynamics. To\naddress this, we introduce a novel \\textbf{Retrieval-Augmented Prediction\n(RAP)} framework, a hybrid paradigm that synergizes the predictive power of\ndeep networks with the grounded truth of historical data. The core philosophy\nof RAP is to leverage historical evolutionary exemplars as a non-parametric\nestimate of the system's local dynamics. For any given state, RAP efficiently\nretrieves the most similar historical analog from a large-scale database. The\ntrue future evolution of this analog then serves as a \\textbf{reference\ntarget}. Critically, this target is not a hard constraint in the loss function\nbut rather a powerful conditional input to a specialized dual-stream\narchitecture. It provides strong \\textbf{dynamic guidance}, steering the\nmodel's predictions towards physically viable trajectories. In extensive\nbenchmarks across meteorology, turbulence, and fire simulation, RAP not only\nsurpasses state-of-the-art methods but also significantly outperforms a strong\n\\textbf{analog-only forecasting baseline}. More importantly, RAP generates\npredictions that are more physically realistic by effectively suppressing error\ndivergence in long-term rollouts.\n","authors":["Hao Jia","Penghao Zhao","Hao Wu","Yuan Gao","Yangyu Tao","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2510.24049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24046v1","updated":"2025-10-28T04:02:49Z","published":"2025-10-28T04:02:49Z","title":"Causal-Aware Generative Adversarial Networks with Reinforcement Learning","summary":"  The utility of tabular data for tasks ranging from model training to\nlarge-scale data analysis is often constrained by privacy concerns or\nregulatory hurdles. While existing data generation methods, particularly those\nbased on Generative Adversarial Networks (GANs), have shown promise, they\nfrequently struggle with capturing complex causal relationship, maintaining\ndata utility, and providing provable privacy guarantees suitable for enterprise\ndeployment. We introduce CA-GAN, a novel generative framework specifically\nengineered to address these challenges for real-world tabular datasets. CA-GAN\nutilizes a two-step approach: causal graph extraction to learn a robust,\ncomprehensive causal relationship in the data's manifold, followed by a custom\nConditional WGAN-GP (Wasserstein GAN with Gradient Penalty) that operates\nexclusively as per the structure of nodes in the causal graph. More\nimportantly, the generator is trained with a new Reinforcement Learning-based\nobjective that aligns the causal graphs constructed from real and fake data,\nensuring the causal awareness in both training and sampling phases. We\ndemonstrate CA-GAN superiority over six SOTA methods across 14 tabular\ndatasets. Our evaluations, focused on core data engineering metrics: causal\npreservation, utility preservation, and privacy preservation. Our method offers\na practical, high-performance solution for data engineers seeking to create\nhigh-quality, privacy-compliant synthetic datasets to benchmark database\nsystems, accelerate software development, and facilitate secure data-driven\nresearch.\n","authors":["Tu Anh Hoang Nguyen","Dang Nguyen","Tri-Nhan Vo","Thuc Duy Le","Sunil Gupta"],"pdf_url":"https://arxiv.org/pdf/2510.24046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17281v2","updated":"2025-10-28T04:01:30Z","published":"2025-10-20T08:16:12Z","title":"MemoryBench: A Benchmark for Memory and Continual Learning in LLM\n  Systems","summary":"  Scaling up data, parameters, and test-time computation has been the\nmainstream methods to improve LLM systems (LLMsys), but their upper bounds are\nalmost reached due to the gradual depletion of high-quality data and marginal\ngains obtained from larger computational resource consumption. Inspired by the\nabilities of human and traditional AI systems in learning from practice,\nconstructing memory and continual learning frameworks for LLMsys has become an\nimportant and popular research direction in recent literature. Yet, existing\nbenchmarks for LLM memory often focus on evaluating the system on homogeneous\nreading comprehension tasks with long-form inputs rather than testing their\nabilities to learn from accumulated user feedback in service time. Therefore,\nwe propose a user feedback simulation framework and a comprehensive benchmark\ncovering multiple domains, languages, and types of tasks to evaluate the\ncontinual learning abilities of LLMsys. Experiments show that the effectiveness\nand efficiency of state-of-the-art baselines are far from satisfying, and we\nhope this benchmark could pave the way for future studies on LLM memory and\noptimization algorithms.\n","authors":["Qingyao Ai","Yichen Tang","Changyue Wang","Jianming Long","Weihang Su","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2510.17281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01068v4","updated":"2025-10-28T04:00:18Z","published":"2025-02-03T05:25:09Z","title":"FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation","summary":"  While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV.\n","authors":["Dongwon Jo","Jiwon Song","Yulhwa Kim","Jae-Joon Kim"],"pdf_url":"https://arxiv.org/pdf/2502.01068v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.11829v2","updated":"2025-10-28T03:59:44Z","published":"2025-10-13T18:29:15Z","title":"Schrödinger bridge for generative AI: Soft-constrained formulation and\n  convergence analysis","summary":"  Generative AI can be framed as the problem of learning a model that maps\nsimple reference measures into complex data distributions, and it has recently\nfound a strong connection to the classical theory of the Schr\\\"odinger bridge\nproblems (SBPs) due partly to their common nature of interpolating between\nprescribed marginals via entropy-regularized stochastic dynamics. However, the\nclassical SBP enforces hard terminal constraints, which often leads to\ninstability in practical implementations, especially in high-dimensional or\ndata-scarce regimes. To address this challenge, we follow the idea of the\nso-called soft-constrained Schr\\\"odinger bridge problem (SCSBP), in which the\nterminal constraint is replaced by a general penalty function. This relaxation\nleads to a more flexible stochastic control formulation of McKean-Vlasov type.\n  We establish the existence of optimal solutions for all penalty levels and\nprove that, as the penalty grows, both the controls and value functions\nconverge to those of the classical SBP at a linear rate. Our analysis builds on\nDoob's h-transform representations, the stability results of Schr\\\"odinger\npotentials, Gamma-convergence, and a novel fixed-point argument that couples an\noptimization problem over the space of measures with an auxiliary entropic\noptimal transport problem. These results not only provide the first\nquantitative convergence guarantees for soft-constrained bridges but also shed\nlight on how penalty regularization enables robust generative modeling,\nfine-tuning, and transfer learning.\n","authors":["Jin Ma","Ying Tan","Renyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2510.11829v2.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2510.24044v1","updated":"2025-10-28T03:56:20Z","published":"2025-10-28T03:56:20Z","title":"Mitigating Negative Transfer via Reducing Environmental Disagreement","summary":"  Unsupervised Domain Adaptation~(UDA) focuses on transferring knowledge from a\nlabeled source domain to an unlabeled target domain, addressing the challenge\nof \\emph{domain shift}. Significant domain shifts hinder effective knowledge\ntransfer, leading to \\emph{negative transfer} and deteriorating model\nperformance. Therefore, mitigating negative transfer is essential. This study\nrevisits negative transfer through the lens of causally disentangled learning,\nemphasizing cross-domain discriminative disagreement on non-causal\nenvironmental features as a critical factor. Our theoretical analysis reveals\nthat overreliance on non-causal environmental features as the environment\nevolves can cause discriminative disagreements~(termed \\emph{environmental\ndisagreement}), thereby resulting in negative transfer. To address this, we\npropose Reducing Environmental Disagreement~(RED), which disentangles each\nsample into domain-invariant causal features and domain-specific non-causal\nenvironmental features via adversarially training domain-specific environmental\nfeature extractors in the opposite domains. Subsequently, RED estimates and\nreduces environmental disagreement based on domain-specific non-causal\nenvironmental features. Experimental results confirm that RED effectively\nmitigates negative transfer and achieves state-of-the-art performance.\n","authors":["Hui Sun","Zheng Xie","Hao-Yuan He","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2510.24044v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.22802v2","updated":"2025-10-28T03:55:35Z","published":"2025-06-28T08:08:16Z","title":"Riemannian-Geometric Fingerprints of Generative Models","summary":"  Recent breakthroughs and rapid integration of generative models (GMs) have\nsparked interest in the problem of model attribution and their fingerprints.\nFor instance, service providers need reliable methods of authenticating their\nmodels to protect their IP, while users and law enforcement seek to verify the\nsource of generated content for accountability and trust. In addition, a\ngrowing threat of model collapse is arising, as more model-generated data are\nbeing fed back into sources (e.g., YouTube) that are often harvested for\ntraining (\"regurgitative training\"), heightening the need to differentiate\nsynthetic from human data. Yet, a gap still exists in understanding generative\nmodels' fingerprints, we believe, stemming from the lack of a formal framework\nthat can define, represent, and analyze the fingerprints in a principled way.\nTo address this gap, we take a geometric approach and propose a new definition\nof artifact and fingerprint of GMs using Riemannian geometry, which allows us\nto leverage the rich theory of differential geometry. Our new definition\ngeneralizes previous work (Song et al., 2024) to non-Euclidean manifolds by\nlearning Riemannian metrics from data and replacing the Euclidean distances and\nnearest-neighbor search with geodesic distances and kNN-based Riemannian center\nof mass. We apply our theory to a new gradient-based algorithm for computing\nthe fingerprints in practice. Results show that it is more effective in\ndistinguishing a large array of GMs, spanning across 4 different datasets in 2\ndifferent resolutions (64 by 64, 256 by 256), 27 model architectures, and 2\nmodalities (Vision, Vision-Language). Using our proposed definition\nsignificantly improves the performance on model attribution, as well as a\ngeneralization to unseen datasets, model types, and modalities, suggesting its\npractical efficacy.\n","authors":["Hae Jin Song","Laurent Itti"],"pdf_url":"https://arxiv.org/pdf/2506.22802v2.pdf","comment":"ICCV 2025 Highlight paper"},{"id":"http://arxiv.org/abs/2510.24043v1","updated":"2025-10-28T03:53:46Z","published":"2025-10-28T03:53:46Z","title":"Localized Kernel Projection Outlyingness: A Two-Stage Approach for\n  Multi-Modal Outlier Detection","summary":"  This paper presents Two-Stage LKPLO, a novel multi-stage outlier detection\nframework that overcomes the coexisting limitations of conventional\nprojection-based methods: their reliance on a fixed statistical metric and\ntheir assumption of a single data structure. Our framework uniquely synthesizes\nthree key concepts: (1) a generalized loss-based outlyingness measure (PLO)\nthat replaces the fixed metric with flexible, adaptive loss functions like our\nproposed SVM-like loss; (2) a global kernel PCA stage to linearize non-linear\ndata structures; and (3) a subsequent local clustering stage to handle\nmulti-modal distributions. Comprehensive 5-fold cross-validation experiments on\n10 benchmark datasets, with automated hyperparameter optimization, demonstrate\nthat Two-Stage LKPLO achieves state-of-the-art performance. It significantly\noutperforms strong baselines on datasets with challenging structures where\nexisting methods fail, most notably on multi-cluster data (Optdigits) and\ncomplex, high-dimensional data (Arrhythmia). Furthermore, an ablation study\nempirically confirms that the synergistic combination of both the kernelization\nand localization stages is indispensable for its superior performance. This\nwork contributes a powerful new tool for a significant class of outlier\ndetection problems and underscores the importance of hybrid, multi-stage\narchitectures.\n","authors":["Akira Tamamori"],"pdf_url":"https://arxiv.org/pdf/2510.24043v1.pdf","comment":"10 pages, 4 figures; submitted to The IEICE Transactions on\n  Information and Systems"},{"id":"http://arxiv.org/abs/2505.17257v4","updated":"2025-10-28T03:53:33Z","published":"2025-05-22T20:10:55Z","title":"JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model","summary":"  Large language models (LLMs) have revolutionized natural language processing\nand are increasingly applied to other sequential data types, including genetic\nsequences. However, adapting LLMs to genomics presents significant challenges.\nCapturing complex genomic interactions requires modeling long-range\ndependencies within DNA sequences, where interactions often span over 10,000\nbase pairs, even within a single gene, posing substantial computational burdens\nunder conventional model architectures and training paradigms. Moreover,\nstandard LLM training approaches are suboptimal for DNA: autoregressive\ntraining, while efficient, supports only unidirectional understanding. However,\nDNA is inherently bidirectional, e.g., bidirectional promoters regulate\ntranscription in both directions and account for nearly 11% of human gene\nexpression. Masked language models (MLMs) allow bidirectional understanding but\nare inefficient, as only masked tokens contribute to the loss per step. To\naddress these limitations, we introduce JanusDNA, the first bidirectional DNA\nfoundation model built upon a novel pretraining paradigm that combines the\noptimization efficiency of autoregressive modeling with the bidirectional\ncomprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and\nMixture of Experts (MoE) architecture, combining long-range modeling of\nAttention with efficient sequential learning of Mamba. MoE layers further scale\nmodel capacity via sparse activation while keeping computational cost low.\nNotably, JanusDNA processes up to 1 million base pairs at single nucleotide\nresolution on a single 80GB GPU. Extensive experiments and ablations show\nJanusDNA achieves new SOTA results on three genomic representation benchmarks,\noutperforming models with 250x more activated parameters. Code:\nhttps://github.com/Qihao-Duan/JanusDNA\n","authors":["Qihao Duan","Bingding Huang","Zhenqiao Song","Irina Lehmann","Lei Gu","Roland Eils","Benjamin Wild"],"pdf_url":"https://arxiv.org/pdf/2505.17257v4.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.05316v3","updated":"2025-10-28T03:50:11Z","published":"2025-06-05T17:55:43Z","title":"Improving Data Efficiency for LLM Reinforcement Fine-tuning Through\n  Difficulty-targeted Online Data Selection and Rollout Replay","summary":"  Reinforcement learning (RL) has become an effective approach for fine-tuning\nlarge language models (LLMs), particularly to enhance their reasoning\ncapabilities. However, RL fine-tuning remains highly resource-intensive, and\nexisting work has largely overlooked the problem of data efficiency. In this\npaper, we propose two techniques to improve data efficiency in LLM RL\nfine-tuning: difficulty-targeted online data selection and rollout replay. We\nintroduce the notion of adaptive difficulty to guide online data selection,\nprioritizing questions of moderate difficulty that are more likely to yield\ninformative learning signals. To estimate adaptive difficulty efficiently, we\ndevelop an attention-based framework that requires rollouts for only a small\nreference set of questions. The adaptive difficulty of the remaining questions\nis then estimated based on their similarity to this set. To further reduce\nrollout cost, we introduce a rollout replay mechanism inspired by experience\nreplay in traditional RL. This technique reuses recent rollouts, lowering\nper-step computation while maintaining stable updates. Experiments across 6\nLLM-dataset combinations show that our method reduces RL fine-tuning time by\n23% to 62% while reaching the same level of performance as the original GRPO\nalgorithm. Our code is available at\nhttps://github.com/ASTRAL-Group/data-efficient-llm-rl.\n","authors":["Yifan Sun","Jingyan Shen","Yibin Wang","Tianyu Chen","Zhendong Wang","Mingyuan Zhou","Huan Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.05316v3.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24039v1","updated":"2025-10-28T03:49:01Z","published":"2025-10-28T03:49:01Z","title":"Geometric Algorithms for Neural Combinatorial Optimization with\n  Constraints","summary":"  Self-Supervised Learning (SSL) for Combinatorial Optimization (CO) is an\nemerging paradigm for solving combinatorial problems using neural networks. In\nthis paper, we address a central challenge of SSL for CO: solving problems with\ndiscrete constraints. We design an end-to-end differentiable framework that\nenables us to solve discrete constrained optimization problems with neural\nnetworks. Concretely, we leverage algorithmic techniques from the literature on\nconvex geometry and Carath\\'eodory's theorem to decompose neural network\noutputs into convex combinations of polytope corners that correspond to\nfeasible sets. This decomposition-based approach enables self-supervised\ntraining but also ensures efficient quality-preserving rounding of the neural\nnet output into feasible solutions. Extensive experiments in\ncardinality-constrained optimization show that our approach can consistently\noutperform neural baselines. We further provide worked-out examples of how our\nmethod can be applied beyond cardinality-constrained problems to a diverse set\nof combinatorial optimization tasks, including finding independent sets in\ngraphs, and solving matroid-constrained problems.\n","authors":["Nikolaos Karalias","Akbar Rafiey","Yifei Xu","Zhishang Luo","Behrooz Tahmasebi","Connie Jiang","Stefanie Jegelka"],"pdf_url":"https://arxiv.org/pdf/2510.24039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24037v1","updated":"2025-10-28T03:39:18Z","published":"2025-10-28T03:39:18Z","title":"Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for\n  Vision Models","summary":"  Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision\nmodels to downstream tasks. Among PEFT paradigms, sparse tuning achieves\nremarkable performance by adjusting only the weights most relevant to\ndownstream tasks, rather than densely tuning the entire weight matrix. Current\nmethods follow a two-stage paradigm. First, it locates task-relevant weights by\ngradient information, which overlooks the parameter adjustments during\nfine-tuning and limits the performance. Second, it updates only the located\nweights by applying a sparse mask to the gradient of the weight matrix, which\nresults in high memory usage due to the storage of all weight matrices in the\noptimizer. In this paper, we propose a one-stage method named SNELLA to\novercome the above limitations. For memory usage, SNELLA selectively updates\nthe weight matrix by adding it to another sparse matrix that is merged by two\nlow-rank learnable matrices. We extend the low-rank decomposition by\nintroducing nonlinear kernel functions, thereby increasing the rank of the\nresulting merged matrix to prevent the interdependency among weight updates,\nenabling better adaptation to downstream tasks. For locating task-relevant\nweights, we propose an adaptive bi-level sparsity allocation mechanism that\nencourages weights to compete across and inside layers based on their\nimportance scores in an end-to-end manner. Extensive experiments are conducted\non classification, segmentation, and generation tasks using different\npre-trained vision models. The results show that SNELLA achieves SOTA\nperformance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.\n90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.\nCompared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%\nacross models with parameter scales from 86M to 632M. Our source codes are\navailable at https://github.com/ssfgunner/SNELL.\n","authors":["Shufan Shen","Junshu Sun","Shuhui Wang","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2510.24037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24035v1","updated":"2025-10-28T03:36:05Z","published":"2025-10-28T03:36:05Z","title":"GraphNet: A Large-Scale Computational Graph Dataset for Tensor Compiler\n  Research","summary":"  We introduce GraphNet, a dataset of 2.7K real-world deep learning\ncomputational graphs with rich metadata, spanning six major task categories\nacross multiple deep learning frameworks. To evaluate tensor compiler\nperformance on these samples, we propose the benchmark metric Speedup Score\nS(t), which jointly considers runtime speedup and execution correctness under\ntunable tolerance levels, offering a reliable measure of general optimization\ncapability. Furthermore, we extend S(t) to the Error-aware Speedup Score ES(t),\nwhich incorporates error information and helps compiler developers identify key\nperformance bottlenecks. In this report, we benchmark the default tensor\ncompilers, CINN for PaddlePaddle and TorchInductor for PyTorch, on computer\nvision (CV) and natural language processing (NLP) samples to demonstrate the\npracticality of GraphNet. The full construction pipeline with graph extraction\nand compiler evaluation tools is available at\nhttps://github.com/PaddlePaddle/GraphNet .\n","authors":["Xinqi Li","Yiqun Liu","Shan Jiang","Enrong Zheng","Huaijin Zheng","Wenhao Dai","Haodong Deng","Dianhai Yu","Yanjun Ma"],"pdf_url":"https://arxiv.org/pdf/2510.24035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.01161v2","updated":"2025-10-28T03:28:48Z","published":"2025-10-01T17:48:23Z","title":"Prosperity before Collapse: How Far Can Off-Policy RL Reach with Stale\n  Data on LLMs?","summary":"  Reinforcement learning has been central to recent advances in large language\nmodel reasoning, but most algorithms rely on on-policy training that demands\nfresh rollouts at every update, limiting efficiency and scalability.\nAsynchronous RL systems alleviate this by decoupling rollout generation from\ntraining, yet their effectiveness hinges on tolerating large staleness in\nrollout data, a setting where existing methods either degrade in performance or\ncollapse. We revisit this challenge and uncover a prosperity-before-collapse\nphenomenon: stale data can be as informative as on-policy data if exploited\nproperly. Building on this insight, we introduce M2PO (Second-Moment Trust\nPolicy Optimization), which constrains the second moment of importance weights\nto suppress only extreme outliers while preserving informative updates.\nNotably, M2PO sharply reduces the fraction of clipped tokens under high\nstaleness (from 1.22% to 0.06% over training), precisely masking high-variance\ntokens while maintaining stable optimization. Extensive evaluation across six\nmodels (from 1.7B to 32B) and eight benchmarks shows that M2PO delivers stable\noff-policy training even with data stale by at least 256 model updates and\nmatches on-policy performance.\n","authors":["Haizhong Zheng","Jiawei Zhao","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2510.01161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24029v1","updated":"2025-10-28T03:24:02Z","published":"2025-10-28T03:24:02Z","title":"Improved Accuracy of Robot Localization Using 3-D LiDAR in a\n  Hippocampus-Inspired Model","summary":"  Boundary Vector Cells (BVCs) are a class of neurons in the brains of\nvertebrates that encode environmental boundaries at specific distances and\nallocentric directions, playing a central role in forming place fields in the\nhippocampus. Most computational BVC models are restricted to two-dimensional\n(2D) environments, making them prone to spatial ambiguities in the presence of\nhorizontal symmetries in the environment. To address this limitation, we\nincorporate vertical angular sensitivity into the BVC framework, thereby\nenabling robust boundary detection in three dimensions, and leading to\nsignificantly more accurate spatial localization in a biologically-inspired\nrobot model.\n  The proposed model processes LiDAR data to capture vertical contours, thereby\ndisambiguating locations that would be indistinguishable under a purely 2D\nrepresentation. Experimental results show that in environments with minimal\nvertical variation, the proposed 3D model matches the performance of a 2D\nbaseline; yet, as 3D complexity increases, it yields substantially more\ndistinct place fields and markedly reduces spatial aliasing. These findings\nshow that adding a vertical dimension to BVC-based localization can\nsignificantly enhance navigation and mapping in real-world 3D spaces while\nretaining performance parity in simpler, near-planar scenarios.\n","authors":["Andrew Gerstenslager","Bekarys Dukenbaev","Ali A. Minai"],"pdf_url":"https://arxiv.org/pdf/2510.24029v1.pdf","comment":"8 pages, 9 figures, Presented at the 2025 International Joint\n  Conference on Neural Networks, Rome, July 2025"},{"id":"http://arxiv.org/abs/2510.23564v2","updated":"2025-10-28T03:22:35Z","published":"2025-10-27T17:35:15Z","title":"ReCode: Unify Plan and Action for Universal Granularity Control","summary":"  Real-world tasks require decisions at varying granularities, and humans excel\nat this by leveraging a unified cognitive representation where planning is\nfundamentally understood as a high-level form of action. However, current Large\nLanguage Model (LLM)-based agents lack this crucial capability to operate\nfluidly across decision granularities. This limitation stems from existing\nparadigms that enforce a rigid separation between high-level planning and\nlow-level action, which impairs dynamic adaptability and limits generalization.\nWe propose ReCode (Recursive Code Generation), a novel paradigm that addresses\nthis limitation by unifying planning and action within a single code\nrepresentation. In this representation, ReCode treats high-level plans as\nabstract placeholder functions, which the agent then recursively decomposes\ninto finer-grained sub-functions until reaching primitive actions. This\nrecursive approach dissolves the rigid boundary between plan and action,\nenabling the agent to dynamically control its decision granularity.\nFurthermore, the recursive structure inherently generates rich,\nmulti-granularity training data, enabling models to learn hierarchical\ndecision-making processes. Extensive experiments show ReCode significantly\nsurpasses advanced baselines in inference performance and demonstrates\nexceptional data efficiency in training, validating our core insight that\nunifying planning and action through recursive code generation is a powerful\nand effective approach to achieving universal granularity control. The code is\navailable at https://github.com/FoundationAgents/ReCode.\n","authors":["Zhaoyang Yu","Jiayi Zhang","Huixue Su","Yufan Zhao","Yifan Wu","Mingyi Deng","Jinyu Xiang","Yizhang Lin","Lingxiao Tang","Yingchao Li","Yuyu Luo","Bang Liu","Chenglin Wu"],"pdf_url":"https://arxiv.org/pdf/2510.23564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24027v1","updated":"2025-10-28T03:19:06Z","published":"2025-10-28T03:19:06Z","title":"Spatio-temporal Multivariate Time Series Forecast with Chosen Variables","summary":"  Spatio-Temporal Multivariate time series Forecast (STMF) uses the time series\nof $n$ spatially distributed variables in a period of recent past to forecast\ntheir values in a period of near future. It has important applications in\nspatio-temporal sensing forecast such as road traffic prediction and air\npollution prediction. Recent papers have addressed a practical problem of\nmissing variables in the model input, which arises in the sensing applications\nwhere the number $m$ of sensors is far less than the number $n$ of locations to\nbe monitored, due to budget constraints. We observe that the state of the art\nassumes that the $m$ variables (i.e., locations with sensors) in the model\ninput are pre-determined and the important problem of how to choose the $m$\nvariables in the input has never been studied. This paper fills the gap by\nstudying a new problem of STMF with chosen variables, which optimally selects\n$m$-out-of-$n$ variables for the model input in order to maximize the forecast\naccuracy. We propose a unified framework that jointly performs variable\nselection and model optimization for both forecast accuracy and model\nefficiency. It consists of three novel technical components: (1) masked\nvariable-parameter pruning, which progressively prunes less informative\nvariables and attention parameters through quantile-based masking; (2)\nprioritized variable-parameter replay, which replays low-loss past samples to\npreserve learned knowledge for model stability; (3) dynamic extrapolation\nmechanism, which propagates information from variables selected for the input\nto all other variables via learnable spatial embeddings and adjacency\ninformation. Experiments on five real-world datasets show that our work\nsignificantly outperforms the state-of-the-art baselines in both accuracy and\nefficiency, demonstrating the effectiveness of joint variable selection and\nmodel optimization.\n","authors":["Zibo Liu","Zhe Jiang","Zelin Xu","Tingsong Xiao","Yupu Zhang","Zhengkun Xiao","Haibo Wang","Shigang Chen"],"pdf_url":"https://arxiv.org/pdf/2510.24027v1.pdf","comment":"In submission"},{"id":"http://arxiv.org/abs/2510.24026v1","updated":"2025-10-28T03:10:54Z","published":"2025-10-28T03:10:54Z","title":"Efficient Global-Local Fusion Sampling for Physics-Informed Neural\n  Networks","summary":"  The accuracy of Physics-Informed Neural Networks (PINNs) critically depends\non the placement of collocation points, as the PDE loss is approximated through\nsampling over the solution domain. Global sampling ensures stability by\ncovering the entire domain but requires many samples and is computationally\nexpensive, whereas local sampling improves efficiency by focusing on\nhigh-residual regions but may neglect well-learned areas, reducing robustness.\nWe propose a Global-Local Fusion (GLF) Sampling Strategy that combines the\nstrengths of both approaches. Specifically, new collocation points are\ngenerated by perturbing training points with Gaussian noise scaled inversely to\nthe residual, thereby concentrating samples in difficult regions while\npreserving exploration. To further reduce computational overhead, a lightweight\nlinear surrogate is introduced to approximate the global residual-based\ndistribution, achieving similar effectiveness at a fraction of the cost.\nTogether, these components, residual-adaptive sampling and residual-based\napproximation, preserve the stability of global methods while retaining the\nefficiency of local refinement. Extensive experiments on benchmark PDEs\ndemonstrate that GLF consistently improves both accuracy and efficiency\ncompared with global and local sampling strategies. This study provides a\npractical and scalable framework for enhancing the reliability and efficiency\nof PINNs in solving complex and high-dimensional PDEs.\n","authors":["Jiaqi Luo","Shixin Xu","Zhouwang Yang"],"pdf_url":"https://arxiv.org/pdf/2510.24026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.02542v3","updated":"2025-10-28T03:07:50Z","published":"2024-12-03T16:34:49Z","title":"Unveiling Concept Attribution in Diffusion Models","summary":"  Diffusion models have shown remarkable abilities in generating realistic and\nhigh-quality images from text prompts. However, a trained model remains largely\nblack-box; little do we know about the roles of its components in exhibiting a\nconcept such as objects or styles. Recent works employ causal tracing to\nlocalize knowledge-storing layers in generative models without showing how\nother layers contribute to the target concept. In this work, we approach\ndiffusion models' interpretability problem from a more general perspective and\npose a question: \\textit{``How do model components work jointly to demonstrate\nknowledge?''}. To answer this question, we decompose diffusion models using\ncomponent attribution, systematically unveiling the importance of each\ncomponent (specifically the model parameter) in generating a concept. The\nproposed framework, called \\textbf{C}omponent \\textbf{A}ttribution for\n\\textbf{D}iffusion Model (CAD), discovers the localization of concept-inducing\n(positive) components, while interestingly uncovers another type of components\nthat contribute negatively to generating a concept, which is missing in the\nprevious knowledge localization work. Based on this holistic understanding of\ndiffusion models, we introduce two fast, inference-time model editing\nalgorithms, CAD-Erase and CAD-Amplify; in particular, CAD-Erase enables erasure\nand CAD-Amplify allows amplification of a generated concept by ablating the\npositive and negative components, respectively, while retaining knowledge of\nother concepts. Extensive experimental results validate the significance of\nboth positive and negative components pinpointed by our framework,\ndemonstrating the potential of providing a complete view of interpreting\ngenerative models. Our code is available\n\\href{https://github.com/mail-research/CAD-attribution4diffusion}{here}.\n","authors":["Quang H. Nguyen","Hoang Phan","Khoa D. Doan"],"pdf_url":"https://arxiv.org/pdf/2412.02542v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24025v1","updated":"2025-10-28T03:07:06Z","published":"2025-10-28T03:07:06Z","title":"NeuroPathNet: Dynamic Path Trajectory Learning for Brain Functional\n  Connectivity Analysis","summary":"  Understanding the evolution of brain functional networks over time is of\ngreat significance for the analysis of cognitive mechanisms and the diagnosis\nof neurological diseases. Existing methods often have difficulty in capturing\nthe temporal evolution characteristics of connections between specific\nfunctional communities. To this end, this paper proposes a new path-level\ntrajectory modeling framework (NeuroPathNet) to characterize the dynamic\nbehavior of connection pathways between brain functional partitions. Based on\nmedically supported static partitioning schemes (such as Yeo and Smith ICA), we\nextract the time series of connection strengths between each pair of functional\npartitions and model them using a temporal neural network. We validate the\nmodel performance on three public functional Magnetic Resonance Imaging (fMRI)\ndatasets, and the results show that it outperforms existing mainstream methods\nin multiple indicators. This study can promote the development of dynamic graph\nlearning methods for brain network analysis, and provide possible clinical\napplications for the diagnosis of neurological diseases.\n","authors":["Guo Tianqi Guo","Chen Liping","Peng Ciyuan","Guo Jingjing","Ren Jing"],"pdf_url":"https://arxiv.org/pdf/2510.24025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22379v2","updated":"2025-10-28T03:06:09Z","published":"2025-10-25T17:48:46Z","title":"TraceTrans: Translation and Spatial Tracing for Surgical Prediction","summary":"  Image-to-image translation models have achieved notable success in converting\nimages across visual domains and are increasingly used for medical tasks such\nas predicting post-operative outcomes and modeling disease progression.\nHowever, most existing methods primarily aim to match the target distribution\nand often neglect spatial correspondences between the source and translated\nimages. This limitation can lead to structural inconsistencies and\nhallucinations, undermining the reliability and interpretability of the\npredictions. These challenges are accentuated in clinical applications by the\nstringent requirement for anatomical accuracy. In this work, we present\nTraceTrans, a novel deformable image translation model designed for\npost-operative prediction that generates images aligned with the target\ndistribution while explicitly revealing spatial correspondences with the\npre-operative input. The framework employs an encoder for feature extraction\nand dual decoders for predicting spatial deformations and synthesizing the\ntranslated image. The predicted deformation field imposes spatial constraints\non the generated output, ensuring anatomical consistency with the source.\nExtensive experiments on medical cosmetology and brain MRI datasets demonstrate\nthat TraceTrans delivers accurate and interpretable post-operative predictions,\nhighlighting its potential for reliable clinical deployment.\n","authors":["Xiyu Luo","Haodong Li","Xinxing Cheng","He Zhao","Yang Hu","Xuan Song","Tianyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.22379v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.08312v2","updated":"2025-10-28T02:55:43Z","published":"2025-08-09T00:00:17Z","title":"CFM-GP: Unified Conditional Flow Matching to Learn Gene Perturbation\n  Across Cell Types","summary":"  Understanding gene perturbation effects across diverse cellular contexts is a\ncentral challenge in functional genomics, with important implications for\ntherapeutic discovery and precision medicine. Single-cell technologies enable\nhigh-resolution measurement of transcriptional responses, but collecting such\ndata is costly and time-consuming, especially when repeated for each cell type.\nExisting computational methods often require separate models per cell type,\nlimiting scalability and generalization. We present CFM-GP, a method for cell\ntype-agnostic gene perturbation prediction. CFM-GP learns a continuous,\ntime-dependent transformation between unperturbed and perturbed gene expression\ndistributions, conditioned on cell type, allowing a single model to predict\nacross all cell types. Unlike prior approaches that use discrete modeling,\nCFM-GP employs a flow matching objective to capture perturbation dynamics in a\nscalable manner. We evaluate on five datasets: SARS-CoV-2 infection, IFN-beta\nstimulated PBMCs, glioblastoma treated with Panobinostat, lupus under IFN-beta\nstimulation, and Statefate progenitor fate mapping. CFM-GP consistently\noutperforms state-of-the-art baselines in R-squared and Spearman correlation,\nand pathway enrichment analysis confirms recovery of key biological pathways.\nThese results demonstrate the robustness and biological fidelity of CFM-GP as a\nscalable solution for cross-cell type gene perturbation prediction.\n","authors":["Abrar Rahman Abir","Sajib Acharjee Dip","Liqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.08312v2.pdf","comment":"28 Pages, 19 Tables, 8 Figures. The first two authors contributed\n  equally"},{"id":"http://arxiv.org/abs/2508.19563v3","updated":"2025-10-28T02:52:33Z","published":"2025-08-27T04:46:05Z","title":"Robustness is Important: Limitations of LLMs for Data Fitting","summary":"  Large Language Models (LLMs) are being applied in a wide array of settings,\nwell beyond the typical language-oriented use cases. In particular, LLMs are\nincreasingly used as a plug-and-play method for fitting data and generating\npredictions. Prior work has shown that LLMs, via in-context learning or\nsupervised fine-tuning, can perform competitively with many tabular supervised\nlearning techniques in terms of predictive performance. However, we identify a\ncritical vulnerability of using LLMs for data fitting -- making changes to data\nrepresentation that are completely irrelevant to the underlying learning task\ncan drastically alter LLMs' predictions on the same data. For example, simply\nchanging variable names can sway the size of prediction error by as much as 82%\nin certain settings. Such prediction sensitivity with respect to\ntask-irrelevant variations manifests under both in-context learning and\nsupervised fine-tuning, for both close-weight and open-weight general-purpose\nLLMs. Moreover, by examining the attention scores of an open-weight LLM, we\ndiscover a non-uniform attention pattern: training examples and variable\nnames/values which happen to occupy certain positions in the prompt receive\nmore attention when output tokens are generated, even though different\npositions are expected to receive roughly the same attention. This partially\nexplains the sensitivity in the presence of task-irrelevant variations. We also\nconsider a state-of-the-art tabular foundation model (TabPFN) trained\nspecifically for data fitting. Despite being explicitly designed to achieve\nprediction robustness, TabPFN is still not immune to task-irrelevant\nvariations. Overall, despite LLMs' impressive predictive capabilities,\ncurrently they lack even the basic level of robustness to be used as a\nprincipled data-fitting tool.\n","authors":["Hejia Liu","Mochen Yang","Gediminas Adomavicius"],"pdf_url":"https://arxiv.org/pdf/2508.19563v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22940v2","updated":"2025-10-28T02:44:02Z","published":"2025-10-27T02:51:51Z","title":"RL-AUX: Reinforcement Learning for Auxiliary Task Generation","summary":"  Auxiliary Learning (AL) is a special case of Multi-task Learning (MTL) in\nwhich a network trains on auxiliary tasks to improve performance on its main\ntask. This technique is used to improve generalization and, ultimately,\nperformance on the network's main task. AL has been demonstrated to improve\nperformance across multiple domains, including navigation, image\nclassification, and natural language processing. One weakness of AL is the need\nfor labeled auxiliary tasks, which can require human effort and domain\nexpertise to generate. Meta Learning techniques have been used to solve this\nissue by learning an additional auxiliary task generation network that can\ncreate helpful tasks for the primary network. The most prominent techniques\nrely on Bi-Level Optimization, which incurs computational cost and increased\ncode complexity. To avoid the need for Bi-Level Optimization, we present an\nRL-based approach to dynamically create auxiliary tasks. In this framework, an\nRL agent is tasked with selecting auxiliary labels for every data point in a\ntraining set. The agent is rewarded when their selection improves the\nperformance on the primary task. We also experiment with learning optimal\nstrategies for weighing the auxiliary loss per data point. On the 20-Superclass\nCIFAR100 problem, our RL approach outperforms human-labeled auxiliary tasks and\nperforms as well as a prominent Bi-Level Optimization technique. Our weight\nlearning approaches significantly outperform all of these benchmarks. For\nexample, a Weight-Aware RL-based approach helps the VGG16 architecture achieve\n80.9% test accuracy while the human-labeled auxiliary task setup achieved\n75.53%. The goal of this work is to (1) prove that RL is a viable approach to\ndynamically generate auxiliary tasks and (2) demonstrate that per-sample\nauxiliary task weights can be learned alongside the auxiliary task labels and\ncan achieve strong results.\n","authors":["Judah Goldfeder","Matthew So","Hod Lipson"],"pdf_url":"https://arxiv.org/pdf/2510.22940v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24013v1","updated":"2025-10-28T02:43:04Z","published":"2025-10-28T02:43:04Z","title":"Discovering Heuristics with Large Language Models (LLMs) for\n  Mixed-Integer Programs: Single-Machine Scheduling","summary":"  Our study contributes to the scheduling and combinatorial optimization\nliterature with new heuristics discovered by leveraging the power of Large\nLanguage Models (LLMs). We focus on the single-machine total tardiness (SMTT)\nproblem, which aims to minimize total tardiness by sequencing n jobs on a\nsingle processor without preemption, given processing times and due dates. We\ndevelop and benchmark two novel LLM-discovered heuristics, the EDD Challenger\n(EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date\n(EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that\nemployed simpler rule-based heuristics, we evaluate our LLM-discovered\nalgorithms using rigorous criteria, including optimality gaps and solution time\nderived from a mixed-integer programming (MIP) formulation of SMTT. We compare\ntheir performance against state-of-the-art heuristics and exact methods across\nvarious job sizes (20, 100, 200, and 500 jobs). For instances with more than\n100 jobs, exact methods such as MIP and dynamic programming become\ncomputationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD\nrule and another widely used algorithm in the literature. MDDC consistently\noutperforms traditional heuristics and remains competitive with exact\napproaches, particularly on larger and more complex instances. This study shows\nthat human-LLM collaboration can produce scalable, high-performing heuristics\nfor NP-hard constrained combinatorial optimization, even under limited\nresources when effectively configured.\n","authors":["İbrahim Oğuz Çetinkaya","İ. Esra Büyüktahtakın","Parshin Shojaee","Chandan K. Reddy"],"pdf_url":"https://arxiv.org/pdf/2510.24013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24012v1","updated":"2025-10-28T02:37:20Z","published":"2025-10-28T02:37:20Z","title":"Training-Free Safe Text Embedding Guidance for Text-to-Image Diffusion\n  Models","summary":"  Text-to-image models have recently made significant advances in generating\nrealistic and semantically coherent images, driven by advanced diffusion models\nand large-scale web-crawled datasets. However, these datasets often contain\ninappropriate or biased content, raising concerns about the generation of\nharmful outputs when provided with malicious text prompts. We propose Safe Text\nembedding Guidance (STG), a training-free approach to improve the safety of\ndiffusion models by guiding the text embeddings during sampling. STG adjusts\nthe text embeddings based on a safety function evaluated on the expected final\ndenoised image, allowing the model to generate safer outputs without additional\ntraining. Theoretically, we show that STG aligns the underlying model\ndistribution with safety constraints, thereby achieving safer outputs while\nminimally affecting generation quality. Experiments on various safety\nscenarios, including nudity, violence, and artist-style removal, show that STG\nconsistently outperforms both training-based and training-free baselines in\nremoving unsafe content while preserving the core semantic intent of input\nprompts. Our code is available at https://github.com/aailab-kaist/STG.\n","authors":["Byeonghu Na","Mina Kang","Jiseok Kwak","Minsang Park","Jiwoo Shin","SeJoon Jun","Gayoung Lee","Jin-Hwa Kim","Il-Chul Moon"],"pdf_url":"https://arxiv.org/pdf/2510.24012v1.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23409v2","updated":"2025-10-28T02:35:45Z","published":"2025-10-27T15:12:49Z","title":"Eigen-Value: Efficient Domain-Robust Data Valuation via Eigenvalue-Based\n  Approach","summary":"  Data valuation has become central in the era of data-centric AI. It drives\nefficient training pipelines and enables objective pricing in data markets by\nassigning a numeric value to each data point. Most existing data valuation\nmethods estimate the effect of removing individual data points by evaluating\nchanges in model validation performance under in-distribution (ID) settings, as\nopposed to out-of-distribution (OOD) scenarios where data follow different\npatterns. Since ID and OOD data behave differently, data valuation methods\nbased on ID loss often fail to generalize to OOD settings, particularly when\nthe validation set contains no OOD data. Furthermore, although OOD-aware\nmethods exist, they involve heavy computational costs, which hinder practical\ndeployment. To address these challenges, we introduce \\emph{Eigen-Value} (EV),\na plug-and-play data valuation framework for OOD robustness that uses only an\nID data subset, including during validation. EV provides a new spectral\napproximation of domain discrepancy, which is the gap of loss between ID and\nOOD using ratios of eigenvalues of ID data's covariance matrix. EV then\nestimates the marginal contribution of each data point to this discrepancy via\nperturbation theory, alleviating the computational burden. Subsequently, EV\nplugs into ID loss-based methods by adding an EV term without any additional\ntraining loop. We demonstrate that EV achieves improved OOD robustness and\nstable value rankings across real-world datasets, while remaining\ncomputationally lightweight. These results indicate that EV is practical for\nlarge-scale settings with domain shift, offering an efficient path to\nOOD-robust data valuation.\n","authors":["Youngjun Choi","Joonseong Kang","Sungjun Lim","Kyungwoo Song"],"pdf_url":"https://arxiv.org/pdf/2510.23409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24010v1","updated":"2025-10-28T02:34:08Z","published":"2025-10-28T02:34:08Z","title":"Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars\n  Science Tasks","summary":"  Foundation models have enabled rapid progress across many specialized domains\nby leveraging large-scale pre-training on unlabeled data, demonstrating strong\ngeneralization to a variety of downstream tasks. While such models have gained\nsignificant attention in fields like Earth Observation, their application to\nMars science remains limited. A key enabler of progress in other domains has\nbeen the availability of standardized benchmarks that support systematic\nevaluation. In contrast, Mars science lacks such benchmarks and standardized\nevaluation frameworks, which have limited progress toward developing foundation\nmodels for Martian tasks. To address this gap, we introduce Mars-Bench, the\nfirst benchmark designed to systematically evaluate models across a broad range\nof Mars-related tasks using both orbital and surface imagery. Mars-Bench\ncomprises 20 datasets spanning classification, segmentation, and object\ndetection, focused on key geologic features such as craters, cones, boulders,\nand frost. We provide standardized, ready-to-use datasets and baseline\nevaluations using models pre-trained on natural images, Earth satellite data,\nand state-of-the-art vision-language models. Results from all analyses suggest\nthat Mars-specific foundation models may offer advantages over general-domain\ncounterparts, motivating further exploration of domain-adapted pre-training.\nMars-Bench aims to establish a standardized foundation for developing and\ncomparing machine learning models for Mars science. Our data, models, and code\nare available at: https://mars-bench.github.io/.\n","authors":["Mirali Purohit","Bimal Gajera","Vatsal Malaviya","Irish Mehta","Kunal Kasodekar","Jacob Adler","Steven Lu","Umaa Rebbapragada","Hannah Kerner"],"pdf_url":"https://arxiv.org/pdf/2510.24010v1.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.21727v2","updated":"2025-10-28T02:31:06Z","published":"2025-09-27T16:50:03Z","title":"Your Dense Retriever is Secretly an Expeditious Reasoner","summary":"  Dense retrievers enhance retrieval by encoding queries and documents into\ncontinuous vectors, but they often struggle with reasoning-intensive queries.\nAlthough Large Language Models (LLMs) can reformulate queries to capture\ncomplex reasoning, applying them universally incurs significant computational\ncost. In this work, we propose Adaptive Query Reasoning (AdaQR), a hybrid query\nrewriting framework. Within this framework, a Reasoner Router dynamically\ndirects each query to either fast dense reasoning or deep LLM reasoning. The\ndense reasoning is achieved by the Dense Reasoner, which performs LLM-style\nreasoning directly in the embedding space, enabling a controllable trade-off\nbetween efficiency and accuracy. Experiments on large-scale retrieval\nbenchmarks BRIGHT show that AdaQR reduces reasoning cost by 28% while\npreserving-or even improving-retrieval performance by 7%.\n","authors":["Yichi Zhang","Jun Bai","Zhixin Cai","Shuhan Qin","Zhuofan Chen","Jinghua Guan","Wenge Rong"],"pdf_url":"https://arxiv.org/pdf/2510.21727v2.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2501.18092v5","updated":"2025-10-28T02:24:55Z","published":"2025-01-30T02:03:30Z","title":"Learning Provably Improves the Convergence of Gradient Descent","summary":"  Learn to Optimize (L2O) trains deep neural network-based solvers for\noptimization, achieving success in accelerating convex problems and improving\nnon-convex solutions. However, L2O lacks rigorous theoretical backing for its\nown training convergence, as existing analyses often use unrealistic\nassumptions -- a gap this work highlights empirically. We bridge this gap by\nproving the training convergence of L2O models that learn Gradient Descent (GD)\nhyperparameters for quadratic programming, leveraging the Neural Tangent Kernel\n(NTK) theory. We propose a deterministic initialization strategy to support our\ntheoretical results and promote stable training over extended optimization\nhorizons by mitigating gradient explosion. Our L2O framework demonstrates over\n50% better optimality than GD and superior robustness over state-of-the-art L2O\nmethods on synthetic datasets. The code of our method can be found from\nhttps://github.com/NetX-lab/MathL2OProof-Official.\n","authors":["Qingyu Song","Wei Lin","Hong Xu"],"pdf_url":"https://arxiv.org/pdf/2501.18092v5.pdf","comment":"48 pages, 11 figures, NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.08078v3","updated":"2025-10-28T02:16:25Z","published":"2025-10-09T11:08:07Z","title":"Detecting and Mitigating Insertion Hallucination in Video-to-Audio\n  Generation","summary":"  Video-to-Audio generation has made remarkable strides in automatically\nsynthesizing sound for video. However, existing evaluation metrics, which focus\non semantic and temporal alignment, overlook a critical failure mode: models\noften generate acoustic events, particularly speech and music, that have no\ncorresponding visual source. We term this phenomenon Insertion Hallucination\nand identify it as a systemic risk driven by dataset biases, such as the\nprevalence of off-screen sounds, that remains completely undetected by current\nmetrics. To address this challenge, we first develop a systematic evaluation\nframework that employs a majority-voting ensemble of multiple audio event\ndetectors. We also introduce two novel metrics to quantify the prevalence and\nseverity of this issue: IH@vid (the fraction of videos with hallucinations) and\nIH@dur (the fraction of hallucinated duration). Building on this, we propose\nPosterior Feature Correction, a novel training-free inference-time method that\nmitigates IH. PFC operates in a two-pass process: it first generates an initial\naudio output to detect hallucinated segments, and then regenerates the audio\nafter masking the corresponding video features at those timestamps. Experiments\non several mainstream V2A benchmarks first reveal that state-of-the-art models\nsuffer from severe IH. In contrast, our PFC method reduces both the prevalence\nand duration of hallucinations by over 50\\% on average, without degrading, and\nin some cases even improving, conventional metrics for audio quality and\ntemporal synchronization. Our work is the first to formally define,\nsystematically measure, and effectively mitigate Insertion Hallucination,\npaving the way for more reliable and faithful V2A models.\n","authors":["Liyang Chen","Hongkai Chen","Yujun Cai","Sifan Li","Qingwen Ye","Yiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2510.08078v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17354v2","updated":"2025-10-28T02:08:49Z","published":"2025-05-23T00:12:49Z","title":"CT-OT Flow: Estimating Continuous-Time Dynamics from Discrete Temporal\n  Snapshots","summary":"  In many real-world settings--e.g., single-cell RNA sequencing, mobility\nsensing, and environmental monitoring--data are observed only as temporally\naggregated snapshots collected over finite time windows, often with noisy or\nuncertain timestamps, and without access to continuous trajectories. We study\nthe problem of estimating continuous-time dynamics from such snapshots. We\npresent Continuous-Time Optimal Transport Flow (CT-OT Flow), a two-stage\nframework that (i) infers high-resolution time labels by aligning neighboring\nintervals via partial optimal transport (POT) and (ii) reconstructs a\ncontinuous-time data distribution through temporal kernel smoothing, from which\nwe sample pairs of nearby times to train standard ODE/SDE models. Our\nformulation explicitly accounts for snapshot aggregation and time-label\nuncertainty and uses practical accelerations (screening and mini-batch POT),\nmaking it applicable to large datasets. Across synthetic benchmarks and two\nreal datasets (scRNA-seq and typhoon tracks), CT-OT Flow reduces distributional\nand trajectory errors compared with OT-CFM, [SF]\\(^{2}\\)M, TrajectoryNet, MFM,\nand ENOT.\n","authors":["Keisuke Kawano","Takuro Kutsuna","Naoki Hayashi","Yasushi Esaki","Hidenori Tanaka"],"pdf_url":"https://arxiv.org/pdf/2505.17354v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23999v1","updated":"2025-10-28T02:03:39Z","published":"2025-10-28T02:03:39Z","title":"Auto-Adaptive PINNs with Applications to Phase Transitions","summary":"  We propose an adaptive sampling method for the training of Physics Informed\nNeural Networks (PINNs) which allows for sampling based on an arbitrary\nproblem-specific heuristic which may depend on the network and its gradients.\nIn particular we focus our analysis on the Allen-Cahn equations, attempting to\naccurately resolve the characteristic interfacial regions using a PINN without\nany post-hoc resampling. In experiments, we show the effectiveness of these\nmethods over residual-adaptive frameworks.\n","authors":["Kevin Buck","Woojeong Kim"],"pdf_url":"https://arxiv.org/pdf/2510.23999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13723v2","updated":"2025-10-28T01:57:58Z","published":"2025-05-19T20:46:26Z","title":"Turbocharging Gaussian Process Inference with Approximate\n  Sketch-and-Project","summary":"  Gaussian processes (GPs) play an essential role in biostatistics, scientific\nmachine learning, and Bayesian optimization for their ability to provide\nprobabilistic predictions and model uncertainty. However, GP inference\nstruggles to scale to large datasets (which are common in modern applications),\nsince it requires the solution of a linear system whose size scales\nquadratically with the number of samples in the dataset. We propose an\napproximate, distributed, accelerated sketch-and-project algorithm\n($\\texttt{ADASAP}$) for solving these linear systems, which improves\nscalability. We use the theory of determinantal point processes to show that\nthe posterior mean induced by sketch-and-project rapidly converges to the true\nposterior mean. In particular, this yields the first efficient, condition\nnumber-free algorithm for estimating the posterior mean along the top spectral\nbasis functions, showing that our approach is principled for GP inference.\n$\\texttt{ADASAP}$ outperforms state-of-the-art solvers based on conjugate\ngradient and coordinate descent across several benchmark datasets and a\nlarge-scale Bayesian optimization task. Moreover, $\\texttt{ADASAP}$ scales to a\ndataset with $> 3 \\cdot 10^8$ samples, a feat which has not been accomplished\nin the literature.\n","authors":["Pratik Rathore","Zachary Frangella","Sachin Garg","Shaghayegh Fazliani","Michał Dereziński","Madeleine Udell"],"pdf_url":"https://arxiv.org/pdf/2505.13723v2.pdf","comment":"NeurIPS 2025; 31 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2510.23994v1","updated":"2025-10-28T01:51:23Z","published":"2025-10-28T01:51:23Z","title":"Predicting Barge Tow Size on Inland Waterways Using Vessel Trajectory\n  Derived Features: Proof of Concept","summary":"  Accurate, real-time estimation of barge quantity on inland waterways remains\na critical challenge due to the non-self-propelled nature of barges and the\nlimitations of existing monitoring systems. This study introduces a novel\nmethod to use Automatic Identification System (AIS) vessel tracking data to\npredict the number of barges in tow using Machine Learning (ML). To train and\ntest the model, barge instances were manually annotated from satellite scenes\nacross the Lower Mississippi River. Labeled images were matched to AIS vessel\ntracks using a spatiotemporal matching procedure. A comprehensive set of 30\nAIS-derived features capturing vessel geometry, dynamic movement, and\ntrajectory patterns were created and evaluated using Recursive Feature\nElimination (RFE) to identify the most predictive variables. Six regression\nmodels, including ensemble, kernel-based, and generalized linear approaches,\nwere trained and evaluated. The Poisson Regressor model yielded the best\nperformance, achieving a Mean Absolute Error (MAE) of 1.92 barges using 12 of\nthe 30 features. The feature importance analysis revealed that metrics\ncapturing vessel maneuverability such as course entropy, speed variability and\ntrip length were most predictive of barge count. The proposed approach provides\na scalable, readily implementable method for enhancing Maritime Domain\nAwareness (MDA), with strong potential applications in lock scheduling, port\nmanagement, and freight planning. Future work will expand the proof of concept\npresented here to explore model transferability to other inland rivers with\ndiffering operational and environmental conditions.\n","authors":["Geoffery Agorku","Sarah Hernandez","Hayley Hames","Cade Wagner"],"pdf_url":"https://arxiv.org/pdf/2510.23994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23992v1","updated":"2025-10-28T01:50:24Z","published":"2025-10-28T01:50:24Z","title":"Optimal Arm Elimination Algorithms for Combinatorial Bandits","summary":"  Combinatorial bandits extend the classical bandit framework to settings where\nthe learner selects multiple arms in each round, motivated by applications such\nas online recommendation and assortment optimization. While extensions of upper\nconfidence bound (UCB) algorithms arise naturally in this context, adapting arm\nelimination methods has proved more challenging. We introduce a novel\nelimination scheme that partitions arms into three categories (confirmed,\nactive, and eliminated), and incorporates explicit exploration to update these\nsets. We demonstrate the efficacy of our algorithm in two settings: the\ncombinatorial multi-armed bandit with general graph feedback, and the\ncombinatorial linear contextual bandit. In both cases, our approach achieves\nnear-optimal regret, whereas UCB-based methods can provably fail due to\ninsufficient explicit exploration. Matching lower bounds are also provided.\n","authors":["Yuxiao Wen","Yanjun Han","Zhengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.23992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23986v1","updated":"2025-10-28T01:43:54Z","published":"2025-10-28T01:43:54Z","title":"STNet: Spectral Transformation Network for Solving Operator Eigenvalue\n  Problem","summary":"  Operator eigenvalue problems play a critical role in various scientific\nfields and engineering applications, yet numerical methods are hindered by the\ncurse of dimensionality. Recent deep learning methods provide an efficient\napproach to address this challenge by iteratively updating neural networks.\nThese methods' performance relies heavily on the spectral distribution of the\ngiven operator: larger gaps between the operator's eigenvalues will improve\nprecision, thus tailored spectral transformations that leverage the spectral\ndistribution can enhance their performance. Based on this observation, we\npropose the Spectral Transformation Network (STNet). During each iteration,\nSTNet uses approximate eigenvalues and eigenfunctions to perform spectral\ntransformations on the original operator, turning it into an equivalent but\neasier problem. Specifically, we employ deflation projection to exclude the\nsubspace corresponding to already solved eigenfunctions, thereby reducing the\nsearch space and avoiding converging to existing eigenfunctions. Additionally,\nour filter transform magnifies eigenvalues in the desired region and suppresses\nthose outside, further improving performance. Extensive experiments demonstrate\nthat STNet consistently outperforms existing learning-based methods, achieving\nstate-of-the-art performance in accuracy.\n","authors":["Hong Wang","Jiang Yixuan","Jie Wang","Xinyi Li","Jian Luo","Huanshuo Dong"],"pdf_url":"https://arxiv.org/pdf/2510.23986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.23502v3","updated":"2025-10-28T01:42:48Z","published":"2025-03-30T16:24:22Z","title":"Boosting Omnidirectional Stereo Matching with a Pre-trained Depth\n  Foundation Model","summary":"  Omnidirectional depth perception is essential for mobile robotics\napplications that require scene understanding across a full 360{\\deg} field of\nview. Camera-based setups offer a cost-effective option by using stereo depth\nestimation to generate dense, high-resolution depth maps without relying on\nexpensive active sensing. However, existing omnidirectional stereo matching\napproaches achieve only limited depth accuracy across diverse environments,\ndepth ranges, and lighting conditions, due to the scarcity of real-world data.\nWe present DFI-OmniStereo, a novel omnidirectional stereo matching method that\nleverages a large-scale pre-trained foundation model for relative monocular\ndepth estimation within an iterative optimization-based stereo matching\narchitecture. We introduce a dedicated two-stage training strategy to utilize\nthe relative monocular depth features for our omnidirectional stereo matching\nbefore scale-invariant fine-tuning. DFI-OmniStereo achieves state-of-the-art\nresults on the real-world Helvipad dataset, reducing disparity MAE by\napproximately 16% compared to the previous best omnidirectional stereo method.\n","authors":["Jannik Endres","Oliver Hahn","Charles Corbière","Simone Schaub-Meyer","Stefan Roth","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2503.23502v3.pdf","comment":"Accepted at IROS 2025. Project page:\n  https://vita-epfl.github.io/DFI-OmniStereo-website/"},{"id":"http://arxiv.org/abs/2509.12630v2","updated":"2025-10-28T01:41:54Z","published":"2025-09-16T03:49:26Z","title":"High-Energy Concentration for Federated Learning in Frequency Domain","summary":"  Federated Learning (FL) presents significant potential for collaborative\noptimization without data sharing. Since synthetic data is sent to the server,\nleveraging the popular concept of dataset distillation, this FL framework\nprotects real data privacy while alleviating data heterogeneity. However, such\nmethods are still challenged by the redundant information and noise in entire\nspatial-domain designs, which inevitably increases the communication burden. In\nthis paper, we propose a novel Frequency-Domain aware FL method with\nhigh-energy concentration (FedFD) to address this problem. Our FedFD is\ninspired by the discovery that the discrete cosine transform predominantly\ndistributes energy to specific regions, referred to as high-energy\nconcentration. The principle behind FedFD is that low-energy like\nhigh-frequency components usually contain redundant information and noise, thus\nfiltering them helps reduce communication costs and optimize performance. Our\nFedFD is mathematically formulated to preserve the low-frequency components\nusing a binary mask, facilitating an optimal solution through frequency-domain\ndistribution alignment. In particular, real data-driven synthetic\nclassification is imposed into the loss to enhance the quality of the\nlow-frequency components. On five image and speech datasets, FedFD achieves\nsuperior performance than state-of-the-art methods while reducing communication\ncosts. For example, on the CIFAR-10 dataset with Dirichlet coefficient $\\alpha\n= 0.01$, FedFD achieves a minimum reduction of 37.78\\% in the communication\ncost, while attaining a 10.88\\% performance gain.\n","authors":["Haozhi Shi","Weiying Xie","Hangyu Ye","Daixun Li","Jitao Ma","Yunsong Li","Leyuan Fang"],"pdf_url":"https://arxiv.org/pdf/2509.12630v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04852v2","updated":"2025-10-28T01:41:35Z","published":"2025-03-06T03:40:01Z","title":"CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data","summary":"  True intelligence hinges on the ability to uncover and leverage hidden causal\nrelations. Despite significant progress in AI and computer vision (CV), there\nremains a lack of benchmarks for assessing models' abilities to infer latent\ncausality from complex visual data. In this paper, we introduce\n\\textsc{\\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates\nstructured data (tables) with corresponding visual representations (images) to\nevaluate causal reasoning. Designed within a systematic framework, Causal3D\ncomprises 19 3D-scene datasets capturing diverse causal relations, views, and\nbackgrounds, enabling evaluations across scenes of varying complexity. We\nassess multiple state-of-the-art methods, including classical causal discovery,\ncausal representation learning, and large/vision-language models (LLMs/VLMs).\nOur experiments show that as causal structures grow more complex without prior\nknowledge, performance declines significantly, highlighting the challenges even\nadvanced methods face in complex causal scenarios. Causal3D serves as a vital\nresource for advancing causal reasoning in CV and fostering trustworthy AI in\ncritical domains.\n","authors":["Disheng Liu","Yiran Qiao","Wuche Liu","Yiren Lu","Yunlai Zhou","Tuo Liang","Yu Yin","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2503.04852v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23985v1","updated":"2025-10-28T01:36:54Z","published":"2025-10-28T01:36:54Z","title":"Score-based constrained generative modeling via Langevin diffusions with\n  boundary conditions","summary":"  Score-based generative models based on stochastic differential equations\n(SDEs) achieve impressive performance in sampling from unknown distributions,\nbut often fail to satisfy underlying constraints. We propose a constrained\ngenerative model using kinetic (underdamped) Langevin dynamics with specular\nreflection of velocity on the boundary defining constraints. This results in\npiecewise continuously differentiable noising and denoising process where the\nlatter is characterized by a time-reversed dynamics restricted to a domain with\nboundary due to specular boundary condition. In addition, we also contribute to\nexisting reflected SDEs based constrained generative models, where the\nstochastic dynamics is restricted through an abstract local time term. By\npresenting efficient numerical samplers which converge with optimal rate in\nterms of discretizations step, we provide a comprehensive comparison of models\nbased on confined (specularly reflected kinetic) Langevin diffusion with models\nbased on reflected diffusion with local time.\n","authors":["Adam Nordenhög","Akash Sharma"],"pdf_url":"https://arxiv.org/pdf/2510.23985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23980v1","updated":"2025-10-28T01:21:54Z","published":"2025-10-28T01:21:54Z","title":"HyperGraphX: Graph Transductive Learning with Hyperdimensional Computing\n  and Message Passing","summary":"  We present a novel algorithm, \\hdgc, that marries graph convolution with\nbinding and bundling operations in hyperdimensional computing for transductive\ngraph learning. For prediction accuracy \\hdgc outperforms major and popular\ngraph neural network implementations as well as state-of-the-art\nhyperdimensional computing implementations for a collection of homophilic\ngraphs and heterophilic graphs. Compared with the most accurate learning\nmethodologies we have tested, on the same target GPU platform, \\hdgc is on\naverage 9561.0 and 144.5 times faster than \\gcnii, a graph neural network\nimplementation and HDGL, a hyperdimensional computing implementation,\nrespectively. As the majority of the learning operates on binary vectors, we\nexpect outstanding energy performance of \\hdgc on neuromorphic and emerging\nprocess-in-memory devices.\n","authors":["Guojing Cong","Tom Potok","Hamed Poursiami","Maryam Parsa"],"pdf_url":"https://arxiv.org/pdf/2510.23980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23977v1","updated":"2025-10-28T01:18:00Z","published":"2025-10-28T01:18:00Z","title":"Synergistic Neural Forecasting of Air Pollution with Stochastic Sampling","summary":"  Air pollution remains a leading global health and environmental risk,\nparticularly in regions vulnerable to episodic air pollution spikes due to\nwildfires, urban haze and dust storms. Accurate forecasting of particulate\nmatter (PM) concentrations is essential to enable timely public health warnings\nand interventions, yet existing models often underestimate rare but hazardous\npollution events. Here, we present SynCast, a high-resolution neural\nforecasting model that integrates meteorological and air composition data to\nimprove predictions of both average and extreme pollution levels. Built on a\nregionally adapted transformer backbone and enhanced with a diffusion-based\nstochastic refinement module, SynCast captures the nonlinear dynamics driving\nPM spikes more accurately than existing approaches. Leveraging on harmonized\nERA5 and CAMS datasets, our model shows substantial gains in forecasting\nfidelity across multiple PM variables (PM$_1$, PM$_{2.5}$, PM$_{10}$),\nespecially under extreme conditions. We demonstrate that conventional loss\nfunctions underrepresent distributional tails (rare pollution events) and show\nthat SynCast, guided by domain-aware objectives and extreme value theory,\nsignificantly enhances performance in highly impacted regions without\ncompromising global accuracy. This approach provides a scalable foundation for\nnext-generation air quality early warning systems and supports climate-health\nrisk mitigation in vulnerable regions.\n","authors":["Yohan Abeysinghe","Muhammad Akhtar Munir","Sanoojan Baliah","Ron Sarafian","Fahad Shahbaz Khan","Yinon Rudich","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2510.23977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23974v1","updated":"2025-10-28T01:10:15Z","published":"2025-10-28T01:10:15Z","title":"Diffusion Adaptive Text Embedding for Text-to-Image Diffusion Models","summary":"  Text-to-image diffusion models rely on text embeddings from a pre-trained\ntext encoder, but these embeddings remain fixed across all diffusion timesteps,\nlimiting their adaptability to the generative process. We propose Diffusion\nAdaptive Text Embedding (DATE), which dynamically updates text embeddings at\neach diffusion timestep based on intermediate perturbed data. We formulate an\noptimization problem and derive an update rule that refines the text embeddings\nat each sampling step to improve alignment and preference between the mean\npredicted image and the text. This allows DATE to dynamically adapts the text\nconditions to the reverse-diffused images throughout diffusion sampling without\nrequiring additional model training. Through theoretical analysis and empirical\nresults, we show that DATE maintains the generative capability of the model\nwhile providing superior text-image alignment over fixed text embeddings across\nvarious tasks, including multi-concept generation and text-guided image\nediting. Our code is available at https://github.com/aailab-kaist/DATE.\n","authors":["Byeonghu Na","Minsang Park","Gyuwon Sim","Donghyeok Shin","HeeSun Bae","Mina Kang","Se Jung Kwon","Wanmo Kang","Il-Chul Moon"],"pdf_url":"https://arxiv.org/pdf/2510.23974v1.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23972v1","updated":"2025-10-28T01:09:19Z","published":"2025-10-28T01:09:19Z","title":"An efficient probabilistic hardware architecture for diffusion-like\n  models","summary":"  The proliferation of probabilistic AI has promoted proposals for specialized\nstochastic computers. Despite promising efficiency gains, these proposals have\nfailed to gain traction because they rely on fundamentally limited modeling\ntechniques and exotic, unscalable hardware. In this work, we address these\nshortcomings by proposing an all-transistor probabilistic computer that\nimplements powerful denoising models at the hardware level. A system-level\nanalysis indicates that devices based on our architecture could achieve\nperformance parity with GPUs on a simple image benchmark using approximately\n10,000 times less energy.\n","authors":["Andraž Jelinčič","Owen Lockwood","Akhil Garlapati","Guillaume Verdon","Trevor McCourt"],"pdf_url":"https://arxiv.org/pdf/2510.23972v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.14291v5","updated":"2025-10-28T00:48:13Z","published":"2025-06-17T08:05:08Z","title":"Equivariance Everywhere All At Once: A Recipe for Graph Foundation\n  Models","summary":"  Graph machine learning architectures are typically tailored to specific tasks\non specific datasets, which hinders their broader applicability. This has led\nto a new quest in graph machine learning: how to build graph foundation models\ncapable of generalizing across arbitrary graphs and features? In this work, we\npresent a recipe for designing graph foundation models for node-level tasks\nfrom first principles. The key ingredient underpinning our study is a\nsystematic investigation of the symmetries that a graph foundation model must\nrespect. In a nutshell, we argue that label permutation-equivariance alongside\nfeature permutation-invariance are necessary in addition to the common node\npermutation-equivariance on each local neighborhood of the graph. To this end,\nwe first characterize the space of linear transformations that are equivariant\nto permutations of nodes and labels, and invariant to permutations of features.\nWe then prove that the resulting network is a universal approximator on\nmultisets that respect the aforementioned symmetries. Our recipe uses such\nlayers on the multiset of features induced by the local neighborhood of the\ngraph to obtain a class of graph foundation models for node property\nprediction. We validate our approach through extensive experiments on 29\nreal-world node classification datasets, demonstrating both strong zero-shot\nempirical performance and consistent improvement as the number of training\ngraphs increases.\n","authors":["Ben Finkelshtein","İsmail İlkan Ceylan","Michael Bronstein","Ron Levie"],"pdf_url":"https://arxiv.org/pdf/2506.14291v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23966v1","updated":"2025-10-28T00:44:25Z","published":"2025-10-28T00:44:25Z","title":"A Pragmatic Way to Measure Chain-of-Thought Monitorability","summary":"  While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI\nsafety, this opportunity could be lost through shifts in training practices or\nmodel architecture. To help preserve monitorability, we propose a pragmatic way\nto measure two components of it: legibility (whether the reasoning can be\nfollowed by a human) and coverage (whether the CoT contains all the reasoning\nneeded for a human to also produce the final output). We implement these\nmetrics with an autorater prompt that enables any capable LLM to compute the\nlegibility and coverage of existing CoTs. After sanity-checking our prompted\nautorater with synthetic CoT degradations, we apply it to several frontier\nmodels on challenging benchmarks, finding that they exhibit high\nmonitorability. We present these metrics, including our complete autorater\nprompt, as a tool for developers to track how design decisions impact\nmonitorability. While the exact prompt we share is still a preliminary version\nunder ongoing development, we are sharing it now in the hopes that others in\nthe community will find it useful. Our method helps measure the default\nmonitorability of CoT - it should be seen as a complement, not a replacement,\nfor the adversarial stress-testing needed to test robustness against\ndeliberately evasive models.\n","authors":["Scott Emmons","Roland S. Zimmermann","David K. Elson","Rohin Shah"],"pdf_url":"https://arxiv.org/pdf/2510.23966v1.pdf","comment":"The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2506.23550v3","updated":"2025-10-28T00:43:01Z","published":"2025-06-30T06:49:31Z","title":"Seeding neural network quantum states with tensor network states","summary":"  We find an efficient approach to approximately convert matrix product states\n(MPSs) into restricted Boltzmann machine wave functions consisting of a\nmultinomial hidden unit through a canonical polyadic (CP) decomposition of the\nMPSs. This method allows us to generate well-behaved initial neural network\nquantum states for quantum many-body ground-state calculations in polynomial\ntime of the number of variational parameters and systematically shorten the\ndistance between the initial states and the ground states while increasing the\nrank of the CP decomposition. We demonstrate the efficiency of our method by\ntaking the transverse-field Ising model as an example and discuss possible\napplications of our method to more general quantum many-body systems in which\nthe ground-state wave functions possess complex nodal structures.\n","authors":["Ryui Kaneko","Shimpei Goto"],"pdf_url":"https://arxiv.org/pdf/2506.23550v3.pdf","comment":"15 pages, 15 figures, All codes and data used in this manuscript are\n  available at https://github.com/ryuikaneko/mps2rbm"},{"id":"http://arxiv.org/abs/2510.23965v1","updated":"2025-10-28T00:42:38Z","published":"2025-10-28T00:42:38Z","title":"The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity","summary":"  Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.\n","authors":["Aymane El Gadarri","Ali Aouad","Vivek F. Farias"],"pdf_url":"https://arxiv.org/pdf/2510.23965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22138v2","updated":"2025-10-28T00:35:27Z","published":"2025-10-25T03:21:20Z","title":"Tractable Shapley Values and Interactions via Tensor Networks","summary":"  We show how to replace the O(2^n) coalition enumeration over n features\nbehind Shapley values and Shapley-style interaction indices with a\nfew-evaluation scheme on a tensor-network (TN) surrogate: TN-SHAP. The key idea\nis to represent a predictor's local behavior as a factorized multilinear map,\nso that coalitional quantities become linear probes of a coefficient tensor.\nTN-SHAP replaces exhaustive coalition sweeps with just a small number of\ntargeted evaluations to extract order-k Shapley interactions. In particular,\nboth order-1 (single-feature) and order-2 (pairwise) computations have cost\nO(n*poly(chi) + n^2), where chi is the TN's maximal cut rank. We provide\ntheoretical guarantees on the approximation error and tractability of TN-SHAP.\nOn UCI datasets, our method matches enumeration on the fitted surrogate while\nreducing evaluation by orders of magnitude and achieves 25-1000x wall-clock\nspeedups over KernelSHAP-IQ at comparable accuracy, while amortizing training\nacross local cohorts.\n","authors":["Farzaneh Heidari","Chao Li","Guillaume Rabusseau"],"pdf_url":"https://arxiv.org/pdf/2510.22138v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17323v2","updated":"2025-10-28T00:28:59Z","published":"2025-05-22T22:24:12Z","title":"Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)","summary":"  Humans are remarkably adept at collaboration, able to infer the strengths and\nweaknesses of new partners in order to work successfully towards shared goals.\nTo build AI systems with this capability, we must first understand its building\nblocks: does such flexibility require explicit, dedicated mechanisms for\nmodelling others -- or can it emerge spontaneously from the pressures of\nopen-ended cooperative interaction? To investigate this question, we train\nsimple model-free RNN agents to collaborate with a population of diverse\npartners. Using the `Overcooked-AI' environment, we collect data from thousands\nof collaborative teams, and analyse agents' internal hidden states. Despite a\nlack of additional architectural features, inductive biases, or auxiliary\nobjectives, the agents nevertheless develop structured internal representations\nof their partners' task abilities, enabling rapid adaptation and generalisation\nto novel collaborators. We investigated these internal models through probing\ntechniques, and large-scale behavioural analysis. Notably, we find that\nstructured partner modelling emerges when agents can influence partner\nbehaviour by controlling task allocation. Our results show that partner\nmodelling can arise spontaneously in model-free agents -- but only under\nenvironmental conditions that impose the right kind of social pressure.\n","authors":["Ruaridh Mon-Williams","Max Taylor-Davies","Elizabeth Mieczkowski","Natalia Velez","Neil R. Bramley","Yanwei Wang","Thomas L. Griffiths","Christopher G. Lucas"],"pdf_url":"https://arxiv.org/pdf/2505.17323v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20958v2","updated":"2025-10-28T00:13:13Z","published":"2025-10-23T19:36:59Z","title":"NeuroPilot: A Realtime Brain-Computer Interface system to enhance\n  concentration of students in online learning","summary":"  The prevalence of online learning poses a vital challenge in real-time\nmonitoring of students' concentration. Traditional methods such as\nquestionnaire assessments require manual intervention, and webcam-based\nmonitoring fails to provide accurate insights about learners' mental focus as\nit is deceived by mere screen fixation without cognitive engagement. Existing\nBCI-based approaches lack real-time validation and evaluation procedures. To\naddress these limitations, a Brain-Computer Interface (BCI) system is developed\nusing a non-invasive Electroencephalogram (EEG) headband, FocusCalm, to record\nbrainwave activity under attentive and non-attentive states. 20 minutes of data\nwere collected from each of 20 participants watching a pre-recorded educational\nvideo. The data validation employed a novel intra-video questionnaire\nassessment. Subsequently, collected signals were segmented (sliding window),\nfiltered (Butterworth bandpass), and cleaned (removal of high- amplitude and\nEOG artifacts such as eye blinks). Time, frequency, wavelet, and statistical\nfeatures were extracted, followed by recursive feature elimination (RFE) with\nsupport vector machines (SVMs) to classify attention and non-attention states.\nThe leave-one-subject-out (LOSO) cross-validation accuracy was found to be\n88.77%. The system provides feedback alerts upon detection of a non-attention\nstate and maintains focus profile logs. A pilot study was conducted to evaluate\nthe effectiveness of real-time feedback. Five participants underwent a\n10-minute session comprising a 5-minute baseline phase devoid of feedback,\nsucceeded by a 5-minute feedback phase, during which alerts were activated if\nparticipants exhibited inattention for approximately 8 consecutive seconds. A\npaired t-test (t = 5.73, p = 0.007) indicated a statistically significant\nimprovement in concentration during the feedback phase.\n","authors":["Asif Islam","Farhan Ishtiaque","Md. Muhyminul Haque","Farhana Sarker","Ravi Vaidyanathan","Khondaker A. Mamun"],"pdf_url":"https://arxiv.org/pdf/2510.20958v2.pdf","comment":"V2: Author list updated with consent from all authors. Minor\n  grammatical errors fixed. This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2510.23948v1","updated":"2025-10-28T00:02:52Z","published":"2025-10-28T00:02:52Z","title":"ChessQA: Evaluating Large Language Models for Chess Understanding","summary":"  Chess provides an ideal testbed for evaluating the reasoning, modeling, and\nabstraction capabilities of large language models (LLMs), as it has\nwell-defined structure and objective ground truth while admitting a wide\nspectrum of skill levels. However, existing evaluations of LLM ability in chess\nare ad hoc and narrow in scope, making it difficult to accurately measure LLM\nchess understanding and how it varies with scale, post-training methodologies,\nor architecture choices. We present ChessQA, a comprehensive benchmark that\nassesses LLM chess understanding across five task categories (Structural,\nMotifs, Short Tactics, Position Judgment, and Semantic), which approximately\ncorrespond to the ascending abstractions that players master as they accumulate\nchess knowledge, from understanding basic rules and learning tactical motifs to\ncorrectly calculating tactics, evaluating positions, and semantically\ndescribing high-level concepts. In this way, ChessQA captures a more\ncomprehensive picture of chess ability and understanding, going significantly\nbeyond the simple move quality evaluations done previously, and offers a\ncontrolled, consistent setting for diagnosis and comparison. Furthermore,\nChessQA is inherently dynamic, with prompts, answer keys, and construction\nscripts that can evolve as models improve. Evaluating a range of contemporary\nLLMs, we find persistent weaknesses across all five categories and provide\nresults and error analyses by category. We will release the code, periodically\nrefreshed datasets, and a public leaderboard to support further research.\n","authors":["Qianfeng Wen","Zhenwei Tang","Ashton Anderson"],"pdf_url":"https://arxiv.org/pdf/2510.23948v1.pdf","comment":"33 pages,8 figures"},{"id":"http://arxiv.org/abs/2510.25039v1","updated":"2025-10-28T23:53:36Z","published":"2025-10-28T23:53:36Z","title":"Automating Benchmark Design","summary":"  The rapid progress and widespread deployment of LLMs and LLM-powered agents\nhas outpaced our ability to evaluate them. Hand-crafted, static benchmarks are\nthe primary tool for assessing model capabilities, but these quickly become\nsaturated. In contrast, dynamic benchmarks evolve alongside the models they\nevaluate, but are expensive to create and continuously update. To address these\nchallenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a\nframework that leverages environment design principles to automate the process\nof dynamic benchmark design. BeTaL works by parameterizing key design choices\nin base benchmark templates and uses LLMs to reason through the resulting\nparameter space to obtain target properties (such as difficulty and realism) in\na cost-efficient manner. We validate this approach on its ability to create\nbenchmarks with desired difficulty levels. Using BeTaL, we create two new\nbenchmarks and extend a popular agentic benchmark $\\tau$-bench. Extensive\nevaluation on these three tasks and multiple target difficulty levels shows\nthat BeTaL produces benchmarks much closer to the desired difficulty, with\naverage deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the\nbaselines.\n","authors":["Amanda Dsouza","Harit Vishwakarma","Zhengyang Qi","Justin Bauer","Derek Pham","Thomas Walshe","Armin Parchami","Frederic Sala","Paroma Varma"],"pdf_url":"https://arxiv.org/pdf/2510.25039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25037v1","updated":"2025-10-28T23:38:43Z","published":"2025-10-28T23:38:43Z","title":"Graph Distance Based on Cause-Effect Estimands with Latents","summary":"  Causal discovery aims to recover graphs that represent causal relations among\ngiven variables from observations, and new methods are constantly being\nproposed. Increasingly, the community raises questions about how much progress\nis made, because properly evaluating discovered graphs remains notoriously\ndifficult, particularly under latent confounding. We propose a graph distance\nmeasure for acyclic directed mixed graphs (ADMGs) based on the downstream task\nof cause-effect estimation under unobserved confounding. Our approach uses\nidentification via fixing and a symbolic verifier to quantify how graph\ndifferences distort cause-effect estimands for different treatment-outcome\npairs. We analyze the behavior of the measure under different graph\nperturbations and compare it against existing distance metrics.\n","authors":["Zhufeng Li","Niki Kilbertus"],"pdf_url":"https://arxiv.org/pdf/2510.25037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.11307v2","updated":"2025-10-28T23:09:56Z","published":"2025-08-15T08:22:01Z","title":"Approximating the universal thermal climate index using sparse\n  regression with orthogonal polynomials","summary":"  This article explores novel data-driven modeling approaches for analyzing and\napproximating the Universal Thermal Climate Index (UTCI), a\nphysiologically-based metric integrating multiple atmospheric variables to\nassess thermal comfort. Given the nonlinear, multivariate structure of UTCI, we\ninvestigate symbolic and sparse regression techniques as tools for\ninterpretable and efficient function approximation. In particular, we highlight\nthe benefits of using orthogonal polynomial bases-such as Legendre\npolynomials-in sparse regression frameworks, demonstrating their advantages in\nstability, convergence, and hierarchical interpretability compared to standard\npolynomial expansions. We demonstrate that our models achieve significantly\nlower root-mean squared losses than the widely used sixth-degree polynomial\nbenchmark-while using the same or fewer parameters. By leveraging Legendre\npolynomial bases, we construct models that efficiently populate a Pareto front\nof accuracy versus complexity and exhibit stable, hierarchical coefficient\nstructures across varying model capacities. Training on just 20% of the data,\nour models generalize robustly to the remaining 80%, with consistent\nperformance under bootstrapping. The decomposition effectively approximates the\nUTCI as a Fourier-like expansion in an orthogonal basis, yielding results near\nthe theoretical optimum in the L2 (least squares) sense. We also connect these\nfindings to the broader context of equation discovery in environmental\nmodeling, referencing probabilistic grammar-based methods that enforce domain\nconsistency and compactness in symbolic expressions. Taken together, these\nresults illustrate how combining sparsity, orthogonality, and symbolic\nstructure enables robust, interpretable modeling of complex environmental\nindices like UTCI - and significantly outperforms the state-of-the-art\napproximation in both accuracy and efficiency.\n","authors":["Sabin Roman","Gregor Skok","Ljupco Todorovski","Saso Dzeroski"],"pdf_url":"https://arxiv.org/pdf/2508.11307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25026v1","updated":"2025-10-28T22:54:43Z","published":"2025-10-28T22:54:43Z","title":"Machine Learning based Analysis for Radiomics Features Robustness in\n  Real-World Deployment Scenarios","summary":"  Radiomics-based machine learning models show promise for clinical decision\nsupport but are vulnerable to distribution shifts caused by variations in\nimaging protocols, positioning, and segmentation. This study systematically\ninvestigates the robustness of radiomics-based machine learning models under\ndistribution shifts across five MRI sequences. We evaluated how different\nacquisition protocols and segmentation strategies affect model reliability in\nterms of predictive power and uncertainty-awareness. Using a phantom of 16\nfruits, we evaluated distribution shifts through: (1) protocol variations\nacross T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2)\nsegmentation variations (full, partial, rotated); and (3) inter-observer\nvariability. We trained XGBoost classifiers on 8 consistent robust features\nversus sequence-specific features, testing model performance under in-domain\nand out-of-domain conditions. Results demonstrate that models trained on\nprotocol-invariant features maintain F1-scores >0.85 across distribution\nshifts, while models using all features showed 40% performance degradation\nunder protocol changes. Dataset augmentation substantially improved the quality\nof uncertainty estimates and reduced the expected calibration error (ECE) by\n35% without sacrificing accuracy. Temperature scaling provided minimal\ncalibration benefits, confirming XGBoost's inherent reliability. Our findings\nreveal that protocol-aware feature selection and controlled phantom studies\neffectively predict model behavior under distribution shifts, providing a\nframework for developing robust radiomics models resilient to real-world\nprotocol variations.\n","authors":["Sarmad Ahmad Khan","Simon Bernatz","Zahra Moslehi","Florian Buettner"],"pdf_url":"https://arxiv.org/pdf/2510.25026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25025v1","updated":"2025-10-28T22:54:19Z","published":"2025-10-28T22:54:19Z","title":"Secure Retrieval-Augmented Generation against Poisoning Attacks","summary":"  Large language models (LLMs) have transformed natural language processing\n(NLP), enabling applications from content generation to decision support.\nRetrieval-Augmented Generation (RAG) improves LLMs by incorporating external\nknowledge but also introduces security risks, particularly from data poisoning,\nwhere the attacker injects poisoned texts into the knowledge database to\nmanipulate system outputs. While various defenses have been proposed, they\noften struggle against advanced attacks. To address this, we introduce RAGuard,\na detection framework designed to identify poisoned texts. RAGuard first\nexpands the retrieval scope to increase the proportion of clean texts, reducing\nthe likelihood of retrieving poisoned content. It then applies chunk-wise\nperplexity filtering to detect abnormal variations and text similarity\nfiltering to flag highly similar texts. This non-parametric approach enhances\nRAG security, and experiments on large-scale datasets demonstrate its\neffectiveness in detecting and mitigating poisoning attacks, including strong\nadaptive attacks.\n","authors":["Zirui Cheng","Jikai Sun","Anjun Gao","Yueyang Quan","Zhuqing Liu","Xiaohua Hu","Minghong Fang"],"pdf_url":"https://arxiv.org/pdf/2510.25025v1.pdf","comment":"To appear in IEEE BigData 2025"},{"id":"http://arxiv.org/abs/2510.25023v1","updated":"2025-10-28T22:45:52Z","published":"2025-10-28T22:45:52Z","title":"Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent\n  Modeling Framework for Deep Brain Stimulation","summary":"  Disentangling shared network-level dynamics from region-specific activity is\na central challenge in modeling multi-region neural data. We introduce SPIRE\n(Shared-Private Inter-Regional Encoder), a deep multi-encoder autoencoder that\nfactorizes recordings into shared and private latent subspaces with novel\nalignment and disentanglement losses. Trained solely on baseline data, SPIRE\nrobustly recovers cross-regional structure and reveals how external\nperturbations reorganize it. On synthetic benchmarks with ground-truth latents,\nSPIRE outperforms classical probabilistic models under nonlinear distortions\nand temporal misalignments. Applied to intracranial deep brain stimulation\n(DBS) recordings, SPIRE shows that shared latents reliably encode\nstimulation-specific signatures that generalize across sites and frequencies.\nThese results establish SPIRE as a practical, reproducible tool for analyzing\nmulti-region neural dynamics under stimulation.\n","authors":["Rahil Soroushmojdehi","Sina Javadzadeh","Mehrnaz Asadi","Terence D. Sanger"],"pdf_url":"https://arxiv.org/pdf/2510.25023v1.pdf","comment":"25 pages total. Main paper (including references): 13 pages with 7\n  figures. Appendix: 12 pages with 5 figures and 4 tables. Submitted to ICLR\n  2026"},{"id":"http://arxiv.org/abs/2510.25016v1","updated":"2025-10-28T22:29:11Z","published":"2025-10-28T22:29:11Z","title":"Towards Human-AI Synergy in Requirements Engineering: A Framework and\n  Preliminary Study","summary":"  The future of Requirements Engineering (RE) is increasingly driven by\nartificial intelligence (AI), reshaping how we elicit, analyze, and validate\nrequirements. Traditional RE is based on labor-intensive manual processes prone\nto errors and complexity. AI-powered approaches, specifically large language\nmodels (LLMs), natural language processing (NLP), and generative AI, offer\ntransformative solutions and reduce inefficiencies. However, the use of AI in\nRE also brings challenges like algorithmic bias, lack of explainability, and\nethical concerns related to automation. To address these issues, this study\nintroduces the Human-AI RE Synergy Model (HARE-SM), a conceptual framework that\nintegrates AI-driven analysis with human oversight to improve requirements\nelicitation, analysis, and validation. The model emphasizes ethical AI use\nthrough transparency, explainability, and bias mitigation. We outline a\nmulti-phase research methodology focused on preparing RE datasets, fine-tuning\nAI models, and designing collaborative human-AI workflows. This preliminary\nstudy presents the conceptual framework and early-stage prototype\nimplementation, establishing a research agenda and practical design direction\nfor applying intelligent data science techniques to semi-structured and\nunstructured RE data in collaborative environments.\n","authors":["Mateen Ahmed Abbasi","Petri Ihantola","Tommi Mikkonen","Niko Mäkitalo"],"pdf_url":"https://arxiv.org/pdf/2510.25016v1.pdf","comment":"Accepted at the 2025 Sixth International Conference on Intelligent\n  Data Science Technologies and Applications (IDSTA 2025),8 pages, 4 figures.\n  Published in IEEE"},{"id":"http://arxiv.org/abs/2510.25013v1","updated":"2025-10-28T22:25:19Z","published":"2025-10-28T22:25:19Z","title":"Emergence of Minimal Circuits for Indirect Object Identification in\n  Attention-Only Transformers","summary":"  Mechanistic interpretability aims to reverse-engineer large language models\n(LLMs) into human-understandable computational circuits. However, the\ncomplexity of pretrained models often obscures the minimal mechanisms required\nfor specific reasoning tasks. In this work, we train small, attention-only\ntransformers from scratch on a symbolic version of the Indirect Object\nIdentification (IOI) task -- a benchmark for studying coreference -- like\nreasoning in transformers. Surprisingly, a single-layer model with only two\nattention heads achieves perfect IOI accuracy, despite lacking MLPs and\nnormalization layers. Through residual stream decomposition, spectral analysis,\nand embedding interventions, we find that the two heads specialize into\nadditive and contrastive subcircuits that jointly implement IOI resolution.\nFurthermore, we show that a two-layer, one-head model achieves similar\nperformance by composing information across layers through query-value\ninteractions. These results demonstrate that task-specific training induces\nhighly interpretable, minimal circuits, offering a controlled testbed for\nprobing the computational foundations of transformer reasoning.\n","authors":["Rabin Adhikari"],"pdf_url":"https://arxiv.org/pdf/2510.25013v1.pdf","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2510.25007v1","updated":"2025-10-28T22:06:59Z","published":"2025-10-28T22:06:59Z","title":"Taming the Real-world Complexities in CPT E/M Coding with Large Language\n  Models","summary":"  Evaluation and Management (E/M) coding, under the Current Procedural\nTerminology (CPT) taxonomy, documents medical services provided to patients by\nphysicians. Used primarily for billing purposes, it is in physicians' best\ninterest to provide accurate CPT E/M codes. %While important, it is an\nauxiliary task that adds to physicians' documentation burden. Automating this\ncoding task will help alleviate physicians' documentation burden, improve\nbilling efficiency, and ultimately enable better patient care. However, a\nnumber of real-world complexities have made E/M encoding automation a\nchallenging task. In this paper, we elaborate some of the key complexities and\npresent ProFees, our LLM-based framework that tackles them, followed by a\nsystematic evaluation. On an expert-curated real-world dataset, ProFees\nachieves an increase in coding accuracy of more than 36\\% over a commercial CPT\nE/M coding system and almost 5\\% over our strongest single-prompt baseline,\ndemonstrating its effectiveness in addressing the real-world complexities.\n","authors":["Islam Nassar","Yang Lin","Yuan Jin","Rongxin Zhu","Chang Wei Tan","Zenan Zhai","Nitika Mathur","Thanh Tien Vu","Xu Zhong","Long Duong","Yuan-Fang Li"],"pdf_url":"https://arxiv.org/pdf/2510.25007v1.pdf","comment":"EMNLP 2025 Industry Track"},{"id":"http://arxiv.org/abs/2510.25005v1","updated":"2025-10-28T22:03:01Z","published":"2025-10-28T22:03:01Z","title":"Cyclic Counterfactuals under Shift-Scale Interventions","summary":"  Most counterfactual inference frameworks traditionally assume acyclic\nstructural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However,\nmany real-world systems (e.g. biological systems) contain feedback loops or\ncyclic dependencies that violate acyclicity. In this work, we study\ncounterfactual inference in cyclic SCMs under shift-scale interventions, i.e.,\nsoft, policy-style changes that rescale and/or shift a variable's mechanism.\n","authors":["Saptarshi Saha","Dhruv Vansraj Rathore","Utpal Garain"],"pdf_url":"https://arxiv.org/pdf/2510.25005v1.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25001v1","updated":"2025-10-28T22:00:30Z","published":"2025-10-28T22:00:30Z","title":"Bayesian Neural Networks vs. Mixture Density Networks: Theoretical and\n  Empirical Insights for Uncertainty-Aware Nonlinear Modeling","summary":"  This paper investigates two prominent probabilistic neural modeling\nparadigms: Bayesian Neural Networks (BNNs) and Mixture Density Networks (MDNs)\nfor uncertainty-aware nonlinear regression. While BNNs incorporate epistemic\nuncertainty by placing prior distributions over network parameters, MDNs\ndirectly model the conditional output distribution, thereby capturing\nmultimodal and heteroscedastic data-generating mechanisms. We present a unified\ntheoretical and empirical framework comparing these approaches. On the\ntheoretical side, we derive convergence rates and error bounds under H\\\"older\nsmoothness conditions, showing that MDNs achieve faster Kullback-Leibler (KL)\ndivergence convergence due to their likelihood-based nature, whereas BNNs\nexhibit additional approximation bias induced by variational inference.\nEmpirically, we evaluate both architectures on synthetic nonlinear datasets and\na radiographic benchmark (RSNA Pediatric Bone Age Challenge). Quantitative and\nqualitative results demonstrate that MDNs more effectively capture multimodal\nresponses and adaptive uncertainty, whereas BNNs provide more interpretable\nepistemic uncertainty under limited data. Our findings clarify the\ncomplementary strengths of posterior-based and likelihood-based probabilistic\nlearning, offering guidance for uncertainty-aware modeling in nonlinear\nsystems.\n","authors":["Riddhi Pratim Ghosh","Ian Barnett"],"pdf_url":"https://arxiv.org/pdf/2510.25001v1.pdf","comment":"20 pages, 2 figures"},{"id":"http://arxiv.org/abs/2510.25000v1","updated":"2025-10-28T21:59:49Z","published":"2025-10-28T21:59:49Z","title":"What Really Matters in Matrix-Whitening Optimizers?","summary":"  A range of recent optimizers have emerged that approximate the same\n\"matrix-whitening\" transformation in various ways. In this work, we\nsystematically deconstruct such optimizers, aiming to disentangle the key\ncomponents that explain performance. Across tuned hyperparameters across the\nboard, all flavors of matrix-whitening methods reliably outperform elementwise\ncounterparts, such as Adam. Matrix-whitening is often related to spectral\ndescent -- however, experiments reveal that performance gains are *not\nexplained solely by accurate spectral normalization* -- particularly, SOAP\ndisplays the largest per-step gain, even though Muon more accurately descends\nalong the steepest spectral descent direction. Instead, we argue that\nmatrix-whitening serves two purposes, and the variance adaptation component of\nmatrix-whitening is the overlooked ingredient explaining this performance gap.\nExperiments show that variance-adapted versions of optimizers consistently\noutperform their sign-descent counterparts, including an adaptive version of\nMuon. We further ablate variance adaptation strategies, finding that while\nlookahead style approximations are not as effective, low-rank variance\nestimators can effectively reduce memory costs without a performance loss.\n","authors":["Kevin Frans","Pieter Abbeel","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2510.25000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24988v1","updated":"2025-10-28T21:34:23Z","published":"2025-10-28T21:34:23Z","title":"Enhancing Hierarchical Reinforcement Learning through Change Point\n  Detection in Time Series","summary":"  Hierarchical Reinforcement Learning (HRL) enhances the scalability of\ndecision-making in long-horizon tasks by introducing temporal abstraction\nthrough options-policies that span multiple timesteps. Despite its theoretical\nappeal, the practical implementation of HRL suffers from the challenge of\nautonomously discovering semantically meaningful subgoals and learning optimal\noption termination boundaries. This paper introduces a novel architecture that\nintegrates a self-supervised, Transformer-based Change Point Detection (CPD)\nmodule into the Option-Critic framework, enabling adaptive segmentation of\nstate trajectories and the discovery of options. The CPD module is trained\nusing heuristic pseudo-labels derived from intrinsic signals to infer latent\nshifts in environment dynamics without external supervision. These inferred\nchange-points are leveraged in three critical ways: (i) to serve as supervisory\nsignals for stabilizing termination function gradients, (ii) to pretrain\nintra-option policies via segment-wise behavioral cloning, and (iii) to enforce\nfunctional specialization through inter-option divergence penalties over\nCPD-defined state partitions. The overall optimization objective enhances the\nstandard actor-critic loss using structure-aware auxiliary losses. In our\nframework, option discovery arises naturally as CPD-defined trajectory segments\nare mapped to distinct intra-option policies, enabling the agent to\nautonomously partition its behavior into reusable, semantically meaningful\nskills. Experiments on the Four-Rooms and Pinball tasks demonstrate that\nCPD-guided agents exhibit accelerated convergence, higher cumulative returns,\nand significantly improved option specialization. These findings confirm that\nintegrating structural priors via change-point segmentation leads to more\ninterpretable, sample-efficient, and robust hierarchical policies in complex\nenvironments.\n","authors":["Hemanath Arumugam","Falong Fan","Bo Liu"],"pdf_url":"https://arxiv.org/pdf/2510.24988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06568v3","updated":"2025-10-28T21:32:09Z","published":"2024-11-10T19:11:48Z","title":"Meta-Learning Objectives for Preference Optimization","summary":"  Evaluating preference optimization (PO) algorithms on LLM alignment is a\nchallenging task that presents prohibitive costs, noise, and several variables\nlike model size and hyper-parameters. In this work, we show that it is possible\nto gain insights on the efficacy of PO algorithm on simpler benchmarks. We\ndesign a diagnostic suite of MuJoCo tasks and datasets, which we use to\nsystematically evaluate PO algorithms, establishing a more controlled and\ncheaper benchmark. We then propose a novel family of PO algorithms based on\nmirror descent, which we call Mirror Preference Optimization (MPO). Through\nevolutionary strategies, we search this class to discover algorithms\nspecialized to specific properties of preference datasets, such as\nmixed-quality or noisy data. We demonstrate that our discovered PO algorithms\noutperform all known algorithms in the targeted MuJoCo settings. Finally, based\non the insights gained from our MuJoCo experiments, we design a PO algorithm\nthat significantly outperform existing baselines in an LLM alignment task.\n","authors":["Carlo Alfano","Silvia Sapora","Jakob Nicolaus Foerster","Patrick Rebeschini","Yee Whye Teh"],"pdf_url":"https://arxiv.org/pdf/2411.06568v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24987v1","updated":"2025-10-28T21:28:39Z","published":"2025-10-28T21:28:39Z","title":"scMRDR: A scalable and flexible framework for unpaired single-cell\n  multi-omics data integration","summary":"  Advances in single-cell sequencing have enabled high-resolution profiling of\ndiverse molecular modalities, while integrating unpaired multi-omics\nsingle-cell data remains challenging. Existing approaches either rely on pair\ninformation or prior correspondences, or require computing a global pairwise\ncoupling matrix, limiting their scalability and flexibility. In this paper, we\nintroduce a scalable and flexible generative framework called single-cell\nMulti-omics Regularized Disentangled Representations (scMRDR) for unpaired\nmulti-omics integration. Specifically, we disentangle each cell's latent\nrepresentations into modality-shared and modality-specific components using a\nwell-designed $\\beta$-VAE architecture, which are augmented with isometric\nregularization to preserve intra-omics biological heterogeneity, adversarial\nobjective to encourage cross-modal alignment, and masked reconstruction loss\nstrategy to address the issue of missing features across modalities. Our method\nachieves excellent performance on benchmark datasets in terms of batch\ncorrection, modality alignment, and biological signal preservation. Crucially,\nit scales effectively to large-level datasets and supports integration of more\nthan two omics, offering a powerful and flexible solution for large-scale\nmulti-omics data integration and downstream biological discovery.\n","authors":["Jianle Sun","Chaoqi Liang","Ran Wei","Peng Zheng","Lei Bai","Wanli Ouyang","Hongliang Yan","Peng Ye"],"pdf_url":"https://arxiv.org/pdf/2510.24987v1.pdf","comment":"Accepted at NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2506.17488v2","updated":"2025-10-28T21:28:31Z","published":"2025-06-20T21:49:17Z","title":"Online Adaptation for Flying Quadrotors in Tight Formations","summary":"  The task of flying in tight formations is challenging for teams of quadrotors\nbecause the complex aerodynamic wake interactions can destabilize individual\nteam members as well as the team. Furthermore, these aerodynamic effects are\nhighly nonlinear and fast-paced, making them difficult to model and predict. To\novercome these challenges, we present L1 KNODE-DW MPC, an adaptive, mixed\nexpert learning based control framework that allows individual quadrotors to\naccurately track trajectories while adapting to time-varying aerodynamic\ninteractions during formation flights. We evaluate L1 KNODE-DW MPC in two\ndifferent three-quadrotor formations and show that it outperforms several MPC\nbaselines. Our results show that the proposed framework is capable of enabling\nthe three-quadrotor team to remain vertically aligned in close proximity\nthroughout the flight. These findings show that the L1 adaptive module\ncompensates for unmodeled disturbances most effectively when paired with an\naccurate dynamics model. A video showcasing our framework and the physical\nexperiments is available here: https://youtu.be/9QX1Q5Ut9Rs\n","authors":["Pei-An Hsieh","Kong Yao Chee","M. Ani Hsieh"],"pdf_url":"https://arxiv.org/pdf/2506.17488v2.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.24986v1","updated":"2025-10-28T21:28:18Z","published":"2025-10-28T21:28:18Z","title":"Epileptic Seizure Detection and Prediction from EEG Data: A Machine\n  Learning Approach with Clinical Validation","summary":"  In recent years, machine learning has become an increasingly powerful tool\nfor supporting seizure detection and monitoring in epilepsy care. Traditional\napproaches focus on identifying seizures only after they begin, which limits\nthe opportunity for early intervention and proactive treatment. In this study,\nwe propose a novel approach that integrates both real-time seizure detection\nand prediction, aiming to capture subtle temporal patterns in EEG data that may\nindicate an upcoming seizure. Our approach was evaluated using the CHB-MIT\nScalp EEG Database, which includes 969 hours of recordings and 173 seizures\ncollected from 23 pediatric and young adult patients with drug-resistant\nepilepsy. To support seizure detection, we implemented a range of supervised\nmachine learning algorithms, including K-Nearest Neighbors, Logistic\nRegression, Random Forest, and Support Vector Machine. The Logistic Regression\nachieved 90.9% detection accuracy with 89.6% recall, demonstrating balanced\nperformance suitable for clinical screening. Random Forest and Support Vector\nMachine models achieved higher accuracy (94.0%) but with 0% recall, failing to\ndetect any seizures, illustrating that accuracy alone is insufficient for\nevaluating medical ML models with class imbalance. For seizure prediction, we\nemployed Long Short-Term Memory (LSTM) networks, which use deep learning to\nmodel temporal dependencies in EEG data. The LSTM model achieved 89.26%\nprediction accuracy. These results highlight the potential of developing\naccessible, real-time monitoring tools that not only detect seizures as\ntraditionally done, but also predict them before they occur. This ability to\npredict seizures marks a significant shift from reactive seizure management to\na more proactive approach, allowing patients to anticipate seizures and take\nprecautionary measures to reduce the risk of injury or other complications.\n","authors":["Ria Jayanti","Tanish Jain"],"pdf_url":"https://arxiv.org/pdf/2510.24986v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2510.24983v1","updated":"2025-10-28T21:26:18Z","published":"2025-10-28T21:26:18Z","title":"LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies","summary":"  Diffusion policies are competitive for offline reinforcement learning (RL)\nbut are typically guided at sampling time by heuristics that lack a statistical\nnotion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that\ntreats each denoising step as a sequential hypothesis test between the\nunconditional prior and the state-conditional policy head. Concretely, we\naccumulate a log-likelihood ratio and gate the conditional mean with a logistic\ncontroller whose threshold tau is calibrated once under H0 to meet a\nuser-specified Type-I level alpha. This turns guidance from a fixed push into\nan evidence-driven adjustment with a user-interpretable risk budget.\nImportantly, we deliberately leave training vanilla (two heads with standard\nepsilon-prediction) under the structure of DDPM. LRT guidance composes\nnaturally with Q-gradients: critic-gradient updates can be taken at the\nunconditional mean, at the LRT-gated mean, or a blend, exposing a continuum\nfrom exploitation to conservatism. We standardize states and actions\nconsistently at train and test time and report a state-conditional\nout-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,\nLRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines\nin our implementation while honoring the desired alpha. Theoretically, we\nestablish level-alpha calibration, concise stability bounds, and a return\ncomparison showing when LRT surpasses Q-guidance-especially when off-support\nerrors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method\nthat adds principled, calibrated risk control to diffusion policies for offline\nRL.\n","authors":["Ximan Sun","Xiang Cheng"],"pdf_url":"https://arxiv.org/pdf/2510.24983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20995v2","updated":"2025-10-28T21:25:00Z","published":"2025-10-23T20:46:49Z","title":"AL-CoLe: Augmented Lagrangian for Constrained Learning","summary":"  Despite the non-convexity of most modern machine learning parameterizations,\nLagrangian duality has become a popular tool for addressing constrained\nlearning problems. We revisit Augmented Lagrangian methods, which aim to\nmitigate the duality gap in non-convex settings while requiring only minimal\nmodifications, and have remained comparably unexplored in constrained learning\nsettings. We establish strong duality results under mild conditions, prove\nconvergence of dual ascent algorithms to feasible and optimal primal solutions,\nand provide PAC-style generalization guarantees. Finally, we demonstrate its\neffectiveness on fairness constrained classification tasks.\n","authors":["Ignacio Boero","Ignacio Hounie","Alejandro Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2510.20995v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24982v1","updated":"2025-10-28T21:24:43Z","published":"2025-10-28T21:24:43Z","title":"Strategic inputs: feature selection from game-theoretic perspective","summary":"  The exponential growth of data volumes has led to escalating computational\ncosts in machine learning model training. However, many features fail to\ncontribute positively to model performance while consuming substantial\ncomputational resources. This paper presents an end-to-end feature selection\nframework for tabular data based on game theory. We formulate feature selection\nprocedure based on a cooperative game where features are modeled as players,\nand their importance is determined through the evaluation of synergistic\ninteractions and marginal contributions. The proposed framework comprises four\ncore components: sample selection, game-theoretic feature importance\nevaluation, redundant feature elimination, and optimized model training.\nExperimental results demonstrate that the proposed method achieves substantial\ncomputation reduction while preserving predictive performance, thereby offering\nan efficient solution of the computational challenges of large-scale machine\nlearning. The source code is available at\nhttps://github.com/vectorsss/strategy_inputs.\n","authors":["Chi Zhao","Jing Liu","Elena Parilina"],"pdf_url":"https://arxiv.org/pdf/2510.24982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.00662v3","updated":"2025-10-28T21:18:02Z","published":"2023-02-01T18:40:53Z","title":"Robust Fitted-Q-Evaluation and Iteration under Sequentially Exogenous\n  Unobserved Confounders","summary":"  Offline reinforcement learning is important in domains such as medicine,\neconomics, and e-commerce where online experimentation is costly, dangerous or\nunethical, and where the true model is unknown. However, most methods assume\nall covariates used in the behavior policy's action decisions are observed.\nThough this assumption, sequential ignorability/unconfoundedness, likely does\nnot hold in observational data, most of the data that accounts for selection\ninto treatment may be observed, motivating sensitivity analysis. We study\nrobust policy evaluation and policy optimization in the presence of\nsequentially-exogenous unobserved confounders under a sensitivity model. We\npropose and analyze orthogonalized robust fitted-Q-iteration that uses\nclosed-form solutions of the robust Bellman operator to derive a loss\nminimization problem for the robust Q function, and adds a bias-correction to\nquantile estimation. Our algorithm enjoys the computational ease of\nfitted-Q-iteration and statistical improvements (reduced dependence on quantile\nestimation error) from orthogonalization. We provide sample complexity bounds,\ninsights, and show effectiveness both in simulations and on real-world\nlongitudinal healthcare data of treating sepsis. In particular, our model of\nsequential unobserved confounders yields an online Markov decision process,\nrather than partially observed Markov decision process: we illustrate how this\ncan enable warm-starting optimistic reinforcement learning algorithms with\nvalid robust bounds from observational data.\n","authors":["David Bruns-Smith","Angela Zhou"],"pdf_url":"https://arxiv.org/pdf/2302.00662v3.pdf","comment":"updated with new warmstarting, complex healthcare data case study"},{"id":"http://arxiv.org/abs/2506.00917v3","updated":"2025-10-28T21:14:04Z","published":"2025-06-01T09:11:24Z","title":"Q-learning with Posterior Sampling","summary":"  Bayesian posterior sampling techniques have demonstrated superior empirical\nperformance in many exploration-exploitation settings. However, their\ntheoretical analysis remains a challenge, especially in complex settings like\nreinforcement learning. In this paper, we introduce Q-Learning with Posterior\nSampling (PSQL), a simple Q-learning-based algorithm that uses Gaussian\nposteriors on Q-values for exploration, akin to the popular Thompson Sampling\nalgorithm in the multi-armed bandit setting. We show that in the tabular\nepisodic MDP setting, PSQL achieves a regret bound of $\\tilde\nO(H^2\\sqrt{SAT})$, closely matching the known lower bound of\n$\\Omega(H\\sqrt{SAT})$. Here, S, A denote the number of states and actions in\nthe underlying Markov Decision Process (MDP), and $T=KH$ with $K$ being the\nnumber of episodes and $H$ being the planning horizon. Our work provides\nseveral new technical insights into the core challenges in combining posterior\nsampling with dynamic programming and TD-learning-based RL algorithms, along\nwith novel ideas for resolving those difficulties. We hope this will form a\nstarting point for analyzing this efficient and important algorithmic technique\nin even more complex RL settings.\n","authors":["Priyank Agrawal","Shipra Agrawal","Azmat Azati"],"pdf_url":"https://arxiv.org/pdf/2506.00917v3.pdf","comment":"Updated version"},{"id":"http://arxiv.org/abs/2510.24974v1","updated":"2025-10-28T21:13:37Z","published":"2025-10-28T21:13:37Z","title":"Conformational Rank Conditioned Committees for Machine Learning-Assisted\n  Directed Evolution","summary":"  Machine Learning-assisted directed evolution (MLDE) is a powerful tool for\nefficiently navigating antibody fitness landscapes. Many structure-aware MLDE\npipelines rely on a single conformation or a single committee across all\nconformations, limiting their ability to separate conformational uncertainty\nfrom epistemic uncertainty. Here, we introduce a rank -conditioned committee\n(RCC) framework that leverages ranked conformations to assign a deep neural\nnetwork committee per rank. This design enables a principled separation between\nepistemic uncertainty and conformational uncertainty. We validate our approach\non SARS-CoV-2 antibody docking, demonstrating significant improvements over\nbaseline strategies. Our results offer a scalable route for therapeutic\nantibody discovery while directly addressing the challenge of modeling\nconformational uncertainty.\n","authors":["Mia Adler","Carrie Liang","Brian Peng","Oleg Presnyakov","Justin M. Baker","Jannelle Lauffer","Himani Sharma","Barry Merriman"],"pdf_url":"https://arxiv.org/pdf/2510.24974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09066v5","updated":"2025-10-28T21:13:11Z","published":"2024-03-14T03:13:01Z","title":"Hyperparameters in Continual Learning: A Reality Check","summary":"  Continual learning (CL) aims to train a model on a sequence of tasks (i.e., a\nCL scenario) while balancing the trade-off between plasticity (learning new\ntasks) and stability (retaining prior knowledge). The dominantly adopted\nconventional evaluation protocol for CL algorithms selects the best\nhyperparameters (e.g., learning rate, mini-batch size, regularization\nstrengths, etc.) within a given scenario and then evaluates the algorithms\nusing these hyperparameters in the same scenario. However, this protocol has\nsignificant shortcomings: it overestimates the CL capacity of algorithms and\nrelies on unrealistic hyperparameter tuning, which is not feasible for\nreal-world applications. From the fundamental principles of evaluation in\nmachine learning, we argue that the evaluation of CL algorithms should focus on\nassessing the generalizability of their CL capacity to unseen scenarios. Based\non this, we propose the Generalizable Two-phase Evaluation Protocol (GTEP)\nconsisting of hyperparameter tuning and evaluation phases. Both phases share\nthe same scenario configuration (e.g., number of tasks) but are generated from\ndifferent datasets. Hyperparameters of CL algorithms are tuned in the first\nphase and applied in the second phase to evaluate the algorithms. We apply this\nprotocol to class-incremental learning, both with and without pretrained\nmodels. Across more than 8,000 experiments, our results show that most\nstate-of-the-art algorithms fail to replicate their reported performance,\nhighlighting that their CL capacity has been significantly overestimated in the\nconventional evaluation protocol. Our implementation can be found in\nhttps://github.com/csm9493/GTEP.\n","authors":["Sungmin Cha","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2403.09066v5.pdf","comment":"TMLR 2025 camera ready version"},{"id":"http://arxiv.org/abs/2505.13111v2","updated":"2025-10-28T21:12:26Z","published":"2025-05-19T13:39:47Z","title":"Why Knowledge Distillation Works in Generative Models: A Minimal Working\n  Explanation","summary":"  Knowledge distillation (KD) is a core component in the training and\ndeployment of modern generative models, particularly large language models\n(LLMs). While its empirical benefits are well documented -- enabling smaller\nstudent models to emulate the performance of much larger teachers -- the\nunderlying mechanisms by which KD improves generative quality remain poorly\nunderstood. In this work, we present a minimal working explanation of KD in\ngenerative modeling. Using a controlled simulation with mixtures of Gaussians,\nwe demonstrate that distillation induces a trade-off between precision and\nrecall in the student model. As the teacher distribution becomes more\nselective, the student concentrates more probability mass on high-likelihood\nregions at the expense of coverage -- a behavior modulated by a single\nentropy-controlling parameter. We then validate this effect in a large-scale\nlanguage modeling setup using the SmolLM2 family of models. Empirical results\nreveal the same precision-recall dynamics observed in simulation, where\nprecision corresponds to sample quality and recall to distributional coverage.\nThis precision-recall trade-off in LLMs is found to be especially beneficial in\nscenarios where sample quality is more important than diversity, such as\ninstruction tuning or downstream generation. Our analysis provides a simple and\ngeneral explanation for the effectiveness of KD in generative modeling.\n","authors":["Sungmin Cha","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2505.13111v2.pdf","comment":"NeurIPS 2025 camera ready version"},{"id":"http://arxiv.org/abs/2308.08705v4","updated":"2025-10-28T21:12:00Z","published":"2023-08-16T23:42:03Z","title":"Partially Observable Multi-Agent Reinforcement Learning with Information\n  Sharing","summary":"  We study provable multi-agent reinforcement learning (RL) in the general\nframework of partially observable stochastic games (POSGs). To circumvent the\nknown hardness results and the use of computationally intractable oracles, we\nadvocate leveraging the potential \\emph{information-sharing} among agents, a\ncommon practice in empirical multi-agent RL, and a standard model for\nmulti-agent control systems with communication. We first establish several\ncomputational complexity results to justify the necessity of\ninformation-sharing, as well as the observability assumption that has enabled\nquasi-polynomial time and sample single-agent RL with partial observations, for\ntractably solving POSGs. Inspired by the inefficiency of planning in the\nground-truth model, we then propose to further \\emph{approximate} the shared\ncommon information to construct an approximate model of the POSG, in which an\napproximate \\emph{equilibrium} (of the original POSG) can be found in\nquasi-polynomial-time, under the aforementioned assumptions. Furthermore, we\ndevelop a partially observable multi-agent RL algorithm whose time and sample\ncomplexities are \\emph{both} quasi-polynomial. Finally, beyond equilibrium\nlearning, we extend our algorithmic framework to finding the \\emph{team-optimal\nsolution} in cooperative POSGs, i.e., decentralized partially observable Markov\ndecision processes, a more challenging goal. We establish concrete\ncomputational and sample complexities under several structural assumptions of\nthe model. We hope our study could open up the possibilities of leveraging and\neven designing different \\emph{information structures}, a well-studied notion\nin control theory, for developing both sample- and computation-efficient\npartially observable multi-agent RL.\n","authors":["Xiangyu Liu","Kaiqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.08705v4.pdf","comment":"Journal extension of the conference version at ICML 2023 accepted to\n  SIAM Journal on Control and Optimization (SICON)"},{"id":"http://arxiv.org/abs/2507.14109v2","updated":"2025-10-28T21:11:23Z","published":"2025-07-18T17:42:20Z","title":"An Adversarial-Driven Experimental Study on Deep Learning for RF\n  Fingerprinting","summary":"  Radio frequency (RF) fingerprinting, which extracts unique hardware\nimperfections of radio devices, has emerged as a promising physical-layer\ndevice identification mechanism in zero trust architectures and beyond 5G\nnetworks. In particular, deep learning (DL) methods have demonstrated\nstate-of-the-art performance in this domain. However, existing approaches have\nprimarily focused on enhancing system robustness against temporal and spatial\nvariations in wireless environments, while the security vulnerabilities of\nthese DL-based approaches have often been overlooked. In this work, we\nsystematically investigate the security risks of DL-based RF fingerprinting\nsystems through an adversarial-driven experimental analysis. We observe a\nconsistent misclassification behavior for DL models under domain shifts, where\na device is frequently misclassified as another specific one. Our analysis\nbased on extensive real-world experiments demonstrates that this behavior can\nbe exploited as an effective backdoor to enable external attackers to intrude\ninto the system. Furthermore, we show that training DL models on raw received\nsignals causes the models to entangle RF fingerprints with environmental and\nsignal-pattern features, creating additional attack vectors that cannot be\nmitigated solely through post-processing security methods such as confidence\nthresholds.\n","authors":["Xinyu Cao","Bimal Adhikari","Shangqing Zhao","Jingxian Wu","Yanjun Pan"],"pdf_url":"https://arxiv.org/pdf/2507.14109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.16656v3","updated":"2025-10-28T21:00:46Z","published":"2025-06-20T00:00:22Z","title":"Mesh-Informed Neural Operator : A Transformer Generative Approach","summary":"  Generative models in function spaces, situated at the intersection of\ngenerative modeling and operator learning, are attracting increasing attention\ndue to their immense potential in diverse scientific and engineering\napplications. While functional generative models are theoretically domain- and\ndiscretization-agnostic, current implementations heavily rely on the Fourier\nNeural Operator (FNO), limiting their applicability to regular grids and\nrectangular domains. To overcome these critical limitations, we introduce the\nMesh-Informed Neural Operator (MINO). By leveraging graph neural operators and\ncross-attention mechanisms, MINO offers a principled, domain- and\ndiscretization-agnostic backbone for generative modeling in function spaces.\nThis advancement significantly expands the scope of such models to more diverse\napplications in generative, inverse, and regression tasks. Furthermore, MINO\nprovides a unified perspective on integrating neural operators with general\nadvanced deep learning architectures. Finally, we introduce a suite of\nstandardized evaluation metrics that enable objective comparison of functional\ngenerative models, addressing another critical gap in the field.\n","authors":["Yaozhong Shi","Zachary E. Ross","Domniki Asimaki","Kamyar Azizzadenesheli"],"pdf_url":"https://arxiv.org/pdf/2506.16656v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01183v2","updated":"2025-10-28T20:58:26Z","published":"2025-06-01T21:34:37Z","title":"Doubly Robust Alignment for Large Language Models","summary":"  This paper studies reinforcement learning from human feedback (RLHF) for\naligning large language models with human preferences. While RLHF has\ndemonstrated promising results, many algorithms are highly sensitive to\nmisspecifications in the underlying preference model (e.g., the Bradley-Terry\nmodel), the reference policy, or the reward function, resulting in undesirable\nfine-tuning. To address model misspecification, we propose a doubly robust\npreference optimization algorithm that remains consistent when either the\npreference model or the reference policy is correctly specified (without\nrequiring both). Our proposal demonstrates superior and more robust performance\nthan state-of-the-art algorithms, both in theory and in practice. The code is\navailable at https://github.com/DRPO4LLM/DRPO4LLM\n","authors":["Erhan Xu","Kai Ye","Hongyi Zhou","Luhan Zhu","Francesco Quinzan","Chengchun Shi"],"pdf_url":"https://arxiv.org/pdf/2506.01183v2.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24966v1","updated":"2025-10-28T20:55:58Z","published":"2025-10-28T20:55:58Z","title":"Sequences of Logits Reveal the Low Rank Structure of Language Models","summary":"  A major problem in the study of large language models is to understand their\ninherent low-dimensional structure. We introduce an approach to study the\nlow-dimensional structure of language models at a model-agnostic level: as\nsequential probabilistic models. We first empirically demonstrate that a wide\nrange of modern language models exhibit low-rank structure: in particular,\nmatrices built from the model's logits for varying sets of prompts and\nresponses have low approximate rank. We then show that this low-rank structure\ncan be leveraged for generation -- in particular, we can generate a response to\na target prompt using a linear combination of the model's outputs on unrelated,\nor even nonsensical prompts.\n  On the theoretical front, we observe that studying the approximate rank of\nlanguage models in the sense discussed above yields a simple universal\nabstraction whose theoretical predictions parallel our experiments. We then\nanalyze the representation power of the abstraction and give provable learning\nguarantees.\n","authors":["Noah Golowich","Allen Liu","Abhishek Shetty"],"pdf_url":"https://arxiv.org/pdf/2510.24966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24951v1","updated":"2025-10-28T20:34:53Z","published":"2025-10-28T20:34:53Z","title":"Resource-Efficient and Robust Inference of Deep and Bayesian Neural\n  Networks on Embedded and Analog Computing Platforms","summary":"  While modern machine learning has transformed numerous application domains,\nits growing computational demands increasingly constrain scalability and\nefficiency, particularly on embedded and resource-limited platforms. In\npractice, neural networks must not only operate efficiently but also provide\nreliable predictions under distributional shifts or unseen data. Bayesian\nneural networks offer a principled framework for quantifying uncertainty, yet\ntheir computational overhead further compounds these challenges.\n  This work advances resource-efficient and robust inference for both\nconventional and Bayesian neural networks through the joint pursuit of\nalgorithmic and hardware efficiency. The former reduces computation through\nmodel compression and approximate Bayesian inference, while the latter\noptimizes deployment on digital accelerators and explores analog hardware,\nbridging algorithmic design and physical realization. The first contribution,\nGalen, performs automatic layer-specific compression guided by sensitivity\nanalysis and hardware-in-the-loop feedback. Analog accelerators offer\nefficiency gains at the cost of noise; this work models device imperfections\nand extends noisy training to nonstationary conditions, improving robustness\nand stability. A second line of work advances probabilistic inference,\ndeveloping analytic and ensemble approximations that replace costly sampling,\nintegrate into a compiler stack, and optimize embedded inference. Finally,\nprobabilistic photonic computing introduces a paradigm where controlled analog\nnoise acts as an intrinsic entropy source, enabling fast, energy-efficient\nprobabilistic inference directly in hardware.\n  Together, these studies demonstrate how efficiency and reliability can be\nadvanced jointly through algorithm-hardware co-design, laying the foundation\nfor the next generation of trustworthy, energy-efficient machine-learning\nsystems.\n","authors":["Bernhard Klein"],"pdf_url":"https://arxiv.org/pdf/2510.24951v1.pdf","comment":"Ph.D. dissertation, Heidelberg University, October 2025"},{"id":"http://arxiv.org/abs/2510.24949v1","updated":"2025-10-28T20:31:19Z","published":"2025-10-28T20:31:19Z","title":"SCOUT: A Lightweight Framework for Scenario Coverage Assessment in\n  Autonomous Driving","summary":"  Assessing scenario coverage is crucial for evaluating the robustness of\nautonomous agents, yet existing methods rely on expensive human annotations or\ncomputationally intensive Large Vision-Language Models (LVLMs). These\napproaches are impractical for large-scale deployment due to cost and\nefficiency constraints. To address these shortcomings, we propose SCOUT\n(Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate\nmodel designed to predict scenario coverage labels directly from an agent's\nlatent sensor representations. SCOUT is trained through a distillation process,\nlearning to approximate LVLM-generated coverage labels while eliminating the\nneed for continuous LVLM inference or human annotation. By leveraging\nprecomputed perception features, SCOUT avoids redundant computations and\nenables fast, scalable scenario coverage estimation. We evaluate our method\nacross a large dataset of real-life autonomous navigation scenarios,\ndemonstrating that it maintains high accuracy while significantly reducing\ncomputational cost. Our results show that SCOUT provides an effective and\npractical alternative for large-scale coverage analysis. While its performance\ndepends on the quality of LVLM-generated training labels, SCOUT represents a\nmajor step toward efficient scenario coverage oversight in autonomous systems.\n","authors":["Anil Yildiz","Sarah M. Thornton","Carl Hildebrandt","Sreeja Roy-Singh","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2510.24949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24942v1","updated":"2025-10-28T20:14:37Z","published":"2025-10-28T20:14:37Z","title":"Finding Culture-Sensitive Neurons in Vision-Language Models","summary":"  Despite their impressive performance, vision-language models (VLMs) still\nstruggle on culturally situated inputs. To understand how VLMs process\nculturally grounded information, we study the presence of culture-sensitive\nneurons, i.e. neurons whose activations show preferential sensitivity to inputs\nassociated with particular cultural contexts. We examine whether such neurons\nare important for culturally diverse visual question answering and where they\nare located. Using the CVQA benchmark, we identify neurons of culture\nselectivity and perform causal tests by deactivating the neurons flagged by\ndifferent identification methods. Experiments on three VLMs across 25 cultural\ngroups demonstrate the existence of neurons whose ablation disproportionately\nharms performance on questions about the corresponding cultures, while having\nminimal effects on others. Moreover, we propose a new margin-based selector -\nContrastive Activation Selection (CAS), and show that it outperforms existing\nprobability- and entropy-based methods in identifying culture-sensitive\nneurons. Finally, our layer-wise analyses reveals that such neurons tend to\ncluster in certain decoder layers. Overall, our findings shed new light on the\ninternal organization of multimodal representations.\n","authors":["Xiutian Zhao","Rochelle Choenni","Rohit Saxena","Ivan Titov"],"pdf_url":"https://arxiv.org/pdf/2510.24942v1.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2510.24941v1","updated":"2025-10-28T20:14:02Z","published":"2025-10-28T20:14:02Z","title":"Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps\n  in Chain-of-Thought","summary":"  Recent large language models (LLMs) can generate long Chain-of-Thought (CoT)\nat test time, enabling them to solve complex tasks. These reasoning steps in\nCoT are often assumed as a faithful reflection of the model's internal thinking\nprocess, and used to monitor unsafe intentions. However, we find many reasoning\nsteps don't truly contribute to LLMs' prediction. We measure the step-wise\ncausal influence of each reasoning step on the model's final prediction with a\nproposed True Thinking Score (TTS). We reveal that LLMs often interleave\nbetween true-thinking steps (which are genuinely used to produce the final\noutput) and decorative-thinking steps (which only give the appearance of\nreasoning but have minimal causal impact). Notably, only a small subset of the\ntotal reasoning steps have a high TTS that causally drive the model's\nprediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning\nsteps in CoT have a TTS >= 0.7 (range: 0-1) under the Qwen-2.5 model.\nFurthermore, we identify a TrueThinking direction in the latent space of LLMs.\nBy steering along or against this direction, we can force the model to perform\nor disregard certain CoT steps when computing the final result. Finally, we\nhighlight that self-verification steps in CoT (i.e., aha moments) can also be\ndecorative, where LLMs do not truly verify their solution. Steering along the\nTrueThinking direction can force internal reasoning over these steps, resulting\nin a change in the final results. Overall, our work reveals that LLMs often\nverbalize reasoning steps without actually performing them internally, which\nundermines both the efficiency of LLM reasoning and the trustworthiness of CoT.\n","authors":["Jiachen Zhao","Yiyou Sun","Weiyan Shi","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2510.24941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12913v2","updated":"2025-10-28T20:06:01Z","published":"2025-06-15T17:03:11Z","title":"Jailbreak Transferability Emerges from Shared Representations","summary":"  Jailbreak transferability is the surprising phenomenon when an adversarial\nattack compromising one model also elicits harmful responses from other models.\nDespite widespread demonstrations, there is little consensus on why transfer is\npossible: is it a quirk of safety training, an artifact of model families, or a\nmore fundamental property of representation learning? We present evidence that\ntransferability emerges from shared representations rather than incidental\nflaws. Across 20 open-weight models and 33 jailbreak attacks, we find two\nfactors that systematically shape transfer: (1) representational similarity\nunder benign prompts, and (2) the strength of the jailbreak on the source\nmodel. To move beyond correlation, we show that deliberately increasing\nsimilarity through benign only distillation causally increases transfer. Our\nqualitative analyses reveal systematic transferability patterns across\ndifferent types of jailbreaks. For example, persona-style jailbreaks transfer\nfar more often than cipher-based prompts, consistent with the idea that\nnatural-language attacks exploit models' shared representation space, whereas\ncipher-based attacks rely on idiosyncratic quirks that do not generalize.\nTogether, these results reframe jailbreak transfer as a consequence of\nrepresentation alignment rather than a fragile byproduct of safety training.\n","authors":["Rico Angell","Jannik Brinkmann","He He"],"pdf_url":"https://arxiv.org/pdf/2506.12913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21671v3","updated":"2025-10-28T20:04:11Z","published":"2025-05-27T18:48:42Z","title":"Adaptive Frontier Exploration on Graphs with Applications to\n  Network-Based Disease Testing","summary":"  We study a sequential decision-making problem on a $n$-node graph\n$\\mathcal{G}$ where each node has an unknown label from a finite set\n$\\mathbf{\\Omega}$, drawn from a joint distribution $\\mathcal{P}$ that is Markov\nwith respect to $\\mathcal{G}$. At each step, selecting a node reveals its label\nand yields a label-dependent reward. The goal is to adaptively choose nodes to\nmaximize expected accumulated discounted rewards. We impose a frontier\nexploration constraint, where actions are limited to neighbors of previously\nselected nodes, reflecting practical constraints in settings such as contact\ntracing and robotic exploration. We design a Gittins index-based policy that\napplies to general graphs and is provably optimal when $\\mathcal{G}$ is a\nforest. Our implementation runs in $\\mathcal{O}(n^2 \\cdot |\\mathbf{\\Omega}|^2)$\ntime while using $\\mathcal{O}(n \\cdot |\\mathbf{\\Omega}|^2)$ oracle calls to\n$\\mathcal{P}$ and $\\mathcal{O}(n^2 \\cdot |\\mathbf{\\Omega}|)$ space. Experiments\non synthetic and real-world graphs show that our method consistently\noutperforms natural baselines, including in non-tree, budget-limited, and\nundiscounted settings. For example, in HIV testing simulations on real-world\nsexual interaction networks, our policy detects nearly all positive cases with\nonly half the population tested, substantially outperforming other baselines.\n","authors":["Davin Choo","Yuqi Pan","Tonghan Wang","Milind Tambe","Alastair van Heerden","Cheryl Johnson"],"pdf_url":"https://arxiv.org/pdf/2505.21671v3.pdf","comment":"Accepted into NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24927v1","updated":"2025-10-28T19:56:13Z","published":"2025-10-28T19:56:13Z","title":"WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for\n  Inductive Learning","summary":"  Link prediction in bipartite graphs is crucial for applications like\nrecommendation systems and failure detection, yet it is less studied than in\nmonopartite graphs. Contrastive methods struggle with inefficient and biased\nnegative sampling, while non-contrastive approaches rely solely on positive\nsamples. Existing models perform well in transductive settings, but their\neffectiveness in inductive, weighted, and bipartite scenarios remains untested.\nTo address this, we propose Weighted Bipartite Triplet-Bootstrapped Graph\nLatents (WBT-BGRL), a non-contrastive framework that enhances bootstrapped\nlearning with a novel weighting mechanism in the triplet loss. Using a\nbipartite architecture with dual GCN encoders, WBT-BGRL is evaluated against\nadapted state-of-the-art models (T-BGRL, BGRL, GBT, CCA-SSG). Results on\nreal-world datasets (Industry and E-commerce) show competitive performance,\nespecially when weighting is applied during pretraining-highlighting the value\nof weighted, non-contrastive learning for inductive link prediction in\nbipartite graphs.\n","authors":["Joel Frank Huarayo Quispe","Lilian Berton","Didier Vega-Oliveros"],"pdf_url":"https://arxiv.org/pdf/2510.24927v1.pdf","comment":"5 pages, submitted to the 12th International Conference on Soft\n  Computing and Machine Intelligence (ISCMI 2025)"},{"id":"http://arxiv.org/abs/2510.22510v2","updated":"2025-10-28T19:55:41Z","published":"2025-10-26T03:24:31Z","title":"CANDI: Hybrid Discrete-Continuous Diffusion Models","summary":"  While continuous diffusion has shown remarkable success in continuous domains\nsuch as image generation, its direct application to discrete data has\nunderperformed compared to purely discrete formulations. This gap is\ncounterintuitive, given that continuous diffusion learns score functions that\nenable joint evolution across multiple positions. To understand this gap, we\nintroduce token identifiability as an analytical framework for understanding\nhow Gaussian noise corrupts discrete data through two mechanisms: discrete\nidentity corruption and continuous rank degradation. We reveal that these\nmechanisms scale differently with vocabulary size, creating a temporal\ndissonance: at noise levels where discrete corruption preserves enough\nstructure for conditional learning, continuous denoising is trivial; at noise\nlevels where continuous denoising is meaningful, discrete corruption destroys\nnearly all conditional structure. To solve this, we propose CANDI (Continuous\nANd DIscrete diffusion), a hybrid framework that decouples discrete and\ncontinuous corruption, enabling simultaneous learning of both conditional\nstructure and continuous geometry. We empirically validate the temporal\ndissonance phenomenon and demonstrate that CANDI successfully avoids it. This\nunlocks the benefits of continuous diffusion for discrete spaces: on controlled\ngeneration, CANDI enables classifier-based guidance with off-the-shelf\nclassifiers through simple gradient addition; on text generation, CANDI\noutperforms masked diffusion at low NFE, demonstrating the value of learning\ncontinuous gradients for discrete spaces. We include the code on the project\npage available here: https://patrickpynadath1.github.io/candi-lander\n","authors":["Patrick Pynadath","Jiaxin Shi","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.22510v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24926v1","updated":"2025-10-28T19:55:29Z","published":"2025-10-28T19:55:29Z","title":"KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution\n  Network for an Accurate Ice Sheet Emulator","summary":"  We introduce KAN-GCN, a fast and accurate emulator for ice sheet modeling\nthat places a Kolmogorov-Arnold Network (KAN) as a feature-wise calibrator\nbefore graph convolution networks (GCNs). The KAN front end applies learnable\none-dimensional warps and a linear mixing step, improving feature conditioning\nand nonlinear encoding without increasing message-passing depth. We employ this\narchitecture to improve the performance of emulators for numerical ice sheet\nmodels. Our emulator is trained and tested using 36 melting-rate simulations\nwith 3 mesh-size settings for Pine Island Glacier, Antarctica. Across 2- to\n5-layer architectures, KAN-GCN matches or exceeds the accuracy of pure GCN and\nMLP-GCN baselines. Despite a small parameter overhead, KAN-GCN improves\ninference throughput on coarser meshes by replacing one edge-wise\nmessage-passing layer with a node-wise transform; only the finest mesh shows a\nmodest cost. Overall, KAN-first designs offer a favorable accuracy vs.\nefficiency trade-off for large transient scenario sweeps.\n","authors":["Zesheng Liu","YoungHyun Koo","Maryam Rahnemoonfar"],"pdf_url":"https://arxiv.org/pdf/2510.24926v1.pdf","comment":"Accept for NeurIPS 2025 Workshop: New Perspectives in Graph Machine\n  Learning"},{"id":"http://arxiv.org/abs/2510.21779v2","updated":"2025-10-28T19:53:22Z","published":"2025-10-18T05:07:57Z","title":"What Causes Postoperative Aspiration?","summary":"  Background: Aspiration, the inhalation of foreign material into the lungs,\nsignificantly impacts surgical patient morbidity and mortality. This study\ndevelops a machine learning (ML) model to predict postoperative aspiration,\nenabling timely preventative interventions.\n  Methods: From the MIMIC-IV database of over 400,000 hospital admissions, we\nidentified 826 surgical patients (mean age: 62, 55.7\\% male) who experienced\naspiration within seven days post-surgery, along with a matched non-aspiration\ncohort. Three ML models: XGBoost, Multilayer Perceptron, and Random Forest were\ntrained using pre-surgical hospitalization data to predict postoperative\naspiration. To investigate causation, we estimated Average Treatment Effects\n(ATE) using Augmented Inverse Probability Weighting.\n  Results: Our ML model achieved an AUROC of 0.86 and 77.3\\% sensitivity on a\nheld-out test set. Maximum daily opioid dose, length of stay, and patient age\nemerged as the most important predictors. ATE analysis identified significant\ncausative factors: opioids (0.25 +/- 0.06) and operative site (neck: 0.20 +/-\n0.13, head: 0.19 +/- 0.13). Despite equal surgery rates across genders, men\nwere 1.5 times more likely to aspirate and received 27\\% higher maximum daily\nopioid dosages compared to women.\n  Conclusion: ML models can effectively predict postoperative aspiration risk,\nenabling targeted preventative measures. Maximum daily opioid dosage and\noperative site significantly influence aspiration risk. The gender disparity in\nboth opioid administration and aspiration rates warrants further investigation.\nThese findings have important implications for improving postoperative care\nprotocols and aspiration prevention strategies.\n","authors":["Supriya Nagesh","Karina Covarrubias","Robert El-Kareh","Shiva Prasad Kasiviswanathan","Nina Mishra"],"pdf_url":"https://arxiv.org/pdf/2510.21779v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24919v1","updated":"2025-10-28T19:44:20Z","published":"2025-10-28T19:44:20Z","title":"Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient\n  Modulation for Harmonized Multimodal Learning","summary":"  In multimodal learning, dominant modalities often overshadow others, limiting\ngeneralization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM),\na model-agnostic framework that applies to many modalities and supports early\nand late fusion scenarios. In every iteration, M-SAM in three steps optimizes\nlearning. \\textbf{First, it identifies the dominant modality} based on\nmodalities' contribution in the accuracy using Shapley. \\textbf{Second, it\ndecomposes the loss landscape}, or in another language, it modulates the loss\nto prioritize the robustness of the model in favor of the dominant modality,\nand \\textbf{third, M-SAM updates the weights} by backpropagation of modulated\ngradients. This ensures robust learning for the dominant modality while\nenhancing contributions from others, allowing the model to explore and exploit\ncomplementary features that strengthen overall performance. Extensive\nexperiments on four diverse datasets show that M-SAM outperforms the latest\nstate-of-the-art optimization and gradient manipulation methods and\nsignificantly balances and improves multimodal learning.\n","authors":["Hossein R. Nowdeh","Jie Ji","Xiaolong Ma","Fatemeh Afghah"],"pdf_url":"https://arxiv.org/pdf/2510.24919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24918v1","updated":"2025-10-28T19:38:36Z","published":"2025-10-28T19:38:36Z","title":"Topic Analysis with Side Information: A Neural-Augmented LDA Approach","summary":"  Traditional topic models such as Latent Dirichlet Allocation (LDA) have been\nwidely used to uncover latent structures in text corpora, but they often\nstruggle to integrate auxiliary information such as metadata, user attributes,\nor document labels. These limitations restrict their expressiveness,\npersonalization, and interpretability. To address this, we propose nnLDA, a\nneural-augmented probabilistic topic model that dynamically incorporates side\ninformation through a neural prior mechanism. nnLDA models each document as a\nmixture of latent topics, where the prior over topic proportions is generated\nby a neural network conditioned on auxiliary features. This design allows the\nmodel to capture complex nonlinear interactions between side information and\ntopic distributions that static Dirichlet priors cannot represent. We develop a\nstochastic variational Expectation-Maximization algorithm to jointly optimize\nthe neural and probabilistic components. Across multiple benchmark datasets,\nnnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression in\ntopic coherence, perplexity, and downstream classification. These results\nhighlight the benefits of combining neural representation learning with\nprobabilistic topic modeling in settings where side information is available.\n","authors":["Biyi Fang","Kripa Rajshekhar","Truong Vo","Diego Klabjan"],"pdf_url":"https://arxiv.org/pdf/2510.24918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09767v3","updated":"2025-10-28T19:36:28Z","published":"2025-02-13T20:51:25Z","title":"Non-Markovian Discrete Diffusion with Causal Language Models","summary":"  Discrete diffusion models offer a flexible, controllable approach to\nstructured sequence generation, yet they still lag behind causal language\nmodels in expressive power. A key limitation lies in their reliance on the\nMarkovian assumption, which restricts each step to condition only on the\ncurrent state, leading to potential uncorrectable error accumulation. In this\npaper, we introduce CaDDi (Causal Discrete Diffusion Model), a discrete\ndiffusion model that conditions on the entire generative trajectory, thereby\nlifting the Markov constraint and allowing the model to revisit and improve\npast states. By unifying sequential (causal) and temporal (diffusion) reasoning\nin a single non-Markovian transformer, CaDDi also treats standard causal\nlanguage models as a special case and permits the direct reuse of pretrained\nLLM weights with no architectural changes. Empirically, CaDDi outperforms\nstate-of-the-art discrete diffusion baselines on natural-language benchmarks,\nsubstantially narrowing the remaining gap to large autoregressive transformers.\n","authors":["Yangtian Zhang","Sizhuang He","Daniel Levine","Lawrence Zhao","David Zhang","Syed A Rizvi","Shiyang Zhang","Emanuele Zappala","Rex Ying","David van Dijk"],"pdf_url":"https://arxiv.org/pdf/2502.09767v3.pdf","comment":"39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)"},{"id":"http://arxiv.org/abs/2412.02968v2","updated":"2025-10-28T19:25:28Z","published":"2024-12-04T02:31:28Z","title":"How Many Ratings per Item are Necessary for Reliable Significance\n  Testing?","summary":"  A cornerstone of machine learning evaluation is the (often hidden) assumption\nthat model and human responses are reliable enough to evaluate models against\nunitary, authoritative, ``gold standard'' data, via simple metrics such as\naccuracy, precision, and recall. The generative AI revolution would seem to\nexplode this assumption, given the critical role stochastic inference plays.\nYet, in spite of public demand for more transparency in AI -- along with strong\nevidence that humans are unreliable judges -- estimates of model reliability\nare conventionally based on, at most, a few output responses per input item. We\nadapt a method, previously used to evaluate the reliability of various metrics\nand estimators for machine learning evaluation, to determine whether an\n(existing or planned) dataset has enough responses per item to assure reliable\nnull hypothesis statistical testing. We show that, for many common metrics,\ncollecting even 5-10 responses per item (from each model and team of human\nevaluators) is not sufficient. We apply our methods to several of the very few\nextant gold standard test sets with multiple disaggregated responses per item\nand show that even these datasets lack enough responses per item. We show how\nour methods can help AI researchers make better decisions about how to collect\ndata for AI evaluation.\n","authors":["Christopher Homan","Flip Korn","Deepak Pandita","Chris Welty"],"pdf_url":"https://arxiv.org/pdf/2412.02968v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21849v2","updated":"2025-10-28T19:23:06Z","published":"2025-10-22T17:02:48Z","title":"TowerVision: Understanding and Improving Multilinguality in\n  Vision-Language Models","summary":"  Despite significant advances in vision-language models (VLMs), most existing\nwork follows an English-centric design process, limiting their effectiveness in\nmultilingual settings. In this work, we provide a comprehensive empirical study\nanalyzing the impact of several multilingual design choices, such as training\ndata composition, encoder selection, and text backbones. The result is\nTowerVision, a family of open multilingual VLMs for both image-text and\nvideo-text tasks, built upon the multilingual text-only model Tower+.\nTowerVision achieves competitive performance on multiple multimodal\nmultilingual benchmarks and shows particular strength in culturally grounded\ntasks and multimodal translation. By incorporating visual and cultural context\nduring fine-tuning, our models surpass existing approaches trained on\nsubstantially larger datasets, as demonstrated on ALM-Bench and Multi30K (image\ntasks) and ViMUL-Bench (video tasks). Alongside the models, we release\nVisionBlocks, a high-quality, curated vision-language dataset. Our findings\nhighlight that multilingual vision-language training data substantially\nimproves cross-lingual generalization -- both from high-resource to\nunderrepresented languages and vice versa -- and that instruction-tuned LLMs\nare not always the optimal initialization point. To support further research,\nwe publicly release all models, data, and training recipes.\n","authors":["André G. Viveiros","Patrick Fernandes","Saul Santos","Sonal Sannigrahi","Emmanouil Zaranis","Nuno M. Guerreiro","Amin Farajian","Pierre Colombo","Graham Neubig","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2510.21849v2.pdf","comment":"15 pages, 7 figures, submitted to arXiv October 2025. All models,\n  datasets, and training code will be released at\n  https://huggingface.co/collections/utter-project/towervision"},{"id":"http://arxiv.org/abs/2510.24907v1","updated":"2025-10-28T19:19:35Z","published":"2025-10-28T19:19:35Z","title":"Understanding Multi-View Transformers","summary":"  Multi-view transformers such as DUSt3R are revolutionizing 3D vision by\nsolving 3D tasks in a feed-forward manner. However, contrary to previous\noptimization-based pipelines, the inner mechanisms of multi-view transformers\nare unclear. Their black-box nature makes further improvements beyond data\nscaling challenging and complicates usage in safety- and reliability-critical\napplications. Here, we present an approach for probing and visualizing 3D\nrepresentations from the residual connections of the multi-view transformers'\nlayers. In this manner, we investigate a variant of the DUSt3R model, shedding\nlight on the development of its latent state across blocks, the role of the\nindividual layers, and suggest how it differs from methods with stronger\ninductive biases of explicit global pose. Finally, we show that the\ninvestigated variant of DUSt3R estimates correspondences that are refined with\nreconstructed geometry. The code used for the analysis is available at\nhttps://github.com/JulienGaubil/und3rstand .\n","authors":["Michal Stary","Julien Gaubil","Ayush Tewari","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2510.24907v1.pdf","comment":"Presented at the ICCV 2025 E2E3D Workshop"},{"id":"http://arxiv.org/abs/2505.17468v2","updated":"2025-10-28T19:08:31Z","published":"2025-05-23T04:49:14Z","title":"Efficient Adaptive Experimentation with Noncompliance","summary":"  We study the problem of estimating the average treatment effect (ATE) in\nadaptive experiments where treatment can only be encouraged -- rather than\ndirectly assigned -- via a binary instrumental variable. Building on\nsemiparametric efficiency theory, we derive the efficiency bound for ATE\nestimation under arbitrary, history-dependent instrument-assignment policies,\nand show it is minimized by a variance-aware allocation rule that balances\noutcome noise and compliance variability. Leveraging this insight, we introduce\nAMRIV -- an Adaptive, Multiply-Robust estimator for Instrumental-Variable\nsettings with variance-optimal assignment. AMRIV pairs (i) an online policy\nthat adaptively approximates the optimal allocation with (ii) a sequential,\ninfluence-function-based estimator that attains the semiparametric efficiency\nbound while retaining multiply-robust consistency. We establish asymptotic\nnormality, explicit convergence rates, and anytime-valid asymptotic confidence\nsequences that enable sequential inference. Finally, we demonstrate the\npractical effectiveness of our approach through empirical studies, showing that\nadaptive instrument assignment, when combined with the AMRIV estimator, yields\nimproved efficiency and robustness compared to existing baselines.\n","authors":["Miruna Oprescu","Brian M Cho","Nathan Kallus"],"pdf_url":"https://arxiv.org/pdf/2505.17468v2.pdf","comment":"37 pages, 4 figures, 2 tables, NeurIPS 2025"},{"id":"http://arxiv.org/abs/2509.21609v3","updated":"2025-10-28T18:57:29Z","published":"2025-09-25T21:21:00Z","title":"VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster\n  Assessment","summary":"  Immediate damage assessment is essential after natural catastrophes; yet,\nconventional hand evaluation techniques are sluggish and perilous. Although\nsatellite and unmanned aerial vehicle (UAV) photos offer extensive perspectives\nof impacted regions, current computer vision methodologies generally yield just\nclassification labels or segmentation masks, so constraining their capacity to\ndeliver a thorough situational comprehension. We introduce the Vision Language\nCaption Enhancer (VLCE), a multimodal system designed to produce comprehensive,\ncontextually-informed explanations of disaster imagery. VLCE employs a\ndual-architecture approach: a CNN-LSTM model with a ResNet50 backbone\npretrained on EuroSat satellite imagery for the xBD dataset, and a Vision\nTransformer (ViT) model pretrained on UAV pictures for the RescueNet dataset.\nBoth systems utilize external semantic knowledge from ConceptNet and WordNet to\nexpand vocabulary coverage and improve description accuracy. We assess VLCE in\ncomparison to leading vision-language models (LLaVA and QwenVL) utilizing\nCLIPScore for semantic alignment and InfoMetIC for caption informativeness.\nExperimental findings indicate that VLCE markedly surpasses baseline models,\nattaining a maximum of 95.33% on InfoMetIC while preserving competitive\nsemantic alignment. Our dual-architecture system demonstrates significant\npotential for improving disaster damage assessment by automating the production\nof actionable, information-dense descriptions from satellite and drone photos.\n","authors":["Md. Mahfuzur Rahman","Kishor Datta Gupta","Marufa Kamal","Fahad Rahman","Sunzida Siddique","Ahmed Rafi Hasan","Mohd Ariful Haque","Roy George"],"pdf_url":"https://arxiv.org/pdf/2509.21609v3.pdf","comment":"29 pages, 40 figures, 3 algorithms"},{"id":"http://arxiv.org/abs/2509.10516v2","updated":"2025-10-28T18:54:59Z","published":"2025-09-03T11:28:57Z","title":"Privacy-Preserving Personalization in Education: A Federated Recommender\n  System for Student Performance Prediction","summary":"  The increasing digitalization of education presents unprecedented\nopportunities for data-driven personalization, but it also introduces\nsignificant challenges to student data privacy. Conventional recommender\nsystems rely on centralized data, a paradigm often incompatible with modern\ndata protection regulations. A novel privacy-preserving recommender system is\nproposed and evaluated to address this critical issue using Federated Learning\n(FL). The approach utilizes a Deep Neural Network (DNN) with rich, engineered\nfeatures from the large-scale ASSISTments educational dataset. A rigorous\ncomparative analysis of federated aggregation strategies was conducted,\nidentifying FedProx as a significantly more stable and effective method for\nhandling heterogeneous student data than the standard FedAvg baseline. The\noptimized federated model achieves a high-performance F1-Score of 76.28%,\ncorresponding to 92% of the performance of a powerful, centralized XGBoost\nmodel. These findings validate that a federated approach can provide highly\neffective content recommendations without centralizing sensitive student data.\nConsequently, our work presents a viable and robust solution to the\npersonalization-privacy dilemma in modern educational platforms.\n","authors":["Rodrigo Tertulino","Ricardo Almeida"],"pdf_url":"https://arxiv.org/pdf/2509.10516v2.pdf","comment":"This paper has been prepared to be submitted to the Informatics in\n  Education"},{"id":"http://arxiv.org/abs/2510.24891v1","updated":"2025-10-28T18:54:51Z","published":"2025-10-28T18:54:51Z","title":"Idea2Plan: Exploring AI-Powered Research Planning","summary":"  Large language models (LLMs) have demonstrated significant potential to\naccelerate scientific discovery as valuable tools for analyzing data,\ngenerating hypotheses, and supporting innovative approaches in various\nscientific fields. In this work, we investigate how LLMs can handle the\ntransition from conceptual research ideas to well-structured research plans.\nEffective research planning not only supports scientists in advancing their\nresearch but also represents a crucial capability for the development of\nautonomous research agents. Despite its importance, the field lacks a\nsystematic understanding of LLMs' research planning capability. To rigorously\nmeasure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a\nbenchmark built from 200 ICML 2025 Spotlight and Oral papers released after\nmajor LLM training cutoffs. Each benchmark instance includes a research idea\nand a grading rubric capturing the key components of valid plans. We further\npropose Idea2Plan JudgeEval, a complementary benchmark to assess the\nreliability of LLM-based judges against expert annotations. Experimental\nresults show that GPT-5 and GPT-5-mini achieve the strongest performance on the\nbenchmark, though substantial headroom remains for future improvement. Our\nstudy provides new insights into LLMs' capability for research planning and lay\nthe groundwork for future progress.\n","authors":["Jin Huang","Silviu Cucerzan","Sujay Kumar Jauhar","Ryen W. White"],"pdf_url":"https://arxiv.org/pdf/2510.24891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21112v2","updated":"2025-10-28T18:52:14Z","published":"2025-07-12T23:10:59Z","title":"InsurTech innovation using natural language processing","summary":"  With the rapid rise of InsurTech, traditional insurance companies are\nincreasingly exploring alternative data sources and advanced technologies to\nsustain their competitive edge. This paper provides both a conceptual overview\nand practical case studies of natural language processing (NLP) and its\nemerging applications within insurance operations, focusing on transforming\nraw, unstructured text into structured data suitable for actuarial analysis and\ndecision-making. Leveraging real-world alternative data provided by an\nInsurTech industry partner that enriches traditional insurance data sources, we\napply various NLP techniques to demonstrate feature de-biasing, feature\ncompression, and industry classification in the commercial insurance context.\nThese enriched, text-derived insights not only add to and refine traditional\nrating factors for commercial insurance pricing but also offer novel\nperspectives for assessing underlying risk by introducing novel industry\nclassification techniques. Through these demonstrations, we show that NLP is\nnot merely a supplementary tool but a foundational element of modern,\ndata-driven insurance analytics.\n","authors":["Panyi Dong","Zhiyu Quan"],"pdf_url":"https://arxiv.org/pdf/2507.21112v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24889v1","updated":"2025-10-28T18:48:48Z","published":"2025-10-28T18:48:48Z","title":"Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep\n  Q-learning thresholding","summary":"  Rapid triage of suspected stroke needs accurate, bedside-deployable tools;\nEEG is promising but underused at first contact. We present an adaptive\nmultitask EEG classifier that converts 32-channel signals to power spectral\ndensity features (Welch), uses a recurrent-convolutional network (GRU-TCN) to\npredict stroke type (healthy, ischemic, hemorrhagic), hemispheric\nlateralization, and severity, and applies a deep Q-network (DQN) to tune\ndecision thresholds in real time. Using a patient-wise split of the UCLH Stroke\nEIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the\nprimary outcome was stroke-type performance; secondary outcomes were severity\nand lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for\nstroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%)\nfor lateralization. With DQN threshold adaptation, stroke-type accuracy\nincreased to about 98.0% (F1 97.7%). We also tested robustness on an\nindependent, low-density EEG cohort (ZJU4H) and report paired patient-level\nstatistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies\n(index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis;\npatient-wise evaluation). Adaptive thresholding shifts the operating point to\nclinically preferred sensitivity-specificity trade-offs, while integrated\nscalp-map and spectral visualizations support interpretability.\n","authors":["Shakeel Abdulkareem","Bora Yimenicioglu","Andrea Yang","Khartik Uppalapati","Aneesh Gudipati","Zhaoyang Fan"],"pdf_url":"https://arxiv.org/pdf/2510.24889v1.pdf","comment":"10 pages, 6 figures. Equal contribution: Shakeel Abdulkareem and Bora\n  Yimenicioglu. Compiled with pdfLaTeX (wlscirep class)"},{"id":"http://arxiv.org/abs/2510.24884v1","updated":"2025-10-28T18:35:57Z","published":"2025-10-28T18:35:57Z","title":"Aggregation Hides Out-of-Distribution Generalization Failures from\n  Spurious Correlations","summary":"  Benchmarks for out-of-distribution (OOD) generalization frequently show a\nstrong positive correlation between in-distribution (ID) and OOD accuracy\nacross models, termed \"accuracy-on-the-line.\" This pattern is often taken to\nimply that spurious correlations - correlations that improve ID but reduce OOD\nperformance - are rare in practice. We find that this positive correlation is\noften an artifact of aggregating heterogeneous OOD examples. Using a simple\ngradient-based method, OODSelect, we identify semantically coherent OOD subsets\nwhere accuracy on the line does not hold. Across widely used distribution shift\nbenchmarks, the OODSelect uncovers subsets, sometimes over half of the standard\nOOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings\nindicate that aggregate metrics can obscure important failure modes of OOD\nrobustness. We release code and the identified subsets to facilitate further\nresearch.\n","authors":["Olawale Salaudeen","Haoran Zhang","Kumail Alhamoud","Sara Beery","Marzyeh Ghassemi"],"pdf_url":"https://arxiv.org/pdf/2510.24884v1.pdf","comment":"Accepted as a Spotlight paper at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.17585v2","updated":"2025-10-28T18:06:24Z","published":"2025-06-21T04:48:05Z","title":"Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language\n  Models","summary":"  Trustworthy language models should provide both correct and verifiable\nanswers. However, citations generated directly by standalone LLMs are often\nunreliable. As a result, current systems insert citations by querying an\nexternal retriever at inference time, introducing latency, infrastructure\ndependence, and vulnerability to retrieval noise. We explore whether LLMs can\nbe made to reliably attribute to the documents seen during continual\npretraining without test-time retrieval, by revising the training process. To\nstudy this, we construct CitePretrainBench, a benchmark that mixes real-world\ncorpora (Wikipedia, Common Crawl, arXiv) with novel documents and probes both\nshort-form (single-fact) and long-form (multi-fact) citation tasks. Our\napproach follows a two-stage process: (1) continual pretraining to index\nfactual knowledge by binding it to persistent document identifiers; and (2)\ninstruction tuning to elicit citation behavior. We introduce Active Indexing\nfor the first stage, which creates generalizable, source-anchored bindings by\naugmenting training with synthetic data that (i) restate each fact in diverse,\ncompositional forms and (ii) enforce bidirectional training (source-to-fact and\nfact-to-source). This equips the model to both generate content from a cited\nsource and attribute its own answers, improving robustness to paraphrase and\ncomposition. Experiments with Qwen-2.5-7B&3B show that Active Indexing\nconsistently outperforms a Passive Indexing baseline, which simply appends an\nidentifier to each document, achieving citation precision gains of up to 30.2%\nacross all tasks and models. Our ablation studies reveal that performance\ncontinues to improve as we scale the amount of augmented data, showing a clear\nupward trend even at 16x the original token count. Finally, we show that\ninternal citations complement external ones by making the model more robust to\nretrieval noise.\n","authors":["Yukun Huang","Sanxing Chen","Jian Pei","Manzil Zaheer","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2506.17585v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.23923v2","updated":"2025-10-28T18:04:14Z","published":"2025-09-28T14:58:58Z","title":"Graph Mixing Additive Networks","summary":"  We introduce GMAN, a flexible, interpretable, and expressive framework that\nextends Graph Neural Additive Networks (GNANs) to learn from sets of sparse\ntime-series data. GMAN represents each time-dependent trajectory as a directed\ngraph and applies an enriched, more expressive GNAN to each graph. It allows\nusers to control the interpretability-expressivity trade-off by grouping\nfeatures and graphs to encode priors, and it provides feature, node, and\ngraph-level interpretability. On real-world datasets, including mortality\nprediction from blood tests and fake-news detection, GMAN outperforms strong\nnon-interpretable black-box baselines while delivering actionable,\ndomain-aligned explanations.\n","authors":["Maya Bechler-Speicher","Andrea Zerio","Maor Huri","Marie Vibeke Vestergaard","Ran Gilad-Bachrach","Tine Jess","Samir Bhatt","Aleksejs Sazonovs"],"pdf_url":"https://arxiv.org/pdf/2509.23923v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2505.19193"},{"id":"http://arxiv.org/abs/2510.24830v1","updated":"2025-10-28T16:42:53Z","published":"2025-10-28T16:42:53Z","title":"The Generation Phases of Flow Matching: a Denoising Perspective","summary":"  Flow matching has achieved remarkable success, yet the factors influencing\nthe quality of its generation process remain poorly understood. In this work,\nwe adopt a denoising perspective and design a framework to empirically probe\nthe generation process. Laying down the formal connections between flow\nmatching models and denoisers, we provide a common ground to compare their\nperformances on generation and denoising. This enables the design of principled\nand controlled perturbations to influence sample generation: noise and drift.\nThis leads to new insights on the distinct dynamical phases of the generative\nprocess, enabling us to precisely characterize at which stage of the generative\nprocess denoisers succeed or fail and why this matters.\n","authors":["Anne Gagneux","Ségolène Martin","Rémi Gribonval","Mathurin Massias"],"pdf_url":"https://arxiv.org/pdf/2510.24830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24829v1","updated":"2025-10-28T16:18:14Z","published":"2025-10-28T16:18:14Z","title":"Send Less, Save More: Energy-Efficiency Benchmark of Embedded CNN\n  Inference vs. Data Transmission in IoT","summary":"  The integration of the Internet of Things (IoT) and Artificial Intelligence\noffers significant opportunities to enhance our ability to monitor and address\necological changes. As environmental challenges become increasingly pressing,\nthe need for effective remote monitoring solutions is more critical than ever.\nA major challenge in designing IoT applications for environmental monitoring -\nparticularly those involving image data - is to create energy-efficient IoT\ndevices capable of long-term operation in remote areas with limited power\navailability. Advancements in the field of Tiny Machine Learning allow the use\nof Convolutional Neural Networks (CNNs) on resource-constrained,\nbattery-operated microcontrollers. Since data transfer is energy-intensive,\nperforming inference directly on microcontrollers to reduce the message size\ncan extend the operational lifespan of IoT nodes. This work evaluates the use\nof common Low Power Wide Area Networks and compressed CNNs trained on domain\nspecific datasets on an ESP32-S3. Our experiments demonstrate, among other\nthings, that executing CNN inference on-device and transmitting only the\nresults reduces the overall energy consumption by a factor of up to five\ncompared to sending raw image data. %The compression of the model using Post\nTraining Quantization is accompanied by an acceptable reduction in accuracy of\nonly a few percentage points compared to a non-quantized model. These findings\nadvocate the development of IoT applications with reduced carbon footprint and\ncapable of operating autonomously in environmental monitoring scenarios by\nincorporating Embedded Machine Learning.\n","authors":["Benjamin Karic","Nina Herrmann","Jan Stenkamp","Paula Scharf","Fabian Gieseke","Angela Schwering"],"pdf_url":"https://arxiv.org/pdf/2510.24829v1.pdf","comment":"11 Pages, Paper lists the categories for the ACM Computing\n  Classification System"},{"id":"http://arxiv.org/abs/2510.24826v1","updated":"2025-10-28T15:50:58Z","published":"2025-10-28T15:50:58Z","title":"Augmenting Biological Fitness Prediction Benchmarks with Landscapes\n  Features from GraphFLA","summary":"  Machine learning models increasingly map biological sequence-fitness\nlandscapes to predict mutational effects. Effective evaluation of these models\nrequires benchmarks curated from empirical data. Despite their impressive\nscales, existing benchmarks lack topographical information regarding the\nunderlying fitness landscapes, which hampers interpretation and comparison of\nmodel performance beyond averaged scores. Here, we introduce GraphFLA, a Python\nframework that constructs and analyzes fitness landscapes from mutagensis data\nin diverse modalities (e.g., DNA, RNA, protein, and beyond) with up to millions\nof mutants. GraphFLA calculates 20 biologically relevant features that\ncharacterize 4 fundamental aspects of landscape topography. By applying\nGraphFLA to over 5,300 landscapes from ProteinGym, RNAGym, and CIS-BP, we\ndemonstrate its utility in interpreting and comparing the performance of dozens\nof fitness prediction models, highlighting factors influencing model accuracy\nand respective advantages of different models. In addition, we release 155\ncombinatorially complete empirical fitness landscapes, encompassing over 2.2\nmillion sequences across various modalities. All the codes and datasets are\navailable at https://github.com/COLA-Laboratory/GraphFLA.\n","authors":["Mingyu Huang","Shasha Zhou","Ke Li"],"pdf_url":"https://arxiv.org/pdf/2510.24826v1.pdf","comment":"56 apges, 18 figures, 8 tables, accepted as a conference paper at\n  NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24523v1","updated":"2025-10-28T15:34:23Z","published":"2025-10-28T15:34:23Z","title":"Unsupervised Machine-Learning Pipeline for Data-Driven Defect Detection\n  and Characterisation: Application to Displacement Cascades","summary":"  Neutron irradiation produces, within a few picoseconds, displacement cascades\nthat are sequences of atomic collisions generating point and extended defects\nwhich subsequently affects the long-term evolution of materials. The diversity\nof these defects, characterized morphologically and statistically, defines what\nis called the \"primary damage\". In this work, we present a fully unsupervised\nmachine learning (ML) workflow that detects and classifies these defects\ndirectly from molecular dynamics data. Local environments are encoded by the\nSmooth Overlap of Atomic Positions (SOAP) vector, anomalous atoms are isolated\nwith autoencoder neural networks (AE), embedded with Uniform Manifold\nApproximation and Projection (UMAP) and clustered using Hierarchical\nDensity-Based Spatial Clustering of Applications with Noise (HDBSCAN). Applied\nto 80 keV displacement cascades in Ni, Fe$_7$0Ni$_{10}$Cr$_{20}$, and Zr, the\nAE successfully identify the small fraction of outlier atoms that participate\nin defect formation. HDBSCAN then partitions the UMAP latent space of\nAE-flagged SOAP descriptors into well defined groups representing vacancy- and\ninterstitial-dominated regions and, within each, separates small from large\naggregates, assigning 99.7 % of outliers to compact physical motifs. A signed\ncluster-identification score confirms this separation, and cluster size scales\nwith net defect counts (R2 > 0.89). Statistical cross analyses between the ML\noutlier map and several conventional detectors (centrosymmetry, dislocation\nextraction, etc.) reveal strong overlap and complementary coverage, all\nachieved without template or threshold tuning. This ML workflow thus provides\nan efficient tool for the quantitative mapping of structural anomalies in\nmaterials, particularly those arising from irradiation damage in displacement\ncascades.\n","authors":["Samuel Del Fré","Andrée de Backer","Christophe Domain","Ludovic Thuinet","Charlotte S. Becquart"],"pdf_url":"https://arxiv.org/pdf/2510.24523v1.pdf","comment":"22 pages, 1 graphical abstract, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2504.05349v3","updated":"2025-10-28T14:10:13Z","published":"2025-04-06T16:09:18Z","title":"The Neural Pruning Law Hypothesis","summary":"  Network pruning is used to reduce inference latency and power consumption in\nlarge neural networks. However, most current pruning methods rely on ad-hoc\nheuristics that are poorly understood. We introduce Hyperflux, a\nconceptually-grounded pruning method, and use it to study the pruning process.\nHyperflux models this process as an interaction between weight flux, the\ngradient's response to the weight's removal, and network pressure, a global\nregularization driving weights towards pruning. We postulate properties that\narise naturally from our framework and find that the relationship between\nminimum flux among weights and density follows a power-law equation.\nFurthermore, we hypothesize the power-law relationship to hold for any\neffective saliency metric and call this idea the Neural Pruning Law Hypothesis.\nWe validate our hypothesis on several families of pruning methods (magnitude,\ngradients, $L_0$), providing a potentially unifying property for neural\npruning.\n","authors":["Eugen Barbulescu","Antonio Alexoaie","Lucian Busoniu"],"pdf_url":"https://arxiv.org/pdf/2504.05349v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07297v2","updated":"2025-10-28T13:27:06Z","published":"2025-04-09T21:40:15Z","title":"Data Fusion of Deep Learned Molecular Embeddings for Property Prediction","summary":"  Data-driven approaches such as deep learning can result in predictive models\nfor material properties with exceptional accuracy and efficiency. However, in\nmany applications, data is sparse, severely limiting their accuracy and\napplicability. To improve predictions, techniques such as transfer learning and\nmultitask learning have been used. The performance of multitask learning models\ndepends on the strength of the underlying correlations between tasks and the\ncompleteness of the data set. Standard multitask models tend to underperform\nwhen trained on sparse data sets with weakly correlated properties. To address\nthis gap, we fuse deep-learned embeddings generated by independent pretrained\nsingle-task models, resulting in a multitask model that inherits rich,\nproperty-specific representations. By reusing (rather than retraining) these\nembeddings, the resulting fused model outperforms standard multitask models and\ncan be extended with fewer trainable parameters. We demonstrate this technique\non a widely used benchmark data set of quantum chemistry data for small\nmolecules as well as a newly compiled sparse data set of experimental data\ncollected from literature and our own quantum chemistry and thermochemical\ncalculations.\n","authors":["Robert J Appleton","Brian C Barnes","Alejandro Strachan"],"pdf_url":"https://arxiv.org/pdf/2504.07297v2.pdf","comment":"J. Chem. Inf. Model. 2025"},{"id":"http://arxiv.org/abs/2510.24287v1","updated":"2025-10-28T10:49:42Z","published":"2025-10-28T10:49:42Z","title":"Towards actionable hypotension prediction -- predicting catecholamine\n  therapy initiation in the intensive care unit","summary":"  Hypotension in critically ill ICU patients is common and life-threatening.\nEscalation to catecholamine therapy marks a key management step, with both\nundertreatment and overtreatment posing risks. Most machine learning (ML)\nmodels predict hypotension using fixed MAP thresholds or MAP forecasting,\noverlooking the clinical decision behind treatment escalation. Predicting\ncatecholamine initiation, the start of vasoactive or inotropic agent\nadministration offers a more clinically actionable target reflecting real\ndecision-making. Using the MIMIC-III database, we modeled catecholamine\ninitiation as a binary event within a 15-minute prediction window. Input\nfeatures included statistical descriptors from a two-hour sliding MAP context\nwindow, along with demographics, biometrics, comorbidities, and ongoing\ntreatments. An Extreme Gradient Boosting (XGBoost) model was trained and\ninterpreted via SHapley Additive exPlanations (SHAP). The model achieved an\nAUROC of 0.822 (0.813-0.830), outperforming the hypotension baseline (MAP < 65,\nAUROC 0.686 [0.675-0.699]). SHAP analysis highlighted recent MAP values, MAP\ntrends, and ongoing treatments (e.g., sedatives, electrolytes) as dominant\npredictors. Subgroup analysis showed higher performance in males, younger\npatients (<53 years), those with higher BMI (>32), and patients without\ncomorbidities or concurrent medications. Predicting catecholamine initiation\nbased on MAP dynamics, treatment context, and patient characteristics supports\nthe critical decision of when to escalate therapy, shifting focus from\nthreshold-based alarms to actionable decision support. This approach is\nfeasible across a broad ICU cohort under natural event imbalance. Future work\nshould enrich temporal and physiological context, extend label definitions to\ninclude therapy escalation, and benchmark against existing hypotension\nprediction systems.\n","authors":["Richard Koebe","Noah Saibel","Juan Miguel Lopez Alcaraz","Simon Schäfer","Nils Strodthoff"],"pdf_url":"https://arxiv.org/pdf/2510.24287v1.pdf","comment":"27 pages, 8 figures, source code under\n  https://github.com/AI4HealthUOL/actionable-hypotension"},{"id":"http://arxiv.org/abs/2510.24817v1","updated":"2025-10-28T10:06:49Z","published":"2025-10-28T10:06:49Z","title":"Towards a Method for Synthetic Generation of PWA Transcripts","summary":"  In aphasia research, Speech-Language Pathologists (SLPs) devote extensive\ntime to manually coding speech samples using Correct Information Units (CIUs),\na measure of how informative an individual sample of speech is. Developing\nautomated systems to recognize aphasic language is limited by data scarcity.\nFor example, only about 600 transcripts are available in AphasiaBank yet\nbillions of tokens are used to train large language models (LLMs). In the\nbroader field of machine learning (ML), researchers increasingly turn to\nsynthetic data when such are sparse. Therefore, this study constructs and\nvalidates two methods to generate synthetic transcripts of the AphasiaBank Cat\nRescue picture description task. One method leverages a procedural programming\napproach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct\nLLMs. The methods generate transcripts across four severity levels (Mild,\nModerate, Severe, Very Severe) through word dropping, filler insertion, and\nparaphasia substitution. Overall, we found, compared to human-elicited\ntranscripts, Mistral 7b Instruct best captures key aspects of linguistic\ndegradation observed in aphasia, showing realistic directional changes in NDW,\nword count, and word length amongst the synthetic generation methods. Based on\nthe results, future work should plan to create a larger dataset, fine-tune\nmodels for better aphasic representation, and have SLPs assess the realism and\nusefulness of the synthetic transcripts.\n","authors":["Jason M. Pittman","Anton Phillips Jr.","Yesenia Medina-Santos","Brielle C. Stark"],"pdf_url":"https://arxiv.org/pdf/2510.24817v1.pdf","comment":"19 pages, 1 figure, 7 tables"},{"id":"http://arxiv.org/abs/2510.24815v1","updated":"2025-10-28T09:49:01Z","published":"2025-10-28T09:49:01Z","title":"Tree Ensemble Explainability through the Hoeffding Functional\n  Decomposition and TreeHFD Algorithm","summary":"  Tree ensembles have demonstrated state-of-the-art predictive performance\nacross a wide range of problems involving tabular data. Nevertheless, the\nblack-box nature of tree ensembles is a strong limitation, especially for\napplications with critical decisions at stake. The Hoeffding or ANOVA\nfunctional decomposition is a powerful explainability method, as it breaks down\nblack-box models into a unique sum of lower-dimensional functions, provided\nthat input variables are independent. In standard learning settings, input\nvariables are often dependent, and the Hoeffding decomposition is generalized\nthrough hierarchical orthogonality constraints. Such generalization leads to\nunique and sparse decompositions with well-defined main effects and\ninteractions. However, the practical estimation of this decomposition from a\ndata sample is still an open problem. Therefore, we introduce the TreeHFD\nalgorithm to estimate the Hoeffding decomposition of a tree ensemble from a\ndata sample. We show the convergence of TreeHFD, along with the main properties\nof orthogonality, sparsity, and causal variable selection. The high performance\nof TreeHFD is demonstrated through experiments on both simulated and real data,\nusing our treehfd Python package (https://github.com/ThalesGroup/treehfd).\nBesides, we empirically show that the widely used TreeSHAP method, based on\nShapley values, is strongly connected to the Hoeffding decomposition.\n","authors":["Clément Bénard"],"pdf_url":"https://arxiv.org/pdf/2510.24815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24234v1","updated":"2025-10-28T09:42:15Z","published":"2025-10-28T09:42:15Z","title":"Sparse Optimistic Information Directed Sampling","summary":"  Many high-dimensional online decision-making problems can be modeled as\nstochastic sparse linear bandits. Most existing algorithms are designed to\nachieve optimal worst-case regret in either the data-rich regime, where\npolynomial dependence on the ambient dimension is unavoidable, or the data-poor\nregime, where dimension-independence is possible at the cost of worse\ndependence on the number of rounds. In contrast, the sparse Information\nDirected Sampling (IDS) algorithm satisfies a Bayesian regret bound that has\nthe optimal rate in both regimes simultaneously. In this work, we explore the\nuse of Sparse Optimistic Information Directed Sampling (SOIDS) to achieve the\nsame adaptivity in the worst-case setting, without Bayesian assumptions.\nThrough a novel analysis that enables the use of a time-dependent learning\nrate, we show that SOIDS can optimally balance information and regret. Our\nresults extend the theoretical guarantees of IDS, providing the first algorithm\nthat simultaneously achieves optimal worst-case regret in both the data-rich\nand data-poor regimes. We empirically demonstrate the good performance of\nSOIDS.\n","authors":["Ludovic Schwartz","Hamish Flynn","Gergely Neu"],"pdf_url":"https://arxiv.org/pdf/2510.24234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24215v1","updated":"2025-10-28T09:29:46Z","published":"2025-10-28T09:29:46Z","title":"What Can Be Recovered Under Sparse Adversarial Corruption?\n  Assumption-Free Theory for Linear Measurements","summary":"  Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ be an arbitrary, known matrix\nand $\\mathbf{e}$ a $q$-sparse adversarial vector. Given $\\mathbf{y} =\n\\mathbf{A} x^* + \\mathbf{e}$ and $q$, we seek the smallest set containing\n$x^*$-hence the one conveying maximal information about $x^*$-that is uniformly\nrecoverable from $\\mathbf{y}$ without knowing $\\mathbf{e}$. While exact\nrecovery of $x^*$ via strong (and often impractical) structural assumptions on\n$\\mathbf{A}$ or $x^*$ (for example, restricted isometry, sparsity) is well\nstudied, recoverability for arbitrary $\\mathbf{A}$ and $x^*$ remains open. Our\nmain result shows that the best that one can hope to recover is $x^* +\n\\ker(\\mathbf{U})$, where $\\mathbf{U}$ is the unique projection matrix onto the\nintersection of rowspaces of all possible submatrices of $\\mathbf{A}$ obtained\nby deleting $2q$ rows. Moreover, we prove that every $x$ that minimizes the\n$\\ell_0$-norm of $\\mathbf{y} - \\mathbf{A} x$ lies in $x^* + \\ker(\\mathbf{U})$,\nwhich then gives a constructive approach to recover this set.\n","authors":["Vishal Halder","Alexandre Reiffers-Masson","Abdeldjalil Aïssa-El-Bey","Gugan Thoppe"],"pdf_url":"https://arxiv.org/pdf/2510.24215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17734v2","updated":"2025-10-28T09:06:30Z","published":"2025-05-23T10:54:53Z","title":"URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous\n  Vehicles","summary":"  Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future\nurban networks, potentially by optimizing their routing decisions. Unlike for\nhuman drivers, these decisions can be made with collective, data-driven\npolicies, developed using machine learning algorithms. Reinforcement learning\n(RL) can facilitate the development of such collective routing strategies, yet\nstandardized and realistic benchmarks are missing. To that end, we present URB:\nUrban Routing Benchmark for RL-equipped Connected Autonomous Vehicles. URB is a\ncomprehensive benchmarking environment that unifies evaluation across 29\nreal-world traffic networks paired with realistic demand patterns. URB comes\nwith a catalog of predefined tasks, multi-agent RL (MARL) algorithm\nimplementations, three baseline methods, domain-specific performance metrics,\nand a modular configuration scheme. Our results show that, despite the lengthy\nand costly training, state-of-the-art MARL algorithms rarely outperformed\nhumans. The experimental results reported in this paper initiate the first\nleaderboard for MARL in large-scale urban routing optimization. They reveal\nthat current approaches struggle to scale, emphasizing the urgent need for\nadvancements in this domain.\n","authors":["Ahmet Onur Akman","Anastasia Psarou","Michał Hoffmann","Łukasz Gorczyca","Łukasz Kowalski","Paweł Gora","Grzegorz Jamróz","Rafał Kucharski"],"pdf_url":"https://arxiv.org/pdf/2505.17734v2.pdf","comment":"Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025), Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2510.24812v1","updated":"2025-10-28T07:53:24Z","published":"2025-10-28T07:53:24Z","title":"From Linear to Nonlinear: Provable Weak-to-Strong Generalization through\n  Feature Learning","summary":"  Weak-to-strong generalization refers to the phenomenon where a stronger model\ntrained under supervision from a weaker one can outperform its teacher. While\nprior studies aim to explain this effect, most theoretical insights are limited\nto abstract frameworks or linear/random feature models. In this paper, we\nprovide a formal analysis of weak-to-strong generalization from a linear CNN\n(weak) to a two-layer ReLU CNN (strong). We consider structured data composed\nof label-dependent signals of varying difficulty and label-independent noise,\nand analyze gradient descent dynamics when the strong model is trained on data\nlabeled by the pretrained weak model. Our analysis identifies two regimes --\ndata-scarce and data-abundant -- based on the signal-to-noise characteristics\nof the dataset, and reveals distinct mechanisms of weak-to-strong\ngeneralization. In the data-scarce regime, generalization occurs via benign\noverfitting or fails via harmful overfitting, depending on the amount of data,\nand we characterize the transition boundary. In the data-abundant regime,\ngeneralization emerges in the early phase through label correction, but we\nobserve that overtraining can subsequently degrade performance.\n","authors":["Junsoo Oh","Jerry Song","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2510.24812v1.pdf","comment":"NeurIPS 2025 camera-ready version, 70 pages"},{"id":"http://arxiv.org/abs/2510.24811v1","updated":"2025-10-28T06:34:15Z","published":"2025-10-28T06:34:15Z","title":"ProofSketch: Efficient Verified Reasoning for Large Language Models","summary":"  Reasoning methods such as chain-of-thought prompting and self-consistency\nhave shown immense potential to improve the accuracy of large language models\nacross various reasoning tasks. However such methods involve generation of\nlengthy reasoning chains, which substantially increases token consumption,\ncomputational cost, and latency. To address this inefficiency, we propose\nProofSketch, a verification-guided reasoning framework that integrates symbolic\nclosure computation, lexicographic verification and adaptive sketch generation.\nOur experiments show that ProofSketch consistently reduces token usage while\nimproving accuracy, demonstrating that this approach offers a promising path\nfor efficient and trustworthy reasoning.\n","authors":["Disha Sheshanarayana","Tanishka Magar"],"pdf_url":"https://arxiv.org/pdf/2510.24811v1.pdf","comment":"Accepted at NeurIPS 2025, ER Workshop"},{"id":"http://arxiv.org/abs/2502.04242v4","updated":"2025-10-28T06:15:39Z","published":"2025-02-06T17:32:49Z","title":"A High-Dimensional Statistical Method for Optimizing Transfer Quantities\n  in Multi-Source Transfer Learning","summary":"  Multi-source transfer learning provides an effective solution to data\nscarcity in real-world supervised learning scenarios by leveraging multiple\nsource tasks. In this field, existing works typically use all available samples\nfrom sources in training, which constrains their training efficiency and may\nlead to suboptimal results. To address this, we propose a theoretical framework\nthat answers the question: what is the optimal quantity of source samples\nneeded from each source task to jointly train the target model? Specifically,\nwe introduce a generalization error measure based on K-L divergence, and\nminimize it based on high-dimensional statistical analysis to determine the\noptimal transfer quantity for each source task. Additionally, we develop an\narchitecture-agnostic and data-efficient algorithm OTQMS to implement our\ntheoretical results for target model training in multi-source transfer\nlearning. Experimental studies on diverse architectures and two real-world\nbenchmark datasets show that our proposed algorithm significantly outperforms\nstate-of-the-art approaches in both accuracy and data efficiency. The code and\nsupplementary materials are available in https://github.com/zqy0126/OTQMS.\n","authors":["Qingyue Zhang","Haohao Fu","Guanbo Huang","Yaoyuan Liang","Chang Chu","Tianren Peng","Yanru Wu","Qi Li","Yang Li","Shao-Lun Huang"],"pdf_url":"https://arxiv.org/pdf/2502.04242v4.pdf","comment":"NeurIPS 2025 Poster"},{"id":"http://arxiv.org/abs/2510.24807v1","updated":"2025-10-28T04:32:42Z","published":"2025-10-28T04:32:42Z","title":"Learning to Attack: Uncovering Privacy Risks in Sequential Data Releases","summary":"  Privacy concerns have become increasingly critical in modern AI and data\nscience applications, where sensitive information is collected, analyzed, and\nshared across diverse domains such as healthcare, finance, and mobility. While\nprior research has focused on protecting privacy in a single data release, many\nreal-world systems operate under sequential or continuous data publishing,\nwhere the same or related data are released over time. Such sequential\ndisclosures introduce new vulnerabilities, as temporal correlations across\nreleases may enable adversaries to infer sensitive information that remains\nhidden in any individual release. In this paper, we investigate whether an\nattacker can compromise privacy in sequential data releases by exploiting\ndependencies between consecutive publications, even when each individual\nrelease satisfies standard privacy guarantees. To this end, we propose a novel\nattack model that captures these sequential dependencies by integrating a\nHidden Markov Model with a reinforcement learning-based bi-directional\ninference mechanism. This enables the attacker to leverage both earlier and\nlater observations in the sequence to infer private information. We instantiate\nour framework in the context of trajectory data, demonstrating how an adversary\ncan recover sensitive locations from sequential mobility datasets. Extensive\nexperiments on Geolife, Porto Taxi, and SynMob datasets show that our model\nconsistently outperforms baseline approaches that treat each release\nindependently. The results reveal a fundamental privacy risk inherent to\nsequential data publishing, where individually protected releases can\ncollectively leak sensitive information when analyzed temporally. These\nfindings underscore the need for new privacy-preserving frameworks that\nexplicitly model temporal dependencies, such as time-aware differential privacy\nor sequential data obfuscation strategies.\n","authors":["Ziyao Cui","Minxing Zhang","Jian Pei"],"pdf_url":"https://arxiv.org/pdf/2510.24807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20958v2","updated":"2025-10-28T00:13:13Z","published":"2025-10-23T19:36:59Z","title":"NeuroPilot: A Realtime Brain-Computer Interface system to enhance\n  concentration of students in online learning","summary":"  The prevalence of online learning poses a vital challenge in real-time\nmonitoring of students' concentration. Traditional methods such as\nquestionnaire assessments require manual intervention, and webcam-based\nmonitoring fails to provide accurate insights about learners' mental focus as\nit is deceived by mere screen fixation without cognitive engagement. Existing\nBCI-based approaches lack real-time validation and evaluation procedures. To\naddress these limitations, a Brain-Computer Interface (BCI) system is developed\nusing a non-invasive Electroencephalogram (EEG) headband, FocusCalm, to record\nbrainwave activity under attentive and non-attentive states. 20 minutes of data\nwere collected from each of 20 participants watching a pre-recorded educational\nvideo. The data validation employed a novel intra-video questionnaire\nassessment. Subsequently, collected signals were segmented (sliding window),\nfiltered (Butterworth bandpass), and cleaned (removal of high-amplitude and EOG\nartifacts such as eye blinks). Time, frequency, wavelet, and statistical\nfeatures were extracted, followed by recursive feature elimination (RFE) with\nsupport vector machines (SVMs) to classify attention and non-attention states.\nThe leave-one-subject-out (LOSO) cross-validation accuracy was found to be\n88.77%. The system provides feedback alerts upon detection of a non-attention\nstate and maintains focus profile logs. A pilot study was conducted to evaluate\nthe effectiveness of real-time feedback. Five participants underwent a\n10-minute session comprising a 5-minute baseline phase devoid of feedback,\nsucceeded by a 5-minute feedback phase, during which alerts were activated if\nparticipants exhibited inattention for approximately 8 consecutive seconds. A\npaired t-test (t = 5.73, p = 0.007) indicated a statistically significant\nimprovement in concentration during the feedback phase.\n","authors":["Asif Islam","Farhan Ishtiaque","Md. Muhyminul Haque","Farhana Sarker","Ravi Vaidyanathan","Khondaker A. Mamun"],"pdf_url":"https://arxiv.org/pdf/2510.20958v2.pdf","comment":"V2: Author list updated with consent from all authors. Minor\n  grammatical errors fixed. This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2510.25796v1","updated":"2025-10-28T23:21:27Z","published":"2025-10-28T23:21:27Z","title":"Non-myopic Matching and Rebalancing in Large-Scale On-Demand\n  Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning","summary":"  Ride-pooling, also known as ride-sharing, shared ride-hailing, or\nmicrotransit, is a service wherein passengers share rides. This service can\nreduce costs for both passengers and operators and reduce congestion and\nenvironmental impacts. A key limitation, however, is its myopic\ndecision-making, which overlooks long-term effects of dispatch decisions. To\naddress this, we propose a simulation-informed reinforcement learning (RL)\napproach. While RL has been widely studied in the context of ride-hailing\nsystems, its application in ride-pooling systems has been less explored. In\nthis study, we extend the learning and planning framework of Xu et al. (2018)\nfrom ride-hailing to ride-pooling by embedding a ride-pooling simulation within\nthe learning mechanism to enable non-myopic decision-making. In addition, we\npropose a complementary policy for rebalancing idle vehicles. By employing\nn-step temporal difference learning on simulated experiences, we derive\nspatiotemporal state values and subsequently evaluate the effectiveness of the\nnon-myopic policy using NYC taxi request data. Results demonstrate that the\nnon-myopic policy for matching can increase the service rate by up to 8.4%\nversus a myopic policy while reducing both in-vehicle and wait times for\npassengers. Furthermore, the proposed non-myopic policy can decrease fleet size\nby over 25% compared to a myopic policy, while maintaining the same level of\nperformance, thereby offering significant cost savings for operators.\nIncorporating rebalancing operations into the proposed framework cuts wait time\nby up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%\ncompared to using the framework for matching decisions alone at the cost of\nincreased vehicle minutes traveled per passenger.\n","authors":["Farnoosh Namdarpour","Joseph Y. J. Chow"],"pdf_url":"https://arxiv.org/pdf/2510.25796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25793v1","updated":"2025-10-28T21:52:33Z","published":"2025-10-28T21:52:33Z","title":"Optimal Information Combining for Multi-Agent Systems Using Adaptive\n  Bias Learning","summary":"  Modern multi-agent systems ranging from sensor networks monitoring critical\ninfrastructure to crowdsourcing platforms aggregating human intelligence can\nsuffer significant performance degradation due to systematic biases that vary\nwith environmental conditions. Current approaches either ignore these biases,\nleading to suboptimal decisions, or require expensive calibration procedures\nthat are often infeasible in practice. This performance gap has real\nconsequences: inaccurate environmental monitoring, unreliable financial\npredictions, and flawed aggregation of human judgments. This paper addresses\nthe fundamental question: when can we learn and correct for these unknown\nbiases to recover near-optimal performance, and when is such learning futile?\nWe develop a theoretical framework that decomposes biases into learnable\nsystematic components and irreducible stochastic components, introducing the\nconcept of learnability ratio as the fraction of bias variance predictable from\nobservable covariates. This ratio determines whether bias learning is\nworthwhile for a given system. We prove that the achievable performance\nimprovement is fundamentally bounded by this learnability ratio, providing\nsystem designers with quantitative guidance on when to invest in bias learning\nversus simpler approaches. We present the Adaptive Bias Learning and Optimal\nCombining (ABLOC) algorithm, which iteratively learns bias-correcting\ntransformations while optimizing combination weights through closedform\nsolutions, guaranteeing convergence to these theoretical bounds. Experimental\nvalidation demonstrates that systems with high learnability ratios can recover\nsignificant performance (we achieved 40%-70% of theoretical maximum improvement\nin our examples), while those with low learnability show minimal benefit,\nvalidating our diagnostic criteria for practical deployment decisions.\n","authors":["Siavash M. Alamouti","Fay Arjomandi"],"pdf_url":"https://arxiv.org/pdf/2510.25793v1.pdf","comment":"22 pages, 2 Figures, 62 equations, 47 references"},{"id":"http://arxiv.org/abs/2510.25791v1","updated":"2025-10-28T20:14:26Z","published":"2025-10-28T20:14:26Z","title":"The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in\n  Transformers?","summary":"  Chain-of-thought (CoT) supervision can substantially improve transformer\nperformance, yet the mechanisms by which models learn to follow and benefit\nfrom CoT remain poorly understood. We investigate these learning dynamics\nthrough the lens of grokking by pretraining transformers on symbolic reasoning\ntasks with tunable algorithmic complexity and controllable data composition to\nstudy their generalization. Models were trained under two settings: (i)\nproducing only final answers, and (ii) emitting explicit CoT traces before\nanswering. Our results show that while CoT generally improves task performance,\nits benefits depend on task complexity. To quantify these effects, we model the\naccuracy of the logarithmic training steps with a three-parameter logistic\ncurve, revealing how the learning speed and shape vary with task complexity,\ndata distribution, and the presence of CoT supervision. We also uncover a\ntransient trace unfaithfulness phase: early in training, models often produce\ncorrect answers while skipping or contradicting CoT steps, before later\naligning their reasoning traces with answers. Empirically, we (1) demonstrate\nthat CoT accelerates generalization but does not overcome tasks with higher\nalgorithmic complexity, such as finding list intersections; (2) introduce a\nkinetic modeling framework for understanding transformer learning; (3)\ncharacterize trace faithfulness as a dynamic property that emerges over\ntraining; and (4) show CoT alters internal transformer computation\nmechanistically.\n","authors":["Zihan Pengmei","Costas Mavromatis","Zhengyuan Shen","Yunyi Zhang","Vassilis N. Ioannidis","Huzefa Rangwala"],"pdf_url":"https://arxiv.org/pdf/2510.25791v1.pdf","comment":"10 pages, 7 figures, with appendix"}]},"2025-10-26T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2510.22868v1","updated":"2025-10-26T23:19:28Z","published":"2025-10-26T23:19:28Z","title":"Seeing the Unseen: Towards Zero-Shot Inspection for Wind Turbine Blades\n  using Knowledge-Augmented Vision Language Models","summary":"  Wind turbine blades operate in harsh environments, making timely damage\ndetection essential for preventing failures and optimizing maintenance.\nDrone-based inspection and deep learning are promising, but typically depend on\nlarge, labeled datasets, which limit their ability to detect rare or evolving\ndamage types. To address this, we propose a zero-shot-oriented inspection\nframework that integrates Retrieval-Augmented Generation (RAG) with\nVision-Language Models (VLM). A multimodal knowledge base is constructed,\ncomprising technical documentation, representative reference images, and\ndomain-specific guidelines. A hybrid text-image retriever with keyword-aware\nreranking assembles the most relevant context to condition the VLM at\ninference, injecting domain knowledge without task-specific training. We\nevaluate the framework on 30 labeled blade images covering diverse damage\ncategories. Although the dataset is small due to the difficulty of acquiring\nverified blade imagery, it covers multiple representative defect types. On this\ntest set, the RAG-grounded VLM correctly classified all samples, whereas the\nsame VLM without retrieval performed worse in both accuracy and precision. We\nfurther compare against open-vocabulary baselines and incorporate uncertainty\nClopper-Pearson confidence intervals to account for the small-sample setting.\nAblation studies indicate that the key advantage of the framework lies in\nexplainability and generalizability: retrieved references ground the reasoning\nprocess and enable the detection of previously unseen defects by leveraging\ndomain knowledge rather than relying solely on visual cues. This research\ncontributes a data-efficient solution for industrial inspection that reduces\ndependence on extensive labeled datasets.\n","authors":["Yang Zhang","Qianyu Zhou","Farhad Imani","Jiong Tang"],"pdf_url":"https://arxiv.org/pdf/2510.22868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16421v3","updated":"2025-10-26T22:58:59Z","published":"2025-03-20T17:59:42Z","title":"MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance","summary":"  Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.\n","authors":["Quanhao Li","Zhen Xing","Rui Wang","Hui Zhang","Qi Dai","Zuxuan Wu"],"pdf_url":"https://arxiv.org/pdf/2503.16421v3.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2507.15857v7","updated":"2025-10-26T22:38:20Z","published":"2025-07-21T17:59:57Z","title":"Diffusion Beats Autoregressive in Data-Constrained Settings","summary":"  Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings where\ntraining involves repeated passes over limited data and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We find new scaling laws for\ndiffusion models and derive a closed-form expression for the critical compute\nthreshold at which diffusion begins to outperform AR. Finally, we explain why\ndiffusion models excel in this regime: their randomized masking objective\nimplicitly trains over a rich distribution of token orderings, acting as an\nimplicit data augmentation that AR's fixed left-to-right factorization lacks.\nOur results suggest that when data, not compute, is the bottleneck, diffusion\nmodels offer a compelling alternative to the standard AR paradigm. Our code is\navailable at: https://diffusion-scaling.github.io.\n","authors":["Mihir Prabhudesai","Mengning Wu","Amir Zadeh","Katerina Fragkiadaki","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2507.15857v7.pdf","comment":"Project Webpage: https://diffusion-scaling.github.io"},{"id":"http://arxiv.org/abs/2506.21656v2","updated":"2025-10-26T22:18:43Z","published":"2025-06-26T18:00:00Z","title":"Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs","summary":"  Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.\n","authors":["Yifan Shen","Yuanzhe Liu","Jingyuan Zhu","Xu Cao","Xiaofeng Zhang","Yixiao He","Wenming Ye","James Matthew Rehg","Ismini Lourentzou"],"pdf_url":"https://arxiv.org/pdf/2506.21656v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22851v1","updated":"2025-10-26T22:04:17Z","published":"2025-10-26T22:04:17Z","title":"Semantic Surgery: Zero-Shot Concept Erasure in Diffusion Models","summary":"  Concept erasure in text-to-image diffusion models is crucial for mitigating\nharmful content, yet existing methods often compromise generative quality. We\nintroduce Semantic Surgery, a novel training-free, zero-shot framework for\nconcept erasure that operates directly on text embeddings before the diffusion\nprocess. It dynamically estimates the presence of target concepts in a prompt\nand performs a calibrated vector subtraction to neutralize their influence at\nthe source, enhancing both erasure completeness and locality. The framework\nincludes a Co-Occurrence Encoding module for robust multi-concept erasure and a\nvisual feedback loop to address latent concept persistence. As a training-free\nmethod, Semantic Surgery adapts dynamically to each prompt, ensuring precise\ninterventions. Extensive experiments on object, explicit content, artistic\nstyle, and multi-celebrity erasure tasks show our method significantly\noutperforms state-of-the-art approaches. We achieve superior completeness and\nrobustness while preserving locality and image quality (e.g., 93.58 H-score in\nobject erasure, reducing explicit content to just 1 instance, and 8.09 H_a in\nstyle erasure with no quality degradation). This robustness also allows our\nframework to function as a built-in threat detection system, offering a\npractical solution for safer text-to-image generation.\n","authors":["Lexiang Xiong","Chengyu Liu","Jingwen Ye","Yan Liu","Yuecong Xu"],"pdf_url":"https://arxiv.org/pdf/2510.22851v1.pdf","comment":"Accepted to the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025). Code is available at\n  https://github.com/Lexiang-Xiong/Semantic-Surgery"},{"id":"http://arxiv.org/abs/2505.20759v3","updated":"2025-10-26T21:52:55Z","published":"2025-05-27T06:03:56Z","title":"PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding","summary":"  Real-world objects are composed of distinctive, object-specific parts.\nIdentifying these parts is key to performing fine-grained, compositional\nreasoning-yet, large multimodal models (LMMs) struggle to perform this\nseemingly straightforward task. In this work, we introduce PARTONOMY, an LMM\nbenchmark designed for pixel-level part grounding. We construct PARTONOMY from\nexisting part datasets and our own rigorously annotated set of images,\nencompassing 862 part labels and 534 object labels for evaluation. Unlike\nexisting datasets that simply ask models to identify generic parts, PARTONOMY\nuses specialized concepts (e.g., agricultural airplane), and challenges models\nto compare objects' parts, consider part-whole relationships, and justify\ntextual predictions with visual segmentations. Our experiments demonstrate\nsignificant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only\n5.9% gIoU), highlighting a critical gap in their part grounding abilities. We\nnote that existing segmentation-enabled LMMs (segmenting LMMs) have two key\narchitectural shortcomings: they use special [SEG] tokens not seen during\npretraining which induce distribution shift, and they discard predicted\nsegmentations instead of using past predictions to guide future ones. To\naddress these deficiencies, we train several part-centric LMMs and propose\nPLUM, a novel segmenting LMM that uses span tagging instead of segmentation\ntokens and that conditions on prior predictions in a feedback loop. We find\nthat pretrained PLUM outperforms existing segmenting LMMs on reasoning\nsegmentation, VQA, and visual hallucination benchmarks. In addition, PLUM\nfinetuned on our proposed Explanatory Part Segmentation task is competitive\nwith segmenting LMMs trained on significantly more segmentation data. Our work\nopens up new avenues towards enabling fine-grained, grounded visual\nunderstanding in LMMs.\n","authors":["Ansel Blume","Jeonghwan Kim","Hyeonjeong Ha","Elen Chatikyan","Xiaomeng Jin","Khanh Duy Nguyen","Nanyun Peng","Kai-Wei Chang","Derek Hoiem","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2505.20759v3.pdf","comment":"NeurIPS 2025 Spotlight; project page:\n  https://wjdghks950.github.io/partonomy.github.io/"},{"id":"http://arxiv.org/abs/2510.22842v1","updated":"2025-10-26T21:21:27Z","published":"2025-10-26T21:21:27Z","title":"FastJAM: a Fast Joint Alignment Model for Images","summary":"  Joint Alignment (JA) of images aims to align a collection of images into a\nunified coordinate frame, such that semantically-similar features appear at\ncorresponding spatial locations. Most existing approaches often require long\ntraining times, large-capacity models, and extensive hyperparameter tuning. We\nintroduce FastJAM, a rapid, graph-based method that drastically reduces the\ncomputational complexity of joint alignment tasks. FastJAM leverages pairwise\nmatches computed by an off-the-shelf image matcher, together with a rapid\nnonparametric clustering, to construct a graph representing intra- and\ninter-image keypoint relations. A graph neural network propagates and\naggregates these correspondences, efficiently predicting per-image homography\nparameters via image-level pooling. Utilizing an inverse-compositional loss,\nthat eliminates the need for a regularization term over the predicted\ntransformations (and thus also obviates the hyperparameter tuning associated\nwith such terms), FastJAM performs image JA quickly and effectively.\nExperimental results on several benchmarks demonstrate that FastJAM achieves\nresults better than existing modern JA methods in terms of alignment quality,\nwhile reducing computation time from hours or minutes to mere seconds. Our code\nis available at our project webpage, https://bgu-cs-vil.github.io/FastJAM/\n","authors":["Omri Hirsch","Ron Shapira Weber","Shira Ifergane","Oren Freifeld"],"pdf_url":"https://arxiv.org/pdf/2510.22842v1.pdf","comment":"Accepted to NeurIPS 2025. Pages 1-10 are the Main Paper. Pages 23-31\n  are Supplemental Material. FastJAM website -\n  https://bgu-cs-vil.github.io/FastJAM/"},{"id":"http://arxiv.org/abs/2510.22838v1","updated":"2025-10-26T21:11:46Z","published":"2025-10-26T21:11:46Z","title":"Semantic-Preserving Cross-Style Visual Reasoning for Robust Multi-Modal\n  Understanding in Large Vision-Language Models","summary":"  The \"style trap\" poses a significant challenge for Large Vision-Language\nModels (LVLMs), hindering robust semantic understanding across diverse visual\nstyles, especially in in-context learning (ICL). Existing methods often fail to\neffectively decouple style from content, hindering generalization. To address\nthis, we propose the Semantic-Preserving Cross-Style Visual Reasoner (SP-CSVR),\na novel framework for stable semantic understanding and adaptive cross-style\nvisual reasoning. SP-CSVR integrates a Cross-Style Feature Encoder (CSFE) for\nstyle-content disentanglement, a Semantic-Aligned In-Context Decoder (SAICD)\nfor efficient few-shot style adaptation, and an Adaptive Semantic Consistency\nModule (ASCM) employing multi-task contrastive learning to enforce cross-style\nsemantic invariance. Extensive experiments on a challenging multi-style dataset\ndemonstrate SP-CSVR's state-of-the-art performance across visual captioning,\nvisual question answering, and in-context style adaptation. Comprehensive\nevaluations, including ablation studies and generalization analysis, confirm\nSP-CSVR's efficacy in enhancing robustness, generalization, and efficiency\nacross diverse visual styles.\n","authors":["Aya Nakayama","Brian Wong","Yuji Nishimura","Kaito Tanaka"],"pdf_url":"https://arxiv.org/pdf/2510.22838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22829v1","updated":"2025-10-26T20:51:52Z","published":"2025-10-26T20:51:52Z","title":"LLM-based Fusion of Multi-modal Features for Commercial Memorability\n  Prediction","summary":"  This paper addresses the prediction of commercial (brand) memorability as\npart of \"Subtask 2: Commercial/Ad Memorability\" within the \"Memorability:\nPredicting movie and commercial memorability\" task at the MediaEval 2025\nworkshop competition. We propose a multimodal fusion system with a Gemma-3 LLM\nbackbone that integrates pre-computed visual (ViT) and textual (E5) features by\nmulti-modal projections. The model is adapted using Low-Rank Adaptation (LoRA).\nA heavily-tuned ensemble of gradient boosted trees serves as a baseline. A key\ncontribution is the use of LLM-generated rationale prompts, grounded in\nexpert-derived aspects of memorability, to guide the fusion model. The results\ndemonstrate that the LLM-based system exhibits greater robustness and\ngeneralization performance on the final test set, compared to the baseline.\n  The paper's codebase can be found at\nhttps://github.com/dsgt-arc/mediaeval-2025-memorability\n","authors":["Aleksandar Pramov"],"pdf_url":"https://arxiv.org/pdf/2510.22829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22827v1","updated":"2025-10-26T20:43:48Z","published":"2025-10-26T20:43:48Z","title":"FairJudge: MLLM Judging for Social Attributes and Prompt Image Alignment","summary":"  Text-to-image (T2I) systems lack simple, reproducible ways to evaluate how\nwell images match prompts and how models treat social attributes. Common\nproxies -- face classifiers and contrastive similarity -- reward surface cues,\nlack calibrated abstention, and miss attributes only weakly visible (for\nexample, religion, culture, disability). We present FairJudge, a lightweight\nprotocol that treats instruction-following multimodal LLMs as fair judges. It\nscores alignment with an explanation-oriented rubric mapped to [-1, 1];\nconstrains judgments to a closed label set; requires evidence grounded in the\nvisible content; and mandates abstention when cues are insufficient. Unlike\nCLIP-only pipelines, FairJudge yields accountable, evidence-aware decisions;\nunlike mitigation that alters generators, it targets evaluation fairness. We\nevaluate gender, race, and age on FairFace, PaTA, and FairCoT; extend to\nreligion, culture, and disability; and assess profession correctness and\nalignment on IdenProf, FairCoT-Professions, and our new DIVERSIFY-Professions.\nWe also release DIVERSIFY, a 469-image corpus of diverse, non-iconic scenes.\nAcross datasets, judge models outperform contrastive and face-centric baselines\non demographic prediction and improve mean alignment while maintaining high\nprofession accuracy, enabling more reliable, reproducible fairness audits.\n","authors":["Zahraa Al Sahili","Maryam Fetanat","Maimuna Nowaz","Ioannis Patras","Matthew Purver"],"pdf_url":"https://arxiv.org/pdf/2510.22827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22204v2","updated":"2025-10-26T20:37:17Z","published":"2025-03-28T07:36:51Z","title":"Segment then Splat: Unified 3D Open-Vocabulary Segmentation via Gaussian\n  Splatting","summary":"  Open-vocabulary querying in 3D space is crucial for enabling more intelligent\nperception in applications such as robotics, autonomous systems, and augmented\nreality. However, most existing methods rely on 2D pixel-level parsing, leading\nto multi-view inconsistencies and poor 3D object retrieval. Moreover, they are\nlimited to static scenes and struggle with dynamic scenes due to the\ncomplexities of motion modeling. In this paper, we propose Segment then Splat,\na 3D-aware open vocabulary segmentation approach for both static and dynamic\nscenes based on Gaussian Splatting. Segment then Splat reverses the long\nestablished approach of \"segmentation after reconstruction\" by dividing\nGaussians into distinct object sets before reconstruction. Once reconstruction\nis complete, the scene is naturally segmented into individual objects,\nachieving true 3D segmentation. This design eliminates both geometric and\nsemantic ambiguities, as well as Gaussian-object misalignment issues in dynamic\nscenes. It also accelerates the optimization process, as it eliminates the need\nfor learning a separate language field. After optimization, a CLIP embedding is\nassigned to each object to enable open-vocabulary querying. Extensive\nexperiments one various datasets demonstrate the effectiveness of our proposed\nmethod in both static and dynamic scenarios.\n","authors":["Yiren Lu","Yunlai Zhou","Yiran Qiao","Chaoda Song","Tuo Liang","Jing Ma","Huan Wang","Yu Yin"],"pdf_url":"https://arxiv.org/pdf/2503.22204v2.pdf","comment":"NeurIPS 2025. Project page:\n  https://vulab-ai.github.io/Segment-then-Splat/"},{"id":"http://arxiv.org/abs/2506.12116v3","updated":"2025-10-26T20:20:07Z","published":"2025-06-13T14:07:44Z","title":"Unsupervised Document and Template Clustering using Multimodal\n  Embeddings","summary":"  We study unsupervised clustering of documents at both the category and\ntemplate levels using frozen multimodal encoders and classical clustering\nalgorithms. We systematize a model-agnostic pipeline that (i) projects\nheterogeneous last-layer states from text-layout-vision encoders into\ntoken-type-aware document vectors and (ii) performs clustering with centroid-\nor density-based methods, including an HDBSCAN + $k$-NN assignment to eliminate\nunlabeled points. We evaluate eight encoders (text-only, layout-aware,\nvision-only, and vision-language) with $k$-Means, DBSCAN, HDBSCAN + $k$-NN, and\nBIRCH on five corpora spanning clean synthetic invoices, their heavily degraded\nprint-and-scan counterparts, scanned receipts, and real identity and\ncertificate documents. The study reveals modality-specific failure modes and a\nrobustness-accuracy trade-off, with vision features nearly solving template\ndiscovery on clean pages while text dominates under covariate shift, and fused\nencoders offering the best balance. We detail a reproducible, oracle-free\ntuning protocol and the curated evaluation settings to guide future work on\nunsupervised document organization.\n","authors":["Phillipe R. Sampaio","Helene Maxcici"],"pdf_url":"https://arxiv.org/pdf/2506.12116v3.pdf","comment":"24 pages, 12 figures"},{"id":"http://arxiv.org/abs/2504.04252v2","updated":"2025-10-26T20:01:32Z","published":"2025-04-05T19:14:51Z","title":"Progressive Multi-Source Domain Adaptation for Personalized Facial\n  Expression Recognition","summary":"  Personalized facial expression recognition (FER) involves adapting a machine\nlearning model using samples from labeled sources and unlabeled target domains.\nGiven the challenges of recognizing subtle expressions with considerable\ninterpersonal variability, state-of-the-art unsupervised domain adaptation\n(UDA) methods focus on the multi-source UDA (MSDA) setting, where each domain\ncorresponds to a specific subject, and improve model accuracy and robustness.\nHowever, when adapting to a specific target, the diverse nature of multiple\nsource domains translates to a large shift between source and target data.\nState-of-the-art MSDA methods for FER address this domain shift by considering\nall the sources to adapt to the target representations. Nevertheless, adapting\nto a target subject presents significant challenges due to large distributional\ndifferences between source and target domains, often resulting in negative\ntransfer. In addition, integrating all sources simultaneously increases\ncomputational costs and causes misalignment with the target. To address these\nissues, we propose a progressive MSDA approach that gradually introduces\ninformation from subjects based on their similarity to the target subject. This\nwill ensure that only the most relevant sources from the target are selected,\nwhich helps avoid the negative transfer caused by dissimilar sources. We first\nexploit the closest sources to reduce the distribution shift with the target\nand then move towards the furthest while only considering the most relevant\nsources based on the predetermined threshold. Furthermore, to mitigate\ncatastrophic forgetting caused by the incremental introduction of source\nsubjects, we implemented a density-based memory mechanism that preserves the\nmost relevant historical source samples for adaptation. Our extensive\nexperiments on Biovid, UNBC-McMaster, Aff-Wild2, BAH, and in a cross-dataset\nsetting.\n","authors":["Muhammad Osama Zeeshan","Marco Pedersoli","Alessandro Lameiras Koerich","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2504.04252v2.pdf","comment":"Transactions on Affective Computing 2025"},{"id":"http://arxiv.org/abs/2510.22810v1","updated":"2025-10-26T19:49:31Z","published":"2025-10-26T19:49:31Z","title":"MAGIC-Talk: Motion-aware Audio-Driven Talking Face Generation with\n  Customizable Identity Control","summary":"  Audio-driven talking face generation has gained significant attention for\napplications in digital media and virtual avatars. While recent methods improve\naudio-lip synchronization, they often struggle with temporal consistency,\nidentity preservation, and customization, especially in long video generation.\nTo address these issues, we propose MAGIC-Talk, a one-shot diffusion-based\nframework for customizable and temporally stable talking face generation.\nMAGIC-Talk consists of ReferenceNet, which preserves identity and enables\nfine-grained facial editing via text prompts, and AnimateNet, which enhances\nmotion coherence using structured motion priors. Unlike previous methods\nrequiring multiple reference images or fine-tuning, MAGIC-Talk maintains\nidentity from a single image while ensuring smooth transitions across frames.\nAdditionally, a progressive latent fusion strategy is introduced to improve\nlong-form video quality by reducing motion inconsistencies and flickering.\nExtensive experiments demonstrate that MAGIC-Talk outperforms state-of-the-art\nmethods in visual quality, identity preservation, and synchronization accuracy,\noffering a robust solution for talking face generation.\n","authors":["Fatemeh Nazarieh","Zhenhua Feng","Diptesh Kanojia","Muhammad Awais","Josef Kittler"],"pdf_url":"https://arxiv.org/pdf/2510.22810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02093v2","updated":"2025-10-26T19:46:57Z","published":"2025-06-02T17:07:10Z","title":"Are Pixel-Wise Metrics Reliable for Sparse-View Computed Tomography\n  Reconstruction?","summary":"  Widely adopted evaluation metrics for sparse-view CT reconstruction--such as\nStructural Similarity Index Measure and Peak Signal-to-Noise Ratio--prioritize\npixel-wise fidelity but often fail to capture the completeness of critical\nanatomical structures, particularly small or thin regions that are easily\nmissed. To address this limitation, we propose a suite of novel anatomy-aware\nevaluation metrics designed to assess structural completeness across anatomical\nstructures, including large organs, small organs, intestines, and vessels.\nBuilding on these metrics, we introduce CARE, a Completeness-Aware\nReconstruction Enhancement framework that incorporates structural penalties\nduring training to encourage anatomical preservation of significant structures.\nCARE is model-agnostic and can be seamlessly integrated into analytical,\nimplicit, and generative methods. When applied to these methods, CARE\nsubstantially improves structural completeness in CT reconstructions, achieving\nup to +32% improvement for large organs, +22% for small organs, +40% for\nintestines, and +36% for vessels.\n","authors":["Tianyu Lin","Xinran Li","Chuntung Zhuang","Qi Chen","Yuanhao Cai","Kai Ding","Alan L. Yuille","Zongwei Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.02093v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.22803v1","updated":"2025-10-26T19:23:20Z","published":"2025-10-26T19:23:20Z","title":"MedXplain-VQA: Multi-Component Explainable Medical Visual Question\n  Answering","summary":"  Explainability is critical for the clinical adoption of medical visual\nquestion answering (VQA) systems, as physicians require transparent reasoning\nto trust AI-generated diagnoses. We present MedXplain-VQA, a comprehensive\nframework integrating five explainable AI components to deliver interpretable\nmedical image analysis. The framework leverages a fine-tuned BLIP-2 backbone,\nmedical query reformulation, enhanced Grad-CAM attention, precise region\nextraction, and structured chain-of-thought reasoning via multi-modal language\nmodels. To evaluate the system, we introduce a medical-domain-specific\nframework replacing traditional NLP metrics with clinically relevant\nassessments, including terminology coverage, clinical structure quality, and\nattention region relevance. Experiments on 500 PathVQA histopathology samples\ndemonstrate substantial improvements, with the enhanced system achieving a\ncomposite score of 0.683 compared to 0.378 for baseline methods, while\nmaintaining high reasoning confidence (0.890). Our system identifies 3-5\ndiagnostically relevant regions per sample and generates structured\nexplanations averaging 57 words with appropriate clinical terminology. Ablation\nstudies reveal that query reformulation provides the most significant initial\nimprovement, while chain-of-thought reasoning enables systematic diagnostic\nprocesses. These findings underscore the potential of MedXplain-VQA as a\nrobust, explainable medical VQA system. Future work will focus on validation\nwith medical experts and large-scale clinical datasets to ensure clinical\nreadiness.\n","authors":["Hai-Dang Nguyen","Minh-Anh Dang","Minh-Tan Le","Minh-Tuan Le"],"pdf_url":"https://arxiv.org/pdf/2510.22803v1.pdf","comment":"10 pages, 4 figures, IEEE conference format"},{"id":"http://arxiv.org/abs/2510.22785v1","updated":"2025-10-26T18:37:12Z","published":"2025-10-26T18:37:12Z","title":"Self-Calibrated Consistency can Fight Back for Adversarial Robustness in\n  Vision-Language Models","summary":"  Pre-trained vision-language models (VLMs) such as CLIP have demonstrated\nstrong zero-shot capabilities across diverse domains, yet remain highly\nvulnerable to adversarial perturbations that disrupt image-text alignment and\ncompromise reliability. Existing defenses typically rely on adversarial\nfine-tuning with labeled data, limiting their applicability in zero-shot\nsettings. In this work, we identify two key weaknesses of current CLIP\nadversarial attacks -- lack of semantic guidance and vulnerability to view\nvariations -- collectively termed semantic and viewpoint fragility. To address\nthese challenges, we propose Self-Calibrated Consistency (SCC), an effective\ntest-time defense. SCC consists of two complementary modules: Semantic\nconsistency, which leverages soft pseudo-labels from counterattack warm-up and\nmulti-view predictions to regularize cross-modal alignment and separate the\ntarget embedding from confusable negatives; and Spatial consistency, aligning\nperturbed visual predictions via augmented views to stabilize inference under\nadversarial perturbations. Together, these modules form a plug-and-play\ninference strategy. Extensive experiments on 22 benchmarks under diverse attack\nsettings show that SCC consistently improves the zero-shot robustness of CLIP\nwhile maintaining accuracy, and can be seamlessly integrated with other VLMs\nfor further gains. These findings highlight the great potential of establishing\nan adversarially robust paradigm from CLIP, with implications extending to\nbroader vision-language domains such as BioMedCLIP.\n","authors":["Jiaxiang Liu","Jiawei Du","Xiao Liu","Prayag Tiwari","Mingkun Xu"],"pdf_url":"https://arxiv.org/pdf/2510.22785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10466v2","updated":"2025-10-26T18:22:06Z","published":"2025-01-15T15:47:49Z","title":"Efficient Semi-Supervised Adversarial Training via Latent\n  Clustering-Based Data Reduction","summary":"  Achieving high model robustness under adversarial settings is widely\nrecognized as demanding considerable training samples. Recent works propose\nsemi-supervised adversarial training (SSAT) methods with external unlabeled or\nsynthetically generated data, which are the current state-of-the-art. However,\nSSAT requires substantial extra data to attain high robustness, resulting in\nprolonged training time and increased memory usage. In this paper, we propose\nunlabeled data reduction strategies to improve the efficiency of SSAT.\nSpecifically, we design novel latent clustering-based techniques to select or\ngenerate a small critical subset of data samples near the model's decision\nboundary. While focusing on boundary-adjacent points, our methods maintain a\nbalanced ratio between boundary and non-boundary data points to avoid\noverfitting. Comprehensive experiments on benchmark datasets demonstrate that\nour methods can significantly reduce SSAT's data requirement and computation\ncosts while preserving its strong robustness advantages. In particular, our\nlatent-space selection scheme based on k-means clustering and our guided DDPM\nfine-tuning approach with LCG-KM are the most effective, achieving nearly\nidentical robust accuracies with 5x to 10x less unlabeled data and\napproximately 4x less total runtime.\n","authors":["Somrita Ghosh","Yuelin Xu","Xiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.10466v2.pdf","comment":"Shorter version of this work accepted by NextGenAISafety Workshop at\n  ICML 2024"},{"id":"http://arxiv.org/abs/2510.22772v1","updated":"2025-10-26T17:42:28Z","published":"2025-10-26T17:42:28Z","title":"Neural-HAR: A Dimension-Gated CNN Accelerator for Real-Time Radar Human\n  Activity Recognition","summary":"  Radar-based human activity recognition (HAR) is attractive for unobtrusive\nand privacy-preserving monitoring, yet many CNN/RNN solutions remain too heavy\nfor edge deployment, and even lightweight ViT/SSM variants often exceed\npractical compute and memory budgets. We introduce Neural-HAR, a\ndimension-gated CNN accelerator tailored for real-time radar HAR on\nresource-constrained platforms. At its core is GateCNN, a parameter-efficient\nDoppler-temporal network that (i) embeds Doppler vectors to emphasize frequency\nevolution over time and (ii) applies dual-path gated convolutions that modulate\nDoppler-aware content features with temporal gates, complemented by a residual\npath for stable training. On the University of Glasgow UoG2020 continuous radar\ndataset, GateCNN attains 86.4% accuracy with only 2.7k parameters and 0.28M\nFLOPs per inference, comparable to CNN-BiGRU at a fraction of the complexity.\nOur FPGA prototype on Xilinx Zynq-7000 Z-7007S reaches 107.5 $\\mu$s latency and\n15 mW dynamic power using LUT-based ROM and distributed RAM only (zero\nDSP/BRAM), demonstrating real-time, energy-efficient edge inference. Code and\nHLS conversion scripts are available at https://github.com/lab-emi/AIRHAR.\n","authors":["Yizhuo Wu","Francesco Fioranelli","Chang Gao"],"pdf_url":"https://arxiv.org/pdf/2510.22772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22760v1","updated":"2025-10-26T17:18:48Z","published":"2025-10-26T17:18:48Z","title":"Understanding What Is Not Said:Referring Remote Sensing Image\n  Segmentation with Scarce Expressions","summary":"  Referring Remote Sensing Image Segmentation (RRSIS) aims to segment instances\nin remote sensing images according to referring expressions. Unlike Referring\nImage Segmentation on general images, acquiring high-quality referring\nexpressions in the remote sensing domain is particularly challenging due to the\nprevalence of small, densely distributed objects and complex backgrounds. This\npaper introduces a new learning paradigm, Weakly Referring Expression Learning\n(WREL) for RRSIS, which leverages abundant class names as weakly referring\nexpressions together with a small set of accurate ones to enable efficient\ntraining under limited annotation conditions. Furthermore, we provide a\ntheoretical analysis showing that mixed-referring training yields a provable\nupper bound on the performance gap relative to training with fully annotated\nreferring expressions, thereby establishing the validity of this new setting.\nWe also propose LRB-WREL, which integrates a Learnable Reference Bank (LRB) to\nrefine weakly referring expressions through sample-specific prompt embeddings\nthat enrich coarse class-name inputs. Combined with a teacher-student\noptimization framework using dynamically scheduled EMA updates, LRB-WREL\nstabilizes training and enhances cross-modal generalization under noisy weakly\nreferring supervision. Extensive experiments on our newly constructed benchmark\nwith varying weakly referring data ratios validate both the theoretical\ninsights and the practical effectiveness of WREL and LRB-WREL, demonstrating\nthat they can approach or even surpass models trained with fully annotated\nreferring expressions.\n","authors":["Kai Ye","Bowen Liu","Jianghang Lin","Jiayi Ji","Pingyang Dai","Liujuan Cao"],"pdf_url":"https://arxiv.org/pdf/2510.22760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22743v1","updated":"2025-10-26T16:34:43Z","published":"2025-10-26T16:34:43Z","title":"ConMatFormer: A Multi-attention and Transformer Integrated ConvNext\n  based Deep Learning Model for Enhanced Diabetic Foot Ulcer Classification","summary":"  Diabetic foot ulcer (DFU) detection is a clinically significant yet\nchallenging task due to the scarcity and variability of publicly available\ndatasets. To solve these problems, we propose ConMatFormer, a new hybrid deep\nlearning architecture that combines ConvNeXt blocks, multiple attention\nmechanisms convolutional block attention module (CBAM) and dual attention\nnetwork (DANet), and transformer modules in a way that works together. This\ndesign facilitates the extraction of better local features and understanding of\nthe global context, which allows us to model small skin patterns across\ndifferent types of DFU very accurately. To address the class imbalance, we used\ndata augmentation methods. A ConvNeXt block was used to obtain detailed local\nfeatures in the initial stages. Subsequently, we compiled the model by adding a\ntransformer module to enhance long-range dependency. This enabled us to\npinpoint the DFU classes that were underrepresented or constituted minorities.\nTests on the DS1 (DFUC2021) and DS2 (diabetic foot ulcer (DFU)) datasets showed\nthat ConMatFormer outperformed state-of-the-art (SOTA) convolutional neural\nnetwork (CNN) and Vision Transformer (ViT) models in terms of accuracy,\nreliability, and flexibility. The proposed method achieved an accuracy of\n0.8961 and a precision of 0.9160 in a single experiment, which is a significant\nimprovement over the current standards for classifying DFUs. In addition, by\n4-fold cross-validation, the proposed model achieved an accuracy of 0.9755 with\na standard deviation of only 0.0031. We further applied explainable artificial\nintelligence (XAI) methods, such as Grad-CAM, Grad-CAM++, and LIME, to\nconsistently monitor the transparency and trustworthiness of the\ndecision-making process.. Our findings set a new benchmark for DFU\nclassification and provide a hybrid attention transformer framework for medical\nimage analysis.\n","authors":["Raihan Ahamed Rifat","Fuyad Hasan Bhoyan","Md Humaion Kabir Mehedi","Md Kaviul Hossain","Md. Jakir Hossen","M. F. Mridha"],"pdf_url":"https://arxiv.org/pdf/2510.22743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09920v3","updated":"2025-10-26T16:16:19Z","published":"2025-06-11T16:41:34Z","title":"Structural-Spectral Graph Convolution with Evidential Edge Learning for\n  Hyperspectral Image Clustering","summary":"  Hyperspectral image (HSI) clustering groups pixels into clusters without\nlabeled data, which is an important yet challenging task. For large-scale HSIs,\nmost methods rely on superpixel segmentation and perform superpixel-level\nclustering based on graph neural networks (GNNs). However, existing GNNs cannot\nfully exploit the spectral information of the input HSI, and the inaccurate\nsuperpixel topological graph may lead to the confusion of different class\nsemantics during information aggregation. To address these challenges, we first\npropose a structural-spectral graph convolutional operator (SSGCO) tailored for\ngraph-structured HSI superpixels to improve their representation quality\nthrough the co-extraction of spatial and spectral features. Second, we propose\nan evidence-guided adaptive edge learning (EGAEL) module that adaptively\npredicts and refines edge weights in the superpixel topological graph. We\nintegrate the proposed method into a contrastive learning framework to achieve\nclustering, where representation learning and clustering are simultaneously\nconducted. Experiments demonstrate that the proposed method improves clustering\naccuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best compared methods on\nfour HSI datasets. Our code is available at\nhttps://github.com/jhqi/SSGCO-EGAEL.\n","authors":["Jianhan Qi","Yuheng Jia","Hui Liu","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2506.09920v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22736v1","updated":"2025-10-26T16:09:53Z","published":"2025-10-26T16:09:53Z","title":"Cross-view Localization and Synthesis -- Datasets, Challenges and\n  Opportunities","summary":"  Cross-view localization and synthesis are two fundamental tasks in cross-view\nvisual understanding, which deals with cross-view datasets: overhead (satellite\nor aerial) and ground-level imagery. These tasks have gained increasing\nattention due to their broad applications in autonomous navigation, urban\nplanning, and augmented reality. Cross-view localization aims to estimate the\ngeographic position of ground-level images based on information provided by\noverhead imagery while cross-view synthesis seeks to generate ground-level\nimages based on information from the overhead imagery. Both tasks remain\nchallenging due to significant differences in viewing perspective, resolution,\nand occlusion, which are widely embedded in cross-view datasets. Recent years\nhave witnessed rapid progress driven by the availability of large-scale\ndatasets and novel approaches. Typically, cross-view localization is formulated\nas an image retrieval problem where ground-level features are matched with\ntiled overhead images feature, extracted by convolutional neural networks\n(CNNs) or vision transformers (ViTs) for cross-view feature embedding.\nCross-view synthesis, on the other hand, seeks to generate ground-level views\nbased on information from overhead imagery, generally using generative\nadversarial networks (GANs) or diffusion models. This paper presents a\ncomprehensive survey of advances in cross-view localization and synthesis,\nreviewing widely used datasets, highlighting key challenges, and providing an\norganized overview of state-of-the-art techniques. Furthermore, it discusses\ncurrent limitations, offers comparative analyses, and outlines promising\ndirections for future research. We also include the project page via\nhttps://github.com/GDAOSU/Awesome-Cross-View-Methods.\n","authors":["Ningli Xu","Rongjun Qin"],"pdf_url":"https://arxiv.org/pdf/2510.22736v1.pdf","comment":"15 Figures"},{"id":"http://arxiv.org/abs/2510.22728v1","updated":"2025-10-26T15:57:14Z","published":"2025-10-26T15:57:14Z","title":"S-Chain: Structured Visual Chain-of-Thought For Medicine","summary":"  Faithful reasoning in medical vision-language models (VLMs) requires not only\naccurate predictions but also transparent alignment between textual rationales\nand visual evidence. While Chain-of-Thought (CoT) prompting has shown promise\nin medical visual question answering (VQA), no large-scale expert-level dataset\nhas captured stepwise reasoning with precise visual grounding. We introduce\nS-Chain, the first large-scale dataset of 12,000 expert-annotated medical\nimages with bounding boxes and structured visual CoT (SV-CoT), explicitly\nlinking visual regions to reasoning steps. The dataset further supports 16\nlanguages, totaling over 700k VQA pairs for broad multilingual applicability.\nUsing S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med,\nLLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that\nSV-CoT supervision significantly improves interpretability, grounding fidelity,\nand robustness. Beyond benchmarking, we study its synergy with\nretrieval-augmented generation, revealing how domain knowledge and visual\ngrounding interact during autoregressive reasoning. Finally, we propose a new\nmechanism that strengthens the alignment between visual evidence and reasoning,\nimproving both reliability and efficiency. S-Chain establishes a new benchmark\nfor grounded medical reasoning and paves the way toward more trustworthy and\nexplainable medical VLMs.\n","authors":["Khai Le-Duc","Duy M. H. Nguyen","Phuong T. H. Trinh","Tien-Phat Nguyen","Nghiem T. Diep","An Ngo","Tung Vu","Trinh Vuong","Anh-Tien Nguyen","Mau Nguyen","Van Trung Hoang","Khai-Nguyen Nguyen","Hy Nguyen","Chris Ngo","Anji Liu","Nhat Ho","Anne-Christin Hauschild","Khanh Xuan Nguyen","Thanh Nguyen-Tang","Pengtao Xie","Daniel Sonntag","James Zou","Mathias Niepert","Anh Totti Nguyen"],"pdf_url":"https://arxiv.org/pdf/2510.22728v1.pdf","comment":"First version"},{"id":"http://arxiv.org/abs/2509.11774v2","updated":"2025-10-26T15:47:25Z","published":"2025-09-15T10:53:28Z","title":"SA-UNetv2: Rethinking Spatial Attention U-Net for Retinal Vessel\n  Segmentation","summary":"  Retinal vessel segmentation is essential for early diagnosis of diseases such\nas diabetic retinopathy, hypertension, and neurodegenerative disorders.\nAlthough SA-UNet introduces spatial attention in the bottleneck, it underuses\nattention in skip connections and does not address the severe\nforeground-background imbalance. We propose SA-UNetv2, a lightweight model that\ninjects cross-scale spatial attention into all skip connections to strengthen\nmulti-scale feature fusion and adopts a weighted Binary Cross-Entropy (BCE)\nplus Matthews Correlation Coefficient (MCC) loss to improve robustness to class\nimbalance. On the public DRIVE and STARE datasets, SA-UNetv2 achieves\nstate-of-the-art performance with only 1.2MB memory and 0.26M parameters (less\nthan 50% of SA-UNet), and 1 second CPU inference on 592 x 592 x 3 images,\ndemonstrating strong efficiency and deployability in resource-constrained,\nCPU-only settings.\n","authors":["Changlu Guo","Anders Nymark Christensen","Anders Bjorholm Dahl","Yugen Yi","Morten Rieger Hannemose"],"pdf_url":"https://arxiv.org/pdf/2509.11774v2.pdf","comment":"The code is available at github.com/clguo/SA-UNetv2"},{"id":"http://arxiv.org/abs/2510.22718v1","updated":"2025-10-26T15:33:29Z","published":"2025-10-26T15:33:29Z","title":"Edge Collaborative Gaussian Splatting with Integrated Rendering and\n  Communication","summary":"  Gaussian splatting (GS) struggles with degraded rendering quality on low-cost\ndevices. To address this issue, we present edge collaborative GS (ECO-GS),\nwhere each user can switch between a local small GS model to guarantee\ntimeliness and a remote large GS model to guarantee fidelity. However, deciding\nhow to engage the large GS model is nontrivial, due to the interdependency\nbetween rendering requirements and resource conditions. To this end, we propose\nintegrated rendering and communication (IRAC), which jointly optimizes\ncollaboration status (i.e., deciding whether to engage large GS) and edge power\nallocation (i.e., enabling remote rendering) under communication constraints\nacross different users by minimizing a newly-derived GS switching function.\nDespite the nonconvexity of the problem, we propose an efficient penalty\nmajorization minimization (PMM) algorithm to obtain the critical point\nsolution. Furthermore, we develop an imitation learning optimization (ILO)\nalgorithm, which reduces the computational time by over 100x compared to PMM.\nExperiments demonstrate the superiority of PMM and the real-time execution\ncapability of ILO.\n","authors":["Yujie Wan","Chenxuan Liu","Shuai Wang","Tong Zhang","James Jianqiao Yu","Kejiang Ye","Dusit Niyato","Chengzhong Xu"],"pdf_url":"https://arxiv.org/pdf/2510.22718v1.pdf","comment":"5 pages and 7 figures, submitted for possible publication"},{"id":"http://arxiv.org/abs/2510.22716v1","updated":"2025-10-26T15:21:42Z","published":"2025-10-26T15:21:42Z","title":"LRW-Persian: Lip-reading in the Wild Dataset for Persian Language","summary":"  Lipreading has emerged as an increasingly important research area for\ndeveloping robust speech recognition systems and assistive technologies for the\nhearing-impaired. However, non-English resources for visual speech recognition\nremain limited. We introduce LRW-Persian, the largest in-the-wild Persian\nword-level lipreading dataset, comprising $743$ target words and over\n$414{,}000$ video samples extracted from more than $1{,}900$ hours of footage\nacross $67$ television programs. Designed as a benchmark-ready resource,\nLRW-Persian provides speaker-disjoint training and test splits, wide regional\nand dialectal coverage, and rich per-clip metadata including head pose, age,\nand gender. To ensure large-scale data quality, we establish a fully automated\nend-to-end curation pipeline encompassing transcription based on Automatic\nSpeech Recognition(ASR), active-speaker localization, quality filtering, and\npose/mask screening. We further fine-tune two widely used lipreading\narchitectures on LRW-Persian, establishing reference performance and\ndemonstrating the difficulty of Persian visual speech recognition. By filling a\ncritical gap in low-resource languages, LRW-Persian enables rigorous\nbenchmarking, supports cross-lingual transfer, and provides a foundation for\nadvancing multimodal speech research in underrepresented linguistic contexts.\nThe dataset is publicly available at: https://lrw-persian.vercel.app.\n","authors":["Zahra Taghizadeh","Mohammad Shahverdikondori","Arian Noori","Alireza Dadgarnia"],"pdf_url":"https://arxiv.org/pdf/2510.22716v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.06380v5","updated":"2025-10-26T15:17:05Z","published":"2025-02-10T12:01:05Z","title":"Structure-preserving contrastive learning for spatial time series","summary":"  The effectiveness of neural network models largely relies on learning\nmeaningful latent patterns from data, where self-supervised learning of\ninformative representations can enhance model performance and generalisability.\nHowever, self-supervised representation learning for spatially characterised\ntime series, which are ubiquitous in transportation domain, poses unique\nchallenges due to the necessity of maintaining fine-grained spatio-temporal\nsimilarities in the latent space. In this study, we introduce two\nstructure-preserving regularisers for the contrastive learning of spatial time\nseries: one regulariser preserves the topology of similarities between\ninstances, and the other preserves the graph geometry of similarities across\nspatial and temporal dimensions. To balance the contrastive learning objective\nand the need for structure preservation, we propose a dynamic weighting\nmechanism that adaptively manages this trade-off and stabilises training. We\nvalidate the proposed method through extensive experiments, including\nmultivariate time series classification to demonstrate its general\napplicability, as well as macroscopic and microscopic traffic prediction to\nhighlight its particular usefulness in encoding traffic interactions. Across\nall tasks, our method preserves the similarity structures more effectively and\nimproves state-of-the-art task performances. This method can be integrated with\nan arbitrary neural network model and is particularly beneficial for time\nseries data with spatial or geographical features. Furthermore, our findings\nsuggest that well-preserved similarity structures in the latent space indicate\nmore informative and useful representations. This provides insights to design\nmore effective neural networks for data-driven transportation research. Our\ncode is made openly accessible with all resulting data at\nhttps://github.com/yiru-jiao/spclt\n","authors":["Yiru Jiao","Sander van Cranenburgh","Simeon Calvert","Hans van Lint"],"pdf_url":"https://arxiv.org/pdf/2502.06380v5.pdf","comment":"TL;DR: Preserving certain structures of similarity relations in\n  spatio-temporal data can improve downstream task performance via contrastive\n  learning"},{"id":"http://arxiv.org/abs/2510.22702v1","updated":"2025-10-26T14:53:36Z","published":"2025-10-26T14:53:36Z","title":"Atlas Urban Index: A VLM-Based Approach for Spatially and Temporally\n  Calibrated Urban Development Monitoring","summary":"  We introduce the {\\em Atlas Urban Index} (AUI), a metric for measuring urban\ndevelopment computed using Sentinel-2 \\citep{spoto2012sentinel2} satellite\nimagery. Existing approaches, such as the {\\em Normalized Difference Built-up\nIndex} (NDBI), often struggle to accurately capture urban development due to\nfactors like atmospheric noise, seasonal variation, and cloud cover. These\nlimitations hinder large-scale monitoring of human development and\nurbanization. To address these challenges, we propose an approach that\nleverages {\\em Vision-Language Models }(VLMs) to provide a development score\nfor regions. Specifically, we collect a time series of Sentinel-2 images for\neach region. Then, we further process the images within fixed time windows to\nget an image with minimal cloud cover, which serves as the representative image\nfor that time window. To ensure consistent scoring, we adopt two strategies:\n(i) providing the VLM with a curated set of reference images representing\ndifferent levels of urbanization, and (ii) supplying the most recent past image\nto both anchor temporal consistency and mitigate cloud-related noise in the\ncurrent image. Together, these components enable AUI to overcome the challenges\nof traditional urbanization indices and produce more reliable and stable\ndevelopment scores. Our qualitative experiments on Bangalore suggest that AUI\noutperforms standard indices such as NDBI.\n","authors":["Mithul Chander","Sai Pragnya Ranga","Prathamesh Mayekar"],"pdf_url":"https://arxiv.org/pdf/2510.22702v1.pdf","comment":"An abridged version of this paper will be presented at and appear in\n  the Proceedings of ACM IKDD CODS 2025"},{"id":"http://arxiv.org/abs/2507.17343v2","updated":"2025-10-26T14:53:32Z","published":"2025-07-23T09:12:25Z","title":"Principled Multimodal Representation Learning","summary":"  Multimodal representation learning seeks to create a unified representation\nspace by integrating diverse data modalities to improve multimodal\nunderstanding. Traditional methods often depend on pairwise contrastive\nlearning, which relies on a predefined anchor modality, restricting alignment\nacross all modalities. Recent advances have investigated the simultaneous\nalignment of multiple modalities, yet several challenges remain, such as\nlimitations imposed by fixed anchor points and instability arising from\noptimizing the product of singular values. To address the challenges, in this\npaper, we propose Principled Multimodal Representation Learning (PMRL), a novel\nframework that achieves simultaneous alignment of multiple modalities without\nanchor dependency in a more stable manner. Specifically, grounded in the\ntheoretical insight that full alignment corresponds to a rank-1 Gram matrix,\nPMRL optimizes the dominant singular value of the representation matrix to\nalign modalities along a shared leading direction. We propose a softmax-based\nloss function that treats singular values as logits to prioritize the largest\nsingular value. Besides, instance-wise contrastive regularization on the\nleading eigenvectors maintains inter-instance separability and prevents\nrepresentation collapse. Extensive experiments across diverse tasks demonstrate\nPMRL's superiority compared to baseline methods. The source code will be\npublicly available.\n","authors":["Xiaohao Liu","Xiaobo Xia","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2507.17343v2.pdf","comment":"Corrected typos and updated experimental results. 32 pages, 9\n  figures, 10 tables"},{"id":"http://arxiv.org/abs/2510.22697v1","updated":"2025-10-26T14:45:30Z","published":"2025-10-26T14:45:30Z","title":"WaveMAE: Wavelet decomposition Masked Auto-Encoder for Remote Sensing","summary":"  Self-supervised learning (SSL) has recently emerged as a key strategy for\nbuilding foundation models in remote sensing, where the scarcity of annotated\ndata limits the applicability of fully supervised approaches. In this work, we\nintroduce WaveMAE, a masked autoencoding framework tailored for multispectral\nsatellite imagery. Unlike conventional pixel-based reconstruction, WaveMAE\nleverages a multi-level Discrete Wavelet Transform (DWT) to disentangle\nfrequency components and guide the encoder toward learning scale-aware\nhigh-frequency representations. We further propose a Geo-conditioned Positional\nEncoding (GPE), which incorporates geographical priors via Spherical Harmonics,\nencouraging embeddings that respect both semantic and geospatial structure. To\nensure fairness in evaluation, all methods are pretrained on the same dataset\n(fMoW-S2) and systematically evaluated on the diverse downstream tasks of the\nPANGAEA benchmark, spanning semantic segmentation, regression, change\ndetection, and multilabel classification. Extensive experiments demonstrate\nthat WaveMAE achieves consistent improvements over prior state-of-the-art\napproaches, with substantial gains on segmentation and regression benchmarks.\nThe effectiveness of WaveMAE pretraining is further demonstrated by showing\nthat even a lightweight variant, containing only 26.4% of the parameters,\nachieves state-of-the-art performance. Our results establish WaveMAE as a\nstrong and geographically informed foundation model for multispectral remote\nsensing imagery.\n","authors":["Vittorio Bernuzzi","Leonardo Rossi","Tomaso Fontanini","Massimo Bertozzi","Andrea Prati"],"pdf_url":"https://arxiv.org/pdf/2510.22697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22694v1","updated":"2025-10-26T14:36:16Z","published":"2025-10-26T14:36:16Z","title":"Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation","summary":"  Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising\nmethod to generate factual and up-to-date responses of Multimodal Large\nLanguage Models (MLLMs) by incorporating non-parametric knowledge from external\nknowledge bases. However, existing MRAG approaches suffer from static retrieval\nstrategies, inflexible modality selection, and suboptimal utilization of\nretrieved information, leading to three critical challenges: determining when\nto retrieve, what modality to incorporate, and how to utilize retrieved\ninformation effectively. To address these challenges, we introduce Windsock, a\nquery-dependent module making decisions on retrieval necessity and modality\nselection, effectively reducing computational overhead and improving response\nquality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction\nTuning, an adaptive training strategy that enhances MLLMs' ability to utilize\nretrieved information while maintaining robustness against noise. Moreover, we\nadopt a self-assessment approach leveraging knowledge within MLLMs to convert\nquestion-answering datasets to MRAG training datasets. Extensive experiments\ndemonstrate that our proposed method significantly improves the generation\nquality by 17.07% while reducing 8.95% retrieval times.\n","authors":["Shu Zhao","Tianyi Shen","Nilesh Ahuja","Omesh Tickoo","Vijaykrishnan Narayanan"],"pdf_url":"https://arxiv.org/pdf/2510.22694v1.pdf","comment":"Accepted at NeurIPS 2025 UniReps Workshop"},{"id":"http://arxiv.org/abs/2505.23209v2","updated":"2025-10-26T14:22:42Z","published":"2025-05-29T07:50:32Z","title":"Navigating the Accuracy-Size Trade-Off with Flexible Model Merging","summary":"  Model merging has emerged as an efficient method to combine multiple\nsingle-task fine-tuned models. The merged model can enjoy multi-task\ncapabilities without expensive training. While promising, merging into a single\nmodel often suffers from an accuracy gap with respect to the fine-tuned models.\nOn the other hand, deploying all individual fine-tuned models incurs high\nstorage costs. We propose FlexMerge, a novel data-free model merging framework\nthat: (a) flexibly generates merged models of varying sizes, spanning the full\nspectrum from a single merged model to retaining all fine-tuned models; and (b)\nsupports multiple merging algorithms in a unified framework. Using FlexMerge,\nwe systematically characterize the accuracy-size trade-off of different\nalgorithms. Our study reveals two key findings: first, even modestly larger\nmerged models can yield steep accuracy gains (up to 13.5% when just doubling\nthe size); second, algorithm rankings are not consistent as size increases,\nwith some methods overtaking others beyond the one-model regime. These results\nuncover a new design dimension for model merging: developing and comparing\nalgorithms across the full spectrum of sizes rather than only at the\nsingle-model limit. Extensive experiments on vision and NLP benchmarks, with up\nto 30 tasks, confirm the generality and practicality of FlexMerge.\n","authors":["Akash Dhasade","Divyansh Jhunjhunwala","Milos Vujasinovic","Gauri Joshi","Anne-Marie Kermarrec"],"pdf_url":"https://arxiv.org/pdf/2505.23209v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10833v2","updated":"2025-10-26T14:10:22Z","published":"2025-04-15T03:24:13Z","title":"Measuring the (Un)Faithfulness of Concept-Based Explanations","summary":"  Post-hoc, unsupervised concept-based explanation methods (U-CBEMs) translate\na vision model's internal reasoning into human-understandable concepts, leading\nto interpretable explanations. However, we find that many state-of-the-art\n(SOTA) U-CBEMs are not faithful: their concepts seem interpretable but fail to\nreproduce the model's predictions. We argue that this deficiency has gone\nunnoticed due to fragmented evaluation - each paper proposes its own\nfaithfulness measure, with no measure-over-measure comparison or broad\nbenchmarking. We close this gap by (i) organizing prior metrics in a unified\nframework, discussing their limitations, and identifying desiderata for a\nfaithfulness measure; (ii) introducing the Surrogate Faithfulness (SURF)\nmeasure, which quantifies faithfulness via the predictive loss of a surrogate\nthat maps explanations to the model's outputs; and (iii) delivering the first\ncomprehensive U-CBEM faithfulness benchmark across diverse tasks and\narchitectures. In a controlled setting, SURF outperforms prior faithfulness\nmeasures in measure-over-measure comparisons, and applying SURF to SOTA U-CBEMs\nreveals that many visually appealing U-CBEMs are surprisingly unfaithful. We\ndemonstrate SURF applicability in two downstream settings - (i) faithfulness\nversus the number of concepts used in the explanation and (ii) U-CBEM\nrobustness to adversarial attacks - underscoring SURF's value as a reliable\nfaithfulness measure. Code to be released.\n","authors":["Shubham Kumar","Narendra Ahuja"],"pdf_url":"https://arxiv.org/pdf/2504.10833v2.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2510.22684v1","updated":"2025-10-26T13:57:08Z","published":"2025-10-26T13:57:08Z","title":"RoboSVG: A Unified Framework for Interactive SVG Generation with\n  Multi-modal Guidance","summary":"  Scalable Vector Graphics (SVGs) are fundamental to digital design and robot\ncontrol, encoding not only visual structure but also motion paths in\ninteractive drawings. In this work, we introduce RoboSVG, a unified multimodal\nframework for generating interactive SVGs guided by textual, visual, and\nnumerical signals. Given an input query, the RoboSVG model first produces\nmultimodal guidance, then synthesizes candidate SVGs through dedicated\ngeneration modules, and finally refines them under numerical guidance to yield\nhigh-quality outputs. To support this framework, we construct RoboDraw, a\nlarge-scale dataset of one million examples, each pairing an SVG generation\ncondition (e.g., text, image, and partial SVG) with its corresponding\nground-truth SVG code. RoboDraw dataset enables systematic study of four tasks,\nincluding basic generation (Text-to-SVG, Image-to-SVG) and interactive\ngeneration (PartialSVG-to-SVG, PartialImage-to-SVG). Extensive experiments\ndemonstrate that RoboSVG achieves superior query compliance and visual fidelity\nacross tasks, establishing a new state of the art in versatile SVG generation.\nThe dataset and source code of this project will be publicly available soon.\n","authors":["Jiuniu Wang","Gongjie Zhang","Quanhao Qian","Junlong Gao","Deli Zhao","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2510.22684v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.22683v1","updated":"2025-10-26T13:54:41Z","published":"2025-10-26T13:54:41Z","title":"Estimation of Fireproof Structure Class and Construction Year for\n  Disaster Risk Assessment","summary":"  Structural fireproof classification is vital for disaster risk assessment and\ninsurance pricing in Japan. However, key building metadata such as construction\nyear and structure type are often missing or outdated, particularly in the\nsecond-hand housing market. This study proposes a multi-task learning model\nthat predicts these attributes from facade images. The model jointly estimates\nthe construction year, building structure, and property type, from which the\nstructural fireproof class - defined as H (non-fireproof), T (semi-fireproof),\nor M (fireproof) - is derived via a rule-based mapping based on official\ninsurance criteria. We trained and evaluated the model using a large-scale\ndataset of Japanese residential images, applying rigorous filtering and\ndeduplication. The model achieved high accuracy in construction-year regression\nand robust classification across imbalanced categories. Qualitative analyses\nshow that it captures visual cues related to building age and materials. Our\napproach demonstrates the feasibility of scalable, interpretable, image-based\nrisk-profiling systems, offering potential applications in insurance, urban\nplanning, and disaster preparedness.\n","authors":["Hibiki Ayabe","Kazushi Okamoto","Koki Karube","Atsushi Shibata","Kei Harada"],"pdf_url":"https://arxiv.org/pdf/2510.22683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.26096v2","updated":"2025-10-26T13:32:18Z","published":"2025-09-30T11:15:07Z","title":"EVODiff: Entropy-aware Variance Optimized Diffusion Inference","summary":"  Diffusion models (DMs) excel in image generation, but suffer from slow\ninference and the training-inference discrepancies. Although gradient-based\nsolvers like DPM-Solver accelerate the denoising inference, they lack\ntheoretical foundations in information transmission efficiency. In this work,\nwe introduce an information-theoretic perspective on the inference processes of\nDMs, revealing that successful denoising fundamentally reduces conditional\nentropy in reverse transitions. This principle leads to our key insights into\nthe inference processes: (1) data prediction parameterization outperforms its\nnoise counterpart, and (2) optimizing conditional variance offers a\nreference-free way to minimize both transition and reconstruction errors. Based\non these insights, we propose an entropy-aware variance optimized method for\nthe generative process of DMs, called EVODiff, which systematically reduces\nuncertainty by optimizing conditional entropy during denoising. Extensive\nexperiments on DMs validate our insights and demonstrate that our method\nsignificantly and consistently outperforms state-of-the-art (SOTA)\ngradient-based solvers. For example, compared to the DPM-Solver++, EVODiff\nreduces the reconstruction error by up to 45.5\\% (FID improves from 5.10 to\n2.78) at 10 function evaluations (NFE) on CIFAR-10, cuts the NFE cost by 25\\%\n(from 20 to 15 NFE) for high-quality samples on ImageNet-256, and improves\ntext-to-image generation while reducing artifacts. Code is available at\nhttps://github.com/ShiguiLi/EVODiff.\n","authors":["Shigui Li","Wei Chen","Delu Zeng"],"pdf_url":"https://arxiv.org/pdf/2509.26096v2.pdf","comment":"NeurIPS 2025, 40 pages, 14 figures"},{"id":"http://arxiv.org/abs/2510.22675v1","updated":"2025-10-26T13:29:26Z","published":"2025-10-26T13:29:26Z","title":"DAMap: Distance-aware MapNet for High Quality HD Map Construction","summary":"  Predicting High-definition (HD) map elements with high quality (high\nclassification and localization scores) is crucial to the safety of autonomous\ndriving vehicles. However, current methods perform poorly in high quality\npredictions due to inherent task misalignment. Two main factors are responsible\nfor misalignment: 1) inappropriate task labels due to one-to-many matching\nqueries sharing the same labels, and 2) sub-optimal task features due to\ntask-shared sampling mechanism. In this paper, we reveal two inherent defects\nin current methods and develop a novel HD map construction method named DAMap\nto address these problems. Specifically, DAMap consists of three components:\nDistance-aware Focal Loss (DAFL), Hybrid Loss Scheme (HLS), and Task Modulated\nDeformable Attention (TMDA). The DAFL is introduced to assign appropriate\nclassification labels for one-to-many matching samples. The TMDA is proposed to\nobtain discriminative task-specific features. Furthermore, the HLS is proposed\nto better utilize the advantages of the DAFL. We perform extensive experiments\nand consistently achieve performance improvement on the NuScenes and Argoverse2\nbenchmarks under different metrics, baselines, splits, backbones, and\nschedules. Code will be available at https://github.com/jpdong-xjtu/DAMap.\n","authors":["Jinpeng Dong","Chen Li","Yutong Lin","Jingwen Fu","Sanping Zhou","Nanning Zheng"],"pdf_url":"https://arxiv.org/pdf/2510.22675v1.pdf","comment":"Accepted to ICCV2025"},{"id":"http://arxiv.org/abs/2510.22673v1","updated":"2025-10-26T13:28:28Z","published":"2025-10-26T13:28:28Z","title":"Alias-Free ViT: Fractional Shift Invariance via Linear Attention","summary":"  Transformers have emerged as a competitive alternative to convnets in vision\ntasks, yet they lack the architectural inductive bias of convnets, which may\nhinder their potential performance. Specifically, Vision Transformers (ViTs)\nare not translation-invariant and are more sensitive to minor image\ntranslations than standard convnets. Previous studies have shown, however, that\nconvnets are also not perfectly shift-invariant, due to aliasing in\ndownsampling and nonlinear layers. Consequently, anti-aliasing approaches have\nbeen proposed to certify convnets' translation robustness. Building on this\nline of work, we propose an Alias-Free ViT, which combines two main components.\nFirst, it uses alias-free downsampling and nonlinearities. Second, it uses\nlinear cross-covariance attention that is shift-equivariant to both integer and\nfractional translations, enabling a shift-invariant global representation. Our\nmodel maintains competitive performance in image classification and outperforms\nsimilar-sized models in terms of robustness to adversarial translations.\n","authors":["Hagay Michaeli","Daniel Soudry"],"pdf_url":"https://arxiv.org/pdf/2510.22673v1.pdf","comment":"Accepted at NeurIPS 2025. Code is available at\n  https://github.com/hmichaeli/alias_free_vit"},{"id":"http://arxiv.org/abs/2510.22669v1","updated":"2025-10-26T13:16:39Z","published":"2025-10-26T13:16:39Z","title":"LVD-GS: Gaussian Splatting SLAM for Dynamic Scenes via Hierarchical\n  Explicit-Implicit Representation Collaboration Rendering","summary":"  3D Gaussian Splatting SLAM has emerged as a widely used technique for\nhigh-fidelity mapping in spatial intelligence. However, existing methods often\nrely on a single representation scheme, which limits their performance in\nlarge-scale dynamic outdoor scenes and leads to cumulative pose errors and\nscale ambiguity. To address these challenges, we propose \\textbf{LVD-GS}, a\nnovel LiDAR-Visual 3D Gaussian Splatting SLAM system. Motivated by the human\nchain-of-thought process for information seeking, we introduce a hierarchical\ncollaborative representation module that facilitates mutual reinforcement for\nmapping optimization, effectively mitigating scale drift and enhancing\nreconstruction robustness. Furthermore, to effectively eliminate the influence\nof dynamic objects, we propose a joint dynamic modeling module that generates\nfine-grained dynamic masks by fusing open-world segmentation with implicit\nresidual constraints, guided by uncertainty estimates from DINO-Depth features.\nExtensive evaluations on KITTI, nuScenes, and self-collected datasets\ndemonstrate that our approach achieves state-of-the-art performance compared to\nexisting methods.\n","authors":["Wenkai Zhu","Xu Li","Qimin Xu","Benwu Wang","Kun Wei","Yiming Peng","Zihang Wang"],"pdf_url":"https://arxiv.org/pdf/2510.22669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22665v1","updated":"2025-10-26T13:04:50Z","published":"2025-10-26T13:04:50Z","title":"SARCLIP: A Vision Language Foundation Model for Semantic Understanding\n  and Target Recognition in SAR Imagery","summary":"  Synthetic Aperture Radar (SAR) has emerged as a crucial imaging modality due\nto its all-weather capabilities. While recent advancements in self-supervised\nlearning and Masked Image Modeling (MIM) have paved the way for SAR foundation\nmodels, these approaches primarily focus on low-level visual features, often\noverlooking multimodal alignment and zero-shot target recognition within SAR\nimagery. To address this limitation, we construct SARCLIP-1M, a large-scale\nvision language dataset comprising over one million text-image pairs aggregated\nfrom existing datasets. We further introduce SARCLIP, the first vision language\nfoundation model tailored for the SAR domain. Our SARCLIP model is trained\nusing a contrastive vision language learning approach by domain transferring\nstrategy, enabling it to bridge the gap between SAR imagery and textual\ndescriptions. Extensive experiments on image-text retrieval and zero-shot\nclassification tasks demonstrate the superior performance of SARCLIP in feature\nextraction and interpretation, significantly outperforming state-of-the-art\nfoundation models and advancing the semantic understanding of SAR imagery. The\ncode and datasets will be released soon.\n","authors":["Qiwei Ma","Zhiyu Wang","Wang Liu","Xukun Lu","Bin Deng","Puhong Duan","Xudong Kang","Shutao Li"],"pdf_url":"https://arxiv.org/pdf/2510.22665v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.23881v2","updated":"2025-10-26T12:57:19Z","published":"2025-06-30T14:10:51Z","title":"Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution\n  Detection","summary":"  Out-of-distribution (OOD) detection is crucial for ensuring the reliability\nand safety of machine learning models in real-world applications, where they\nfrequently face data distributions unseen during training. Despite progress,\nexisting methods are often vulnerable to spurious correlations that mislead\nmodels and compromise robustness. To address this, we propose SPROD, a novel\nprototype-based OOD detection approach that explicitly addresses the challenge\nposed by unknown spurious correlations. Our post-hoc method refines class\nprototypes to mitigate bias from spurious features without additional data or\nhyperparameter tuning, and is broadly applicable across diverse backbones and\nOOD detection settings. We conduct a comprehensive spurious correlation OOD\ndetection benchmarking, comparing our method against existing approaches and\ndemonstrating its superior performance across challenging OOD datasets, such as\nCelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced\nAnimals MetaCoCo. On average, SPROD improves AUROC by 4.8% and FPR@95 by 9.4%\nover the second best.\n","authors":["Reihaneh Zohrabi","Hosein Hasani","Mahdieh Soleymani Baghshah","Anna Rohrbach","Marcus Rohrbach","Mohammad Hossein Rohban"],"pdf_url":"https://arxiv.org/pdf/2506.23881v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.22650v1","updated":"2025-10-26T12:22:56Z","published":"2025-10-26T12:22:56Z","title":"Self-Attention Decomposition For Training Free Diffusion Editing","summary":"  Diffusion models achieve remarkable fidelity in image synthesis, yet precise\ncontrol over their outputs for targeted editing remains challenging. A key step\ntoward controllability is to identify interpretable directions in the model's\nlatent representations that correspond to semantic attributes. Existing\napproaches for finding interpretable directions typically rely on sampling\nlarge sets of images or training auxiliary networks, which limits efficiency.\nWe propose an analytical method that derives semantic editing directions\ndirectly from the pretrained parameters of diffusion models, requiring neither\nadditional data nor fine-tuning. Our insight is that self-attention weight\nmatrices encode rich structural information about the data distribution learned\nduring training. By computing the eigenvectors of these weight matrices, we\nobtain robust and interpretable editing directions. Experiments demonstrate\nthat our method produces high-quality edits across multiple datasets while\nreducing editing time significantly by 60% over current benchmarks.\n","authors":["Tharun Anand","Mohammad Hassan Vali","Arno Solin"],"pdf_url":"https://arxiv.org/pdf/2510.22650v1.pdf","comment":"4 pages (ICASSP Format)"},{"id":"http://arxiv.org/abs/2510.22647v1","updated":"2025-10-26T12:18:15Z","published":"2025-10-26T12:18:15Z","title":"A Critical Study on Tea Leaf Disease Detection using Deep Learning\n  Techniques","summary":"  The proposed solution is Deep Learning Technique that will be able classify\nthree types of tea leaves diseases from which two diseases are caused by the\npests and one due to pathogens (infectious organisms) and environmental\nconditions and also show the area damaged by a disease in leaves. Namely Red\nRust, Helopeltis and Red spider mite respectively. In this paper we have\nevaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for\nthe object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU\nrange of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.\nWhile Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95\nand recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than\nSSD. Also used Mask R-CNN for Object Instance Segmentation where we have\nimplemented our custom method to calculate the damaged diseased portion of\nleaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red\nSpider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.\n","authors":["Nabajyoti Borah","Raju Moni Borah","Bandan Boruah","Purnendu Bikash Acharjee","Sajal Saha","Ripjyoti Hazarika"],"pdf_url":"https://arxiv.org/pdf/2510.22647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04789v2","updated":"2025-10-26T12:06:58Z","published":"2025-06-05T09:14:42Z","title":"Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations","summary":"  Learning effective multi-modal 3D representations of objects is essential for\nnumerous applications, such as augmented reality and robotics. Existing methods\noften rely on task-specific embeddings that are tailored either for semantic\nunderstanding or geometric reconstruction. As a result, these embeddings\ntypically cannot be decoded into explicit geometry and simultaneously reused\nacross tasks. In this paper, we propose Object-X, a versatile multi-modal\nobject representation framework capable of encoding rich object embeddings\n(e.g. images, point cloud, text) and decoding them back into detailed geometric\nand visual reconstructions. Object-X operates by geometrically grounding the\ncaptured modalities in a 3D voxel grid and learning an unstructured embedding\nfusing the information from the voxels with the object attributes. The learned\nembedding enables 3D Gaussian Splatting-based object reconstruction, while also\nsupporting a range of downstream tasks, including scene alignment, single-image\n3D object reconstruction, and localization. Evaluations on two challenging\nreal-world datasets demonstrate that Object-X produces high-fidelity novel-view\nsynthesis comparable to standard 3D Gaussian Splatting, while significantly\nimproving geometric accuracy. Moreover, Object-X achieves competitive\nperformance with specialized methods in scene alignment and localization.\nCritically, our object-centric descriptors require 3-4 orders of magnitude less\nstorage compared to traditional image- or point cloud-based approaches,\nestablishing Object-X as a scalable and highly practical solution for\nmulti-modal 3D scene representation.\n","authors":["Gaia Di Lorenzo","Federico Tombari","Marc Pollefeys","Daniel Barath"],"pdf_url":"https://arxiv.org/pdf/2506.04789v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22630v1","updated":"2025-10-26T11:24:55Z","published":"2025-10-26T11:24:55Z","title":"Robust Atypical Mitosis Classification with DenseNet121: Stain-Aware\n  Augmentation and Hybrid Loss for Domain Generalization","summary":"  Atypical mitotic figures are important biomarkers of tumor aggressiveness in\nhistopathology, yet reliable recognition remains challenging due to severe\nclass imbalance and variability across imaging domains. We present a\nDenseNet-121-based framework tailored for atypical mitosis classification in\nthe MIDOG 2025 (Track 2) setting. Our method integrates stain-aware\naugmentation (Macenko), geometric and intensity transformations, and\nimbalance-aware learning via weighted sampling with a hybrid objective\ncombining class-weighted binary cross-entropy and focal loss. Trained\nend-to-end with AdamW and evaluated across multiple independent domains, the\nmodel demonstrates strong generalization under scanner and staining shifts,\nachieving balanced accuracy 85.0%, AUROC 0.927, sensitivity 89.2%, and\nspecificity 80.9% on the official test set. These results indicate that\ncombining DenseNet-121 with stain-aware augmentation and imbalance-adaptive\nobjectives yields a robust, domain-generalizable framework for atypical mitosis\nclassification suitable for real-world computational pathology workflows.\n","authors":["Adinath Dukre","Ankan Deria","Yutong Xie","Imran Razzak"],"pdf_url":"https://arxiv.org/pdf/2510.22630v1.pdf","comment":"MIDOG 2025 MICCAI Workshop accepted"},{"id":"http://arxiv.org/abs/2009.12991v5","updated":"2025-10-26T11:09:24Z","published":"2020-09-28T00:32:11Z","title":"Long-Tailed Classification by Keeping the Good and Removing the Bad\n  Momentum Causal Effect","summary":"  As the class size grows, maintaining a balanced dataset across many classes\nis challenging because the data are long-tailed in nature; it is even\nimpossible when the sample-of-interest co-exists with each other in one\ncollectable unit, e.g., multiple visual instances in one image. Therefore,\nlong-tailed classification is the key to deep learning at scale. However,\nexisting methods are mainly based on re-weighting/re-sampling heuristics that\nlack a fundamental theory. In this paper, we establish a causal inference\nframework, which not only unravels the whys of previous methods, but also\nderives a new principled solution. Specifically, our theory shows that the SGD\nmomentum is essentially a confounder in long-tailed classification. On one\nhand, it has a harmful causal effect that misleads the tail prediction biased\ntowards the head. On the other hand, its induced mediation also benefits the\nrepresentation learning and head prediction. Our framework elegantly\ndisentangles the paradoxical effects of the momentum, by pursuing the direct\ncausal effect caused by an input sample. In particular, we use causal\nintervention in training, and counterfactual reasoning in inference, to remove\nthe \"bad\" while keep the \"good\". We achieve new state-of-the-arts on three\nlong-tailed visual recognition benchmarks: Long-tailed CIFAR-10/-100,\nImageNet-LT for image classification and LVIS for instance segmentation.\n","authors":["Kaihua Tang","Jianqiang Huang","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2009.12991v5.pdf","comment":"This paper is accepted by NeurIPS 2020. The code is available on\n  GitHub: https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch"},{"id":"http://arxiv.org/abs/2002.11949v4","updated":"2025-10-26T11:07:57Z","published":"2020-02-27T07:29:53Z","title":"Unbiased Scene Graph Generation from Biased Training","summary":"  Today's scene graph generation (SGG) task is still far from practical, mainly\ndue to the severe training bias, e.g., collapsing diverse \"human walk on / sit\non / lay on beach\" into \"human on beach\". Given such SGG, the down-stream tasks\nsuch as VQA can hardly infer better scene structures than merely a bag of\nobjects. However, debiasing in SGG is not trivial because traditional debiasing\nmethods cannot distinguish between the good and bad bias, e.g., good context\nprior (e.g., \"person read book\" rather than \"eat\") and bad long-tailed bias\n(e.g., \"near\" dominating \"behind / in front of\"). In this paper, we present a\nnovel SGG framework based on causal inference but not the conventional\nlikelihood. We first build a causal graph for SGG, and perform traditional\nbiased training with the graph. Then, we propose to draw the counterfactual\ncausality from the trained graph to infer the effect from the bad bias, which\nshould be removed. In particular, we use Total Direct Effect (TDE) as the\nproposed final predicate score for unbiased SGG. Note that our framework is\nagnostic to any SGG model and thus can be widely applied in the community who\nseeks unbiased predictions. By using the proposed Scene Graph Diagnosis toolkit\non the SGG benchmark Visual Genome and several prevailing models, we observed\nsignificant improvements over the previous state-of-the-art methods.\n","authors":["Kaihua Tang","Yulei Niu","Jianqiang Huang","Jiaxin Shi","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2002.11949v4.pdf","comment":"This paper is accepted by CVPR 2020. The code is publicly available\n  on GitHub: https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch"},{"id":"http://arxiv.org/abs/2412.12772v3","updated":"2025-10-26T10:48:34Z","published":"2024-12-17T10:33:36Z","title":"Optimize the Unseen -- Fast NeRF Cleanup with Free Space Prior","summary":"  Neural Radiance Fields (NeRF) have advanced photorealistic novel view\nsynthesis, but their reliance on photometric reconstruction introduces\nartifacts, commonly known as \"floaters\". These artifacts degrade novel view\nquality, especially in areas unseen by the training cameras. We present a fast,\npost-hoc NeRF cleanup method that eliminates such artifacts by enforcing our\nFree Space Prior, effectively minimizing floaters without disrupting the NeRF's\nrepresentation of observed regions. Unlike existing approaches that rely on\neither Maximum Likelihood (ML) estimation to fit the data or a complex, local\ndata-driven prior, our method adopts a Maximum-a-Posteriori (MAP) approach,\nselecting the optimal model parameters under a simple global prior assumption\nthat unseen regions should remain empty. This enables our method to clean\nartifacts in both seen and unseen areas, enhancing novel view quality even in\nchallenging scene regions. Our method is comparable with existing NeRF cleanup\nmodels while being 2.5x faster in inference time, requires no additional memory\nbeyond the original NeRF, and achieves cleanup training in less than 30\nseconds. Our code will be made publically available.\n","authors":["Leo Segre","Shai Avidan"],"pdf_url":"https://arxiv.org/pdf/2412.12772v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22622v1","updated":"2025-10-26T10:40:52Z","published":"2025-10-26T10:40:52Z","title":"DeepfakeBench-MM: A Comprehensive Benchmark for Multimodal Deepfake\n  Detection","summary":"  The misuse of advanced generative AI models has resulted in the widespread\nproliferation of falsified data, particularly forged human-centric audiovisual\ncontent, which poses substantial societal risks (e.g., financial fraud and\nsocial instability). In response to this growing threat, several works have\npreliminarily explored countermeasures. However, the lack of sufficient and\ndiverse training data, along with the absence of a standardized benchmark,\nhinder deeper exploration. To address this challenge, we first build Mega-MMDF,\na large-scale, diverse, and high-quality dataset for multimodal deepfake\ndetection. Specifically, we employ 21 forgery pipelines through the combination\nof 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face\nreenactment methods. Mega-MMDF currently contains 0.1 million real samples and\n1.1 million forged samples, making it one of the largest and most diverse\nmultimodal deepfake datasets, with plans for continuous expansion. Building on\nit, we present DeepfakeBench-MM, the first unified benchmark for multimodal\ndeepfake detection. It establishes standardized protocols across the entire\ndetection pipeline and serves as a versatile platform for evaluating existing\nmethods as well as exploring novel approaches. DeepfakeBench-MM currently\nsupports 5 datasets and 11 multimodal deepfake detectors. Furthermore, our\ncomprehensive evaluations and in-depth analyses uncover several key findings\nfrom multiple perspectives (e.g., augmentation, stacked forgery). We believe\nthat DeepfakeBench-MM, together with our large-scale Mega-MMDF, will serve as\nfoundational infrastructures for advancing multimodal deepfake detection.\n","authors":["Kangran Zhao","Yupeng Chen","Xiaoyu Zhang","Yize Chen","Weinan Guan","Baicheng Chen","Chengzhe Sun","Soumyya Kanti Datta","Qingshan Liu","Siwei Lyu","Baoyuan Wu"],"pdf_url":"https://arxiv.org/pdf/2510.22622v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2510.22618v1","updated":"2025-10-26T10:31:22Z","published":"2025-10-26T10:31:22Z","title":"Cross-Species Transfer Learning in Agricultural AI: Evaluating ZebraPose\n  Adaptation for Dairy Cattle Pose Estimation","summary":"  Pose estimation serves as a cornerstone of computer vision for understanding\nanimal posture, behavior, and welfare. Yet, agricultural applications remain\nconstrained by the scarcity of large, annotated datasets for livestock,\nespecially dairy cattle. This study evaluates the potential and limitations of\ncross-species transfer learning by adapting ZebraPose - a vision\ntransformer-based model trained on synthetic zebra imagery - for 27-keypoint\ndetection in dairy cows under real barn conditions. Using three configurations\n- a custom on-farm dataset (375 images, Sussex, New Brunswick, Canada), a\nsubset of the APT-36K benchmark dataset, and their combination, we\nsystematically assessed model accuracy and generalization across environments.\nWhile the combined model achieved promising performance (AP = 0.86, AR = 0.87,\nPCK 0.5 = 0.869) on in-distribution data, substantial generalization failures\noccurred when applied to unseen barns and cow populations. These findings\nexpose the synthetic-to-real domain gap as a major obstacle to agricultural AI\ndeployment and emphasize that morphological similarity between species is\ninsufficient for cross-domain transfer. The study provides practical insights\ninto dataset diversity, environmental variability, and computational\nconstraints that influence real-world deployment of livestock monitoring\nsystems. We conclude with a call for agriculture-first AI design, prioritizing\nfarm-level realism, cross-environment robustness, and open benchmark datasets\nto advance trustworthy and scalable animal-centric technologies.\n","authors":["Mackenzie Tapp","Sibi Chakravarthy Parivendan","Kashfia Sailunaz","Suresh Neethirajan"],"pdf_url":"https://arxiv.org/pdf/2510.22618v1.pdf","comment":"20 pages, 11 figures, 6 Tables"},{"id":"http://arxiv.org/abs/2501.06488v3","updated":"2025-10-26T10:20:24Z","published":"2025-01-11T09:12:43Z","title":"NVS-SQA: Exploring Self-Supervised Quality Representation Learning for\n  Neurally Synthesized Scenes without References","summary":"  Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting,\neffectively creates photorealistic scenes from sparse viewpoints, typically\nevaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However,\nthese full-reference methods, which compare synthesized views to reference\nviews, may not fully capture the perceptual quality of neurally synthesized\nscenes (NSS), particularly due to the limited availability of dense reference\nviews. Furthermore, the challenges in acquiring human perceptual labels hinder\nthe creation of extensive labeled datasets, risking model overfitting and\nreduced generalizability. To address these issues, we propose NVS-SQA, a NSS\nquality assessment method to learn no-reference quality representations through\nself-supervision without reliance on human labels. Traditional self-supervised\nlearning predominantly relies on the \"same instance, similar representation\"\nassumption and extensive datasets. However, given that these conditions do not\napply in NSS quality assessment, we employ heuristic cues and quality scores as\nlearning objectives, along with a specialized contrastive pair preparation\nprocess to improve the effectiveness and efficiency of learning. The results\nshow that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e.,\non average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second\nbest) and even exceeds 16 full-reference methods across all evaluation metrics\n(i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).\n","authors":["Qiang Qu","Yiran Shen","Xiaoming Chen","Yuk Ying Chung","Weidong Cai","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2501.06488v3.pdf","comment":"Accepted by TPAMI"},{"id":"http://arxiv.org/abs/2510.22607v1","updated":"2025-10-26T10:05:48Z","published":"2025-10-26T10:05:48Z","title":"SWAN: Self-supervised Wavelet Neural Network for Hyperspectral Image\n  Unmixing","summary":"  In this article, we present SWAN: a three-stage, self-supervised wavelet\nneural network for joint estimation of endmembers and abundances from\nhyperspectral imagery. The contiguous and overlapping hyperspectral band images\nare first expanded to Biorthogonal wavelet basis space that provides sparse,\ndistributed, and multi-scale representations. The idea is to exploit latent\nsymmetries from thus obtained invariant and covariant features using a\nself-supervised learning paradigm. The first stage, SWANencoder maps the input\nwavelet coefficients to a compact lower-dimensional latent space. The second\nstage, SWANdecoder uses the derived latent representation to reconstruct the\ninput wavelet coefficients. Interestingly, the third stage SWANforward learns\nthe underlying physics of the hyperspectral image. A three-stage combined loss\nfunction is formulated in the image acquisition domain that eliminates the need\nfor ground truth and enables self-supervised training. Adam is employed for\noptimizing the proposed loss function, while Sigmoid with a dropout of 0.3 is\nincorporated to avoid possible overfitting. Kernel regularizers bound the\nmagnitudes and preserve spatial variations in the estimated endmember\ncoefficients. The output of SWANencoder represents estimated abundance maps\nduring inference, while weights of SWANdecoder are retrieved to extract\nendmembers. Experiments are conducted on two benchmark synthetic data sets with\ndifferent signal-to-noise ratios as well as on three real benchmark\nhyperspectral data sets while comparing the results with several\nstate-of-the-art neural network-based unmixing methods. The qualitative,\nquantitative, and ablation results show performance enhancement by learning a\nresilient unmixing function as well as promoting self-supervision and compact\nnetwork parameters for practical applications.\n","authors":["Yassh Ramchandani","Vijayashekhar S S","Jignesh S. Bhatt"],"pdf_url":"https://arxiv.org/pdf/2510.22607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22605v1","updated":"2025-10-26T10:00:27Z","published":"2025-10-26T10:00:27Z","title":"Projection Embedded Diffusion Bridge for CT Reconstruction from\n  Incomplete Data","summary":"  Reconstructing CT images from incomplete projection data remains challenging\ndue to the ill-posed nature of the problem. Diffusion bridge models have\nrecently shown promise in restoring clean images from their corresponding\nFiltered Back Projection (FBP) reconstructions, but incorporating data\nconsistency into these models remains largely underexplored. Incorporating data\nconsistency can improve reconstruction fidelity by aligning the reconstructed\nimage with the observed projection data, and can enhance detail recovery by\nintegrating structural information contained in the projections. In this work,\nwe propose the Projection Embedded Diffusion Bridge (PEDB). PEDB introduces a\nnovel reverse stochastic differential equation (SDE) to sample from the\ndistribution of clean images conditioned on both the FBP reconstruction and the\nincomplete projection data. By explicitly conditioning on the projection data\nin sampling the clean images, PEDB naturally incorporates data consistency. We\nembed the projection data into the score function of the reverse SDE. Under\ncertain assumptions, we derive a tractable expression for the posterior score.\nIn addition, we introduce a free parameter to control the level of\nstochasticity in the reverse process. We also design a discretization scheme\nfor the reverse SDE to mitigate discretization error. Extensive experiments\ndemonstrate that PEDB achieves strong performance in CT reconstruction from\nthree types of incomplete data, including sparse-view, limited-angle, and\ntruncated projections. For each of these types, PEDB outperforms evaluated\nstate-of-the-art diffusion bridge models across standard, noisy, and\ndomain-shift evaluations.\n","authors":["Yuang Wang","Pengfei Jin","Siyeop Yoon","Matthew Tivnan","Shaoyang Zhang","Li Zhang","Quanzheng Li","Zhiqiang Chen","Dufan Wu"],"pdf_url":"https://arxiv.org/pdf/2510.22605v1.pdf","comment":"53 pages, 7 figures, submitted to Medical Image Analysis"},{"id":"http://arxiv.org/abs/2502.09110v3","updated":"2025-10-26T09:49:49Z","published":"2025-02-13T09:40:26Z","title":"Pulling Back the Curtain: Unsupervised Adversarial Detection via\n  Contrastive Auxiliary Networks","summary":"  Deep learning models are widely employed in safety-critical applications yet\nremain susceptible to adversarial attacks -- imperceptible perturbations that\ncan significantly degrade model performance. Conventional defense mechanisms\npredominantly focus on either enhancing model robustness or detecting\nadversarial inputs independently. In this work, we propose an Unsupervised\nadversarial detection via Contrastive Auxiliary Networks (U-CAN) to uncover\nadversarial behavior within auxiliary feature representations, without the need\nfor adversarial examples. U-CAN is embedded within selected intermediate layers\nof the target model. These auxiliary networks, comprising projection layers and\nArcFace-based linear layers, refine feature representations to more effectively\ndistinguish between benign and adversarial inputs. Comprehensive experiments\nacross multiple datasets (CIFAR-10, Mammals, and a subset of ImageNet) and\narchitectures (ResNet-50, VGG-16, and ViT) demonstrate that our method\nsurpasses existing unsupervised adversarial detection techniques, achieving\nsuperior F1 scores against four distinct attack methods. The proposed framework\nprovides a scalable and effective solution for enhancing the security and\nreliability of deep learning systems.\n","authors":["Eylon Mizrahi","Raz Lapid","Moshe Sipper"],"pdf_url":"https://arxiv.org/pdf/2502.09110v3.pdf","comment":"Accepted for Oral Presentation at SafeMM-AI @ ICCV 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.22603v1","updated":"2025-10-26T09:44:20Z","published":"2025-10-26T09:44:20Z","title":"Mitigating Attention Sinks and Massive Activations in Audio-Visual\n  Speech Recognition with LLMS","summary":"  Large language models (LLMs) have recently advanced auditory speech\nrecognition (ASR), visual speech recognition (VSR), and audio-visual speech\nrecognition (AVSR). However, understanding of their internal dynamics under\nfine-tuning remains limited. In natural language processing, recent work has\nrevealed attention sinks, tokens that attract disproportionately high\nattention, and associated massive activations in which some features of sink\ntokens exhibit huge activation in LLMs. In this work, we are the first to study\nthese phenomena in multimodal speech recognition. Through a detailed analysis\nof audio-visual LLMs, we identify attention sinks and massive activations not\nonly at the BOS token but also at intermediate low-semantic tokens across ASR,\nVSR, and AVSR. We show that massive activations originate in the MLP layers and\ncorrespond to fixed feature indices across all sink tokens. We further show\nthat intermediate sink tokens exhibit high cosine similarity to the BOS token,\nthereby amplifying attention and activation. Building on these insights, we\nintroduce a simple decorrelation loss that reduces cosine similarity between\nBOS and other tokens, effectively mitigating intermediate sinks and massive\nactivations. Furthermore, our method improves word error rate (WER) under high\naudio-visual feature downsampling while remaining stable at lower downsampling\nrates.\n","authors":[" Anand","Umberto Cappellazzo","Stavros Petridis","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2510.22603v1.pdf","comment":"The code is available at\n  https://github.com/umbertocappellazzo/Llama-AVSR"},{"id":"http://arxiv.org/abs/2510.20819v2","updated":"2025-10-26T09:13:56Z","published":"2025-10-23T17:59:54Z","title":"Towards General Modality Translation with Contrastive and Predictive\n  Latent Diffusion Bridge","summary":"  Recent advances in generative modeling have positioned diffusion models as\nstate-of-the-art tools for sampling from complex data distributions. While\nthese models have shown remarkable success across single-modality domains such\nas images and audio, extending their capabilities to Modality Translation (MT),\ntranslating information across different sensory modalities, remains an open\nchallenge. Existing approaches often rely on restrictive assumptions, including\nshared dimensionality, Gaussian source priors, and modality-specific\narchitectures, which limit their generality and theoretical grounding. In this\nwork, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a\ngeneral-purpose framework for modality translation based on a latent-variable\nextension of Denoising Diffusion Bridge Models. By operating in a shared latent\nspace, our method learns a bridge between arbitrary modalities without\nrequiring aligned dimensions. We introduce a contrastive alignment loss to\nenforce semantic consistency between paired samples and design a\ndomain-agnostic encoder-decoder architecture tailored for noise prediction in\nlatent space. Additionally, we propose a predictive loss to guide training\ntoward accurate cross-domain translation and explore several training\nstrategies to improve stability. Our approach supports arbitrary modality pairs\nand performs strongly on diverse MT tasks, including multi-view to 3D shape\ngeneration, image super-resolution, and multi-view scene synthesis.\nComprehensive experiments and ablations validate the effectiveness of our\nframework, establishing a new strong baseline in general modality translation.\nFor more information, see our project page:\nhttps://sites.google.com/view/lddbm/home.\n","authors":["Nimrod Berman","Omkar Joglekar","Eitan Kosman","Dotan Di Castro","Omri Azencot"],"pdf_url":"https://arxiv.org/pdf/2510.20819v2.pdf","comment":"Accepted as a poster at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.22589v1","updated":"2025-10-26T09:09:52Z","published":"2025-10-26T09:09:52Z","title":"PSScreen V2: Partially Supervised Multiple Retinal Disease Screening","summary":"  In this work, we propose PSScreen V2, a partially supervised self-training\nframework for multiple retinal disease screening. Unlike previous methods that\nrely on fully labelled or single-domain datasets, PSScreen V2 is designed to\nlearn from multiple partially labelled datasets with different distributions,\naddressing both label absence and domain shift challenges. To this end,\nPSScreen V2 adopts a three-branch architecture with one teacher and two student\nnetworks. The teacher branch generates pseudo labels from weakly augmented\nimages to address missing labels, while the two student branches introduce\nnovel feature augmentation strategies: Low-Frequency Dropout (LF-Dropout),\nwhich enhances domain robustness by randomly discarding domain-related\nlow-frequency components, and Low-Frequency Uncertainty (LF-Uncert), which\nestimates uncertain domain variability via adversarially learned Gaussian\nperturbations of low-frequency statistics. Extensive experiments on multiple\nin-domain and out-of-domain fundus datasets demonstrate that PSScreen V2\nachieves state-of-the-art performance and superior domain generalization\nability. Furthermore, compatibility tests with diverse backbones, including the\nvision foundation model DINOv2, as well as evaluations on chest X-ray datasets,\nhighlight the universality and adaptability of the proposed framework. The\ncodes are available at https://github.com/boyiZheng99/PSScreen_V2.\n","authors":["Boyi Zheng","Yalin Zheng","Hrvoje Bogunović","Qing Liu"],"pdf_url":"https://arxiv.org/pdf/2510.22589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22582v1","updated":"2025-10-26T08:47:20Z","published":"2025-10-26T08:47:20Z","title":"Cross-View UAV Geo-Localization with Precision-Focused Efficient Design:\n  A Hierarchical Distillation Approach with Multi-view Refinement","summary":"  Cross-view geo-localization (CVGL) enables UAV localization by matching\naerial images to geo-tagged satellite databases, which is critical for\nautonomous navigation in GNSS-denied environments. However, existing methods\nrely on resource-intensive fine-grained feature extraction and alignment, where\nmultiple branches and modules significantly increase inference costs, limiting\ntheir deployment on edge devices. We propose Precision-Focused Efficient Design\n(PFED), a resource-efficient framework combining hierarchical knowledge\ntransfer and multi-view representation refinement. This innovative method\ncomprises two key components: 1) During training, Hierarchical Distillation\nparadigm for fast and accurate CVGL (HD-CVGL), coupled with Uncertainty-Aware\nPrediction Alignment (UAPA) to distill essential information and mitigate the\ndata imbalance without incurring additional inference overhead. 2) During\ninference, an efficient Multi-view Refinement Module (MRM) leverages mutual\ninformation to filter redundant samples and effectively utilize the multi-view\ndata. Extensive experiments show that PFED achieves state-of-the-art\nperformance in both accuracy and efficiency, reaching 97.15\\% Recall@1 on\nUniversity-1652 while being over $5 \\times$ more efficient in FLOPs and $3\n\\times$ faster than previous top methods. Furthermore, PFED runs at 251.5 FPS\non the AGX Orin edge device, demonstrating its practical viability for\nreal-time UAV applications. The project is available at\nhttps://github.com/SkyEyeLoc/PFED\n","authors":["Jian Sun","Kangdao Liu","Chi Zhang","Chuangquan Chen","Junge Shen","Chi-Man Vong"],"pdf_url":"https://arxiv.org/pdf/2510.22582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.19612v2","updated":"2025-10-26T08:46:44Z","published":"2025-10-22T14:05:25Z","title":"Beyond sparse denoising in frames: minimax estimation with a scattering\n  transform","summary":"  A considerable amount of research in harmonic analysis has been devoted to\nnon-linear estimators of signals contaminated by additive Gaussian noise. They\nare implemented by thresholding coefficients in a frame, which provide a sparse\nsignal representation, or by minimising their $\\ell^1$ norm. However, sparse\nestimators in frames are not sufficiently rich to adapt to complex signal\nregularities. For cartoon images whose edges are piecewise $\\bf C^\\alpha$\ncurves, wavelet, curvelet and Xlet frames are suboptimal if the Lipschitz\nexponent $\\alpha \\leq 2$ is an unknown parameter. Deep convolutional neural\nnetworks have recently obtained much better numerical results, which reach the\nminimax asymptotic bounds for all $\\alpha$. Wavelet scattering coefficients\nhave been introduced as simplified convolutional neural network models. They\nare computed by transforming the modulus of wavelet coefficients with a second\nwavelet transform. We introduce a denoising estimator by jointly minimising and\nmaximising the $\\ell^1$ norms of different subsets of scattering coefficients.\nWe prove that these $\\ell^1$ norms capture different types of geometric image\nregularity. Numerical experiments show that this denoising estimator reaches\nthe minimax asymptotic bound for cartoon images for all Lipschitz exponents\n$\\alpha \\leq 2$. We state this numerical result as a mathematical conjecture.\nIt provides a different harmonic analysis approach to suppress noise from\nsignals, and to specify the geometric regularity of functions. It also opens a\nmathematical bridge between harmonic analysis and denoising estimators with\ndeep convolutional network.\n","authors":["Nathanaël Cuvelle--Magar","Stéphane Mallat"],"pdf_url":"https://arxiv.org/pdf/2510.19612v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22577v1","updated":"2025-10-26T08:28:05Z","published":"2025-10-26T08:28:05Z","title":"From Pixels to Views: Learning Angular-Aware and Physics-Consistent\n  Representations for Light Field Microscopy","summary":"  Light field microscopy (LFM) has become an emerging tool in neuroscience for\nlarge-scale neural imaging in vivo, notable for its single-exposure volumetric\nimaging, broad field of view, and high temporal resolution. However,\nlearning-based 3D reconstruction in XLFM remains underdeveloped due to two core\nchallenges: the absence of standardized datasets and the lack of methods that\ncan efficiently model its angular-spatial structure while remaining physically\ngrounded. We address these challenges by introducing three key contributions.\nFirst, we construct the XLFM-Zebrafish benchmark, a large-scale dataset and\nevaluation suite for XLFM reconstruction. Second, we propose Masked View\nModeling for Light Fields (MVN-LF), a self-supervised task that learns angular\npriors by predicting occluded views, improving data efficiency. Third, we\nformulate the Optical Rendering Consistency Loss (ORC Loss), a differentiable\nrendering constraint that enforces alignment between predicted volumes and\ntheir PSF-based forward projections. On the XLFM-Zebrafish benchmark, our\nmethod improves PSNR by 7.7% over state-of-the-art baselines.\n","authors":["Feng He","Guodong Tan","Qiankun Li","Jun Yu","Quan Wen"],"pdf_url":"https://arxiv.org/pdf/2510.22577v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.13757v2","updated":"2025-10-26T08:23:58Z","published":"2025-06-16T17:58:50Z","title":"AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous\n  Driving with Adaptive Reasoning and Reinforcement Fine-Tuning","summary":"  Recent advancements in Vision-Language-Action (VLA) models have shown promise\nfor end-to-end autonomous driving by leveraging world knowledge and reasoning\ncapabilities. However, current VLA models often struggle with physically\ninfeasible action outputs, complex model structures, or unnecessarily long\nreasoning. In this paper, we propose AutoVLA, a novel VLA model that unifies\nreasoning and action generation within a single autoregressive generation model\nfor end-to-end autonomous driving. AutoVLA performs semantic reasoning and\ntrajectory planning directly from raw visual inputs and language instructions.\nWe tokenize continuous trajectories into discrete, feasible actions, enabling\ndirect integration into the language model. For training, we employ supervised\nfine-tuning to equip the model with dual thinking modes: fast thinking\n(trajectory-only) and slow thinking (enhanced with chain-of-thought reasoning).\nTo further enhance planning performance and efficiency, we introduce a\nreinforcement fine-tuning method based on Group Relative Policy Optimization\n(GRPO), reducing unnecessary reasoning in straightforward scenarios. Extensive\nexperiments across real-world and simulated datasets and benchmarks, including\nnuPlan, nuScenes, Waymo, and CARLA, demonstrate the competitive performance of\nAutoVLA in both open-loop and closed-loop settings. Qualitative results\nshowcase the adaptive reasoning and accurate planning capabilities of AutoVLA\nin diverse scenarios.\n","authors":["Zewei Zhou","Tianhui Cai","Seth Z. Zhao","Yun Zhang","Zhiyu Huang","Bolei Zhou","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2506.13757v2.pdf","comment":"NeurIPS 2025; Website link:https://autovla.github.io/"},{"id":"http://arxiv.org/abs/2510.22575v1","updated":"2025-10-26T08:18:16Z","published":"2025-10-26T08:18:16Z","title":"MELDAE: A Framework for Micro-Expression Spotting, Detection, and\n  Automatic Evaluation in In-the-Wild Conversational Scenes","summary":"  Accurately analyzing spontaneous, unconscious micro-expressions is crucial\nfor revealing true human emotions, but this task remains challenging in wild\nscenarios, such as natural conversation. Existing research largely relies on\ndatasets from controlled laboratory environments, and their performance\ndegrades dramatically in the real world. To address this issue, we propose\nthree contributions: the first micro-expression dataset focused on\nconversational-in-the-wild scenarios; an end-to-end localization and detection\nframework, MELDAE; and a novel boundary-aware loss function that improves\ntemporal accuracy by penalizing onset and offset errors. Extensive experiments\ndemonstrate that our framework achieves state-of-the-art results on the WDMD\ndataset, improving the key F1_{DR} localization metric by 17.72% over the\nstrongest baseline, while also demonstrating excellent generalization\ncapabilities on existing benchmarks.\n","authors":["Yigui Feng","Qinglin Wang","Yang Liu","Ke Liu","Haotian Mo","Enhao Huang","Gencheng Liu","Mingzhe Liu","Jie Liu"],"pdf_url":"https://arxiv.org/pdf/2510.22575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09744v3","updated":"2025-10-26T08:14:10Z","published":"2024-08-19T07:15:44Z","title":"RealCustom++: Representing Images as Real Textual Word for Real-Time\n  Customization","summary":"  Given a text and an image of a specific subject, text-to-image customization\naims to generate new images that align with both the text and the subject's\nappearance. Existing works follow the pseudo-word paradigm, which represents\nthe subject as a non-existent pseudo word and combines it with other text to\ngenerate images. However, the pseudo word causes semantic conflict from its\ndifferent learning objective and entanglement from overlapping influence scopes\nwith other texts, resulting in a dual-optimum paradox where subject similarity\nand text controllability cannot be optimal simultaneously. To address this, we\npropose RealCustom++, a novel real-word paradigm that represents the subject\nwith a non-conflicting real word to firstly generate a coherent guidance image\nand corresponding subject mask, thereby disentangling the influence scopes of\nthe text and subject for simultaneous optimization. Specifically, RealCustom++\nintroduces a train-inference decoupled framework: (1) during training, it\nlearns a general alignment between visual conditions and all real words in the\ntext; and (2) during inference, a dual-branch architecture is employed, where\nthe Guidance Branch produces the subject guidance mask and the Generation\nBranch utilizes this mask to customize the generation of the specific real word\nexclusively within subject-relevant regions. In contrast to previous methods\nthat excel in either controllability or similarity, RealCustom++ achieves\nsuperior performance in both, with improvements of 7.48% in controllability,\n3.04% in similarity, and 76.43% in generation quality. For multi-subject\ncustomization, RealCustom++ further achieves improvements of 4.6% in\ncontrollability and 6.34% in multi-subject similarity. Our work has been\napplied in JiMeng of ByteDance, and codes are released at\nhttps://github.com/bytedance/RealCustom.\n","authors":["Zhendong Mao","Mengqi Huang","Fei Ding","Mingcong Liu","Qian He","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.09744v3.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2505.21076v2","updated":"2025-10-26T08:05:47Z","published":"2025-05-27T12:01:19Z","title":"DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic\n  City Understanding","summary":"  Multimodal large language models (MLLMs) have demonstrated remarkable\ncapabilities in visual understanding, but their application to long-term Earth\nobservation analysis remains limited, primarily focusing on single-temporal or\nbi-temporal imagery. To address this gap, we introduce DVL-Suite, a\ncomprehensive framework for analyzing long-term urban dynamics through remote\nsensing imagery. Our suite comprises 14,871 high-resolution (1.0m)\nmulti-temporal images spanning 42 major cities in the U.S. from 2005 to 2023,\norganized into two components: DVL-Bench and DVL-Instruct. The DVL-Bench\nincludes six urban understanding tasks, from fundamental change detection\n(pixel-level) to quantitative analyses (regional-level) and comprehensive urban\nnarratives (scene-level), capturing diverse urban dynamics including\nexpansion/transformation patterns, disaster assessment, and environmental\nchallenges. We evaluate 18 state-of-the-art MLLMs and reveal their limitations\nin long-term temporal understanding and quantitative analysis. These challenges\nmotivate the creation of DVL-Instruct, a specialized instruction-tuning dataset\ndesigned to enhance models' capabilities in multi-temporal Earth observation.\nBuilding upon this dataset, we develop DVLChat, a baseline model capable of\nboth image-level question-answering and pixel-level segmentation, facilitating\na comprehensive understanding of city dynamics through language interactions.\n","authors":["Weihao Xuan","Junjue Wang","Heli Qi","Zihang Chen","Zhuo Zheng","Yanfei Zhong","Junshi Xia","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2505.21076v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.22571v1","updated":"2025-10-26T08:04:28Z","published":"2025-10-26T08:04:28Z","title":"STATUS Bench: A Rigorous Benchmark for Evaluating Object State\n  Understanding in Vision-Language Models","summary":"  Object state recognition aims to identify the specific condition of objects,\nsuch as their positional states (e.g., open or closed) and functional states\n(e.g., on or off). While recent Vision-Language Models (VLMs) are capable of\nperforming a variety of multimodal tasks, it remains unclear how precisely they\ncan identify object states. To alleviate this issue, we introduce the STAte and\nTransition UnderStanding Benchmark (STATUS Bench), the first benchmark for\nrigorously evaluating the ability of VLMs to understand subtle variations in\nobject states in diverse situations. Specifically, STATUS Bench introduces a\nnovel evaluation scheme that requires VLMs to perform three tasks\nsimultaneously: object state identification (OSI), image retrieval (IR), and\nstate change identification (SCI). These tasks are defined over our fully\nhand-crafted dataset involving image pairs, their corresponding object state\ndescriptions and state change descriptions. Furthermore, we introduce a\nlarge-scale training dataset, namely STATUS Train, which consists of 13 million\nsemi-automatically created descriptions. This dataset serves as the largest\nresource to facilitate further research in this area. In our experiments, we\ndemonstrate that STATUS Bench enables rigorous consistency evaluation and\nreveal that current state-of-the-art VLMs still significantly struggle to\ncapture subtle object state distinctions. Surprisingly, under the proposed\nrigorous evaluation scheme, most open-weight VLMs exhibited chance-level\nzero-shot performance. After fine-tuning on STATUS Train, Qwen2.5-VL achieved\nperformance comparable to Gemini 2.0 Flash. These findings underscore the\nnecessity of STATUS Bench and Train for advancing object state recognition in\nVLM research.\n","authors":["Mahiro Ukai","Shuhei Kurita","Nakamasa Inoue"],"pdf_url":"https://arxiv.org/pdf/2510.22571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23660v1","updated":"2025-10-26T08:01:34Z","published":"2025-10-26T08:01:34Z","title":"Quanvolutional Neural Networks for Pneumonia Detection: An Efficient\n  Quantum-Assisted Feature Extraction Paradigm","summary":"  Pneumonia poses a significant global health challenge, demanding accurate and\ntimely diagnosis. While deep learning, particularly Convolutional Neural\nNetworks (CNNs), has shown promise in medical image analysis for pneumonia\ndetection, CNNs often suffer from high computational costs, limitations in\nfeature representation, and challenges in generalizing from smaller datasets.\nTo address these limitations, we explore the application of Quanvolutional\nNeural Networks (QNNs), leveraging quantum computing for enhanced feature\nextraction. This paper introduces a novel hybrid quantum-classical model for\npneumonia detection using the PneumoniaMNIST dataset. Our approach utilizes a\nquanvolutional layer with a parameterized quantum circuit (PQC) to process 2x2\nimage patches, employing rotational Y-gates for data encoding and entangling\nlayers to generate non-classical feature representations. These\nquantum-extracted features are then fed into a classical neural network for\nclassification. Experimental results demonstrate that the proposed QNN achieves\na higher validation accuracy of 83.33 percent compared to a comparable\nclassical CNN which achieves 73.33 percent. This enhanced convergence and\nsample efficiency highlight the potential of QNNs for medical image analysis,\nparticularly in scenarios with limited labeled data. This research lays the\nfoundation for integrating quantum computing into deep-learning-driven medical\ndiagnostic systems, offering a computationally efficient alternative to\ntraditional approaches.\n","authors":["Gazi Tanbhir","Md. Farhan Shahriyar","Abdullah Md Raihan Chy"],"pdf_url":"https://arxiv.org/pdf/2510.23660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23659v1","updated":"2025-10-26T07:52:53Z","published":"2025-10-26T07:52:53Z","title":"Quantum Machine Learning for Image Classification: A Hybrid Model of\n  Residual Network with Quantum Support Vector Machine","summary":"  Recently, there has been growing attention on combining quantum machine\nlearning (QML) with classical deep learning approaches, as computational\ntechniques are key to improving the performance of image classification tasks.\nThis study presents a hybrid approach that uses ResNet-50 (Residual Network)\nfor feature extraction and Quantum Support Vector Machines (QSVM) for\nclassification in the context of potato disease detection. Classical machine\nlearning as well as deep learning models often struggle with high-dimensional\nand complex datasets, necessitating advanced techniques like quantum computing\nto improve classification efficiency. In our research, we use ResNet-50 to\nextract deep feature representations from RGB images of potato diseases. These\nfeatures are then subjected to dimensionality reduction using Principal\nComponent Analysis (PCA). The resulting features are processed through QSVM\nmodels which apply various quantum feature maps such as ZZ, Z, and Pauli-X to\ntransform classical data into quantum states. To assess the model performance,\nwe compared it with classical machine learning algorithms such as Support\nVector Machine (SVM) and Random Forest (RF) using five-fold stratified\ncross-validation for comprehensive evaluation. The experimental results\ndemonstrate that the Z-feature map-based QSVM outperforms classical models,\nachieving an accuracy of 99.23 percent, surpassing both SVM and RF models. This\nresearch highlights the advantages of integrating quantum computing into image\nclassification and provides a potential disease detection solution through\nhybrid quantum-classical modeling.\n","authors":["Md. Farhan Shahriyar","Gazi Tanbhir","Abdullah Md Raihan Chy"],"pdf_url":"https://arxiv.org/pdf/2510.23659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22565v1","updated":"2025-10-26T07:44:26Z","published":"2025-10-26T07:44:26Z","title":"Learning Event-guided Exposure-agnostic Video Frame Interpolation via\n  Adaptive Feature Blending","summary":"  Exposure-agnostic video frame interpolation (VFI) is a challenging task that\naims to recover sharp, high-frame-rate videos from blurry, low-frame-rate\ninputs captured under unknown and dynamic exposure conditions. Event cameras\nare sensors with high temporal resolution, making them especially advantageous\nfor this task. However, existing event-guided methods struggle to produce\nsatisfactory results on severely low-frame-rate blurry videos due to the lack\nof temporal constraints. In this paper, we introduce a novel event-guided\nframework for exposure-agnostic VFI, addressing this limitation through two key\ncomponents: a Target-adaptive Event Sampling (TES) and a Target-adaptive\nImportance Mapping (TIM). Specifically, TES samples events around the target\ntimestamp and the unknown exposure time to better align them with the\ncorresponding blurry frames. TIM then generates an importance map that\nconsiders the temporal proximity and spatial relevance of consecutive features\nto the target. Guided by this map, our framework adaptively blends consecutive\nfeatures, allowing temporally aligned features to serve as the primary cues\nwhile spatially relevant ones offer complementary support. Extensive\nexperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of our approach in exposure-agnostic VFI scenarios.\n","authors":["Junsik Jung","Yoonki Cho","Woo Jae Kim","Lin Wang","Sune-eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2510.22565v1.pdf","comment":"Accepted for BMVC2025"},{"id":"http://arxiv.org/abs/2508.06937v2","updated":"2025-10-26T07:19:20Z","published":"2025-08-09T11:06:58Z","title":"CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing","summary":"  Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses this\ntrilemma through two key innovations. First, Selective Canny Control applies\nstructural guidance from a Canny ControlNet only to the unedited regions,\npreserving the original image's details while allowing for precise, text-driven\nchanges in the specified editable area. Second, Dual-Prompt Guidance utilizes\nboth a local prompt for the specific edit and a global prompt for overall scene\ncoherence. Through this synergistic approach, these components enable\ncontrollable local editing for object addition, replacement, and removal,\nachieving a superior trade-off among text adherence, context fidelity, and\nediting seamlessness compared to current region-based methods. Beyond this,\nCannyEdit offers exceptional flexibility: it operates effectively with rough\nmasks or even single-point hints in addition tasks. Furthermore, the framework\ncan seamlessly integrate with vision-language models in a training-free manner\nfor complex instruction-based editing that requires planning and reasoning. Our\nextensive evaluations demonstrate CannyEdit's strong performance against\nleading instruction-based editors in complex object addition scenarios.\n","authors":["Weiyan Xie","Han Gao","Didan Deng","Kaican Li","April Hua Liu","Yongxiang Huang","Nevin L. Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.06937v2.pdf","comment":"Project Page: vaynexie.github.io/CannyEdit/; MindSpore Code:\n  github.com/mindspore-lab/mindone/tree/master/examples/canny_edit; PyTorch\n  Code: github.com/vaynexie/CannyEdit"},{"id":"http://arxiv.org/abs/2505.15510v2","updated":"2025-10-26T06:24:15Z","published":"2025-05-21T13:29:58Z","title":"Visual Thoughts: A Unified Perspective of Understanding Multimodal\n  Chain-of-Thought","summary":"  Large Vision-Language Models (LVLMs) have achieved significant success in\nmultimodal tasks, with multimodal chain-of-thought (MCoT) further enhancing\nperformance and interpretability. Recent MCoT methods fall into two categories:\n(i) Textual-MCoT (T-MCoT), which takes multimodal input and produces textual\noutput; and (ii) Interleaved-MCoT (I-MCoT), which generates interleaved\nimage-text outputs. Despite advances in both approaches, the mechanisms driving\nthese improvements are not fully understood. To fill this gap, we first reveal\nthat MCoT boosts LVLMs by incorporating visual thoughts, which convey image\ninformation to the reasoning process regardless of the MCoT format, depending\nonly on clarity and conciseness of expression. Furthermore, to explore visual\nthoughts systematically, we define four distinct forms of visual thought\nexpressions and analyze them comprehensively. Our findings demonstrate that\nthese forms differ in clarity and conciseness, yielding varying levels of MCoT\nimprovement. Additionally, we explore the internal nature of visual thoughts,\nfinding that visual thoughts serve as intermediaries between the input image\nand reasoning to deeper transformer layers, enabling more advanced visual\ninformation transmission. We hope that the visual thoughts can inspire further\nbreakthroughs for future MCoT research.\n","authors":["Zihui Cheng","Qiguang Chen","Xiao Xu","Jiaqi Wang","Weiyun Wang","Hao Fei","Yidong Wang","Alex Jinpeng Wang","Zhi Chen","Wanxiang Che","Libo Qin"],"pdf_url":"https://arxiv.org/pdf/2505.15510v2.pdf","comment":"Accepted at NeurIPS 2025;"},{"id":"http://arxiv.org/abs/2509.23729v2","updated":"2025-10-26T06:17:42Z","published":"2025-09-28T08:20:00Z","title":"LUQ: Layerwise Ultra-Low Bit Quantization for Multimodal Large Language\n  Models","summary":"  Large Language Models (LLMs) with multimodal capabilities have revolutionized\nvision-language tasks, but their deployment often requires huge memory and\ncomputational resources. While post-training quantization (PTQ) has\nsuccessfully compressed language models to as low as 1-bit precision without\nsignificant performance loss, its effectiveness for multimodal LLMs (MLLMs)\nremains relatively unexplored. In this paper, we present the first study on\nultra-low bit (<4-bit) quantization for multimodal LLMs. Our analysis reveals\nthat multimodal tokens and intermediate layer activations produced by them\nexhibit significantly higher statistical variance and entropy compared to text\ntokens, making them less tolerant to ultra-low bit quantization. However, the\nactivation distributions of multimodal tokens varies significantly over\ndifferent layers, with some layers having lower entropy activation\ndistributions. We empirically show that such layers in these models can better\ntolerate ultra-low bit quantization. Building on these insights, we propose a\nnovel strategy for MLLM quantization, LUQ: Layerwise Ultra-Low Bit\nQuantization, which selectively applies ultra-low bit quantization to layers\nthat are more resilient to it. Additionally, we also show that using a mix of\nmultimodal tokens (image and text) for PTQ boosts VQA performance in the\nultra-low bit regime. We evaluate our method on LLaVA-1.5 and Qwen-2.5-VL\nacross 9 popular VQA benchmarks. The resulting LUQ models use 40% and 31% less\nmemory than their 4-bit counterparts, respectively, while exhibiting a\nperformance degradation of less than 10% on the MME benchmark.\n","authors":["Shubhang Bhatnagar","Andy Xu","Kar-Han Tan","Narendra Ahuja"],"pdf_url":"https://arxiv.org/pdf/2509.23729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07246v6","updated":"2025-10-26T05:55:28Z","published":"2024-08-14T01:16:40Z","title":"ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area","summary":"  Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B.\n","authors":["Junxian Li","Di Zhang","Xunzhi Wang","Zeying Hao","Jingdi Lei","Qian Tan","Cai Zhou","Wei Liu","Yaotian Yang","Xinrui Xiong","Weiyun Wang","Zhe Chen","Wenhai Wang","Wei Li","Shufei Zhang","Mao Su","Wanli Ouyang","Yuqiang Li","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.07246v6.pdf","comment":"11 pages, updated version"},{"id":"http://arxiv.org/abs/2505.14677v3","updated":"2025-10-26T05:33:58Z","published":"2025-05-20T17:58:35Z","title":"Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning","summary":"  Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.\n","authors":["Jiaer Xia","Yuhang Zang","Peng Gao","Sharon Li","Kaiyang Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.14677v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22534v1","updated":"2025-10-26T05:03:55Z","published":"2025-10-26T05:03:55Z","title":"SRSR: Enhancing Semantic Accuracy in Real-World Image Super-Resolution\n  with Spatially Re-Focused Text-Conditioning","summary":"  Existing diffusion-based super-resolution approaches often exhibit semantic\nambiguities due to inaccuracies and incompleteness in their text conditioning,\ncoupled with the inherent tendency for cross-attention to divert towards\nirrelevant pixels. These limitations can lead to semantic misalignment and\nhallucinated details in the generated high-resolution outputs. To address\nthese, we propose a novel, plug-and-play spatially re-focused super-resolution\n(SRSR) framework that consists of two core components: first, we introduce\nSpatially Re-focused Cross-Attention (SRCA), which refines text conditioning at\ninference time by applying visually-grounded segmentation masks to guide\ncross-attention. Second, we introduce a Spatially Targeted Classifier-Free\nGuidance (STCFG) mechanism that selectively bypasses text influences on\nungrounded pixels to prevent hallucinations. Extensive experiments on both\nsynthetic and real-world datasets demonstrate that SRSR consistently\noutperforms seven state-of-the-art baselines in standard fidelity metrics (PSNR\nand SSIM) across all datasets, and in perceptual quality measures (LPIPS and\nDISTS) on two real-world benchmarks, underscoring its effectiveness in\nachieving both high semantic fidelity and perceptual quality in\nsuper-resolution.\n","authors":["Chen Chen","Majid Abdolshah","Violetta Shevchenko","Hongdong Li","Chang Xu","Pulak Purkait"],"pdf_url":"https://arxiv.org/pdf/2510.22534v1.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2412.11762v2","updated":"2025-10-26T05:03:24Z","published":"2024-12-16T13:26:52Z","title":"GS-ProCams: Gaussian Splatting-based Projector-Camera Systems","summary":"  We present GS-ProCams, the first Gaussian Splatting-based framework for\nprojector-camera systems (ProCams). GS-ProCams is not only view-agnostic but\nalso significantly enhances the efficiency of projection mapping (PM) that\nrequires establishing geometric and radiometric mappings between the projector\nand the camera. Previous CNN-based ProCams are constrained to a specific\nviewpoint, limiting their applicability to novel perspectives. In contrast,\nNeRF-based ProCams support view-agnostic projection mapping, however, they\nrequire an additional co-located light source and demand significant\ncomputational and memory resources. To address this issue, we propose\nGS-ProCams that employs 2D Gaussian for scene representations, and enables\nefficient view-agnostic ProCams applications. In particular, we explicitly\nmodel the complex geometric and photometric mappings of ProCams using projector\nresponses, the projection surface's geometry and materials represented by\nGaussians, and the global illumination component. Then, we employ\ndifferentiable physically-based rendering to jointly estimate them from\ncaptured multi-view projections. Compared to state-of-the-art NeRF-based\nmethods, our GS-ProCams eliminates the need for additional devices, achieving\nsuperior ProCams simulation quality. It also uses only 1/10 of the GPU memory\nfor training and is 900 times faster in inference speed. Please refer to our\nproject page for the code and dataset:\nhttps://realqingyue.github.io/GS-ProCams/.\n","authors":["Qingyue Deng","Jijiang Li","Haibin Ling","Bingyao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11762v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01481v4","updated":"2025-10-26T04:54:22Z","published":"2025-05-02T15:58:38Z","title":"VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on\n  Synthetic Video Understanding","summary":"  Vision-Language Models (VLMs) have achieved strong results in video\nunderstanding, yet a key question remains: do they truly comprehend visual\ncontent or only learn shallow correlations between vision and language? Real\nvisual understanding, especially of physics and common sense, is essential for\nAI systems that interact with the physical world. Current evaluations mostly\nuse real-world videos similar to training data, so high benchmark scores may\nnot reflect real reasoning ability. To address this, we propose\nnegative-control tests using videos that depict physically impossible or\nlogically inconsistent events. We introduce VideoHallu, a synthetic dataset of\nphysics- and commonsense-violating scenes generated with Veo2, Sora, and Kling.\nIt includes expert-annotated question-answer pairs across four categories of\nviolations. Tests of leading VLMs (Qwen-2.5-VL, Video-R1, VideoChat-R1) show\nthat, despite strong results on benchmarks such as MVBench and MMVU, they often\nmiss these violations, exposing gaps in visual reasoning. Reinforcement\nlearning fine-tuning on VideoHallu improves recognition of such violations\nwithout reducing standard benchmark performance. Our data is available at\nhttps://github.com/zli12321/VideoHallu.git.\n","authors":["Zongxia Li","Xiyang Wu","Guangyao Shi","Yubin Qin","Hongyang Du","Fuxiao Liu","Tianyi Zhou","Dinesh Manocha","Jordan Lee Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2505.01481v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07227v4","updated":"2025-10-26T04:45:17Z","published":"2025-01-13T11:28:49Z","title":"MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning","summary":"  Video causal reasoning aims to achieve a high-level understanding of videos\nfrom a causal perspective. However, it exhibits limitations in its scope,\nprimarily executed in a question-answering paradigm and focusing on brief video\nsegments containing isolated events and basic causal relations, lacking\ncomprehensive and structured causality analysis for videos with multiple\ninterconnected events. To fill this gap, we introduce a new task and dataset,\nMulti-Event Causal Discovery (MECD). It aims to uncover the causal relations\nbetween events distributed chronologically across long videos. Given visual\nsegments and textual descriptions of events, MECD identifies the causal\nassociations between these events to derive a comprehensive and structured\nevent-level video causal graph explaining why and how the result event\noccurred. To address the challenges of MECD, we devise a novel framework\ninspired by the Granger Causality method, incorporating an efficient mask-based\nevent prediction model to perform an Event Granger Test. It estimates causality\nby comparing the predicted result event when premise events are masked versus\nunmasked. Furthermore, we integrate causal inference techniques such as\nfront-door adjustment and counterfactual inference to mitigate challenges in\nMECD like causality confounding and illusory causality. Additionally, context\nchain reasoning is introduced to conduct more robust and generalized reasoning.\nExperiments validate the effectiveness of our framework in reasoning complete\ncausal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,\nrespectively. Further experiments demonstrate that causal relation graphs can\nalso contribute to downstream video understanding tasks such as video question\nanswering and video event prediction.\n","authors":["Tieyuan Chen","Huabin Liu","Yi Wang","Yihang Chen","Tianyao He","Chaofan Gan","Huanyu He","Weiyao Lin"],"pdf_url":"https://arxiv.org/pdf/2501.07227v4.pdf","comment":"Accepted by IEEE TPAMI (IEEE Transactions on Pattern Analysis and\n  Machine Intelligence). arXiv admin note: substantial text overlap with\n  arXiv:2409.17647"},{"id":"http://arxiv.org/abs/2510.22529v1","updated":"2025-10-26T04:31:01Z","published":"2025-10-26T04:31:01Z","title":"Bag-of-Word-Groups (BoWG): A Robust and Efficient Loop Closure Detection\n  Method Under Perceptual Aliasing","summary":"  Loop closure is critical in Simultaneous Localization and Mapping (SLAM)\nsystems to reduce accumulative drift and ensure global mapping consistency.\nHowever, conventional methods struggle in perceptually aliased environments,\nsuch as narrow pipes, due to vector quantization, feature sparsity, and\nrepetitive textures, while existing solutions often incur high computational\ncosts. This paper presents Bag-of-Word-Groups (BoWG), a novel loop closure\ndetection method that achieves superior precision-recall, robustness, and\ncomputational efficiency. The core innovation lies in the introduction of word\ngroups, which captures the spatial co-occurrence and proximity of visual words\nto construct an online dictionary. Additionally, drawing inspiration from\nprobabilistic transition models, we incorporate temporal consistency directly\ninto similarity computation with an adaptive scheme, substantially improving\nprecision-recall performance. The method is further strengthened by a feature\ndistribution analysis module and dedicated post-verification mechanisms. To\nevaluate the effectiveness of our method, we conduct experiments on both public\ndatasets and a confined-pipe dataset we constructed. Results demonstrate that\nBoWG surpasses state-of-the-art methods, including both traditional and\nlearning-based approaches, in terms of precision-recall and computational\nefficiency. Our approach also exhibits excellent scalability, achieving an\naverage processing time of 16 ms per image across 17,565 images in the\nBicocca25b dataset.\n","authors":["Xiang Fei","Tina Tian","Howie Choset","Lu Li"],"pdf_url":"https://arxiv.org/pdf/2510.22529v1.pdf","comment":"This paper has been accepted by IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2510.22528v1","updated":"2025-10-26T04:30:02Z","published":"2025-10-26T04:30:02Z","title":"AesCrop: Aesthetic-driven Cropping Guided by Composition","summary":"  Aesthetic-driven image cropping is crucial for applications like view\nrecommendation and thumbnail generation, where visual appeal significantly\nimpacts user engagement. A key factor in visual appeal is composition--the\ndeliberate arrangement of elements within an image. Some methods have\nsuccessfully incorporated compositional knowledge through evaluation-based and\nregression-based paradigms. However, evaluation-based methods lack globality\nwhile regression-based methods lack diversity. Recently, hybrid approaches that\nintegrate both paradigms have emerged, bridging the gap between these two to\nachieve better diversity and globality. Notably, existing hybrid methods do not\nincorporate photographic composition guidance, a key attribute that defines\nphotographic aesthetics. In this work, we introduce AesCrop, a\ncomposition-aware hybrid image-cropping model that integrates a VMamba image\nencoder, augmented with a novel Mamba Composition Attention Bias (MCAB) and a\ntransformer decoder to perform end-to-end rank-based image cropping, generating\nmultiple crops along with the corresponding quality scores. By explicitly\nencoding compositional cues into the attention mechanism, MCAB directs AesCrop\nto focus on the most compositionally salient regions. Extensive experiments\ndemonstrate that AesCrop outperforms current state-of-the-art methods,\ndelivering superior quantitative metrics and qualitatively more pleasing crops.\n","authors":["Yen-Hong Wong","Lai-Kuan Wong"],"pdf_url":"https://arxiv.org/pdf/2510.22528v1.pdf","comment":"Accepted at the IEEE/CVF International Conference on Computer Vision\n  (ICCV) Workshops, 2025"},{"id":"http://arxiv.org/abs/2510.22521v1","updated":"2025-10-26T04:13:31Z","published":"2025-10-26T04:13:31Z","title":"Open Multimodal Retrieval-Augmented Factual Image Generation","summary":"  Large Multimodal Models (LMMs) have achieved remarkable progress in\ngenerating photorealistic and prompt-aligned images, but they often produce\noutputs that contradict verifiable knowledge, especially when prompts involve\nfine-grained attributes or time-sensitive events. Conventional\nretrieval-augmented approaches attempt to address this issue by introducing\nexternal information, yet they are fundamentally incapable of grounding\ngeneration in accurate and evolving knowledge due to their reliance on static\nsources and shallow evidence integration. To bridge this gap, we introduce\nORIG, an agentic open multimodal retrieval-augmented framework for Factual\nImage Generation (FIG), a new task that requires both visual realism and\nfactual grounding. ORIG iteratively retrieves and filters multimodal evidence\nfrom the web and incrementally integrates the refined knowledge into enriched\nprompts to guide generation. To support systematic evaluation, we build\nFIG-Eval, a benchmark spanning ten categories across perceptual, compositional,\nand temporal dimensions. Experiments demonstrate that ORIG substantially\nimproves factual consistency and overall image quality over strong baselines,\nhighlighting the potential of open multimodal retrieval for factual image\ngeneration.\n","authors":["Yang Tian","Fan Liu","Jingyuan Zhang","Wei Bi","Yupeng Hu","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2510.22521v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2509.20414v2","updated":"2025-10-26T04:10:24Z","published":"2025-09-24T09:06:41Z","title":"SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and\n  Self-Reflective Agent","summary":"  Indoor scene synthesis has become increasingly important with the rise of\nEmbodied AI, which requires 3D environments that are not only visually\nrealistic but also physically plausible and functionally diverse. While recent\napproaches have advanced visual fidelity, they often remain constrained to\nfixed scene categories, lack sufficient object-level detail and physical\nconsistency, and struggle to align with complex user instructions. In this\nwork, we present SceneWeaver, a reflective agentic framework that unifies\ndiverse scene synthesis paradigms through tool-based iterative refinement. At\nits core, SceneWeaver employs a language model-based planner to select from a\nsuite of extensible scene generation tools, ranging from data-driven generative\nmodels to visual- and LLM-based methods, guided by self-evaluation of physical\nplausibility, visual realism, and semantic alignment with user input. This\nclosed-loop reason-act-reflect design enables the agent to identify semantic\ninconsistencies, invoke targeted tools, and update the environment over\nsuccessive iterations. Extensive experiments on both common and open-vocabulary\nroom types demonstrate that SceneWeaver not only outperforms prior methods on\nphysical, visual, and semantic metrics, but also generalizes effectively to\ncomplex scenes with diverse instructions, marking a step toward general-purpose\n3D environment generation. Project website: https://scene-weaver.github.io/.\n","authors":["Yandan Yang","Baoxiong Jia","Shujie Zhang","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2509.20414v2.pdf","comment":"Accepted by NeurIPS 2025, 26 pages"},{"id":"http://arxiv.org/abs/2410.15068v4","updated":"2025-10-26T03:39:20Z","published":"2024-10-19T11:11:58Z","title":"A Cycle Ride to HDR: Semantics Aware Self-Supervised Framework for\n  Unpaired LDR-to-HDR Image Reconstruction","summary":"  Reconstruction of High Dynamic Range (HDR) from Low Dynamic Range (LDR)\nimages is an important computer vision task. There is a significant amount of\nresearch utilizing both conventional non-learning methods and modern\ndata-driven approaches, focusing on using both single-exposed and multi-exposed\nLDR for HDR image reconstruction. However, most current state-of-the-art\nmethods require high-quality paired {LDR;HDR} datasets with limited literature\nuse of unpaired datasets, that is, methods that learn the LDR-HDR mapping\nbetween domains. This paper proposes CycleHDR, a method that integrates\nself-supervision into a modified semantic- and cycle-consistent adversarial\narchitecture that utilizes unpaired LDR and HDR datasets for training. Our\nmethod introduces novel artifact- and exposure-aware generators to address\nvisual artifact removal. It also puts forward an encoder and loss to address\nsemantic consistency, another under-explored topic. CycleHDR is the first to\nuse semantic and contextual awareness for the LDR-HDR reconstruction task in a\nself-supervised setup. The method achieves state-of-the-art performance across\nseveral benchmark datasets and reconstructs high-quality HDR images. The\nofficial website of this work is available at:\nhttps://github.com/HrishavBakulBarua/Cycle-HDR\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","Lemuel Lai En Che","Abhinav Dhall","KokSheik Wong","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2410.15068v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22507v1","updated":"2025-10-26T03:11:26Z","published":"2025-10-26T03:11:26Z","title":"GateFuseNet: An Adaptive 3D Multimodal Neuroimaging Fusion Network for\n  Parkinson's Disease Diagnosis","summary":"  Accurate diagnosis of Parkinson's disease (PD) from MRI remains challenging\ndue to symptom variability and pathological heterogeneity. Most existing\nmethods rely on conventional magnitude-based MRI modalities, such as\nT1-weighted images (T1w), which are less sensitive to PD pathology than\nQuantitative Susceptibility Mapping (QSM), a phase-based MRI technique that\nquantifies iron deposition in deep gray matter nuclei. In this study, we\npropose GateFuseNet, an adaptive 3D multimodal fusion network that integrates\nQSM and T1w images for PD diagnosis. The core innovation lies in a gated fusion\nmodule that learns modality-specific attention weights and channel-wise gating\nvectors for selective feature modulation. This hierarchical gating mechanism\nenhances ROI-aware features while suppressing irrelevant signals. Experimental\nresults show that our method outperforms three existing state-of-the-art\napproaches, achieving 85.00% accuracy and 92.06% AUC. Ablation studies further\nvalidate the contributions of ROI guidance, multimodal integration, and fusion\npositioning. Grad-CAM visualizations confirm the model's focus on clinically\nrelevant pathological regions. The source codes and pretrained models can be\nfound at https://github.com/YangGaoUQ/GateFuseNet\n","authors":["Rui Jin","Chen Chen","Yin Liu","Hongfu Sun","Min Zeng","Min Li","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2510.22507v1.pdf","comment":"The first two authors contributed equally to this work.\n  Correspondence to: Yang Gao, E-mail: yang.gao@csu.edu.cn"},{"id":"http://arxiv.org/abs/2510.22491v1","updated":"2025-10-26T02:12:20Z","published":"2025-10-26T02:12:20Z","title":"LAMP: Data-Efficient Linear Affine Weight-Space Models for\n  Parameter-Controlled 3D Shape Generation and Extrapolation","summary":"  Generating high-fidelity 3D geometries that satisfy specific parameter\nconstraints has broad applications in design and engineering. However, current\nmethods typically rely on large training datasets and struggle with\ncontrollability and generalization beyond the training distributions. To\novercome these limitations, we introduce LAMP (Linear Affine Mixing of\nParametric shapes), a data-efficient framework for controllable and\ninterpretable 3D generation. LAMP first aligns signed distance function (SDF)\ndecoders by overfitting each exemplar from a shared initialization, then\nsynthesizes new geometries by solving a parameter-constrained mixing problem in\nthe aligned weight space. To ensure robustness, we further propose a safety\nmetric that detects geometry validity via linearity mismatch. We evaluate LAMP\non two 3D parametric benchmarks: DrivAerNet++ and BlendedNet. We found that\nLAMP enables (i) controlled interpolation within bounds with as few as 100\nsamples, (ii) safe extrapolation by up to 100% parameter difference beyond\ntraining ranges, (iii) physics performance-guided optimization under fixed\nparameters. LAMP significantly outperforms conditional autoencoder and Deep\nNetwork Interpolation (DNI) baselines in both extrapolation and data\nefficiency. Our results demonstrate that LAMP advances controllable,\ndata-efficient, and safe 3D generation for design exploration, dataset\ngeneration, and performance-driven optimization.\n","authors":["Ghadi Nehme","Yanxia Zhang","Dule Shu","Matt Klenk","Faez Ahmed"],"pdf_url":"https://arxiv.org/pdf/2510.22491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22480v1","updated":"2025-10-26T01:41:08Z","published":"2025-10-26T01:41:08Z","title":"Single-Teacher View Augmentation: Boosting Knowledge Distillation via\n  Angular Diversity","summary":"  Knowledge Distillation (KD) aims to train a lightweight student model by\ntransferring knowledge from a large, high-capacity teacher. Recent studies have\nshown that leveraging diverse teacher perspectives can significantly improve\ndistillation performance; however, achieving such diversity typically requires\nmultiple teacher networks, leading to high computational costs. In this work,\nwe propose a novel cost-efficient knowledge augmentation method for KD that\ngenerates diverse multi-views by attaching multiple branches to a single\nteacher. To ensure meaningful semantic variation across multi-views, we\nintroduce two angular diversity objectives: 1) constrained inter-angle\ndiversify loss, which maximizes angles between augmented views while preserving\nproximity to the original teacher output, and 2) intra-angle diversify loss,\nwhich encourages an even distribution of views around the original output. The\nensembled knowledge from these angularly diverse views, along with the original\nteacher, is distilled into the student. We further theoretically demonstrate\nthat our objectives increase the diversity among ensemble members and thereby\nreduce the upper bound of the ensemble's expected loss, leading to more\neffective distillation. Experimental results show that our method surpasses an\nexisting knowledge augmentation method across diverse configurations. Moreover,\nthe proposed method is compatible with other KD frameworks in a plug-and-play\nfashion, providing consistent improvements in generalization performance.\n","authors":["Seonghoon Yu","Dongjun Nam","Dina Katabi","Jeany Son"],"pdf_url":"https://arxiv.org/pdf/2510.22480v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.22473v1","updated":"2025-10-26T01:11:13Z","published":"2025-10-26T01:11:13Z","title":"DynaPose4D: High-Quality 4D Dynamic Content Generation via Pose\n  Alignment Loss","summary":"  Recent advancements in 2D and 3D generative models have expanded the\ncapabilities of computer vision. However, generating high-quality 4D dynamic\ncontent from a single static image remains a significant challenge. Traditional\nmethods have limitations in modeling temporal dependencies and accurately\ncapturing dynamic geometry changes, especially when considering variations in\ncamera perspective. To address this issue, we propose DynaPose4D, an innovative\nsolution that integrates 4D Gaussian Splatting (4DGS) techniques with\nCategory-Agnostic Pose Estimation (CAPE) technology. This framework uses 3D\nGaussian Splatting to construct a 3D model from single images, then predicts\nmulti-view pose keypoints based on one-shot support from a chosen view,\nleveraging supervisory signals to enhance motion consistency. Experimental\nresults show that DynaPose4D achieves excellent coherence, consistency, and\nfluidity in dynamic motion generation. These findings not only validate the\nefficacy of the DynaPose4D framework but also indicate its potential\napplications in the domains of computer vision and animation production.\n","authors":["Jing Yang","Yufeng Yang"],"pdf_url":"https://arxiv.org/pdf/2510.22473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.08141v2","updated":"2025-10-26T01:03:16Z","published":"2025-08-11T16:14:17Z","title":"Pindrop it! Audio and Visual Deepfake Countermeasures for Robust\n  Detection and Fine Grained-Localization","summary":"  The field of visual and audio generation is burgeoning with new\nstate-of-the-art methods. This rapid proliferation of new techniques\nunderscores the need for robust solutions for detecting synthetic content in\nvideos. In particular, when fine-grained alterations via localized\nmanipulations are performed in visual, audio, or both domains, these subtle\nmodifications add challenges to the detection algorithms. This paper presents\nsolutions for the problems of deepfake video classification and localization.\nThe methods were submitted to the ACM 1M Deepfakes Detection Challenge,\nachieving the best performance in the temporal localization task and a top four\nranking in the classification task for the TestA split of the evaluation\ndataset.\n","authors":["Nicholas Klein","Hemlata Tak","James Fullwood","Krishna Regmi","Leonidas Spinoulas","Ganesh Sivaraman","Tianxiang Chen","Elie Khoury"],"pdf_url":"https://arxiv.org/pdf/2508.08141v2.pdf","comment":null}]},"2025-10-25T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2001.08747v2","updated":"2025-10-25T23:22:53Z","published":"2020-01-23T18:37:24Z","title":"Reducing the Representation Error of GAN Image Priors Using the Deep\n  Decoder","summary":"  Generative models, such as GANs, learn an explicit low-dimensional\nrepresentation of a particular class of images, and so they may be used as\nnatural image priors for solving inverse problems such as image restoration and\ncompressive sensing. GAN priors have demonstrated impressive performance on\nthese tasks, but they can exhibit substantial representation error for both\nin-distribution and out-of-distribution images, because of the mismatch between\nthe learned, approximate image distribution and the data generating\ndistribution. In this paper, we demonstrate a method for reducing the\nrepresentation error of GAN priors by modeling images as the linear combination\nof a GAN prior with a Deep Decoder. The deep decoder is an underparameterized\nand most importantly unlearned natural signal model similar to the Deep Image\nPrior. No knowledge of the specific inverse problem is needed in the training\nof the GAN underlying our method. For compressive sensing and image\nsuperresolution, our hybrid model exhibits consistently higher PSNRs than both\nthe GAN priors and Deep Decoder separately, both on in-distribution and\nout-of-distribution images. This model provides a method for extensibly and\ncheaply leveraging both the benefits of learned and unlearned image recovery\npriors in inverse problems.\n","authors":["Mara Daniels","Paul Hand","Reinhard Heckel"],"pdf_url":"https://arxiv.org/pdf/2001.08747v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22454v1","updated":"2025-10-25T23:09:22Z","published":"2025-10-25T23:09:22Z","title":"SemiETPicker: Fast and Label-Efficient Particle Picking for CryoET\n  Tomography Using Semi-Supervised Learning","summary":"  Cryogenic Electron Tomography (CryoET) combined with sub-volume averaging\n(SVA) is the only imaging modality capable of resolving protein structures\ninside cells at molecular resolution. Particle picking, the task of localizing\nand classifying target proteins in 3D CryoET volumes, remains the main\nbottleneck. Due to the reliance on time-consuming manual labels, the vast\nreserve of unlabeled tomograms remains underutilized. In this work, we present\na fast, label-efficient semi-supervised framework that exploits this untapped\ndata. Our framework consists of two components: (i) an end-to-end\nheatmap-supervised detection model inspired by keypoint detection, and (ii) a\nteacher-student co-training mechanism that enhances performance under sparse\nlabeling conditions. Furthermore, we introduce multi-view pseudo-labeling and a\nCryoET-specific DropBlock augmentation strategy to further boost performance.\nExtensive evaluations on the large-scale CZII dataset show that our approach\nimproves F1 by 10% over supervised baselines, underscoring the promise of\nsemi-supervised learning for leveraging unlabeled CryoET data.\n","authors":["Linhan Wang","Jianwen Dou","Wang Li","Shengkun Wang","Zhiwu Xie","Chang-Tien Lu","Yinlin Chen"],"pdf_url":"https://arxiv.org/pdf/2510.22454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18047v2","updated":"2025-10-25T22:06:16Z","published":"2025-05-23T15:52:26Z","title":"RestoreVAR: Visual Autoregressive Generation for All-in-One Image\n  Restoration","summary":"  The use of latent diffusion models (LDMs) such as Stable Diffusion has\nsignificantly improved the perceptual quality of All-in-One image Restoration\n(AiOR) methods, while also enhancing their generalization capabilities.\nHowever, these LDM-based frameworks suffer from slow inference due to their\niterative denoising process, rendering them impractical for time-sensitive\napplications. Visual autoregressive modeling (VAR), a recently introduced\napproach for image generation, performs scale-space autoregression and achieves\ncomparable performance to that of state-of-the-art diffusion transformers with\ndrastically reduced computational costs. Moreover, our analysis reveals that\ncoarse scales in VAR primarily capture degradations while finer scales encode\nscene detail, simplifying the restoration process. Motivated by this, we\npropose RestoreVAR, a novel VAR-based generative approach for AiOR that\nsignificantly outperforms LDM-based models in restoration performance while\nachieving over $10\\times$ faster inference. To optimally exploit the advantages\nof VAR for AiOR, we propose architectural modifications and improvements,\nincluding intricately designed cross-attention mechanisms and a latent-space\nrefinement module, tailored for the AiOR task. Extensive experiments show that\nRestoreVAR achieves state-of-the-art performance among generative AiOR methods,\nwhile also exhibiting strong generalization capabilities.\n","authors":["Sudarshan Rajagopalan","Kartik Narayan","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2505.18047v2.pdf","comment":"Project page: https://sudraj2002.github.io/restorevarpage/"},{"id":"http://arxiv.org/abs/2510.22443v1","updated":"2025-10-25T21:54:01Z","published":"2025-10-25T21:54:01Z","title":"Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable\n  Agents","summary":"  There has been a surge of interest in assistive wearable agents: agents\nembodied in wearable form factors (e.g., smart glasses) who take assistive\nactions toward a user's goal/query (e.g. \"Where did I leave my keys?\"). In this\nwork, we consider the important complementary problem of inferring that goal\nfrom multi-modal contextual observations. Solving this \"goal inference\" problem\nholds the promise of eliminating the effort needed to interact with such an\nagent. This work focuses on creating WAGIBench, a strong benchmark to measure\nprogress in solving this problem using vision-language models (VLMs). Given the\nlimited prior work in this area, we collected a novel dataset comprising 29\nhours of multimodal data from 348 participants across 3,477 recordings,\nfeaturing ground-truth goals alongside accompanying visual, audio, digital, and\nlongitudinal contextual observations. We validate that human performance\nexceeds model performance, achieving 93% multiple-choice accuracy compared with\n84% for the best-performing VLM. Generative benchmark results that evaluate\nseveral families of modern vision-language models show that larger models\nperform significantly better on the task, yet remain far from practical\nusefulness, as they produce relevant goals only 55% of the time. Through a\nmodality ablation, we show that models benefit from extra information in\nrelevant modalities with minimal performance degradation from irrelevant\nmodalities.\n","authors":["Vijay Veerabadran","Fanyi Xiao","Nitin Kamra","Pedro Matias","Joy Chen","Caley Drooff","Brett D Roads","Riley Williams","Ethan Henderson","Xuanyi Zhao","Kevin Carlberg","Joseph Tighe","Karl Ridgeway"],"pdf_url":"https://arxiv.org/pdf/2510.22443v1.pdf","comment":"Accepted as a spotlight paper at the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.22436v1","updated":"2025-10-25T21:14:45Z","published":"2025-10-25T21:14:45Z","title":"3D Roadway Scene Object Detection with LIDARs in Snowfall Conditions","summary":"  Because 3D structure of a roadway environment can be characterized directly\nby a Light Detection and Ranging (LiDAR) sensors, they can be used to obtain\nexceptional situational awareness for assitive and autonomous driving systems.\nAlthough LiDARs demonstrate good performance in clean and clear weather\nconditions, their performance significantly deteriorates in adverse weather\nconditions such as those involving atmospheric precipitation. This may render\nperception capabilities of autonomous systems that use LiDAR data in learning\nbased models to perform object detection and ranging ineffective. While efforts\nhave been made to enhance the accuracy of these models, the extent of signal\ndegradation under various weather conditions remains largely not quantified. In\nthis study, we focus on the performance of an automotive grade LiDAR in snowy\nconditions in order to develop a physics-based model that examines failure\nmodes of a LiDAR sensor. Specifically, we investigated how the LiDAR signal\nattenuates with different snowfall rates and how snow particles near the source\nserve as small but efficient reflectors. Utilizing our model, we transform data\nfrom clear conditions to simulate snowy scenarios, enabling a comparison of our\nsynthetic data with actual snowy conditions. Furthermore, we employ this\nsynthetic data, representative of different snowfall rates, to explore the\nimpact on a pre-trained object detection model, assessing its performance under\nvarying levels of snowfall\n","authors":["Ghazal Farhani","Taufiq Rahman","Syed Mostaquim Ali","Andrew Liu","Mohamed Zaki","Dominique Charlebois","Benoit Anctil"],"pdf_url":"https://arxiv.org/pdf/2510.22436v1.pdf","comment":"2024 IEEE 27th International Conference on Intelligent Transportation\n  Systems (ITSC), pp. 1441--1448, Sept. 2024"},{"id":"http://arxiv.org/abs/2510.22431v1","updated":"2025-10-25T20:34:18Z","published":"2025-10-25T20:34:18Z","title":"Hollywood Town: Long-Video Generation via Cross-Modal Multi-Agent\n  Orchestration","summary":"  Recent advancements in multi-agent systems have demonstrated significant\npotential for enhancing creative task performance, such as long video\ngeneration. This study introduces three innovations to improve multi-agent\ncollaboration. First, we propose OmniAgent, a hierarchical, graph-based\nmulti-agent framework for long video generation that leverages a\nfilm-production-inspired architecture to enable modular specialization and\nscalable inter-agent collaboration. Second, inspired by context engineering, we\npropose hypergraph nodes that enable temporary group discussions among agents\nlacking sufficient context, reducing individual memory requirements while\nensuring adequate contextual information. Third, we transition from directed\nacyclic graphs (DAGs) to directed cyclic graphs with limited retries, allowing\nagents to reflect and refine outputs iteratively, thereby improving earlier\nstages through feedback from subsequent nodes. These contributions lay the\ngroundwork for developing more robust multi-agent systems in creative tasks.\n","authors":["Zheng Wei","Mingchen Li","Zeqian Zhang","Ruibin Yuan","Pan Hui","Huamin Qu","James Evans","Maneesh Agrawala","Anyi Rao"],"pdf_url":"https://arxiv.org/pdf/2510.22431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17823v4","updated":"2025-10-25T19:57:55Z","published":"2025-01-29T18:15:49Z","title":"Robust Multimodal Learning via Cross-Modal Proxy Tokens","summary":"  Multimodal models often experience a significant performance drop when one or\nmore modalities are missing during inference. To address this challenge, we\npropose a simple yet effective approach that enhances robustness to missing\nmodalities while maintaining strong performance when all modalities are\navailable. Our method introduces cross-modal proxy tokens (CMPTs), which\napproximate the class token of a missing modality by attending only to the\ntokens of the available modality without requiring explicit modality generation\nor auxiliary networks. To efficiently learn these approximations with minimal\ncomputational overhead, we employ low-rank adapters in frozen unimodal encoders\nand jointly optimize an alignment loss with a task-specific loss. Extensive\nexperiments on five multimodal datasets show that our method outperforms\nstate-of-the-art baselines across various missing rates while achieving\ncompetitive results in complete-modality settings. Overall, our method offers a\nflexible and efficient solution for robust multimodal learning. The code for\nthis paper is available at:\nhttps://github.com/CSIPlab/Cross-Modal-Proxy-Tokens.\n","authors":["Md Kaykobad Reza","Ameya Patil","Mashhour Solh","M. Salman Asif"],"pdf_url":"https://arxiv.org/pdf/2501.17823v4.pdf","comment":"28 Pages, 13 Figures, 11 Tables. Accepted by Transactions on Machine\n  Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2510.22391v1","updated":"2025-10-25T18:27:00Z","published":"2025-10-25T18:27:00Z","title":"Top-Down Semantic Refinement for Image Captioning","summary":"  Large Vision-Language Models (VLMs) face an inherent contradiction in image\ncaptioning: their powerful single-step generation capabilities often lead to a\nmyopic decision-making process. This makes it difficult to maintain global\nnarrative coherence while capturing rich details, a limitation that is\nparticularly pronounced in tasks that require multi-step and complex scene\ndescription. To overcome this fundamental challenge, we redefine image\ncaptioning as a goal-oriented hierarchical refinement planning problem, and\nfurther propose a novel framework, named Top-Down Semantic Refinement (TDSR),\nwhich models the generation process as a Markov Decision Process (MDP).\nHowever, planning within the vast state space of a VLM presents a significant\ncomputational hurdle. Our core contribution, therefore, is the design of a\nhighly efficient Monte Carlo Tree Search (MCTS) algorithm tailored for VLMs. By\nincorporating a visual-guided parallel expansion and a lightweight value\nnetwork, our TDSR reduces the call frequency to the expensive VLM by an order\nof magnitude without sacrificing planning quality. Furthermore, an adaptive\nearly stopping mechanism dynamically matches computational overhead to the\nimage's complexity. Extensive experiments on multiple benchmarks, including\nDetailCaps, COMPOSITIONCAP, and POPE, demonstrate that our TDSR, as a\nplug-and-play module, can significantly enhance the performance of existing\nVLMs (e.g., LLaVA-1.5, Qwen2.5-VL) by achieving state-of-the-art or highly\ncompetitive results in fine-grained description, compositional generalization,\nand hallucination suppression.\n","authors":["Jusheng Zhang","Kaitong Cai","Jing Yang","Jian Wang","Chengpei Tang","Keze Wang"],"pdf_url":"https://arxiv.org/pdf/2510.22391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22390v1","updated":"2025-10-25T18:18:10Z","published":"2025-10-25T18:18:10Z","title":"A Fully Interpretable Statistical Approach for Roadside LiDAR Background\n  Subtraction","summary":"  We present a fully interpretable and flexible statistical method for\nbackground subtraction in roadside LiDAR data, aimed at enhancing\ninfrastructure-based perception in automated driving. Our approach introduces\nboth a Gaussian distribution grid (GDG), which models the spatial statistics of\nthe background using background-only scans, and a filtering algorithm that uses\nthis representation to classify LiDAR points as foreground or background. The\nmethod supports diverse LiDAR types, including multiline 360 degree and\nmicro-electro-mechanical systems (MEMS) sensors, and adapts to various\nconfigurations. Evaluated on the publicly available RCooper dataset, it\noutperforms state-of-the-art techniques in accuracy and flexibility, even with\nminimal background data. Its efficient implementation ensures reliable\nperformance on low-resource hardware, enabling scalable real-world deployment.\n","authors":["Aitor Iglesias","Nerea Aranjuelo","Patricia Javierre","Ainhoa Menendez","Ignacio Arganda-Carreras","Marcos Nieto"],"pdf_url":"https://arxiv.org/pdf/2510.22390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22387v1","updated":"2025-10-25T18:10:05Z","published":"2025-10-25T18:10:05Z","title":"Privacy-Aware Federated nnU-Net for ECG Page Digitization","summary":"  Deep neural networks can convert ECG page images into analyzable waveforms,\nyet centralized training often conflicts with cross-institutional privacy and\ndeployment constraints. A cross-silo federated digitization framework is\npresented that trains a full-model nnU-Net segmentation backbone without\nsharing images and aggregates updates across sites under realistic non-IID\nheterogeneity (layout, grid style, scanner profile, noise).\n  The protocol integrates three standard server-side aggregators--FedAvg,\nFedProx, and FedAdam--and couples secure aggregation with central, user-level\ndifferential privacy to align utility with formal guarantees. Key features\ninclude: (i) end-to-end full-model training and synchronization across clients;\n(ii) secure aggregation so the server only observes a clipped, weighted sum\nonce a participation threshold is met; (iii) central Gaussian DP with Renyi\naccounting applied post-aggregation for auditable user-level privacy; and (iv)\na calibration-aware digitization pipeline comprising page normalization, trace\nsegmentation, grid-leakage suppression, and vectorization to twelve-lead\nsignals.\n  Experiments on ECG pages rendered from PTB-XL show consistently faster\nconvergence and higher late-round plateaus with adaptive server updates\n(FedAdam) relative to FedAvg and FedProx, while approaching centralized\nperformance. The privacy mechanism maintains competitive accuracy while\npreventing exposure of raw images or per-client updates, yielding deployable,\nauditable guarantees suitable for multi-institution settings.\n","authors":["Nader Nemati"],"pdf_url":"https://arxiv.org/pdf/2510.22387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22383v1","updated":"2025-10-25T17:55:13Z","published":"2025-10-25T17:55:13Z","title":"Dynamic Dropout: Leveraging Conway's Game of Life for Neural Networks\n  Regularization","summary":"  Regularization techniques play a crucial role in preventing overfitting and\nimproving the generalization performance of neural networks. Dropout, a widely\nused regularization technique, randomly deactivates units during training to\nintroduce redundancy and prevent co-adaptation among neurons. Despite its\neffectiveness, dropout has limitations, such as its static nature and lack of\ninterpretability. In this paper, we propose a novel approach to regularization\nby substituting dropout with Conway's Game of Life (GoL), a cellular automata\nwith simple rules that govern the evolution of a grid of cells. We introduce\ndynamic unit deactivation during training by representing neural network units\nas cells in a GoL grid and applying the game's rules to deactivate units. This\napproach allows for the emergence of spatial patterns that adapt to the\ntraining data, potentially enhancing the network's ability to generalize. We\ndemonstrate the effectiveness of our approach on the CIFAR-10 dataset, showing\nthat dynamic unit deactivation using GoL achieves comparable performance to\ntraditional dropout techniques while offering insights into the network's\nbehavior through the visualization of evolving patterns. Furthermore, our\ndiscussion highlights the applicability of our proposal in deeper\narchitectures, demonstrating how it enhances the performance of different\ndropout techniques.\n","authors":["David Freire-Obregón","José Salas-Cáceres","Modesto Castrillón-Santana"],"pdf_url":"https://arxiv.org/pdf/2510.22383v1.pdf","comment":"Accepted for presentation at the 5th International Conference on\n  Computing and Machine Intelligence (ICMI 2026)"},{"id":"http://arxiv.org/abs/2510.22380v1","updated":"2025-10-25T17:49:29Z","published":"2025-10-25T17:49:29Z","title":"Efficient Large-Deformation Medical Image Registration via Recurrent\n  Dynamic Correlation","summary":"  Deformable image registration estimates voxel-wise correspondences between\nimages through spatial transformations, and plays a key role in medical\nimaging. While deep learning methods have significantly reduced runtime,\nefficiently handling large deformations remains a challenging task.\nConvolutional networks aggregate local features but lack direct modeling of\nvoxel correspondences, promoting recent works to explore explicit feature\nmatching. Among them, voxel-to-region matching is more efficient for direct\ncorrespondence modeling by computing local correlation features whithin\nneighbourhoods, while region-to-region matching incurs higher redundancy due to\nexcessive correlation pairs across large regions. However, the inherent\nlocality of voxel-to-region matching hinders the capture of long-range\ncorrespondences required for large deformations. To address this, we propose a\nRecurrent Correlation-based framework that dynamically relocates the matching\nregion toward more promising positions. At each step, local matching is\nperformed with low cost, and the estimated offset guides the next search\nregion, supporting efficient convergence toward large deformations. In\naddition, we uses a lightweight recurrent update module with memory capacity\nand decouples motion-related and texture features to suppress semantic\nredundancy. We conduct extensive experiments on brain MRI and abdominal CT\ndatasets under two settings: with and without affine pre-registration. Results\nshow that our method exibits a strong accuracy-computation trade-off,\nsurpassing or matching the state-of-the-art performance. For example, it\nachieves comparable performance on the non-affine OASIS dataset, while using\nonly 9.5% of the FLOPs and running 96% faster than RDP, a representative\nhigh-performing method.\n","authors":["Tianran Li","Marius Staring","Yuchuan Qiao"],"pdf_url":"https://arxiv.org/pdf/2510.22380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16188v6","updated":"2025-10-25T17:43:45Z","published":"2025-03-20T14:37:45Z","title":"Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual\n  Reinforcement Fine-Tuning","summary":"  This paper investigates the role of explicit thinking process in rule-based\nreinforcement fine-tuning (RFT) for MLLMs. We first propose CLS-RL for MLLM\nimage classification, using verifiable rewards for fine-tuning. Experiments\nshow CLS-RL significantly outperforms SFT and yields a cross-dataset\ngeneralization effect. We then rethink and question whether explicit thinking\nin RFT is always necessary. Challenging the convention that explicit thinking\nis crucial for the success of RFT, we introduce No-Thinking-RL, exploring RFT\nwithout thinking by introducing a simple equality accuracy reward. We evaluate\nNo-Thinking-RL on 6 diverse tasks across different model sizes and types.\nExperimental results reveal three key findings: 1). Visual perception tasks do\nnot require thinking during RFT, as No-Thinking-RL consistently outperforms or\nmatches Thinking-based RFT across model sizes. 2).} Models with limited\ncapabilities struggle to generate high-quality CoT for RFT, making\nThinking-based RFT less effective than No-Thinking-RL. 3). There are\ninconsistencies between the answers in the thinking and answer tags for some\nresponses of thinking-based RFT, which show lower accuracy than the overall\naccuracy. We hypothesize that explicit thinking before verifiable answers may\nhinder reward convergence and reduce performance. To test this hypothesis, we\npropose Think-After-Answer, which places thinking after the answer to mitigate\nthis effect for experimental verification. Lastly, we conduct a pilot study to\nexplore whether MLLMs can learn when to think during RFT, introducing an\nAdaptive-Thinking method. Experiments show that it converges to a specific\nprompt depending on model capability and task complexity, achieving comparable\nor better performance than both Thinking and No-Thinking-RL. This suggests\nMLLMs can adaptively decide to think or not based on their capabilities and\ntask complexity.\n","authors":["Ming Li","Jike Zhong","Shitian Zhao","Yuxiang Lai","Haoquan Zhang","Wang Bill Zhu","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.16188v6.pdf","comment":"Neurips 2025 Spotlight"},{"id":"http://arxiv.org/abs/2510.22373v1","updated":"2025-10-25T17:31:02Z","published":"2025-10-25T17:31:02Z","title":"VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations","summary":"  Visualization, a domain-specific yet widely used form of imagery, is an\neffective way to turn complex datasets into intuitive insights, and its value\ndepends on whether data are faithfully represented, clearly communicated, and\naesthetically designed. However, evaluating visualization quality is\nchallenging: unlike natural images, it requires simultaneous judgment across\ndata encoding accuracy, information expressiveness, and visual aesthetics.\nAlthough multimodal large language models (MLLMs) have shown promising\nperformance in aesthetic assessment of natural images, no systematic benchmark\nexists for measuring their capabilities in evaluating visualizations. To\naddress this, we propose VisJudge-Bench, the first comprehensive benchmark for\nevaluating MLLMs' performance in assessing visualization aesthetics and\nquality. It contains 3,090 expert-annotated samples from real-world scenarios,\ncovering single visualizations, multiple visualizations, and dashboards across\n32 chart types. Systematic testing on this benchmark reveals that even the most\nadvanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human\nexperts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a\ncorrelation with human ratings of only 0.429. To address this issue, we propose\nVisJudge, a model specifically designed for visualization aesthetics and\nquality assessment. Experimental results demonstrate that VisJudge\nsignificantly narrows the gap with human judgment, reducing the MAE to 0.442 (a\n19.8% reduction) and increasing the consistency with human experts to 0.681 (a\n58.7% improvement) compared to GPT-5. The benchmark is available at\nhttps://github.com/HKUSTDial/VisJudgeBench.\n","authors":["Yupeng Xie","Zhiyang Zhang","Yifan Wu","Sirong Lu","Jiayi Zhang","Zhaoyang Yu","Jinlin Wang","Sirui Hong","Bang Liu","Chenglin Wu","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2510.22373v1.pdf","comment":"53 pages, 26 figures, 5 tables"},{"id":"http://arxiv.org/abs/2510.22370v1","updated":"2025-10-25T17:27:08Z","published":"2025-10-25T17:27:08Z","title":"BLIP-FusePPO: A Vision-Language Deep Reinforcement Learning Framework\n  for Lane Keeping in Autonomous Vehicles","summary":"  In this paper, we propose Bootstrapped Language-Image Pretraining-driven\nFused State Representation in Proximal Policy Optimization (BLIP-FusePPO), a\nnovel multimodal reinforcement learning (RL) framework for autonomous\nlane-keeping (LK), in which semantic embeddings generated by a vision-language\nmodel (VLM) are directly fused with geometric states, LiDAR observations, and\nProportional-Integral-Derivative-based (PID) control feedback within the agent\nobservation space. The proposed method lets the agent learn driving rules that\nare aware of their surroundings and easy to understand by combining high-level\nscene understanding from the VLM with low-level control and spatial signals.\nOur architecture brings together semantic, geometric, and control-aware\nrepresentations to make policy learning more robust. A hybrid reward function\nthat includes semantic alignment, LK accuracy, obstacle avoidance, and speed\nregulation helps learning to be more efficient and generalizable. Our method is\ndifferent from the approaches that only use semantic models to shape rewards.\nInstead, it directly embeds semantic features into the state representation.\nThis cuts down on expensive runtime inference and makes sure that semantic\nguidance is always available. The simulation results show that the proposed\nmodel is better at LK stability and adaptability than the best vision-based and\nmultimodal RL baselines in a wide range of difficult driving situations. We\nmake our code publicly available.\n","authors":["Seyed Ahmad Hosseini Miangoleh","Amin Jalal Aghdasian","Farzaneh Abdollahi"],"pdf_url":"https://arxiv.org/pdf/2510.22370v1.pdf","comment":"https://github.com/Amin-A96/BLIP-FusePPO-A-Vision-Language-Deep-Reinforcement-Learning-Framework-for-Lane-Keeping-in-Autonomous.git"},{"id":"http://arxiv.org/abs/2508.11696v2","updated":"2025-10-25T17:26:43Z","published":"2025-08-12T19:27:17Z","title":"A Deep Learning-Based CCTV System for Automatic Smoking Detection in\n  Fire Exit Zones","summary":"  A deep learning real-time smoking detection system for CCTV surveillance of\nfire exit areas is proposed due to critical safety requirements. The dataset\ncontains 8,124 images from 20 different scenarios along with 2,708 raw samples\ndemonstrating low-light areas. We evaluated three advanced object detection\nmodels: YOLOv8, YOLOv11, and YOLOv12, followed by development of a custom model\nderived from YOLOv8 with added structures for challenging surveillance\ncontexts. The proposed model outperformed the others, achieving a recall of\n78.90 percent and mAP at 50 of 83.70 percent, delivering optimal object\ndetection across varied environments. Performance evaluation on multiple edge\ndevices using multithreaded operations showed the Jetson Xavier NX processed\ndata at 52 to 97 milliseconds per inference, establishing its suitability for\ntime-sensitive operations. This system offers a robust and adaptable platform\nfor monitoring public safety and enabling automatic regulatory compliance.\n","authors":["Sami Sadat","Mohammad Irtiza Hossain","Junaid Ahmed Sifat","Suhail Haque Rafi","Md. Waseq Alauddin Alvi","Md. Khalilur Rhaman"],"pdf_url":"https://arxiv.org/pdf/2508.11696v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22366v1","updated":"2025-10-25T16:55:55Z","published":"2025-10-25T16:55:55Z","title":"T2SMark: Balancing Robustness and Diversity in Noise-as-Watermark for\n  Diffusion Models","summary":"  Diffusion models have advanced rapidly in recent years, producing\nhigh-fidelity images while raising concerns about intellectual property\nprotection and the misuse of generative AI. Image watermarking for diffusion\nmodels, particularly Noise-as-Watermark (NaW) methods, encode watermark as\nspecific standard Gaussian noise vector for image generation, embedding the\ninfomation seamlessly while maintaining image quality. For detection, the\ngeneration process is inverted to recover the initial noise vector containing\nthe watermark before extraction. However, existing NaW methods struggle to\nbalance watermark robustness with generation diversity. Some methods achieve\nstrong robustness by heavily constraining initial noise sampling, which\ndegrades user experience, while others preserve diversity but prove too fragile\nfor real-world deployment. To address this issue, we propose T2SMark, a\ntwo-stage watermarking scheme based on Tail-Truncated Sampling (TTS). Unlike\nprior methods that simply map bits to positive or negative values, TTS enhances\nrobustness by embedding bits exclusively in the reliable tail regions while\nrandomly sampling the central zone to preserve the latent distribution. Our\ntwo-stage framework then ensures sampling diversity by integrating a randomly\ngenerated session key into both encryption pipelines. We evaluate T2SMark on\ndiffusion models with both U-Net and DiT backbones. Extensive experiments show\nthat it achieves an optimal balance between robustness and diversity. Our code\nis available at\n\\href{https://github.com/0xD009/T2SMark}{https://github.com/0xD009/T2SMark}.\n","authors":["Jindong Yang","Han Fang","Weiming Zhang","Nenghai Yu","Kejiang Chen"],"pdf_url":"https://arxiv.org/pdf/2510.22366v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.14627v2","updated":"2025-10-25T16:54:00Z","published":"2025-10-16T12:38:14Z","title":"GOPLA: Generalizable Object Placement Learning via Synthetic\n  Augmentation of Human Arrangement","summary":"  Robots are expected to serve as intelligent assistants, helping humans with\neveryday household organization. A central challenge in this setting is the\ntask of object placement, which requires reasoning about both semantic\npreferences (e.g., common-sense object relations) and geometric feasibility\n(e.g., collision avoidance). We present GOPLA, a hierarchical framework that\nlearns generalizable object placement from augmented human demonstrations. A\nmulti-modal large language model translates human instructions and visual\ninputs into structured plans that specify pairwise object relationships. These\nplans are then converted into 3D affordance maps with geometric common sense by\na spatial mapper, while a diffusion-based planner generates placement poses\nguided by test-time costs, considering multi-plan distributions and collision\navoidance. To overcome data scarcity, we introduce a scalable pipeline that\nexpands human placement demonstrations into diverse synthetic training data.\nExtensive experiments show that our approach improves placement success rates\nby 30.04 percentage points over the runner-up, evaluated on positioning\naccuracy and physical plausibility, demonstrating strong generalization across\na wide range of real-world robotic placement scenarios.\n","authors":["Yao Zhong","Hanzhi Chen","Simon Schaefer","Anran Zhang","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2510.14627v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05604v2","updated":"2025-10-25T16:42:54Z","published":"2025-07-08T02:33:44Z","title":"Kernel Density Steering: Inference-Time Scaling via Mode Seeking for\n  Image Restoration","summary":"  Diffusion models show promise for image restoration, but existing methods\noften struggle with inconsistent fidelity and undesirable artifacts. To address\nthis, we introduce Kernel Density Steering (KDS), a novel inference-time\nframework promoting robust, high-fidelity outputs through explicit local\nmode-seeking. KDS employs an $N$-particle ensemble of diffusion samples,\ncomputing patch-wise kernel density estimation gradients from their collective\noutputs. These gradients steer patches in each particle towards shared,\nhigher-density regions identified within the ensemble. This collective local\nmode-seeking mechanism, acting as \"collective wisdom\", steers samples away from\nspurious modes prone to artifacts, arising from independent sampling or model\nimperfections, and towards more robust, high-fidelity structures. This allows\nus to obtain better quality samples at the expense of higher compute by\nsimultaneously sampling multiple particles. As a plug-and-play framework, KDS\nrequires no retraining or external verifiers, seamlessly integrating with\nvarious diffusion samplers. Extensive numerical validations demonstrate KDS\nsubstantially improves both quantitative and qualitative performance on\nchallenging real-world super-resolution and image inpainting tasks.\n","authors":["Yuyang Hu","Kangfu Mei","Mojtaba Sahraee-Ardakan","Ulugbek S. Kamilov","Peyman Milanfar","Mauricio Delbracio"],"pdf_url":"https://arxiv.org/pdf/2507.05604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22359v1","updated":"2025-10-25T16:39:04Z","published":"2025-10-25T16:39:04Z","title":"EndoSfM3D: Learning to 3D Reconstruct Any Endoscopic Surgery Scene using\n  Self-supervised Foundation Model","summary":"  3D reconstruction of endoscopic surgery scenes plays a vital role in\nenhancing scene perception, enabling AR visualization, and supporting\ncontext-aware decision-making in image-guided surgery. A critical yet\nchallenging step in this process is the accurate estimation of the endoscope's\nintrinsic parameters. In real surgical settings, intrinsic calibration is\nhindered by sterility constraints and the use of specialized endoscopes with\ncontinuous zoom and telescope rotation. Most existing methods for endoscopic 3D\nreconstruction do not estimate intrinsic parameters, limiting their\neffectiveness for accurate and reliable reconstruction. In this paper, we\nintegrate intrinsic parameter estimation into a self-supervised monocular depth\nestimation framework by adapting the Depth Anything V2 (DA2) model for joint\ndepth, pose, and intrinsics prediction. We introduce an attention-based pose\nnetwork and a Weight-Decomposed Low-Rank Adaptation (DoRA) strategy for\nefficient fine-tuning of DA2. Our method is validated on the SCARED and C3VD\npublic datasets, demonstrating superior performance compared to recent\nstate-of-the-art approaches in self-supervised monocular depth estimation and\n3D reconstruction. Code and model weights can be found in project repository:\nhttps://github.com/MOYF-beta/EndoSfM3D.\n","authors":["Changhao Zhang","Matthew J. Clarkson","Mobarak I. Hoque"],"pdf_url":"https://arxiv.org/pdf/2510.22359v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2505.21381v6","updated":"2025-10-25T16:08:37Z","published":"2025-05-27T16:09:50Z","title":"ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding","summary":"  State Space models (SSMs) such as PointMamba enable efficient feature\nextraction for point cloud self-supervised learning with linear complexity,\noutperforming Transformers in computational efficiency. However, existing\nPointMamba-based methods depend on complex token ordering and random masking,\nwhich disrupt spatial continuity and local semantic correlations. We propose\nZigzagPointMamba to tackle these challenges. The core of our approach is a\nsimple zigzag scan path that globally sequences point cloud tokens, enhancing\nspatial continuity by preserving the proximity of spatially adjacent point\ntokens. Nevertheless, random masking undermines local semantic modeling in\nself-supervised learning. To address this, we introduce a Semantic-Siamese\nMasking Strategy (SMS), which masks semantically similar tokens to facilitate\nreconstruction by integrating local features of original and similar tokens.\nThis overcomes the dependence on isolated local features and enables robust\nglobal semantic modeling. Our pre-trained ZigzagPointMamba weights\nsignificantly improve downstream tasks, achieving a 1.59% mIoU gain on\nShapeNetPart for part segmentation, a 0.4% higher accuracy on ModelNet40 for\nclassification, and 0.19%, 1.22%, and 0.72% higher accuracies respectively for\nthe classification tasks on the OBJ-BG, OBJ-ONLY, and PB-T50-RS subsets of\nScanObjectNN.\n","authors":["Linshuang Diao","Sensen Song","Yurong Qian","Dayong Ren"],"pdf_url":"https://arxiv.org/pdf/2505.21381v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22340v1","updated":"2025-10-25T15:49:45Z","published":"2025-10-25T15:49:45Z","title":"DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical\n  Reasoning of VLMs in Solid Geometry","summary":"  Solid geometry problem solving demands spatial mathematical reasoning that\nintegrates spatial intelligence and symbolic reasoning. However, most existing\nmultimodal mathematical reasoning benchmarks focus primarily on 2D plane\ngeometry, rely on static datasets prone to data contamination and memorization,\nand evaluate models solely by final answers, overlooking the reasoning process.\nTo address these limitations, we introduce DynaSolidGeo, the first dynamic\nbenchmark for evaluating genuine spatial reasoning in Vision-Language Models\n(VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo\ncontains 503 expert-curated seed questions that can, in principle, dynamically\ngenerate an unbounded number of diverse multimodal text-visual instances.\nBeyond answer accuracy, we incorporate process evaluation based on\nexpert-annotated reasoning chains to measure logical validity and causal\ncoherence. Experiments across representative open-source and closed-source VLMs\nreveal large performance gaps, severe degradation in dynamic settings, and poor\nperformance on tasks requiring high-level spatial intelligence, such as mental\nrotation and visualization. The code and dataset are available at\n\\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.\n","authors":["Changti Wu","Shijie Lian","Zihao Liu","Lei Zhang","Laurence Tianruo Yang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2510.22340v1.pdf","comment":"The code and dataset are available at\n  \\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}"},{"id":"http://arxiv.org/abs/2510.22337v1","updated":"2025-10-25T15:40:34Z","published":"2025-10-25T15:40:34Z","title":"GeoDiffusion: A Training-Free Framework for Accurate 3D Geometric\n  Conditioning in Image Generation","summary":"  Precise geometric control in image generation is essential for engineering \\&\nproduct design and creative industries to control 3D object features accurately\nin image space. Traditional 3D editing approaches are time-consuming and demand\nspecialized skills, while current image-based generative methods lack accuracy\nin geometric conditioning. To address these challenges, we propose\nGeoDiffusion, a training-free framework for accurate and efficient geometric\nconditioning of 3D features in image generation. GeoDiffusion employs a\nclass-specific 3D object as a geometric prior to define keypoints and\nparametric correlations in 3D space. We ensure viewpoint consistency through a\nrendered image of a reference 3D object, followed by style transfer to meet\nuser-defined appearance specifications. At the core of our framework is\nGeoDrag, improving accuracy and speed of drag-based image editing on geometry\nguidance tasks and general instructions on DragBench. Our results demonstrate\nthat GeoDiffusion enables precise geometric modifications across various\niterative design workflows.\n","authors":["Phillip Mueller","Talip Uenlue","Sebastian Schmidt","Marcel Kollovieh","Jiajie Fan","Stephan Guennemann","Lars Mikelsons"],"pdf_url":"https://arxiv.org/pdf/2510.22337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22335v1","updated":"2025-10-25T15:40:07Z","published":"2025-10-25T15:40:07Z","title":"Moving Beyond Diffusion: Hierarchy-to-Hierarchy Autoregression for\n  fMRI-to-Image Reconstruction","summary":"  Reconstructing visual stimuli from fMRI signals is a central challenge\nbridging machine learning and neuroscience. Recent diffusion-based methods\ntypically map fMRI activity to a single high-level embedding, using it as fixed\nguidance throughout the entire generation process. However, this fixed guidance\ncollapses hierarchical neural information and is misaligned with the\nstage-dependent demands of image reconstruction. In response, we propose\nMindHier, a coarse-to-fine fMRI-to-image reconstruction framework built on\nscale-wise autoregressive modeling. MindHier introduces three components: a\nHierarchical fMRI Encoder to extract multi-level neural embeddings, a\nHierarchy-to-Hierarchy Alignment scheme to enforce layer-wise correspondence\nwith CLIP features, and a Scale-Aware Coarse-to-Fine Neural Guidance strategy\nto inject these embeddings into autoregression at matching scales. These\ndesigns make MindHier an efficient and cognitively-aligned alternative to\ndiffusion-based methods by enabling a hierarchical reconstruction process that\nsynthesizes global semantics before refining local details, akin to human\nvisual perception. Extensive experiments on the NSD dataset show that MindHier\nachieves superior semantic fidelity, 4.67x faster inference, and more\ndeterministic results than the diffusion-based baselines.\n","authors":["Xu Zhang","Ruijie Quan","Wenguan Wang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2510.22335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.22828v3","updated":"2025-10-25T15:36:26Z","published":"2025-07-30T16:42:02Z","title":"CapRecover: A Cross-Modality Feature Inversion Attack Framework on\n  Vision Language Models","summary":"  As Vision-Language Models (VLMs) are increasingly deployed in split-DNN\nconfigurations--with visual encoders (e.g., ResNet, ViT) operating on user\ndevices and sending intermediate features to the cloud--there is a growing\nprivacy risk from semantic information leakage. Existing approaches to\nreconstructing images from these intermediate features often result in blurry,\nsemantically ambiguous images. To directly address semantic leakage, we propose\nCapRecover, a cross-modality inversion framework that recovers high-level\nsemantic content, such as labels or captions, directly from intermediate\nfeatures without image reconstruction.\n  We evaluate CapRecover on multiple datasets and victim models, demonstrating\nstrong performance in semantic recovery. Specifically, CapRecover achieves up\nto 92.71% Top-1 label accuracy on CIFAR-10 and generates fluent captions from\nResNet50 features on COCO2017 with ROUGE-L scores up to 0.52. Our analysis\nfurther reveals that deeper convolutional layers encode significantly more\nsemantic information compared to shallow layers. To mitigate semantic leakage,\nwe introduce a simple yet effective protection method: adding random noise to\nintermediate features at each layer and removing the noise in the next layer.\nExperimental results show that this approach prevents semantic leakage without\nadditional training costs. Our code is available at\nhttps://jus1mple.github.io/Image2CaptionAttack.\n","authors":["Kedong Xiu","Sai Qian Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.22828v3.pdf","comment":"9 pages, accepted by the 2025 ACM Multimedia Conference. Code is\n  available at https://jus1mple.github.io/Image2CaptionAttack"},{"id":"http://arxiv.org/abs/2510.22322v1","updated":"2025-10-25T15:00:38Z","published":"2025-10-25T15:00:38Z","title":"Beyond Augmentation: Leveraging Inter-Instance Relation in\n  Self-Supervised Representation Learning","summary":"  This paper introduces a novel approach that integrates graph theory into\nself-supervised representation learning. Traditional methods focus on\nintra-instance variations generated by applying augmentations. However, they\noften overlook important inter-instance relationships. While our method retains\nthe intra-instance property, it further captures inter-instance relationships\nby constructing k-nearest neighbor (KNN) graphs for both teacher and student\nstreams during pretraining. In these graphs, nodes represent samples along with\ntheir latent representations. Edges encode the similarity between instances.\nFollowing pretraining, a representation refinement phase is performed. In this\nphase, Graph Neural Networks (GNNs) propagate messages not only among immediate\nneighbors but also across multiple hops, thereby enabling broader contextual\nintegration. Experimental results on CIFAR-10, ImageNet-100, and ImageNet-1K\ndemonstrate accuracy improvements of 7.3%, 3.2%, and 1.0%, respectively, over\nstate-of-the-art methods. These results highlight the effectiveness of the\nproposed graph based mechanism. The code is publicly available at\nhttps://github.com/alijavidani/SSL-GraphNNCLR.\n","authors":["Ali Javidani","Babak Nadjar Araabi","Mohammad Amin Sadeghi"],"pdf_url":"https://arxiv.org/pdf/2510.22322v1.pdf","comment":"Accepted in IEEE Signal Processing Letters, 2025"},{"id":"http://arxiv.org/abs/2510.22319v1","updated":"2025-10-25T14:51:17Z","published":"2025-10-25T14:51:17Z","title":"GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via\n  Regulated Clipping","summary":"  Recently, GRPO-based reinforcement learning has shown remarkable progress in\noptimizing flow-matching models, effectively improving their alignment with\ntask-specific rewards. Within these frameworks, the policy update relies on\nimportance-ratio clipping to constrain overconfident positive and negative\ngradients. However, in practice, we observe a systematic shift in the\nimportance-ratio distribution-its mean falls below 1 and its variance differs\nsubstantially across timesteps. This left-shifted and inconsistent distribution\nprevents positive-advantage samples from entering the clipped region, causing\nthe mechanism to fail in constraining overconfident positive updates. As a\nresult, the policy model inevitably enters an implicit over-optimization\nstage-while the proxy reward continues to increase, essential metrics such as\nimage quality and text-prompt alignment deteriorate sharply, ultimately making\nthe learned policy impractical for real-world use. To address this issue, we\nintroduce GRPO-Guard, a simple yet effective enhancement to existing GRPO\nframeworks. Our method incorporates ratio normalization, which restores a\nbalanced and step-consistent importance ratio, ensuring that PPO clipping\nproperly constrains harmful updates across denoising timesteps. In addition, a\ngradient reweighting strategy equalizes policy gradients over noise conditions,\npreventing excessive updates from particular timestep regions. Together, these\ndesigns act as a regulated clipping mechanism, stabilizing optimization and\nsubstantially mitigating implicit over-optimization without relying on heavy KL\nregularization. Extensive experiments on multiple diffusion backbones (e.g.,\nSD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard\nsignificantly reduces over-optimization while maintaining or even improving\ngeneration quality.\n","authors":["Jing Wang","Jiajun Liang","Jie Liu","Henglin Liu","Gongye Liu","Jun Zheng","Wanyuan Pang","Ao Ma","Zhenyu Xie","Xintao Wang","Meng Wang","Pengfei Wan","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2510.22319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22300v1","updated":"2025-10-25T14:00:26Z","published":"2025-10-25T14:00:26Z","title":"T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense\n  on Text-to-Image Model","summary":"  Using risky text prompts, such as pornography and violent prompts, to test\nthe safety of text-to-image (T2I) models is a critical task. However, existing\nrisky prompt datasets are limited in three key areas: 1) limited risky\ncategories, 2) coarse-grained annotation, and 3) low effectiveness. To address\nthese limitations, we introduce T2I-RiskyPrompt, a comprehensive benchmark\ndesigned for evaluating safety-related tasks in T2I models. Specifically, we\nfirst develop a hierarchical risk taxonomy, which consists of 6 primary\ncategories and 14 fine-grained subcategories. Building upon this taxonomy, we\nconstruct a pipeline to collect and annotate risky prompts. Finally, we obtain\n6,432 effective risky prompts, where each prompt is annotated with both\nhierarchical category labels and detailed risk reasons. Moreover, to facilitate\nthe evaluation, we propose a reason-driven risky image detection method that\nexplicitly aligns the MLLM with safety annotations. Based on T2I-RiskyPrompt,\nwe conduct a comprehensive evaluation of eight T2I models, nine defense\nmethods, five safety filters, and five attack strategies, offering nine key\ninsights into the strengths and limitations of T2I model safety. Finally, we\ndiscuss potential applications of T2I-RiskyPrompt across various research\nfields. The dataset and code are provided in\nhttps://github.com/datar001/T2I-RiskyPrompt.\n","authors":["Chenyu Zhang","Tairen Zhang","Lanjun Wang","Ruidong Chen","Wenhui Li","Anan Liu"],"pdf_url":"https://arxiv.org/pdf/2510.22300v1.pdf","comment":"AAAI under review"},{"id":"http://arxiv.org/abs/2506.04470v2","updated":"2025-10-25T13:56:07Z","published":"2025-06-04T21:40:01Z","title":"A Poisson-Guided Decomposition Network for Extreme Low-Light Image\n  Enhancement","summary":"  Low-light image denoising and enhancement are challenging, especially when\ntraditional noise assumptions, such as Gaussian noise, do not hold in majority.\nIn many real-world scenarios, such as low-light imaging, noise is\nsignal-dependent and is better represented as Poisson noise. In this work, we\naddress the problem of denoising images degraded by Poisson noise under extreme\nlow-light conditions. We introduce a light-weight deep learning-based method\nthat integrates Retinex based decomposition with Poisson denoising into a\nunified encoder-decoder network. The model simultaneously enhances illumination\nand suppresses noise by incorporating a Poisson denoising loss to address\nsignal-dependent noise. Without prior requirement for reflectance and\nillumination, the network learns an effective decomposition process while\nensuring consistent reflectance and smooth illumination without causing any\nform of color distortion. The experimental results demonstrate the\neffectiveness and practicality of the proposed low-light illumination\nenhancement method. Our method significantly improves visibility and brightness\nin low-light conditions, while preserving image structure and color constancy\nunder ambient illumination.\n","authors":["Isha Rao","Ratul Chakraborty","Sanjay Ghosh"],"pdf_url":"https://arxiv.org/pdf/2506.04470v2.pdf","comment":"8 pages, 3 figures and 1 table"},{"id":"http://arxiv.org/abs/2404.17922v2","updated":"2025-10-25T13:48:58Z","published":"2024-04-27T14:20:46Z","title":"Open-Set 3D Semantic Instance Maps for Vision Language Navigation --\n  O3D-SIM","summary":"  Humans excel at forming mental maps of their surroundings, equipping them to\nunderstand object relationships and navigate based on language queries. Our\nprevious work, SI Maps (Nanwani L, Agarwal A, Jain K, et al. Instance-level\nsemantic maps for vision language navigation. In: 2023 32nd IEEE International\nConference on Robot and Human Interactive Communication (RO-MAN). IEEE; 2023\nAug.), showed that having instance-level information and the semantic\nunderstanding of an environment helps significantly improve performance for\nlanguage-guided tasks. We extend this instance-level approach to 3D while\nincreasing the pipeline's robustness and improving quantitative and qualitative\nresults. Our method leverages foundational models for object recognition, image\nsegmentation, and feature extraction. We propose a representation that results\nin a 3D point cloud map with instance-level embeddings, which bring in the\nsemantic understanding that natural language commands can query.\nQuantitatively, the work improves upon the success rate of language-guided\ntasks. At the same time, we qualitatively observe the ability to identify\ninstances more clearly and leverage the foundational models and language and\nimage-aligned embeddings to identify objects that, otherwise, a closed-set\napproach wouldn't be able to identify.\n  Project Page - https://smart-wheelchair-rrc.github.io/o3d-sim-webpage\n","authors":["Laksh Nanwani","Kumaraditya Gupta","Aditya Mathur","Swayam Agrawal","A. H. Abdul Hafez","K. Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2404.17922v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09049v2","updated":"2025-10-25T13:01:56Z","published":"2025-01-15T12:11:33Z","title":"Dynamic-Aware Spatio-temporal Representation Learning for Dynamic MRI\n  Reconstruction","summary":"  Dynamic MRI reconstruction, one of inverse problems, has seen a surge by the\nuse of deep learning techniques. Especially, the practical difficulty of\nobtaining ground truth data has led to the emergence of unsupervised learning\napproaches. A recent promising method among them is implicit neural\nrepresentation (INR), which defines the data as a continuous function that maps\ncoordinate values to the corresponding signal values. This allows for filling\nin missing information only with incomplete measurements and solving the\ninverse problem effectively. Nevertheless, previous works incorporating this\nmethod have faced drawbacks such as long optimization time and the need for\nextensive hyperparameter tuning. To address these issues, we propose\nDynamic-Aware INR (DA-INR), an INR-based model for dynamic MRI reconstruction\nthat captures the spatial and temporal continuity of dynamic MRI data in the\nimage domain and explicitly incorporates the temporal redundancy of the data\ninto the model structure. As a result, DA-INR outperforms other models in\nreconstruction quality even at extreme undersampling ratios while significantly\nreducing optimization time and requiring minimal hyperparameter tuning.\n","authors":["Dayoung Baik","Jaejun Yoo"],"pdf_url":"https://arxiv.org/pdf/2501.09049v2.pdf","comment":"MICCAI2025"},{"id":"http://arxiv.org/abs/2510.22282v1","updated":"2025-10-25T12:56:46Z","published":"2025-10-25T12:56:46Z","title":"CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language\n  Models via Reinforcement Learning","summary":"  Harnessing publicly available, large-scale web data, such as street view and\nsatellite imagery, urban socio-economic sensing is of paramount importance for\nachieving global sustainable development goals. With the emergence of Large\nVision-Language Models (LVLMs), new opportunities have arisen to solve this\ntask by treating it as a multi-modal perception and understanding problem.\nHowever, recent studies reveal that LVLMs still struggle with accurate and\ninterpretable socio-economic predictions from visual data. To address these\nlimitations and maximize the potential of LVLMs, we introduce\n\\textbf{CityRiSE}, a novel framework for \\textbf{R}eason\\textbf{i}ng urban\n\\textbf{S}ocio-\\textbf{E}conomic status in LVLMs through pure reinforcement\nlearning (RL). With carefully curated multi-modal data and verifiable reward\ndesign, our approach guides the LVLM to focus on semantically meaningful visual\ncues, enabling structured and goal-oriented reasoning for generalist\nsocio-economic status prediction. Experiments demonstrate that CityRiSE with\nemergent reasoning process significantly outperforms existing baselines,\nimproving both prediction accuracy and generalization across diverse urban\ncontexts, particularly for prediction on unseen cities and unseen indicators.\nThis work highlights the promise of combining RL and LVLMs for interpretable\nand generalist urban socio-economic sensing.\n","authors":["Tianhui Liu","Hetian Pang","Xin Zhang","Jie Feng","Yong Li","Pan Hui"],"pdf_url":"https://arxiv.org/pdf/2510.22282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22276v1","updated":"2025-10-25T12:42:42Z","published":"2025-10-25T12:42:42Z","title":"WAON: Large-Scale and High-Quality Japanese Image-Text Pair Dataset for\n  Vision-Language Models","summary":"  Large-scale and high-quality image-text pair datasets play an important role\nin developing high-performing Vision-Language Models (VLMs). In this work, we\nintroduce WAON, a large-scale and high-quality Japanese image-text pair dataset\ncontaining approximately 155 million examples, collected from Common Crawl. Our\ndataset construction pipeline employs various techniques, including filtering\nand deduplication, which have been shown to be effective in previous studies.\nTo evaluate its effectiveness, we also construct WAON-Bench, a manually curated\nbenchmark for Japanese cultural image classification, consisting of 374\nclasses. To assess the effectiveness of our dataset, we conduct experiments\nusing both WAON and the Japanese subset of ReLAION, one of the most widely used\nvision-language datasets. We fine-tune SigLIP2, a strong multilingual model, on\nboth datasets. The results demonstrate that WAON enhances model performance on\nWAON-Bench more efficiently than ReLAION and achieves higher accuracy across\nall evaluated benchmarks. Furthermore, the model fine-tuned on WAON achieves\nstate-of-the-art performance on several Japanese cultural benchmarks. We\nrelease our dataset, model, and code at https://speed1313.github.io/WAON.\n","authors":["Issa Sugiura","Shuhei Kurita","Yusuke Oda","Daisuke Kawahara","Yasuo Okabe","Naoaki Okazaki"],"pdf_url":"https://arxiv.org/pdf/2510.22276v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.10635v2","updated":"2025-10-25T12:23:47Z","published":"2025-03-13T17:59:55Z","title":"A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90%\n  Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1","summary":"  Despite promising performance on open-source large vision-language models\n(LVLMs), transfer-based targeted attacks often fail against closed-source\ncommercial LVLMs. Analyzing failed adversarial perturbations reveals that the\nlearned perturbations typically originate from a uniform distribution and lack\nclear semantic details, resulting in unintended responses. This critical\nabsence of semantic information leads commercial black-box LVLMs to either\nignore the perturbation entirely or misinterpret its embedded semantics,\nthereby causing the attack to fail. To overcome these issues, we propose to\nrefine semantic clarity by encoding explicit semantic details within local\nregions, thus ensuring the capture of finer-grained features and inter-model\ntransferability, and by concentrating modifications on semantically rich areas\nrather than applying them uniformly. To achieve this, we propose a simple yet\nhighly effective baseline: at each optimization step, the adversarial image is\ncropped randomly by a controlled aspect ratio and scale, resized, and then\naligned with the target image in the embedding space. While the naive\nsource-target matching method has been utilized before in the literature, we\nare the first to provide a tight analysis, which establishes a close connection\nbetween perturbation optimization and semantics. Experimental results confirm\nour hypothesis. Our adversarial examples crafted with local-aggregated\nperturbations focused on crucial regions exhibit surprisingly good\ntransferability to commercial LVLMs, including GPT-4.5, GPT-4o,\nGemini-2.0-flash, Claude-3.5/3.7-sonnet, and even reasoning models like o1,\nClaude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach achieves\nsuccess rates exceeding 90% on GPT-4.5, 4o, and o1, significantly outperforming\nall prior state-of-the-art attack methods with lower $\\ell_1/\\ell_2$\nperturbations.\n","authors":["Zhaoyi Li","Xiaohan Zhao","Dong-Dong Wu","Jiacheng Cui","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2503.10635v2.pdf","comment":"NeurIPS 2025. Code at: https://github.com/VILA-Lab/M-Attack"},{"id":"http://arxiv.org/abs/2510.22268v1","updated":"2025-10-25T12:16:10Z","published":"2025-10-25T12:16:10Z","title":"GSAlign: Geometric and Semantic Alignment Network for Aerial-Ground\n  Person Re-Identification","summary":"  Aerial-Ground person re-identification (AG-ReID) is an emerging yet\nchallenging task that aims to match pedestrian images captured from drastically\ndifferent viewpoints, typically from unmanned aerial vehicles (UAVs) and\nground-based surveillance cameras. The task poses significant challenges due to\nextreme viewpoint discrepancies, occlusions, and domain gaps between aerial and\nground imagery. While prior works have made progress by learning cross-view\nrepresentations, they remain limited in handling severe pose variations and\nspatial misalignment. To address these issues, we propose a Geometric and\nSemantic Alignment Network (GSAlign) tailored for AG-ReID. GSAlign introduces\ntwo key components to jointly tackle geometric distortion and semantic\nmisalignment in aerial-ground matching: a Learnable Thin Plate Spline (LTPS)\nModule and a Dynamic Alignment Module (DAM). The LTPS module adaptively warps\npedestrian features based on a set of learned keypoints, effectively\ncompensating for geometric variations caused by extreme viewpoint changes. In\nparallel, the DAM estimates visibility-aware representation masks that\nhighlight visible body regions at the semantic level, thereby alleviating the\nnegative impact of occlusions and partial observations in cross-view\ncorrespondence. A comprehensive evaluation on CARGO with four matching\nprotocols demonstrates the effectiveness of GSAlign, achieving significant\nimprovements of +18.8\\% in mAP and +16.8\\% in Rank-1 accuracy over previous\nstate-of-the-art methods on the aerial-ground setting. The code is available\nat: \\textcolor{magenta}{https://github.com/stone96123/GSAlign}.\n","authors":["Qiao Li","Jie Li","Yukang Zhang","Lei Tan","Jing Chen","Jiayi Ji"],"pdf_url":"https://arxiv.org/pdf/2510.22268v1.pdf","comment":"Accepted by Neurips 2025"},{"id":"http://arxiv.org/abs/2406.04772v4","updated":"2025-10-25T12:13:18Z","published":"2024-06-07T09:17:33Z","title":"REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning","summary":"  Recent rehearsal-free continual learning (CL) methods guided by prompts\nachieve strong performance on vision tasks with non-stationary data but remain\nresource-intensive, hindering real-world edge deployment. We introduce\nresource-efficient prompting (REP), which improves the computational and memory\nefficiency of prompt-based rehearsal-free continual learning methods while\nminimizing accuracy trade-offs. Our approach employs swift prompt selection to\nrefine input data using a carefully provisioned model and introduces adaptive\ntoken merging (AToM) and adaptive layer dropping (ALD) for efficient prompt\nupdates. AToM and ALD selectively skip data and model layers while preserving\ntask-specific features during the learning of new tasks. Extensive experiments\non multiple image classification datasets demonstrate REP's superior resource\nefficiency over state-of-the-art rehearsal-free CL methods.\n","authors":["Sungho Jeon","Xinyue Ma","Kwang In Kim","Myeongjae Jeon"],"pdf_url":"https://arxiv.org/pdf/2406.04772v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22260v1","updated":"2025-10-25T11:57:22Z","published":"2025-10-25T11:57:22Z","title":"Accident Anticipation via Temporal Occurrence Prediction","summary":"  Accident anticipation aims to predict potential collisions in an online\nmanner, enabling timely alerts to enhance road safety. Existing methods\ntypically predict frame-level risk scores as indicators of hazard. However,\nthese approaches rely on ambiguous binary supervision (labeling all frames in\naccident videos as positive) despite the fact that risk varies continuously\nover time, leading to unreliable learning and false alarms. To address this, we\npropose a novel paradigm that shifts the prediction target from current-frame\nrisk scoring to directly estimating accident scores at multiple future time\nsteps (e.g., 0.1s-2.0s ahead), leveraging precisely annotated accident\ntimestamps as supervision. Our method employs a snippet-level encoder to\njointly model spatial and temporal dynamics, and a Transformer-based temporal\ndecoder that predicts accident scores for all future horizons simultaneously\nusing dedicated temporal queries. Furthermore, we introduce a refined\nevaluation protocol that reports Time-to-Accident (TTA) and recall (evaluated\nat multiple pre-accident intervals (0.5s, 1.0s, and 1.5s)) only when the false\nalarm rate (FAR) remains within an acceptable range, ensuring practical\nrelevance. Experiments show that our method achieves superior performance in\nboth recall and TTA under realistic FAR constraints.\n","authors":["Tianhao Zhao","Yiyang Zou","Zihao Mao","Peilun Xiao","Yulin Huang","Hongda Yang","Yuxuan Li","Qun Li","Guobin Wu","Yutian Lin"],"pdf_url":"https://arxiv.org/pdf/2510.22260v1.pdf","comment":"Accepted by NIPS 2025"},{"id":"http://arxiv.org/abs/2510.02745v2","updated":"2025-10-25T11:34:54Z","published":"2025-10-03T06:16:58Z","title":"Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient\n  Multimodal Retrieval","summary":"  The success of DeepSeek-R1 demonstrates the immense potential of using\nreinforcement learning (RL) to enhance LLMs' reasoning capabilities. This paper\nintroduces Retrv-R1, the first R1-style MLLM specifically designed for\nmultimodal universal retrieval, achieving higher performance by employing\nstep-by-step reasoning to produce more accurate retrieval results. We find that\ndirectly applying the methods of DeepSeek-R1 to retrieval tasks is not\nfeasible, mainly due to (1) the high computational cost caused by the large\ntoken consumption required for multiple candidates with reasoning processes,\nand (2) the instability and suboptimal results when directly applying RL to\ntrain for retrieval tasks. To address these issues, Retrv-R1 introduces an\ninformation compression module with a details inspection mechanism, which\nenhances computational efficiency by reducing the number of tokens while\nensuring that critical information for challenging candidates is preserved.\nFurthermore, a new training paradigm is proposed, including an activation stage\nusing a retrieval-tailored synthetic CoT dataset for more effective\noptimization, followed by RL with a novel curriculum reward to improve both\nperformance and efficiency. Incorporating these novel designs, Retrv-R1\nachieves SOTA performance, high efficiency, and strong generalization ability,\nas demonstrated by experiments across multiple benchmarks and tasks.\n","authors":["Lanyun Zhu","Deyi Ji","Tianrun Chen","Haiyang Wu","Shiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2510.02745v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2411.00617v3","updated":"2025-10-25T11:20:19Z","published":"2024-11-01T14:25:54Z","title":"Continuous and complete liver vessel segmentation with graph-attention\n  guided diffusion","summary":"  Improving connectivity and completeness are the most challenging aspects of\nliver vessel segmentation, especially for small vessels. These challenges\nrequire both learning the continuous vessel geometry, and focusing on small\nvessel detection. However, current methods do not explicitly address these two\naspects and cannot generalize well when constrained by inconsistent\nannotations. Here, we take advantage of the generalization of the diffusion\nmodel and explicitly integrate connectivity and completeness in our\ndiffusion-based segmentation model. Specifically, we use a graph-attention\nmodule that adds knowledge about vessel geometry, and thus adds continuity.\nAdditionally, we perform the graph-attention at multiple-scales, thus focusing\non small liver vessels. Our method outperforms eight state-of-the-art medical\nsegmentation methods on two public datasets: 3D-ircadb-01 and LiVS. Our code is\navailable at https://github.com/ZhangXiaotong015/GATSegDiff.\n","authors":["Xiaotong Zhang","Alexander Broersen","Gonnie CM van Erp","Silvia L. Pintea","Jouke Dijkstra"],"pdf_url":"https://arxiv.org/pdf/2411.00617v3.pdf","comment":"Accepted by Knowledge-Based Systems"},{"id":"http://arxiv.org/abs/2510.22243v1","updated":"2025-10-25T10:16:22Z","published":"2025-10-25T10:16:22Z","title":"Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using\n  LMIINet with the CGRA4ML Framework","summary":"  Semantic segmentation has emerged as a fundamental problem in computer\nvision, gaining particular importance in real-time applications such as\nautonomous driving. The main challenge is achieving high accuracy while\noperating under computational and hardware constraints. In this research, we\npresent an FPGA-based implementation of real-time semantic segmentation\nleveraging the lightweight LMIINet architecture and the Coarse-Grained\nReconfigurable Array for Machine Learning (CGRA4ML) hardware framework. The\nmodel was trained using Quantization-Aware Training (QAT) with 8-bit precision\non the Cityscapes dataset, reducing memory footprint by a factor of four while\nenabling efficient fixed-point computations. Necessary modifications were\napplied to adapt the model to CGRA4ML constraints, including simplifying skip\nconnections, employing hardware-friendly operations such as depthwise-separable\nand 1A-1 convolutions, and redesigning parts of the Flatten Transformer. Our\nimplementation achieves approximately 90% pixel accuracy and 45% mean\nIntersection-over-Union (mIoU), operating in real-time at 20 frames per second\n(FPS) with 50.1 ms latency on the ZCU104 FPGA board. The results demonstrate\nthe potential of CGRA4ML, with its flexibility in mapping modern layers and\noff-chip memory utilization for skip connections, provides a path for\nimplementing advanced semantic segmentation networks on FPGA for real-time\napplications to outperform traditional GPU solutions in terms of power\nefficiency while maintaining competitive accuracy. The code for this project is\npublicly available at https://github.com/STAmirr/ cgra4ml_semantic_segmentation\n","authors":["Amir Mohammad Khadem Hosseini","Sattar Mirzakuchaki"],"pdf_url":"https://arxiv.org/pdf/2510.22243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19549v2","updated":"2025-10-25T10:08:26Z","published":"2025-04-28T07:55:11Z","title":"DEEMO: De-identity Multimodal Emotion Recognition and Reasoning","summary":"  Emotion understanding is a critical yet challenging task. Most existing\napproaches rely heavily on identity-sensitive information, such as facial\nexpressions and speech, which raises concerns about personal privacy. To\naddress this, we introduce the De-identity Multimodal Emotion Recognition and\nReasoning (DEEMO), a novel task designed to enable emotion understanding using\nde-identified video and audio inputs. The DEEMO dataset consists of two\nsubsets: DEEMO-NFBL, which includes rich annotations of Non-Facial Body\nLanguage (NFBL), and DEEMO-MER, an instruction dataset for Multimodal Emotion\nRecognition and Reasoning using identity-free cues. This design supports\nemotion understanding without compromising identity privacy. In addition, we\npropose DEEMO-LLaMA, a Multimodal Large Language Model (MLLM) that integrates\nde-identified audio, video, and textual information to enhance both emotion\nrecognition and reasoning. Extensive experiments show that DEEMO-LLaMA achieves\nstate-of-the-art performance on both tasks, outperforming existing MLLMs by a\nsignificant margin, achieving 74.49% accuracy and 74.45% F1-score in\nde-identity emotion recognition, and 6.20 clue overlap and 7.66 label overlap\nin de-identity emotion reasoning. Our work contributes to ethical AI by\nadvancing privacy-preserving emotion understanding and promoting responsible\naffective computing.\n","authors":["Deng Li","Bohao Xing","Xin Liu","Baiqiang Xia","Bihan Wen","Heikki Kälviäinen"],"pdf_url":"https://arxiv.org/pdf/2504.19549v2.pdf","comment":"Accepted by ACMMM 2025"},{"id":"http://arxiv.org/abs/2508.15387v6","updated":"2025-10-25T09:55:01Z","published":"2025-08-21T09:23:51Z","title":"DIO: Refining Mutual Information and Causal Chain to Enhance Machine\n  Abstract Reasoning Ability","summary":"  Despite deep learning's broad success, its abstract-reasoning bottleneck\npersists. We tackle Raven's Progressive Matrices (RPM), the benchmark for\npattern, reasoning and problem-solving intelligence. We model the full causal\nchain image $\\rightarrow$ attributes $\\rightarrow$ progressive patterns\n$\\rightarrow$ consistency $\\rightarrow$ answer and build the baseline DIO. Yet\nDIO's mutual-information lower-bound objective does not embed human logic: the\nbound is loose and statistic-based, ignoring causal subject-object links. We\ntherefore present three refinements. 1) Brando introduces trainable negative\noptions to tighten the variational bound. 2) WORLD replaces generation with a\nGaussian-mixture feature model that supplies infinite, weighted negatives,\nfurther tightening the bound. 3) DIEGO adds metadata supervision to rectify the\n\"attributes $\\rightarrow$ patterns\" semantic gap, aligning representations with\nhuman rules. These upgrades substantially boost discriminative RPM accuracy\nand, for the first time, let DIO generate valid answers in open-ended RPM. The\nwork provides causal-driven design guidelines, objective-refinement strategies\nand cross-modal insights for abstract-reasoning research.\n","authors":["Ruizhuo Song","Beiming Yuan"],"pdf_url":"https://arxiv.org/pdf/2508.15387v6.pdf","comment":"15 pages, 9 figures, 8 tables"},{"id":"http://arxiv.org/abs/2510.22236v1","updated":"2025-10-25T09:42:49Z","published":"2025-10-25T09:42:49Z","title":"DiffusionLane: Diffusion Model for Lane Detection","summary":"  In this paper, we present a novel diffusion-based model for lane detection,\ncalled DiffusionLane, which treats the lane detection task as a denoising\ndiffusion process in the parameter space of the lane. Firstly, we add the\nGaussian noise to the parameters (the starting point and the angle) of ground\ntruth lanes to obtain noisy lane anchors, and the model learns to refine the\nnoisy lane anchors in a progressive way to obtain the target lanes. Secondly,\nwe propose a hybrid decoding strategy to address the poor feature\nrepresentation of the encoder, resulting from the noisy lane anchors.\nSpecifically, we design a hybrid diffusion decoder to combine global-level and\nlocal-level decoders for high-quality lane anchors. Then, to improve the\nfeature representation of the encoder, we employ an auxiliary head in the\ntraining stage to adopt the learnable lane anchors for enriching the\nsupervision on the encoder. Experimental results on four benchmarks, Carlane,\nTusimple, CULane, and LLAMAS, show that DiffusionLane possesses a strong\ngeneralization ability and promising detection performance compared to the\nprevious state-of-the-art methods. For example, DiffusionLane with ResNet18\nsurpasses the existing methods by at least 1\\% accuracy on the domain\nadaptation dataset Carlane. Besides, DiffusionLane with MobileNetV4 gets\n81.32\\% F1 score on CULane, 96.89\\% accuracy on Tusimple with ResNet34, and\n97.59\\% F1 score on LLAMAS with ResNet101. Code will be available at\nhttps://github.com/zkyntu/UnLanedet.\n","authors":["Kunyang Zhou","Yeqin Shao"],"pdf_url":"https://arxiv.org/pdf/2510.22236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22229v1","updated":"2025-10-25T09:25:01Z","published":"2025-10-25T09:25:01Z","title":"Diffusion-Driven Two-Stage Active Learning for Low-Budget Semantic\n  Segmentation","summary":"  Semantic segmentation demands dense pixel-level annotations, which can be\nprohibitively expensive - especially under extremely constrained labeling\nbudgets. In this paper, we address the problem of low-budget active learning\nfor semantic segmentation by proposing a novel two-stage selection pipeline.\nOur approach leverages a pre-trained diffusion model to extract rich\nmulti-scale features that capture both global structure and fine details. In\nthe first stage, we perform a hierarchical, representation-based candidate\nselection by first choosing a small subset of representative pixels per image\nusing MaxHerding, and then refining these into a diverse global pool. In the\nsecond stage, we compute an entropy-augmented disagreement score (eDALD) over\nnoisy multi-scale diffusion features to capture both epistemic uncertainty and\nprediction confidence, selecting the most informative pixels for annotation.\nThis decoupling of diversity and uncertainty lets us achieve high segmentation\naccuracy with only a tiny fraction of labeled pixels. Extensive experiments on\nfour benchmarks (CamVid, ADE-Bed, Cityscapes, and Pascal-Context) demonstrate\nthat our method significantly outperforms existing baselines under extreme\npixel-budget regimes. Our code is available at\nhttps://github.com/jn-kim/two-stage-edald.\n","authors":["Jeongin Kim","Wonho Bae","YouLee Han","Giyeong Oh","Youngjae Yu","Danica J. Sutherland","Junhyug Noh"],"pdf_url":"https://arxiv.org/pdf/2510.22229v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.22225v1","updated":"2025-10-25T09:10:29Z","published":"2025-10-25T09:10:29Z","title":"Audio Frequency-Time Dual Domain Evaluation on Depression Diagnosis","summary":"  Depression, as a typical mental disorder, has become a prevalent issue\nsignificantly impacting public health. However, the prevention and treatment of\ndepression still face multiple challenges, including complex diagnostic\nprocedures, ambiguous criteria, and low consultation rates, which severely\nhinder timely assessment and intervention. To address these issues, this study\nadopts voice as a physiological signal and leverages its frequency-time dual\ndomain multimodal characteristics along with deep learning models to develop an\nintelligent assessment and diagnostic algorithm for depression. Experimental\nresults demonstrate that the proposed method achieves excellent performance in\nthe classification task for depression diagnosis, offering new insights and\napproaches for the assessment, screening, and diagnosis of depression.\n","authors":["Yu Luo","Nan Huang","Sophie Yu","Hendry Xu","Jerry Wang","Colin Wang","Zhichao Liu","Chen Zeng"],"pdf_url":"https://arxiv.org/pdf/2510.22225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14350v3","updated":"2025-10-25T09:08:18Z","published":"2025-03-18T15:31:12Z","title":"VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded\n  Generation","summary":"  Recent video diffusion models have enhanced video editing, but it remains\nchallenging to handle instructional editing and diverse tasks (e.g., adding,\nremoving, changing) within a unified framework. In this paper, we introduce\nVEGGIE, a Video Editor with Grounded Generation from Instructions, a simple\nend-to-end framework that unifies video concept editing, grounding, and\nreasoning based on diverse user instructions. Specifically, given a video and\ntext query, VEGGIE first utilizes an MLLM to interpret user intentions in\ninstructions and ground them to the video contexts, generating frame-specific\ngrounded task queries for pixel-space responses. A diffusion model then renders\nthese plans and generates edited videos that align with user intent. To support\ndiverse tasks and complex instructions, we employ a curriculum learning\nstrategy: first aligning the MLLM and video diffusion model with large-scale\ninstructional image editing data, followed by end-to-end fine-tuning on\nhigh-quality multitask video data. Additionally, we introduce a novel data\nsynthesis pipeline to generate paired instructional video editing data for\nmodel training. It transforms static image data into diverse, high-quality\nvideo editing samples by leveraging Image-to-Video models to inject dynamics.\nVEGGIE shows strong performance in instructional video editing with different\nediting skills, outperforming the best instructional baseline as a versatile\nmodel, while other models struggle with multi-tasking. VEGGIE also excels in\nvideo object grounding and reasoning segmentation, where other baselines fail.\nWe further reveal how the multiple tasks help each other and highlight\npromising applications like zero-shot multimodal instructional and in-context\nvideo editing.\n","authors":["Shoubin Yu","Difan Liu","Ziqiao Ma","Yicong Hong","Yang Zhou","Hao Tan","Joyce Chai","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2503.14350v3.pdf","comment":"ICCV 2025; First three authors contributed equally. Project page:\n  https://veggie-gen.github.io/"},{"id":"http://arxiv.org/abs/2506.17113v2","updated":"2025-10-25T08:57:40Z","published":"2025-06-20T16:14:13Z","title":"MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert\n  Aggregation","summary":"  Combining pre-trained expert models offers substantial potential for scalable\nmultimodal reasoning, but building a unified framework remains challenging due\nto the increasing diversity of input modalities and task complexity. For\ninstance, medical diagnosis requires precise reasoning over structured clinical\ntables, while financial forecasting depends on interpreting plot-based data to\nmake informed predictions. To tackle this challenge, we introduce MEXA, a\ntraining-free framework that performs modality- and task-aware aggregation of\nmultiple expert models to enable effective multimodal reasoning across diverse\nand distinct domains. MEXA dynamically selects expert models based on the input\nmodality and the task-specific reasoning demands (i.e., skills). Each expert\nmodel, specialized in a modality task pair, generates interpretable textual\nreasoning outputs. MEXA then aggregates and reasons over these outputs using a\nLarge Reasoning Model (LRM) to produce the final answer. This modular design\nallows flexible and transparent multimodal reasoning across diverse domains\nwithout additional training overhead. We extensively evaluate our approach on\ndiverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D\nUnderstanding, and Medical QA. MEXA consistently delivers performance\nimprovements over strong multimodal baselines, highlighting the effectiveness\nand broad applicability of our expert-driven selection and aggregation in\ndiverse multimodal reasoning tasks.\n","authors":["Shoubin Yu","Yue Zhang","Ziyang Wang","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2506.17113v2.pdf","comment":"EMNLP 2025 Findings; The first two authors contributed equally;\n  Github link: https://github.com/Yui010206/MEXA"},{"id":"http://arxiv.org/abs/2505.09252v2","updated":"2025-10-25T08:57:24Z","published":"2025-05-14T09:54:46Z","title":"Zero-Shot Multi-modal Large Language Model v.s. Supervised Deep\n  Learning: A Comparative Study on CT-Based Intracranial Hemorrhage Subtyping","summary":"  Introduction: Timely identification of intracranial hemorrhage (ICH) subtypes\non non-contrast computed tomography is critical for prognosis prediction and\ntherapeutic decision-making, yet remains challenging due to low contrast and\nblurring boundaries. This study evaluates the performance of zero-shot\nmulti-modal large language models (MLLMs) compared to traditional deep learning\nmethods in ICH binary classification and subtyping. Methods: We utilized a\ndataset provided by RSNA, comprising 192 NCCT volumes. The study compares\nvarious MLLMs, including GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2,\nwith conventional deep learning models, including ResNet50 and Vision\nTransformer. Carefully crafted prompts were used to guide MLLMs in tasks such\nas ICH presence, subtype classification, localization, and volume estimation.\nResults: The results indicate that in the ICH binary classification task,\ntraditional deep learning models outperform MLLMs comprehensively. For subtype\nclassification, MLLMs also exhibit inferior performance compared to traditional\ndeep learning models, with Gemini 2.0 Flash achieving an macro-averaged\nprecision of 0.41 and a macro-averaged F1 score of 0.31. Conclusion: While\nMLLMs excel in interactive capabilities, their overall accuracy in ICH\nsubtyping is inferior to deep networks. However, MLLMs enhance interpretability\nthrough language interactions, indicating potential in medical imaging\nanalysis. Future efforts will focus on model refinement and developing more\nprecise MLLMs to improve performance in three-dimensional medical image\nprocessing.\n","authors":["Yinuo Wang","Yue Zeng","Kai Chen","Cai Meng","Chao Pan","Zhouping Tang"],"pdf_url":"https://arxiv.org/pdf/2505.09252v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.02733v2","updated":"2025-10-25T08:49:34Z","published":"2025-10-03T05:34:24Z","title":"Net2Net: When Un-trained Meets Pre-trained Networks for Robust\n  Real-World Denoising","summary":"  Traditional denoising methods for noise removal have largely relied on\nhandcrafted priors, often perform well in controlled environments but struggle\nto address the complexity and variability of real noise. In contrast, deep\nlearning-based approaches have gained prominence for learning noise\ncharacteristics from large datasets, but these methods frequently require\nextensive labeled data and may not generalize effectively across diverse noise\ntypes and imaging conditions. In this paper, we present an innovative method,\ntermed as Net2Net, that combines the strengths of untrained and pre-trained\nnetworks to tackle the challenges of real-world noise removal. The innovation\nof Net2Net lies in its combination of unsupervised DIP and supervised\npre-trained model DRUNet by regularization by denoising (RED). The untrained\nnetwork adapts to the unique noise characteristics of each input image without\nrequiring labeled data, while the pre-trained network leverages learned\nrepresentations from large-scale datasets to deliver robust denoising\nperformance. This hybrid framework enhances generalization across varying noise\npatterns and improves performance, particularly in scenarios with limited\ntraining data. Extensive experiments on benchmark datasets demonstrate the\nsuperiority of our method for real-world noise removal.\n","authors":["Weimin Yuan","Cai Meng"],"pdf_url":"https://arxiv.org/pdf/2510.02733v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22217v1","updated":"2025-10-25T08:35:22Z","published":"2025-10-25T08:35:22Z","title":"Enpowering Your Pansharpening Models with Generalizability: Unified\n  Distribution is All You Need","summary":"  Existing deep learning-based models for remote sensing pansharpening exhibit\nexceptional performance on training datasets. However, due to sensor-specific\ncharacteristics and varying imaging conditions, these models suffer from\nsubstantial performance degradation when applied to unseen satellite data,\nlacking generalizability and thus limiting their applicability. We argue that\nthe performance drops stem primarily from distributional discrepancies from\ndifferent sources and the key to addressing this challenge lies in bridging the\ngap between training and testing distributions. To validate the idea and\nfurther achieve a \"train once, deploy forever\" capability, this paper\nintroduces a novel and intuitive approach to enpower any pansharpening models\nwith generalizability by employing a unified distribution strategy (UniPAN).\nSpecifically, we construct a distribution transformation function that\nnormalizes the pixels sampled from different sources to conform to an identical\ndistribution. The deep models are trained on the transformed domain, and during\ntesting on new datasets, the new data are also transformed to match the\ntraining distribution. UniPAN aims to train and test the model on a unified and\nconsistent distribution, thereby enhancing its generalizability. Extensive\nexperiments validate the efficacy of UniPAN, demonstrating its potential to\nsignificantly enhance the performance of deep pansharpening models across\ndiverse satellite sensors. Codes: https://github.com/yc-cui/UniPAN.\n","authors":["Yongchuan Cui","Peng Liu","Hui Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.22217v1.pdf","comment":"Accepted to ICCV 2025"},{"id":"http://arxiv.org/abs/2510.22215v1","updated":"2025-10-25T08:27:37Z","published":"2025-10-25T08:27:37Z","title":"Hybrid-Vector Retrieval for Visually Rich Documents: Combining\n  Single-Vector Efficiency and Multi-Vector Accuracy","summary":"  Retrieval over visually rich documents is essential for tasks such as legal\ndiscovery, scientific search, and enterprise knowledge management. Existing\napproaches fall into two paradigms: single-vector retrieval, which is efficient\nbut coarse, and multi-vector retrieval, which is accurate but computationally\nexpensive. To address this trade-off, we propose HEAVEN, a two-stage\nhybrid-vector framework. In the first stage, HEAVEN efficiently retrieves\ncandidate pages using a single-vector method over Visually-Summarized Pages\n(VS-Pages), which assemble representative visual layouts from multiple pages.\nIn the second stage, it reranks candidates with a multi-vector method while\nfiltering query tokens by linguistic importance to reduce redundant\ncomputations. To evaluate retrieval systems under realistic conditions, we also\nintroduce ViMDOC, the first benchmark for visually rich, multi-document, and\nlong-document retrieval. Across four benchmarks, HEAVEN attains 99.87% of the\nRecall@1 performance of multi-vector models on average while reducing per-query\ncomputation by 99.82%, achieving efficiency and accuracy. Our code and datasets\nare available at: https://github.com/juyeonnn/HEAVEN\n","authors":["Juyeon Kim","Geon Lee","Dongwon Choi","Taeuk Kim","Kijung Shin"],"pdf_url":"https://arxiv.org/pdf/2510.22215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22214v1","updated":"2025-10-25T08:26:45Z","published":"2025-10-25T08:26:45Z","title":"GALA: A GlobAl-LocAl Approach for Multi-Source Active Domain Adaptation","summary":"  Domain Adaptation (DA) provides an effective way to tackle target-domain\ntasks by leveraging knowledge learned from source domains. Recent studies have\nextended this paradigm to Multi-Source Domain Adaptation (MSDA), which exploits\nmultiple source domains carrying richer and more diverse transferable\ninformation. However, a substantial performance gap still remains between\nadaptation-based methods and fully supervised learning. In this paper, we\nexplore a more practical and challenging setting, named Multi-Source Active\nDomain Adaptation (MS-ADA), to further enhance target-domain performance by\nselectively acquiring annotations from the target domain. The key difficulty of\nMS-ADA lies in designing selection criteria that can jointly handle inter-class\ndiversity and multi-source domain variation. To address these challenges, we\npropose a simple yet effective GALA strategy (GALA), which combines a global\nk-means clustering step for target-domain samples with a cluster-wise local\nselection criterion, effectively tackling the above two issues in a\ncomplementary manner. Our proposed GALA is plug-and-play and can be seamlessly\nintegrated into existing DA frameworks without introducing any additional\ntrainable parameters. Extensive experiments on three standard DA benchmarks\ndemonstrate that GALA consistently outperforms prior active learning and active\nDA methods, achieving performance comparable to the fully-supervised upperbound\nwhile using only 1% of the target annotations.\n","authors":["Juepeng Zheng","Peifeng Zhang","Yibin Wen","Qingmei Li","Yang Zhang","Haohuan Fu"],"pdf_url":"https://arxiv.org/pdf/2510.22214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22213v1","updated":"2025-10-25T08:21:40Z","published":"2025-10-25T08:21:40Z","title":"DynamicTree: Interactive Real Tree Animation via Sparse Voxel Spectrum","summary":"  Generating dynamic and interactive 3D objects, such as trees, has wide\napplications in virtual reality, games, and world simulation. Nevertheless,\nexisting methods still face various challenges in generating realistic 4D\nmotion for complex real trees. In this paper, we propose DynamicTree, the first\nframework that can generate long-term, interactive animation of 3D Gaussian\nSplatting trees. Unlike prior optimization-based methods, our approach\ngenerates dynamics in a fast feed-forward manner. The key success of our\napproach is the use of a compact sparse voxel spectrum to represent the tree\nmovement. Given a 3D tree from Gaussian Splatting reconstruction, our pipeline\nfirst generates mesh motion using the sparse voxel spectrum and then binds\nGaussians to deform the mesh. Additionally, the proposed sparse voxel spectrum\ncan also serve as a basis for fast modal analysis under external forces,\nallowing real-time interactive responses. To train our model, we also introduce\n4DTree, the first large-scale synthetic 4D tree dataset containing 8,786\nanimated tree meshes with semantic labels and 100-frame motion sequences.\nExtensive experiments demonstrate that our method achieves realistic and\nresponsive tree animations, significantly outperforming existing approaches in\nboth visual quality and computational efficiency.\n","authors":["Yaokun Li","Lihe Ding","Xiao Chen","Guang Tan","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2510.22213v1.pdf","comment":"Project Page:\n  https://dynamictree-dev.github.io/DynamicTree.github.io/"},{"id":"http://arxiv.org/abs/2510.22208v1","updated":"2025-10-25T08:18:41Z","published":"2025-10-25T08:18:41Z","title":"Simplifying Knowledge Transfer in Pretrained Models","summary":"  Pretrained models are ubiquitous in the current deep learning landscape,\noffering strong results on a broad range of tasks. Recent works have shown that\nmodels differing in various design choices exhibit categorically diverse\ngeneralization behavior, resulting in one model grasping distinct data-specific\ninsights unavailable to the other. In this paper, we propose to leverage large\npublicly available model repositories as an auxiliary source of model\nimprovements. We introduce a data partitioning strategy where pretrained models\nautonomously adopt either the role of a student, seeking knowledge, or that of\na teacher, imparting knowledge. Experiments across various tasks demonstrate\nthe effectiveness of our proposed approach. In image classification, we\nimproved the performance of ViT-B by approximately 1.4% through bidirectional\nknowledge transfer with ViT-T. For semantic segmentation, our method boosted\nall evaluation metrics by enabling knowledge transfer both within and across\nbackbone architectures. In video saliency prediction, our approach achieved a\nnew state-of-the-art. We further extend our approach to knowledge transfer\nbetween multiple models, leading to considerable performance improvements for\nall model participants.\n","authors":["Siddharth Jain","Shyamgopal Karthik","Vineet Gandhi"],"pdf_url":"https://arxiv.org/pdf/2510.22208v1.pdf","comment":"12 pages, 3 figures, 6 tables, Accepted at TMLR 2025"},{"id":"http://arxiv.org/abs/2510.22205v1","updated":"2025-10-25T08:08:10Z","published":"2025-10-25T08:08:10Z","title":"TrajGATFormer: A Graph-Based Transformer Approach for Worker and\n  Obstacle Trajectory Prediction in Off-site Construction Environments","summary":"  As the demand grows within the construction industry for processes that are\nnot only faster but also safer and more efficient, offsite construction has\nemerged as a solution, though it brings new safety risks due to the close\ninteraction between workers, machinery, and moving obstacles. Predicting the\nfuture trajectories of workers and taking into account social and environmental\nfactors is a crucial step for developing collision-avoidance systems to\nmitigate such risks. Traditional methods often struggle to adapt to the dynamic\nand unpredictable nature of construction environments. Many rely on simplified\nassumptions or require hand-crafted features, limiting their ability to respond\nto complex, real-time interactions between workers and moving obstacles. While\nrecent data-driven methods have improved the modeling of temporal patterns,\nthey still face challenges in capturing long-term behavior and accounting for\nthe spatial and social context crucial to collision risk assessment. To address\nthese limitations, this paper proposes a framework integrating YOLOv10n and\nDeepSORT for precise detection and tracking, along with two novel trajectory\nprediction models: TrajGATFormer and TrajGATFormer-Obstacle. YOLOv10n serves as\nthe backbone for object detection, accurately identifying workers and obstacles\nin diverse scenes, while DeepSORT efficiently tracks them over time with unique\nIDs for continuity. Both models employ a transformer encoder-decoder with Graph\nAttention Networks (GAT) to capture temporal and spatial interactions.\nTrajGATFormer predicts worker trajectories with an ADE of 1.25 m and FDE of 2.3\nm over a 4.8 s horizon, while TrajGATFormer-Obstacle extends prediction to both\nworkers and obstacles, achieving higher accuracy (ADE 1.15 m, FDE 2.2 m).\nComparative analysis shows both models outperform traditional methods, reducing\nADE and FDE by up to 35% and 38%, respectively.\n","authors":["Mohammed Alduais","Xinming Li","Qipei Mei"],"pdf_url":"https://arxiv.org/pdf/2510.22205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22199v1","updated":"2025-10-25T07:39:02Z","published":"2025-10-25T07:39:02Z","title":"MOGRAS: Human Motion with Grasping in 3D Scenes","summary":"  Generating realistic full-body motion interacting with objects is critical\nfor applications in robotics, virtual reality, and human-computer interaction.\nWhile existing methods can generate full-body motion within 3D scenes, they\noften lack the fidelity for fine-grained tasks like object grasping.\nConversely, methods that generate precise grasping motions typically ignore the\nsurrounding 3D scene. This gap, generating full-body grasping motions that are\nphysically plausible within a 3D scene, remains a significant challenge. To\naddress this, we introduce MOGRAS (Human MOtion with GRAsping in 3D Scenes), a\nlarge-scale dataset that bridges this gap. MOGRAS provides pre-grasping\nfull-body walking motions and final grasping poses within richly annotated 3D\nindoor scenes. We leverage MOGRAS to benchmark existing full-body grasping\nmethods and demonstrate their limitations in scene-aware generation.\nFurthermore, we propose a simple yet effective method to adapt existing\napproaches to work seamlessly within 3D scenes. Through extensive quantitative\nand qualitative experiments, we validate the effectiveness of our dataset and\nhighlight the significant improvements our proposed method achieves, paving the\nway for more realistic human-scene interactions.\n","authors":["Kunal Bhosikar","Siddharth Katageri","Vivek Madhavaram","Kai Han","Charu Sharma"],"pdf_url":"https://arxiv.org/pdf/2510.22199v1.pdf","comment":"British Machine Vision Conference Workshop - From Scene Understanding\n  to Human Modeling"},{"id":"http://arxiv.org/abs/2510.22196v1","updated":"2025-10-25T07:29:26Z","published":"2025-10-25T07:29:26Z","title":"Scaling Non-Parametric Sampling with Representation","summary":"  Scaling and architectural advances have produced strikingly photorealistic\nimage generative models, yet their mechanisms still remain opaque. Rather than\nadvancing scaling, our goal is to strip away complicated engineering tricks and\npropose a simple, non-parametric generative model. Our design is grounded in\nthree principles of natural images-(i) spatial non-stationarity, (ii) low-level\nregularities, and (iii) high-level semantics-and defines each pixel's\ndistribution from its local context window. Despite its minimal architecture\nand no training, the model produces high-fidelity samples on MNIST and visually\ncompelling CIFAR-10 images. This combination of simplicity and strong empirical\nperformance points toward a minimal theory of natural-image structure. The\nmodel's white-box nature also allows us to have a mechanistic understanding of\nhow the model generalizes and generates diverse images. We study it by tracing\neach generated pixel back to its source images. These analyses reveal a simple,\ncompositional procedure for \"part-whole generalization\", suggesting a\nhypothesis for how large neural network generative models learn to generalize.\n","authors":["Vincent Lu","Aaron Truong","Zeyu Yun","Yubei Chen"],"pdf_url":"https://arxiv.org/pdf/2510.22196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08441v2","updated":"2025-10-25T07:05:42Z","published":"2025-07-11T09:32:45Z","title":"Vision Foundation Models as Effective Visual Tokenizers for\n  Autoregressive Image Generation","summary":"  In this work, we present a novel direction to build an image tokenizer\ndirectly on top of a frozen vision foundation model, which is a largely\nunderexplored area. Specifically, we employ a frozen vision foundation model as\nthe encoder of our tokenizer. To enhance its effectiveness, we introduce two\nkey components: (1) a region-adaptive quantization framework that reduces\nredundancy in the pre-trained features on regular 2D grids, and (2) a semantic\nreconstruction objective that aligns the tokenizer's outputs with the\nfoundation model's representations to preserve semantic fidelity. Based on\nthese designs, our proposed image tokenizer, VFMTok, achieves substantial\nimprovements in image reconstruction and generation quality, while also\nenhancing token efficiency. It further boosts autoregressive (AR) generation --\nachieving a gFID of 1.36 on ImageNet benchmarks, while accelerating model\nconvergence by three times, and enabling high-fidelity class-conditional\nsynthesis without the need for classifier-free guidance (CFG). The code is\navailable at https://github.com/CVMI-Lab/VFMTok.\n","authors":["Anlin Zheng","Xin Wen","Xuanyang Zhang","Chuofan Ma","Tiancai Wang","Gang Yu","Xiangyu Zhang","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2507.08441v2.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.11277v2","updated":"2025-10-25T06:08:55Z","published":"2024-12-15T18:49:20Z","title":"Macro2Micro: A Rapid and Precise Cross-modal Magnetic Resonance Imaging\n  Synthesis using Multi-scale Structural Brain Similarity","summary":"  The human brain is a complex system requiring both macroscopic and\nmicroscopic components for comprehensive understanding. However, mapping\nnonlinear relationships between these scales remains challenging due to\ntechnical limitations and the high cost of multimodal Magnetic Resonance\nImaging (MRI) acquisition. To address this, we introduce Macro2Micro, a deep\nlearning framework that predicts brain microstructure from macrostructure using\na Generative Adversarial Network (GAN). Based on the hypothesis that microscale\nstructural information can be inferred from macroscale structures, Macro2Micro\nexplicitly encodes multiscale brain information into distinct processing\nbranches. To enhance artifact elimination and output quality, we propose a\nsimple yet effective auxiliary discriminator and learning objective. Extensive\nexperiments demonstrated that Macro2Micro faithfully translates T1-weighted\nMRIs into corresponding Fractional Anisotropy (FA) images, achieving a 6.8\\%\nimprovement in the Structural Similarity Index Measure (SSIM) compared to\nprevious methods, while retaining the individual biological characteristics of\nthe brain. With an inference time of less than 0.01 seconds per MR modality\ntranslation, Macro2Micro introduces the potential for real-time multimodal\nrendering in medical and research applications. The code will be made available\nupon acceptance.\n","authors":["Sooyoung Kim","Joonwoo Kwon","Junbeom Kwon","Jungyoun Janice Min","Sangyoon Bae","Yuewei Lin","Shinjae Yoo","Jiook Cha"],"pdf_url":"https://arxiv.org/pdf/2412.11277v2.pdf","comment":"The code will be made available upon acceptance"},{"id":"http://arxiv.org/abs/2510.22171v1","updated":"2025-10-25T05:45:18Z","published":"2025-10-25T05:45:18Z","title":"HARMONY: Hidden Activation Representations and Model Output-Aware\n  Uncertainty Estimation for Vision-Language Models","summary":"  The growing deployment of Vision-Language Models (VLMs) in high-stakes\napplications such as autonomous driving and assistive technologies for visually\nimpaired individuals necessitates reliable mechanisms to assess the\ntrustworthiness of their generation. Uncertainty Estimation (UE) plays a\ncentral role in quantifying the reliability of model outputs and reducing\nunsafe generations via selective prediction. In this regard, most existing\nprobability-based UE approaches rely on output probability distributions,\naggregating token probabilities into a single uncertainty score using\npredefined functions such as length-normalization. Another line of research\nleverages model hidden representations and trains MLP-based models to predict\nuncertainty. However, these methods often fail to capture the complex\nmultimodal relationships between semantic and textual tokens and struggle to\nidentify biased probabilities often influenced by language priors. Motivated by\nthese observations, we propose a novel UE framework, HARMONY, that jointly\nleverages fused multimodal information in model activations and the output\ndistribution of the VLM to determine the reliability of responses. The key\nhypothesis of our work is that both the model's internal belief in its visual\nunderstanding, captured by its hidden representations, and the produced token\nprobabilities carry valuable reliability signals that can be jointly leveraged\nto improve UE performance, surpassing approaches that rely on only one of these\ncomponents. Experimental results on three open-ended VQA benchmarks, A-OKVQA,\nVizWiz, and PathVQA, and three state-of-the-art VLMs, LLaVa-7b, LLaVA-13b and\nInstructBLIP demonstrate that our method consistently performs on par with or\nbetter than existing approaches, achieving up to 4\\% improvement in AUROC, and\n6\\% in PRR, establishing new state of the art in uncertainty estimation for\nVLMs.\n","authors":["Erum Mushtaq","Zalan Fabian","Yavuz Faruk Bakman","Anil Ramakrishna","Mahdi Soltanolkotabi","Salman Avestimehr"],"pdf_url":"https://arxiv.org/pdf/2510.22171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.00371v3","updated":"2025-10-25T05:38:36Z","published":"2025-07-01T01:59:59Z","title":"PlantSegNeRF: A few-shot, cross-species method for plant 3D instance\n  point cloud reconstruction via joint-channel NeRF with multi-view image\n  instance matching","summary":"  Organ segmentation of plant point clouds is a prerequisite for the\nhigh-resolution and accurate extraction of organ-level phenotypic traits.\nAlthough the fast development of deep learning has boosted much research on\nsegmentation of plant point clouds, the existing techniques for organ\nsegmentation still face limitations in resolution, segmentation accuracy, and\ngeneralizability across various plant species. In this study, we proposed a\nnovel approach called plant segmentation neural radiance fields (PlantSegNeRF),\naiming to directly generate high-precision instance point clouds from\nmulti-view RGB image sequences for a wide range of plant species. PlantSegNeRF\nperformed 2D instance segmentation on the multi-view images to generate\ninstance masks for each organ with a corresponding ID. The multi-view instance\nIDs corresponding to the same plant organ were then matched and refined using a\nspecially designed instance matching module. The instance NeRF was developed to\nrender an implicit scene, containing color, density, semantic and instance\ninformation. The implicit scene was ultimately converted into high-precision\nplant instance point clouds based on the volume density. The results proved\nthat in semantic segmentation of point clouds, PlantSegNeRF outperformed the\ncommonly used methods, demonstrating an average improvement of 16.1%, 18.3%,\n17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the\nsecond-best results on structurally complex species. More importantly,\nPlantSegNeRF exhibited significant advantages in plant point cloud instance\nsegmentation tasks. Across all plant species, it achieved average improvements\nof 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.\nThis study extends the organ-level plant phenotyping and provides a\nhigh-throughput way to supply high-quality 3D data for the development of\nlarge-scale models in plant science.\n","authors":["Xin Yang","Ruiming Du","Hanyang Huang","Jiayang Xie","Pengyao Xie","Leisen Fang","Ziyue Guo","Nanjun Jiang","Yu Jiang","Haiyan Cen"],"pdf_url":"https://arxiv.org/pdf/2507.00371v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22166v1","updated":"2025-10-25T05:25:37Z","published":"2025-10-25T05:25:37Z","title":"Expert Validation of Synthetic Cervical Spine Radiographs Generated with\n  a Denoising Diffusion Probabilistic Model","summary":"  Machine learning in neurosurgery is limited by challenges in assembling\nlarge, high-quality imaging datasets. Synthetic data offers a scalable,\nprivacy-preserving solution. We evaluated the feasibility of generating\nrealistic lateral cervical spine radiographs using a denoising diffusion\nprobabilistic model (DDPM) trained on 4,963 images from the Cervical Spine\nX-ray Atlas. Model performance was monitored via training/validation loss and\nFrechet inception distance, and synthetic image quality was assessed in a\nblinded \"clinical Turing test\" with six neuroradiologists and two\nspine-fellowship trained neurosurgeons. Experts reviewed 50 quartets containing\none real and three synthetic images, identifying the real image and rating\nrealism on a 4-point Likert scale. Experts correctly identified the real image\nin 29% of trials (Fleiss' kappa=0.061). Mean realism scores were comparable\nbetween real (3.323) and synthetic images (3.228, 3.258, and 3.320; p=0.383,\n0.471, 1.000). Nearest-neighbor analysis found no evidence of memorization. We\nalso provide a dataset of 20,063 synthetic radiographs. These results\ndemonstrate that DDPM-generated cervical spine X-rays are statistically\nindistinguishable in realism and quality from real clinical images, offering a\nnovel approach to creating large-scale neuroimaging datasets for ML\napplications in landmarking, segmentation, and classification.\n","authors":["Austin A. Barr","Brij S. Karmur","Anthony J. Winder","Eddie Guo","John T. Lysack","James N. Scott","William F. Morrish","Muneer Eesa","Morgan Willson","David W. Cadotte","Michael M. H. Yang","Ian Y. M. Chan","Sanju Lama","Garnette R. Sutherland"],"pdf_url":"https://arxiv.org/pdf/2510.22166v1.pdf","comment":"10 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2510.22164v1","updated":"2025-10-25T05:23:50Z","published":"2025-10-25T05:23:50Z","title":"LT-Exosense: A Vision-centric Multi-session Mapping System for Lifelong\n  Safe Navigation of Exoskeletons","summary":"  Self-balancing exoskeletons offer a promising mobility solution for\nindividuals with lower-limb disabilities. For reliable long-term operation,\nthese exoskeletons require a perception system that is effective in changing\nenvironments. In this work, we introduce LT-Exosense, a vision-centric,\nmulti-session mapping system designed to support long-term (semi)-autonomous\nnavigation for exoskeleton users. LT-Exosense extends single-session mapping\ncapabilities by incrementally fusing spatial knowledge across multiple\nsessions, detecting environmental changes, and updating a persistent global\nmap. This representation enables intelligent path planning, which can adapt to\nnewly observed obstacles and can recover previous routes when obstructions are\nremoved. We validate LT-Exosense through several real-world experiments,\ndemonstrating a scalable multi-session map that achieves an average\npoint-to-point error below 5 cm when compared to ground-truth laser scans. We\nalso illustrate the potential application of adaptive path planning in\ndynamically changing indoor environments.\n","authors":["Jianeng Wang","Matias Mattamala","Christina Kassab","Nived Chebrolu","Guillaume Burger","Fabio Elnecave","Marine Petriaux","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2510.22164v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.22161v1","updated":"2025-10-25T05:13:28Z","published":"2025-10-25T05:13:28Z","title":"I2-NeRF: Learning Neural Radiance Fields Under Physically-Grounded Media\n  Interactions","summary":"  Participating in efforts to endow generative AI with the 3D physical world\nperception, we propose I2-NeRF, a novel neural radiance field framework that\nenhances isometric and isotropic metric perception under media degradation.\nWhile existing NeRF models predominantly rely on object-centric sampling,\nI2-NeRF introduces a reverse-stratified upsampling strategy to achieve\nnear-uniform sampling across 3D space, thereby preserving isometry. We further\npresent a general radiative formulation for media degradation that unifies\nemission, absorption, and scattering into a particle model governed by the\nBeer-Lambert attenuation law. By composing the direct and media-induced\nin-scatter radiance, this formulation extends naturally to complex media\nenvironments such as underwater, haze, and even low-light scenes. By treating\nlight propagation uniformly in both vertical and horizontal directions, I2-NeRF\nenables isotropic metric perception and can even estimate medium properties\nsuch as water depth. Experiments on real-world datasets demonstrate that our\nmethod significantly improves both reconstruction fidelity and physical\nplausibility compared to existing approaches.\n","authors":["Shuhong Liu","Lin Gu","Ziteng Cui","Xuangeng Chu","Tatsuya Harada"],"pdf_url":"https://arxiv.org/pdf/2510.22161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23566v4","updated":"2025-10-25T05:08:12Z","published":"2025-05-29T15:41:00Z","title":"Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for\n  Handwritten Mathematical Expression Recognition","summary":"  Handwritten Mathematical Expression Recognition (HMER) remains a persistent\nchallenge in Optical Character Recognition (OCR) due to the inherent freedom of\nsymbol layouts and variability in handwriting styles. Prior methods have faced\nperformance bottlenecks by proposing isolated architectural modifications,\nmaking them difficult to integrate coherently into a unified framework.\nMeanwhile, recent advances in pretrained vision-language models (VLMs) have\ndemonstrated strong cross-task generalization, offering a promising foundation\nfor developing unified solutions. In this paper, we introduce Uni-MuMER, which\nfully fine-tunes a VLM for the HMER task without modifying its architecture,\neffectively injecting domain-specific knowledge into a generalist framework.\nOur method integrates three data-driven tasks: Tree-Aware Chain-of-Thought\n(Tree-CoT) for structured spatial reasoning, Error-Driven Learning (EDL) for\nreducing confusion among visually similar characters, and Symbol Counting (SC)\nfor improving recognition consistency in long expressions. Experiments on the\nCROHME and HME100K datasets show that Uni-MuMER achieves super state-of-the-art\nperformance, outperforming the best lightweight specialized model SSAN by\n16.31\\% and the top-performing VLM Gemini2.5-flash by 24.42\\% under zero-shot\nsetting. Our datasets, models, and code are open-sourced at:\n{https://github.com/BFlameSwift/Uni-MuMER\n","authors":["Yu Li","Jin Jiang","Jianhua Zhu","Shuai Peng","Baole Wei","Yuxuan Zhou","Liangcai Gao"],"pdf_url":"https://arxiv.org/pdf/2505.23566v4.pdf","comment":"Accepted by NeurIPS 2025 as a spotlight"},{"id":"http://arxiv.org/abs/2502.02096v3","updated":"2025-10-25T05:06:24Z","published":"2025-02-04T08:25:58Z","title":"Dual-Flow: Transferable Multi-Target, Instance-Agnostic Attacks via\n  In-the-wild Cascading Flow Optimization","summary":"  Adversarial attacks are widely used to evaluate model robustness, and in\nblack-box scenarios, the transferability of these attacks becomes crucial.\nExisting generator-based attacks have excellent generalization and\ntransferability due to their instance-agnostic nature. However, when training\ngenerators for multi-target tasks, the success rate of transfer attacks is\nrelatively low due to the limitations of the model's capacity. To address these\nchallenges, we propose a novel Dual-Flow framework for multi-target\ninstance-agnostic adversarial attacks, utilizing Cascading Distribution Shift\nTraining to develop an adversarial velocity function. Extensive experiments\ndemonstrate that Dual-Flow significantly improves transferability over previous\nmulti-target generative attacks. For example, it increases the success rate\nfrom Inception-v3 to ResNet-152 by 34.58\\%. Furthermore, our attack method\nshows substantially stronger robustness against defense mechanisms, such as\nadversarially trained models. The code of Dual-Flow is available at:\n$\\href{https://github.com/Chyxx/Dual-Flow}{https://github.com/Chyxx/Dual-Flow}$.\n","authors":["Yixiao Chen","Shikun Sun","Jianshu Li","Ruoyu Li","Zhe Li","Junliang Xing"],"pdf_url":"https://arxiv.org/pdf/2502.02096v3.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.22160v1","updated":"2025-10-25T04:58:18Z","published":"2025-10-25T04:58:18Z","title":"SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation\n  for the Low-Resource Maithili Language","summary":"  Developing benchmark datasets for low-resource languages poses significant\nchallenges, primarily due to the limited availability of native linguistic\nexperts and the substantial time and cost involved in annotation. Given these\nchallenges, Maithili is still underrepresented in natural language processing\nresearch. It is an Indo-Aryan language spoken by more than 13 million people in\nthe Purvanchal region of India, valued for its rich linguistic structure and\ncultural significance. While sentiment analysis has achieved remarkable\nprogress in high-resource languages, resources for low-resource languages, such\nas Maithili, remain scarce, often restricted to coarse-grained annotations and\nlacking interpretability mechanisms. To address this limitation, we introduce a\nnovel dataset comprising 3,221 Maithili sentences annotated for sentiment\npolarity and accompanied by natural language justifications. Moreover, the\ndataset is carefully curated and validated by linguistic experts to ensure both\nlabel reliability and contextual fidelity. Notably, the justifications are\nwritten in Maithili, thereby promoting culturally grounded interpretation and\nenhancing the explainability of sentiment models. Furthermore, extensive\nexperiments using both classical machine learning and state-of-the-art\ntransformer architectures demonstrate the dataset's effectiveness for\ninterpretable sentiment analysis. Ultimately, this work establishes the first\nbenchmark for explainable affective computing in Maithili, thus contributing a\nvaluable resource to the broader advancement of multilingual NLP and\nexplainable AI.\n","authors":["Rahul Ranjan","Mahendra Kumar Gurve"," Anuj"," Nitin","Yamuna Prasad"],"pdf_url":"https://arxiv.org/pdf/2510.22160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22154v1","updated":"2025-10-25T04:17:50Z","published":"2025-10-25T04:17:50Z","title":"Frequency-Spatial Interaction Driven Network for Low-Light Image\n  Enhancement","summary":"  Low-light image enhancement (LLIE) aims at improving the perception or\ninterpretability of an image captured in an environment with poor illumination.\nWith the advent of deep learning, the LLIE technique has achieved significant\nbreakthroughs. However, existing LLIE methods either ignore the important role\nof frequency domain information or fail to effectively promote the propagation\nand flow of information, limiting the LLIE performance. In this paper, we\ndevelop a novel frequency-spatial interaction-driven network (FSIDNet) for LLIE\nbased on two-stage architecture. To be specific, the first stage is designed to\nrestore the amplitude of low-light images to improve the lightness, and the\nsecond stage devotes to restore phase information to refine fine-grained\nstructures. Considering that Frequency domain and spatial domain information\nare complementary and both favorable for LLIE, we further develop two\nfrequency-spatial interaction blocks which mutually amalgamate the\ncomplementary spatial and frequency information to enhance the capability of\nthe model. In addition, we construct the Information Exchange Module (IEM) to\nassociate two stages by adequately incorporating cross-stage and cross-scale\nfeatures to effectively promote the propagation and flow of information in the\ntwo-stage network structure. Finally, we conduct experiments on several widely\nused benchmark datasets (i.e., LOL-Real, LSRW-Huawei, etc.), which demonstrate\nthat our method achieves the excellent performance in terms of visual results\nand quantitative metrics while preserving good model efficiency.\n","authors":["Yunhong Tao","Wenbing Tao","Xiang Xiang"],"pdf_url":"https://arxiv.org/pdf/2510.22154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03610v2","updated":"2025-10-25T04:04:52Z","published":"2025-05-06T15:09:23Z","title":"Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack\n  Detection","summary":"  3D mask presentation attack detection is crucial for protecting face\nrecognition systems against the rising threat of 3D mask attacks. While most\nexisting methods utilize multimodal features or remote photoplethysmography\n(rPPG) signals to distinguish between real faces and 3D masks, they face\nsignificant challenges, such as the high costs associated with multimodal\nsensors and limited generalization ability. Detection-related text descriptions\noffer concise, universal information and are cost-effective to obtain. However,\nthe potential of vision-language multimodal features for 3D mask presentation\nattack detection remains unexplored. In this paper, we propose a novel\nknowledge-based prompt learning framework to explore the strong generalization\ncapability of vision-language models for 3D mask presentation attack detection.\nSpecifically, our approach incorporates entities and triples from knowledge\ngraphs into the prompt learning process, generating fine-grained, task-specific\nexplicit prompts that effectively harness the knowledge embedded in pre-trained\nvision-language models. Furthermore, considering different input images may\nemphasize distinct knowledge graph elements, we introduce a visual-specific\nknowledge filter based on an attention mechanism to refine relevant elements\naccording to the visual context. Additionally, we leverage causal graph theory\ninsights into the prompt learning process to further enhance the generalization\nability of our method. During training, a spurious correlation elimination\nparadigm is employed, which removes category-irrelevant local image patches\nusing guidance from knowledge-based text features, fostering the learning of\ngeneralized causal prompts that align with category-relevant local patches.\nExperimental results demonstrate that the proposed method achieves\nstate-of-the-art intra- and cross-scenario detection performance on benchmark\ndatasets.\n","authors":["Fangling Jiang","Qi Li","Bing Liu","Weining Wang","Caifeng Shan","Zhenan Sun","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2505.03610v2.pdf","comment":"Accepted by TPAMI"},{"id":"http://arxiv.org/abs/2510.22149v1","updated":"2025-10-25T04:02:04Z","published":"2025-10-25T04:02:04Z","title":"Power to the Clients: Federated Learning in a Dictatorship Setting","summary":"  Federated learning (FL) has emerged as a promising paradigm for decentralized\nmodel training, enabling multiple clients to collaboratively learn a shared\nmodel without exchanging their local data. However, the decentralized nature of\nFL also introduces vulnerabilities, as malicious clients can compromise or\nmanipulate the training process. In this work, we introduce dictator clients, a\nnovel, well-defined, and analytically tractable class of malicious participants\ncapable of entirely erasing the contributions of all other clients from the\nserver model, while preserving their own. We propose concrete attack strategies\nthat empower such clients and systematically analyze their effects on the\nlearning process. Furthermore, we explore complex scenarios involving multiple\ndictator clients, including cases where they collaborate, act independently, or\nform an alliance in order to ultimately betray one another. For each of these\nsettings, we provide a theoretical analysis of their impact on the global\nmodel's convergence. Our theoretical algorithms and findings about the complex\nscenarios including multiple dictator clients are further supported by\nempirical evaluations on both computer vision and natural language processing\nbenchmarks.\n","authors":["Mohammadsajad Alipour","Mohammad Mohammadi Amiri"],"pdf_url":"https://arxiv.org/pdf/2510.22149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.04273v3","updated":"2025-10-25T03:36:49Z","published":"2025-08-06T09:58:43Z","title":"Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video\n  Moment Retrieval","summary":"  Video Moment Retrieval (VMR) aims to retrieve a specific moment semantically\nrelated to the given query. To tackle this task, most existing VMR methods\nsolely focus on the visual and textual modalities while neglecting the\ncomplementary but important audio modality. Although a few recent works try to\ntackle the joint audio-vision-text reasoning, they treat all modalities equally\nand simply embed them without fine-grained interaction for moment retrieval.\nThese designs are counter-practical as: Not all audios are helpful for video\nmoment retrieval, and the audio of some videos may be complete noise or\nbackground sound that is meaningless to the moment determination. To this end,\nwe propose a novel Importance-aware Multi-Granularity fusion model (IMG), which\nlearns to dynamically and selectively aggregate the audio-vision-text contexts\nfor VMR. Specifically, after integrating the textual guidance with vision and\naudio separately, we first design a pseudo-label-supervised audio importance\npredictor that predicts the importance score of the audio, and accordingly\nassigns weights to mitigate the interference caused by noisy audio. Then, we\ndesign a multi-granularity audio fusion module that adaptively fuses audio and\nvisual modalities at local-, event-, and global-level, fully capturing their\ncomplementary contexts. We further propose a cross-modal knowledge distillation\nstrategy to address the challenge of missing audio modality during inference.\nTo evaluate our method, we further construct a new VMR dataset, i.e.,\nCharades-AudioMatter, where audio-related samples are manually selected and\nre-organized from the original Charades-STA to validate the model's capability\nin utilizing audio modality. Extensive experiments validate the effectiveness\nof our method, achieving state-of-the-art with audio-video fusion in VMR\nmethods. Our code is available at https://github.com/HuiGuanLab/IMG.\n","authors":["Junan Lin","Daizong Liu","Xianke Chen","Xiaoye Qu","Xun Yang","Jixiang Zhu","Sanyuan Zhang","Jianfeng Dong"],"pdf_url":"https://arxiv.org/pdf/2508.04273v3.pdf","comment":"Accepted to ACM MM 2025"},{"id":"http://arxiv.org/abs/2510.03160v2","updated":"2025-10-25T03:34:09Z","published":"2025-10-03T16:32:02Z","title":"SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the\n  SpineMed-450k Corpus","summary":"  Spine disorders affect 619 million people globally and are a leading cause of\ndisability, yet AI-assisted diagnosis remains limited by the lack of\nlevel-aware, multimodal datasets. Clinical decision-making for spine disorders\nrequires sophisticated reasoning across X-ray, CT, and MRI at specific\nvertebral levels. However, progress has been constrained by the absence of\ntraceable, clinically-grounded instruction data and standardized,\nspine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem\nco-designed with practicing spine surgeons. It features SpineMed-450k, the\nfirst large-scale dataset explicitly designed for vertebral-level reasoning\nacross imaging modalities with over 450,000 instruction instances, and\nSpineBench, a clinically-grounded evaluation framework. SpineMed-450k is\ncurated from diverse sources, including textbooks, guidelines, open datasets,\nand ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline\nwith a two-stage LLM generation method (draft and revision) to ensure\nhigh-quality, traceable data for question-answering, multi-turn consultations,\nand report generation. SpineBench evaluates models on clinically salient axes,\nincluding level identification, pathology assessment, and surgical planning.\nOur comprehensive evaluation of several recently advanced large vision-language\nmodels (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,\nlevel-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k\ndemonstrates consistent and significant improvements across all tasks.\nClinician assessments confirm the diagnostic clarity and practical utility of\nour model's outputs.\n","authors":["Ming Zhao","Wenhui Dong","Yang Zhang","Xiang Zheng","Zhonghao Zhang","Zian Zhou","Yunzhi Guan","Liukun Xu","Wei Peng","Zhaoyang Gong","Zhicheng Zhang","Dachuan Li","Xiaosheng Ma","Yuli Ma","Jianing Ni","Changjiang Jiang","Lixia Tian","Qixin Chen","Kaishun Xia","Pingping Liu","Tongshun Zhang","Zhiqiang Liu","Zhongyan Bi","Chenyang Si","Tiansheng Sun","Caifeng Shan"],"pdf_url":"https://arxiv.org/pdf/2510.03160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08010v5","updated":"2025-10-25T03:27:47Z","published":"2025-06-09T17:59:57Z","title":"Vision Transformers Don't Need Trained Registers","summary":"  We investigate the mechanism underlying a previously identified phenomenon in\nVision Transformers - the emergence of high-norm tokens that lead to noisy\nattention maps (Darcet et al., 2024). We observe that in multiple models (e.g.,\nCLIP, DINOv2), a sparse set of neurons is responsible for concentrating\nhigh-norm activations on outlier tokens, leading to irregular attention\npatterns and degrading downstream visual processing. While the existing\nsolution for removing these outliers involves retraining models from scratch\nwith additional learned register tokens, we use our findings to create a\ntraining-free approach to mitigate these artifacts. By shifting the high-norm\nactivations from our discovered register neurons into an additional untrained\ntoken, we can mimic the effect of register tokens on a model already trained\nwithout registers. We demonstrate that our method produces cleaner attention\nand feature maps, enhances performance over base models across multiple\ndownstream visual tasks, and achieves results comparable to models explicitly\ntrained with register tokens. We then extend test-time registers to\noff-the-shelf vision-language models, yielding cleaner attention-based,\ntext-to-image attribution. Finally, we outline a simple mathematical model that\nreflects the observed behavior of register neurons and high norm tokens. Our\nresults suggest that test-time registers effectively take on the role of\nregister tokens at test-time, offering a training-free solution for any\npre-trained model released without them.\n","authors":["Nick Jiang","Amil Dravid","Alexei Efros","Yossi Gandelsman"],"pdf_url":"https://arxiv.org/pdf/2506.08010v5.pdf","comment":"Project page and code:\n  https://avdravid.github.io/test-time-registers. Accepted to NeurIPS '25\n  (spotlight)"},{"id":"http://arxiv.org/abs/2510.22142v1","updated":"2025-10-25T03:27:26Z","published":"2025-10-25T03:27:26Z","title":"Attention Residual Fusion Network with Contrast for Source-free Domain\n  Adaptation","summary":"  Source-free domain adaptation (SFDA) involves training a model on source\ndomain and then applying it to a related target domain without access to the\nsource data and labels during adaptation. The complexity of scene information\nand lack of the source domain make SFDA a difficult task. Recent studies have\nshown promising results, but many approaches to domain adaptation concentrate\non domain shift and neglect the effects of negative transfer, which may impede\nenhancements of model performance during adaptation. n this paper, addressing\nthis issue, we propose a novel framework of Attention Residual Fusion Network\n(ARFNet) based on contrast learning for SFDA to alleviate negative transfer and\ndomain shift during the progress of adaptation, in which attention residual\nfusion, global-local attention contrast, and dynamic centroid evaluation are\nexploited. Concretely, the attention mechanism is first exploited to capture\nthe discriminative region of the target object. Then, in each block, attention\nfeatures are decomposed into spatial-wise and channel-wise attentions to\nachieve the cross-layer attention residual fusion progressively and\nself-distillation. During adaptation progress, we contrast global and local\nrepresentations to improve the perceptual capabilities of different categories,\nwhich enables the model to discriminate variations between inner-class and\nintra-class. Finally, a dynamic centroid evaluation strategy is exploited to\nevaluate the trustworthy centroids and labels for self-supervised\nself-distillation, which aims to accurately approximate the center of the\nsource domain and pseudo-labels to mitigate domain shift. To validate the\nefficacy, we execute comprehensive experiments on five benchmarks of varying\nscales. Experimental outcomes indicate that our method surpasses other\ntechniques, attaining superior performance across SFDA benchmarks.\n","authors":["Renrong Shao","Wei Zhang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2510.22142v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.22141v1","updated":"2025-10-25T03:27:19Z","published":"2025-10-25T03:27:19Z","title":"LOC: A General Language-Guided Framework for Open-Set 3D Occupancy\n  Prediction","summary":"  Vision-Language Models (VLMs) have shown significant progress in open-set\nchallenges. However, the limited availability of 3D datasets hinders their\neffective application in 3D scene understanding. We propose LOC, a general\nlanguage-guided framework adaptable to various occupancy networks, supporting\nboth supervised and self-supervised learning paradigms. For self-supervised\ntasks, we employ a strategy that fuses multi-frame LiDAR points for\ndynamic/static scenes, using Poisson reconstruction to fill voids, and\nassigning semantics to voxels via K-Nearest Neighbor (KNN) to obtain\ncomprehensive voxel representations. To mitigate feature over-homogenization\ncaused by direct high-dimensional feature distillation, we introduce Densely\nContrastive Learning (DCL). DCL leverages dense voxel semantic information and\npredefined textual prompts. This efficiently enhances open-set recognition\nwithout dense pixel-level supervision, and our framework can also leverage\nexisting ground truth to further improve performance. Our model predicts dense\nvoxel features embedded in the CLIP feature space, integrating textual and\nimage pixel information, and classifies based on text and semantic similarity.\nExperiments on the nuScenes dataset demonstrate the method's superior\nperformance, achieving high-precision predictions for known classes and\ndistinguishing unknown classes without additional training data.\n","authors":["Yuhang Gao","Xiang Xiang","Sheng Zhong","Guoyou Wang"],"pdf_url":"https://arxiv.org/pdf/2510.22141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10133v2","updated":"2025-10-25T03:27:01Z","published":"2023-08-20T02:02:16Z","title":"TransFace++: Rethinking the Face Recognition Paradigm with a Focus on\n  Accuracy, Efficiency, and Security","summary":"  Face Recognition (FR) technology has made significant strides with the\nemergence of deep learning. Typically, most existing FR models are built upon\nConvolutional Neural Networks (CNN) and take RGB face images as the model's\ninput. In this work, we take a closer look at existing FR paradigms from\nhigh-efficiency, security, and precision perspectives, and identify the\nfollowing three problems: (i) CNN frameworks are vulnerable in capturing global\nfacial features and modeling the correlations between local facial features.\n(ii) Selecting RGB face images as the model's input greatly degrades the\nmodel's inference efficiency, increasing the extra computation costs. (iii) In\nthe real-world FR system that operates on RGB face images, the integrity of\nuser privacy may be compromised if hackers successfully penetrate and gain\naccess to the input of this model. To solve these three issues, we propose two\nnovel FR frameworks, i.e., TransFace and TransFace++, which successfully\nexplore the feasibility of applying ViTs and image bytes to FR tasks,\nrespectively. Experiments on popular face benchmarks demonstrate the\nsuperiority of our TransFace and TransFace++. Code is available at\nhttps://github.com/DanJun6737/TransFace_pp.\n","authors":["Jun Dan","Yang Liu","Baigui Sun","Jiankang Deng","Shan Luo"],"pdf_url":"https://arxiv.org/pdf/2308.10133v2.pdf","comment":"This is an extended version of our previous ICCV paper \"TransFace\",\n  with significant new experiments, ablation studies, and improvements\n  published in IEEE TPAMI as \"TransFace++\""},{"id":"http://arxiv.org/abs/2510.22140v1","updated":"2025-10-25T03:23:38Z","published":"2025-10-25T03:23:38Z","title":"STG-Avatar: Animatable Human Avatars via Spacetime Gaussian","summary":"  Realistic animatable human avatars from monocular videos are crucial for\nadvancing human-robot interaction and enhancing immersive virtual experiences.\nWhile recent research on 3DGS-based human avatars has made progress, it still\nstruggles with accurately representing detailed features of non-rigid objects\n(e.g., clothing deformations) and dynamic regions (e.g., rapidly moving limbs).\nTo address these challenges, we present STG-Avatar, a 3DGS-based framework for\nhigh-fidelity animatable human avatar reconstruction. Specifically, our\nframework introduces a rigid-nonrigid coupled deformation framework that\nsynergistically integrates Spacetime Gaussians (STG) with linear blend skinning\n(LBS). In this hybrid design, LBS enables real-time skeletal control by driving\nglobal pose transformations, while STG complements it through spacetime\nadaptive optimization of 3D Gaussians. Furthermore, we employ optical flow to\nidentify high-dynamic regions and guide the adaptive densification of 3D\nGaussians in these regions. Experimental results demonstrate that our method\nconsistently outperforms state-of-the-art baselines in both reconstruction\nquality and operational efficiency, achieving superior quantitative metrics\nwhile retaining real-time rendering capabilities. Our code is available at\nhttps://github.com/jiangguangan/STG-Avatar\n","authors":["Guangan Jiang","Tianzi Zhang","Dong Li","Zhenjun Zhao","Haoang Li","Mingrui Li","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2510.22140v1.pdf","comment":"Accepted by the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2503.19331v3","updated":"2025-10-25T03:14:20Z","published":"2025-03-25T03:45:59Z","title":"ChA-MAEViT: Unifying Channel-Aware Masked Autoencoders and Multi-Channel\n  Vision Transformers for Improved Cross-Channel Learning","summary":"  Prior work using Masked Autoencoders (MAEs) typically relies on random patch\nmasking based on the assumption that images have significant redundancies\nacross different channels, allowing for the reconstruction of masked content\nusing cross-channel correlations. However, this assumption does not hold in\nMulti-Channel Imaging (MCI), where channels may provide complementary\ninformation with minimal feature overlap. Thus, these MAEs primarily learn\nlocal structures within individual channels from patch reconstruction, failing\nto fully leverage cross-channel interactions and limiting their MCI\neffectiveness. In this paper, we present ChA-MAEViT, an MAE-based method that\nenhances feature learning across MCI channels via four key strategies: (1)\ndynamic channel-patch masking, which compels the model to reconstruct missing\nchannels in addition to masked patches, thereby enhancing cross-channel\ndependencies and improving robustness to varying channel configurations; (2)\nmemory tokens, which serve as long-term memory aids to promote information\nsharing across channels, addressing the challenges of reconstructing\nstructurally diverse channels; (3) hybrid token fusion module, which merges\nfine-grained patch tokens with a global class token to capture richer\nrepresentations; and (4) Channel-Aware Decoder, a lightweight decoder utilizes\nchannel tokens to effectively reconstruct image patches. Experiments on\nsatellite and microscopy datasets, CHAMMI, JUMP-CP, and So2Sat, show that\nChA-MAEViT significantly outperforms state-of-the-art MCI-ViTs by 3.0-21.5%,\nhighlighting the importance of cross-channel interactions in MCI. Our code is\npublicly available at https://github.com/chaudatascience/cha_mae_vit.\n","authors":["Chau Pham","Juan C. Caicedo","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2503.19331v3.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.04308v3","updated":"2025-10-25T03:07:10Z","published":"2025-06-04T17:59:27Z","title":"RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language\n  Models for Robotics","summary":"  Spatial referring is a fundamental capability of embodied robots to interact\nwith the 3D physical world. However, even with the powerful pretrained vision\nlanguage models (VLMs), recent approaches are still not qualified to accurately\nunderstand the complex 3D scenes and dynamically reason about the\ninstruction-indicated locations for interaction. To this end, we propose\nRoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding\nby integrating a disentangled but dedicated depth encoder via supervised\nfine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial\nreasoning via reinforcement fine-tuning (RFT), with metric-sensitive process\nreward functions tailored for spatial referring tasks. To support SFT and RFT\ntraining, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x\nprior), covering 31 spatial relations (vs. 15 prior) and supporting complex\nreasoning processes (up to 5 steps). In addition, we introduce\nRefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial\nreferring with multi-step reasoning. Experiments show that SFT-trained\nRoboRefer achieves state-of-the-art spatial understanding, with an average\nsuccess rate of 89.6%. RFT-trained RoboRefer further outperforms all other\nbaselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average\naccuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various\ncontrol policies to execute long-horizon, dynamic tasks across diverse robots\n(e,g., UR5, G1 humanoid) in cluttered real-world scenes. Please see the project\npage at https://zhoues.github.io/RoboRefer.\n","authors":["Enshen Zhou","Jingkun An","Cheng Chi","Yi Han","Shanyu Rong","Chi Zhang","Pengwei Wang","Zhongyuan Wang","Tiejun Huang","Lu Sheng","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.04308v3.pdf","comment":"Accepted by NeurIPS 2025. Project page:\n  https://zhoues.github.io/RoboRefer/"},{"id":"http://arxiv.org/abs/2510.22129v1","updated":"2025-10-25T03:04:51Z","published":"2025-10-25T03:04:51Z","title":"egoEMOTION: Egocentric Vision and Physiological Signals for Emotion and\n  Personality Recognition in Real-World Tasks","summary":"  Understanding affect is central to anticipating human behavior, yet current\negocentric vision benchmarks largely ignore the person's emotional states that\nshape their decisions and actions. Existing tasks in egocentric perception\nfocus on physical activities, hand-object interactions, and attention modeling\n- assuming neutral affect and uniform personality. This limits the ability of\nvision systems to capture key internal drivers of behavior. In this paper, we\npresent egoEMOTION, the first dataset that couples egocentric visual and\nphysiological signals with dense self-reports of emotion and personality across\ncontrolled and real-world scenarios. Our dataset includes over 50 hours of\nrecordings from 43 participants, captured using Meta's Project Aria glasses.\nEach session provides synchronized eye-tracking video, headmounted\nphotoplethysmography, inertial motion data, and physiological baselines for\nreference. Participants completed emotion-elicitation tasks and naturalistic\nactivities while self-reporting their affective state using the Circumplex\nModel and Mikels' Wheel as well as their personality via the Big Five model. We\ndefine three benchmark tasks: (1) continuous affect classification (valence,\narousal, dominance); (2) discrete emotion classification; and (3) trait-level\npersonality inference. We show that a classical learning-based method, as a\nsimple baseline in real-world affect prediction, produces better estimates from\nsignals captured on egocentric vision systems than processing physiological\nsignals. Our dataset establishes emotion and personality as core dimensions in\negocentric perception and opens new directions in affect-driven modeling of\nbehavior, intent, and interaction.\n","authors":["Matthias Jammot","Bjöern Braun","Paul Streli","Rafael Wampfler","Christian Holz"],"pdf_url":"https://arxiv.org/pdf/2510.22129v1.pdf","comment":"Accepted for publication at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.16394v2","updated":"2025-10-25T02:57:21Z","published":"2025-05-22T08:46:53Z","title":"Raw2Drive: Reinforcement Learning with Aligned World Models for\n  End-to-End Autonomous Driving (in CARLA v2)","summary":"  Reinforcement Learning (RL) can mitigate the causal confusion and\ndistribution shift inherent to imitation learning (IL). However, applying RL to\nend-to-end autonomous driving (E2E-AD) remains an open problem for its training\ndifficulty, and IL is still the mainstream paradigm in both academia and\nindustry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated\npromising results in neural planning; however, these methods typically require\nprivileged information as input rather than raw sensor data. We fill this gap\nby designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently\ntrain an auxiliary privileged world model paired with a neural planner that\nuses privileged information as input. Subsequently, we introduce a raw sensor\nworld model trained via our proposed Guidance Mechanism, which ensures\nconsistency between the raw sensor world model and the privileged world model\nduring rollouts. Finally, the raw sensor world model combines the prior\nknowledge embedded in the heads of the privileged world model to effectively\nguide the training of the raw sensor policy. Raw2Drive is so far the only RL\nbased end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it\nachieves state-of-the-art performance.\n","authors":["Zhenjie Yang","Xiaosong Jia","Qifeng Li","Xue Yang","Maoqing Yao","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2505.16394v2.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2502.12520v6","updated":"2025-10-25T02:55:28Z","published":"2025-02-18T04:09:46Z","title":"SafeEraser: Enhancing Safety in Multimodal Large Language Models through\n  Multimodal Machine Unlearning","summary":"  As Multimodal Large Language Models (MLLMs) develop, their potential security\nissues have become increasingly prominent. Machine Unlearning (MU), as an\neffective strategy for forgetting specific knowledge in training data, has been\nwidely used in privacy protection. However, MU for safety in MLLM has yet to be\nfully explored. To address this issue, we propose SAFEERASER, a safety\nunlearning benchmark for MLLMs, consisting of 3,000 images and 28.8K VQA pairs.\nWe comprehensively evaluate unlearning methods from two perspectives: forget\nquality and model utility. Our findings show that existing MU methods struggle\nto maintain model performance while implementing the forget operation and often\nsuffer from over-forgetting. Hence, we introduce Prompt Decouple (PD) Loss to\nalleviate over-forgetting through decouple prompt during unlearning process. To\nquantitatively measure over-forgetting mitigated by PD Loss, we propose a new\nmetric called Safe Answer Refusal Rate (SARR). Experimental results demonstrate\nthat combining PD Loss with existing unlearning methods can effectively prevent\nover-forgetting and achieve a decrease of 79.5% in the SARR metric of LLaVA-7B\nand LLaVA-13B, while maintaining forget quality and model utility. Our code and\ndataset will be released upon acceptance. Warning: This paper contains examples\nof harmful language and images, and reader discretion is recommended.\n","authors":["Junkai Chen","Zhijie Deng","Kening Zheng","Yibo Yan","Shuliang Liu","PeiJun Wu","Peijie Jiang","Jia Liu","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2502.12520v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22127v1","updated":"2025-10-25T02:55:08Z","published":"2025-10-25T02:55:08Z","title":"Mint: A Simple Test-Time Adaptation of Vision-Language Models against\n  Common Corruptions","summary":"  Pretrained vision-language models such as CLIP achieve strong zero-shot\ngeneralization but remain vulnerable to distribution shifts caused by input\ncorruptions. In this work, we investigate how corruptions affect CLIP's image\nembeddings and uncover a consistent phenomenon we term as embedding variance\ncollapse, where both intra-class and inter-class variances shrink as corruption\nseverity increases. We find that this collapse is closely tied to performance\ndegradation, with inter-class variance strongly correlated with classification\naccuracy. To explain this phenomenon, we analyze how corruptions alter the\nstructure of the embedding space. Our theoretical results suggest that the\nvisual encoder tends to encode corruption-related signals, which dilute\nclass-discriminative features and compress the representation geometry. We\nfurther show that maximizing inter-class variance, even when estimated from\npseudo-labels, can provably enhance embedding quality. Based on this insight,\nwe propose Mint, a simple test-time adaptation method that maximizes\npseudo-label-based inter-class variance on the fly using a mean accumulator and\na gradient accumulator. Mint operates effectively with small batch sizes and\nconsistently improves performance across multiple corruption benchmarks and\nCLIP architectures. Our code is available at https://github.com/baowenxuan/Mint .\n","authors":["Wenxuan Bao","Ruxi Deng","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2510.22127v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2503.10270v3","updated":"2025-10-25T02:29:47Z","published":"2025-03-13T11:26:45Z","title":"EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing","summary":"  Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit\n","authors":["Zexuan Yan","Yue Ma","Chang Zou","Wenteng Chen","Qifeng Chen","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.10270v3.pdf","comment":"accepted by ICCV2025"},{"id":"http://arxiv.org/abs/2510.22119v1","updated":"2025-10-25T02:09:04Z","published":"2025-10-25T02:09:04Z","title":"CogStereo: Neural Stereo Matching with Implicit Spatial Cognition\n  Embedding","summary":"  Deep stereo matching has advanced significantly on benchmark datasets through\nfine-tuning but falls short of the zero-shot generalization seen in foundation\nmodels in other vision tasks. We introduce CogStereo, a novel framework that\naddresses challenging regions, such as occlusions or weak textures, without\nrelying on dataset-specific priors. CogStereo embeds implicit spatial cognition\ninto the refinement process by using monocular depth features as priors,\ncapturing holistic scene understanding beyond local correspondences. This\napproach ensures structurally coherent disparity estimation, even in areas\nwhere geometry alone is inadequate. CogStereo employs a dual-conditional\nrefinement mechanism that combines pixel-wise uncertainty with cognition-guided\nfeatures for consistent global correction of mismatches. Extensive experiments\non Scene Flow, KITTI, Middlebury, ETH3D, EuRoc, and real-world demonstrate that\nCogStereo not only achieves state-of-the-art results but also excels in\ncross-domain generalization, shifting stereo vision towards a cognition-driven\napproach.\n","authors":["Lihuang Fang","Xiao Hu","Yuchen Zou","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.22119v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2510.22107v1","updated":"2025-10-25T01:25:50Z","published":"2025-10-25T01:25:50Z","title":"Discovering Latent Graphs with GFlowNets for Diverse Conditional Image\n  Generation","summary":"  Capturing diversity is crucial in conditional and prompt-based image\ngeneration, particularly when conditions contain uncertainty that can lead to\nmultiple plausible outputs. To generate diverse images reflecting this\ndiversity, traditional methods often modify random seeds, making it difficult\nto discern meaningful differences between samples, or diversify the input\nprompt, which is limited in verbally interpretable diversity. We propose\nRainbow, a novel conditional image generation framework, applicable to any\npretrained conditional generative model, that addresses inherent\ncondition/prompt uncertainty and generates diverse plausible images. Rainbow is\nbased on a simple yet effective idea: decomposing the input condition into\ndiverse latent representations, each capturing an aspect of the uncertainty and\ngenerating a distinct image. First, we integrate a latent graph, parameterized\nby Generative Flow Networks (GFlowNets), into the prompt representation\ncomputation. Second, leveraging GFlowNets' advanced graph sampling capabilities\nto capture uncertainty and output diverse trajectories over the graph, we\nproduce multiple trajectories that collectively represent the input condition,\nleading to diverse condition representations and corresponding output images.\nEvaluations on natural image and medical image datasets demonstrate Rainbow's\nimprovement in both diversity and fidelity across image synthesis, image\ngeneration, and counterfactual generation tasks.\n","authors":["Bailey Trang","Parham Saremi","Alan Q. Wang","Fangrui Huang","Zahra TehraniNasab","Amar Kumar","Tal Arbel","Li Fei-Fei","Ehsan Adeli"],"pdf_url":"https://arxiv.org/pdf/2510.22107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.01704v2","updated":"2025-10-25T01:22:08Z","published":"2025-10-02T06:24:12Z","title":"Holistic Order Prediction in Natural Scenes","summary":"  Even in controlled settings, understanding instance-wise geometries is a\nchallenging task for a wide range of visual models. Although specialized\nsystems exist, modern arts rely on expensive input formats (category labels,\nbinary segmentation masks) and inference costs (a quadratic amount of forward\npasses). We mitigate these limitations by proposing InstaFormer, a network\ncapable of holistic order prediction. That is, solely given an input RGB image,\nInstaFormer returns the full occlusion and depth orderings for all the\ninstances in the scene in a single forward pass. At its core, InstaFormer\nrelies on interactions between object queries and latent mask descriptors that\nsemantically represent the same objects while carrying complementary\ninformation. We comprehensively benchmark and ablate our approach to highlight\nits effectiveness. Our code and models are open-source and available at this\nURL: https://github.com/SNU-VGILab/InstaOrder.\n","authors":["Pierre Musacchio","Hyunmin Lee","Jaesik Park"],"pdf_url":"https://arxiv.org/pdf/2510.01704v2.pdf","comment":"24 pages, 11 figures, 6 tables"},{"id":"http://arxiv.org/abs/2510.22102v1","updated":"2025-10-25T00:58:47Z","published":"2025-10-25T00:58:47Z","title":"Mitigating Coordinate Prediction Bias from Positional Encoding Failures","summary":"  Multimodal large language models (MLLMs) excel at vision-language tasks such\nas VQA and document understanding, yet precise coordinate prediction remains\nchallenging. High-resolution inputs exacerbate this difficulty by producing\nlong token sequences that weaken positional encodings and introduce directional\nbiases in coordinate outputs. We investigate this phenomenon by analyzing how\nMLLMs behave when visual positional encodings (VPEs) are deliberately perturbed\nthrough shuffling. Our analysis reveals that such perturbations induce\npredictable, non-random coordinate biases rather than random errors, suggesting\nthat models rely on internal positional priors when spatial grounding signals\nare degraded. Crucially, we observe similar directional error patterns in\nnatural high-resolution datasets, indicating that positional encoding failures\nare a key bottleneck for accurate coordinate prediction at scale. To address\nthis issue, we propose Vision-PE Shuffle Guidance (VPSG), a training-free\ntest-time method that leverages the directional nature of these biases for\ncorrection. VPSG runs auxiliary decoding with shuffled VPEs to isolate\nposition-unconditioned tendencies, then uses this as negative evidence to guide\ndigit prediction while preserving coordinate format through a lightweight\nfinite-state machine. Experiments on ScreenSpot-Pro demonstrate reliable\nimprovements, highlighting positional encoding robustness as a critical factor\nfor spatial reasoning in MLLMs.\n","authors":["Xingjian Tao","Yiwei Wang","Yujun Cai","Yihong Luo","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2510.22102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2101.09858v5","updated":"2025-10-25T00:08:43Z","published":"2021-01-25T02:31:49Z","title":"Weakly Supervised Learning for Facial Behavior Analysis : A Review","summary":"  In the recent years, there has been a shift in facial behavior analysis from\nthe laboratory-controlled conditions to the challenging in-the-wild conditions\ndue to the superior performance of deep learning based approaches for many real\nworld applications.However, the performance of deep learning approaches relies\non the amount of training data. One of the major problems with data acquisition\nis the requirement of annotations for large amount of training data. Labeling\nprocess of huge training data demands lot of human support with strong domain\nexpertise for facial expressions or action units, which is difficult to obtain\nin real-time environments.Moreover, labeling process is highly vulnerable to\nambiguity of expressions or action units, especially for intensities due to the\nbias induced by the domain experts. Therefore, there is an imperative need to\naddress the problem of facial behavior analysis with weak annotations. In this\npaper, we provide a comprehensive review of weakly supervised learning (WSL)\napproaches for facial behavior analysis with both categorical as well as\ndimensional labels along with the challenges and potential research directions\nassociated with it. First, we introduce various types of weak annotations in\nthe context of facial behavior analysis and the corresponding challenges\nassociated with it. We then systematically review the existing state-of-the-art\napproaches and provide a taxonomy of these approaches along with their insights\nand limitations. In addition, widely used data-sets in the reviewed literature\nand the performance of these approaches along with evaluation principles are\nsummarized. Finally, we discuss the remaining challenges and opportunities\nalong with the potential research directions in order to apply facial behavior\nanalysis with weak labels in real life situations.\n","authors":["R. Gnana Praveen","Patrick Cardinal","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2101.09858v5.pdf","comment":"Provided a link of constantly updated papers\n  \\url{https://github.com/praveena2j/\n  awesome-Weakly-Supervised-Facial-Behavior-Analysis}"}]},"2025-10-29T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2510.25772v1","updated":"2025-10-29T17:59:53Z","published":"2025-10-29T17:59:53Z","title":"VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context\n  Learning","summary":"  Visual effects (VFX) are crucial to the expressive power of digital media,\nyet their creation remains a major challenge for generative AI. Prevailing\nmethods often rely on the one-LoRA-per-effect paradigm, which is\nresource-intensive and fundamentally incapable of generalizing to unseen\neffects, thus limiting scalability and creation. To address this challenge, we\nintroduce VFXMaster, the first unified, reference-based framework for VFX video\ngeneration. It recasts effect generation as an in-context learning task,\nenabling it to reproduce diverse dynamic effects from a reference video onto\ntarget content. In addition, it demonstrates remarkable generalization to\nunseen effect categories. Specifically, we design an in-context conditioning\nstrategy that prompts the model with a reference example. An in-context\nattention mask is designed to precisely decouple and inject the essential\neffect attributes, allowing a single unified model to master the effect\nimitation without information leakage. In addition, we propose an efficient\none-shot effect adaptation mechanism to boost generalization capability on\ntough unseen effects from a single user-provided video rapidly. Extensive\nexperiments demonstrate that our method effectively imitates various categories\nof effect information and exhibits outstanding generalization to out-of-domain\neffects. To foster future research, we will release our code, models, and a\ncomprehensive dataset to the community.\n","authors":["Baolu Li","Yiming Zhang","Qinghe Wang","Liqian Ma","Xiaoyu Shi","Xintao Wang","Pengfei Wan","Zhenfei Yin","Yunzhi Zhuge","Huchuan Lu","Xu Jia"],"pdf_url":"https://arxiv.org/pdf/2510.25772v1.pdf","comment":"Project Page URL:https://libaolu312.github.io/VFXMaster/"},{"id":"http://arxiv.org/abs/2510.25765v1","updated":"2025-10-29T17:58:14Z","published":"2025-10-29T17:58:14Z","title":"FreeArt3D: Training-Free Articulated Object Generation using 3D\n  Diffusion","summary":"  Articulated 3D objects are central to many applications in robotics, AR/VR,\nand animation. Recent approaches to modeling such objects either rely on\noptimization-based reconstruction pipelines that require dense-view supervision\nor on feed-forward generative models that produce coarse geometric\napproximations and often overlook surface texture. In contrast, open-world 3D\ngeneration of static objects has achieved remarkable success, especially with\nthe advent of native 3D diffusion models such as Trellis. However, extending\nthese methods to articulated objects by training native 3D diffusion models\nposes significant challenges. In this work, we present FreeArt3D, a\ntraining-free framework for articulated 3D object generation. Instead of\ntraining a new model on limited articulated data, FreeArt3D repurposes a\npre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape\nprior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by\ntreating articulation as an additional generative dimension. Given a few images\ncaptured in different articulation states, FreeArt3D jointly optimizes the\nobject's geometry, texture, and articulation parameters without requiring\ntask-specific training or access to large-scale articulated datasets. Our\nmethod generates high-fidelity geometry and textures, accurately predicts\nunderlying kinematic structures, and generalizes well across diverse object\ncategories. Despite following a per-instance optimization paradigm, FreeArt3D\ncompletes in minutes and significantly outperforms prior state-of-the-art\napproaches in both quality and versatility.\n","authors":["Chuhao Chen","Isabella Liu","Xinyue Wei","Hao Su","Minghua Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25760v1","updated":"2025-10-29T17:55:43Z","published":"2025-10-29T17:55:43Z","title":"Multimodal Spatial Reasoning in the Large Model Era: A Survey and\n  Benchmarks","summary":"  Humans possess spatial reasoning abilities that enable them to understand\nspaces through multimodal observations, such as vision and sound. Large\nmultimodal reasoning models extend these abilities by learning to perceive and\nreason, showing promising performance across diverse spatial tasks. However,\nsystematic reviews and publicly available benchmarks for these models remain\nlimited. In this survey, we provide a comprehensive review of multimodal\nspatial reasoning tasks with large models, categorizing recent progress in\nmultimodal large language models (MLLMs) and introducing open benchmarks for\nevaluation. We begin by outlining general spatial reasoning, focusing on\npost-training techniques, explainability, and architecture. Beyond classical 2D\ntasks, we examine spatial relationship reasoning, scene and layout\nunderstanding, as well as visual question answering and grounding in 3D space.\nWe also review advances in embodied AI, including vision-language navigation\nand action models. Additionally, we consider emerging modalities such as audio\nand egocentric video, which contribute to novel spatial understanding through\nnew sensors. We believe this survey establishes a solid foundation and offers\ninsights into the growing field of multimodal spatial reasoning. Updated\ninformation about this survey, codes and implementation of the open benchmarks\ncan be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.\n","authors":["Xu Zheng","Zihao Dongfang","Lutao Jiang","Boyuan Zheng","Yulong Guo","Zhenquan Zhang","Giuliano Albanese","Runyi Yang","Mengjiao Ma","Zixin Zhang","Chenfei Liao","Dingcheng Zhen","Yuanhuiyi Lyu","Yuqian Fu","Bin Ren","Linfeng Zhang","Danda Pani Paudel","Nicu Sebe","Luc Van Gool","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2510.25760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25739v1","updated":"2025-10-29T17:43:31Z","published":"2025-10-29T17:43:31Z","title":"Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image\n  Generation","summary":"  Autoregressive (AR) image generation models are capable of producing\nhigh-fidelity images but often suffer from slow inference due to their\ninherently sequential, token-by-token decoding process. Speculative decoding,\nwhich employs a lightweight draft model to approximate the output of a larger\nAR model, has shown promise in accelerating text generation without\ncompromising quality. However, its application to image generation remains\nlargely underexplored. The challenges stem from a significantly larger sampling\nspace, which complicates the alignment between the draft and target model\noutputs, coupled with the inadequate use of the two-dimensional spatial\nstructure inherent in images, thereby limiting the modeling of local\ndependencies. To overcome these challenges, we introduce Hawk, a new approach\nthat harnesses the spatial structure of images to guide the speculative model\ntoward more accurate and efficient predictions. Experimental results on\nmultiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR\nmodels, while preserving both image fidelity and diversity.\n","authors":["Zhi-Kai Chen","Jun-Peng Jiang","Han-Jia Ye","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2510.25739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.20600v2","updated":"2025-10-29T17:35:15Z","published":"2025-08-28T09:43:59Z","title":"GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac\n  MRI Reconstruction","summary":"  Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction\nremains a critical challenge due to the trade-off between scan time and image\nquality, particularly when generalizing across diverse acquisition settings. We\npropose GENRE-CMR, a generative adversarial network (GAN)-based architecture\nemploying a residual deep unrolled reconstruction framework to enhance\nreconstruction fidelity and generalization. The architecture unrolls iterative\noptimization into a cascade of convolutional subnetworks, enriched with\nresidual connections to enable progressive feature propagation from shallow to\ndeeper stages. To further improve performance, we integrate two loss functions:\n(1) an Edge-Aware Region (EAR) loss, which guides the network to focus on\nstructurally informative regions and helps prevent common reconstruction\nblurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which\nregularizes the feature space across diverse data distributions via a symmetric\nKL divergence formulation. Extensive experiments confirm that GENRE-CMR\nsurpasses state-of-the-art methods on training and unseen data, achieving\n0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various\nacceleration factors and sampling trajectories. Ablation studies confirm the\ncontribution of each proposed component to reconstruction quality and\ngeneralization. Our framework presents a unified and robust solution for\nhigh-quality CMR reconstruction, paving the way for clinically adaptable\ndeployment across heterogeneous acquisition protocols.\n","authors":["Kian Anvari Hamedani","Narges Razizadeh","Shahabedin Nabavi","Mohsen Ebrahimi Moghaddam"],"pdf_url":"https://arxiv.org/pdf/2508.20600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.24096v2","updated":"2025-10-29T16:13:52Z","published":"2025-06-30T17:48:54Z","title":"MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient\n  Surface Reconstruction","summary":"  While recent advances in Gaussian Splatting have enabled fast reconstruction\nof high-quality 3D scenes from images, extracting accurate surface meshes\nremains a challenge. Current approaches extract the surface through costly\npost-processing steps, resulting in the loss of fine geometric details or\nrequiring significant time and leading to very dense meshes with millions of\nvertices. More fundamentally, the a posteriori conversion from a volumetric to\na surface representation limits the ability of the final mesh to preserve all\ngeometric structures captured during training. We present MILo, a novel\nGaussian Splatting framework that bridges the gap between volumetric and\nsurface representations by differentiably extracting a mesh from the 3D\nGaussians. We design a fully differentiable procedure that constructs the\nmesh-including both vertex locations and connectivity-at every iteration\ndirectly from the parameters of the Gaussians, which are the only quantities\noptimized during training. Our method introduces three key technical\ncontributions: a bidirectional consistency framework ensuring both\nrepresentations-Gaussians and the extracted mesh-capture the same underlying\ngeometry during training; an adaptive mesh extraction process performed at each\ntraining iteration, which uses Gaussians as differentiable pivots for Delaunay\ntriangulation; a novel method for computing signed distance values from the 3D\nGaussians that enables precise surface extraction while avoiding geometric\nerosion. Our approach can reconstruct complete scenes, including backgrounds,\nwith state-of-the-art quality while requiring an order of magnitude fewer mesh\nvertices than previous methods. Due to their light weight and empty interior,\nour meshes are well suited for downstream applications such as physics\nsimulations or animation.\n","authors":["Antoine Guédon","Diego Gomez","Nissim Maruani","Bingchen Gong","George Drettakis","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2506.24096v2.pdf","comment":"10 pages. A presentation video of our approach is available at\n  https://youtu.be/_SGNhhNz0fE"},{"id":"http://arxiv.org/abs/2506.07464v3","updated":"2025-10-29T15:59:41Z","published":"2025-06-09T06:15:54Z","title":"DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO","summary":"  Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training for enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success using a PPO-style reinforcement algorithm\nwith group-normalized rewards. However, the effectiveness of GRPO in Video\nLarge Language Models (VideoLLMs) has still been less studyed. In this paper,\nwe explore GRPO and identify two problems that deteriorate the effective\nlearning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate\nthese challenges, we propose DeepVideo-R1, a video large language model trained\nwith Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation.\nReg-GRPO reformulates the GRPO loss function into a regression task that\ndirectly predicts the advantage in GRPO, eliminating the need for safeguards\nsuch as the clipping and min functions. It directly aligns the model with\nadvantages, providing guidance to prefer better ones. The difficulty-aware data\naugmentation strategy augments input prompts/videos to locate the difficulty of\nsamples at solvable difficulty levels, enabling diverse reward signals. Our\nexperimental results show that our approach significantly improves video\nreasoning performance across multiple benchmarks.\n","authors":["Jinyoung Park","Jeehye Na","Jinyoung Kim","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2506.07464v3.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2309.13672v8","updated":"2025-10-29T15:35:18Z","published":"2023-09-24T15:40:40Z","title":"RL-I2IT: Image-to-Image Translation with Deep Reinforcement Learning","summary":"  Most existing Image-to-Image Translation (I2IT) methods generate images in a\nsingle run of a deep learning (DL) model. However, designing such a single-step\nmodel is always challenging, requiring a huge number of parameters and easily\nfalling into bad global minimums and overfitting. In this work, we reformulate\nI2IT as a step-wise decision-making problem via deep reinforcement learning\n(DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The\nkey feature in the RL-I2IT framework is to decompose a monolithic learning\nprocess into small steps with a lightweight model to progressively transform a\nsource image successively to a target image. Considering that it is challenging\nto handle high dimensional continuous state and action spaces in the\nconventional RL framework, we introduce meta policy with a new concept Plan to\nthe standard Actor-Critic model, which is of a lower dimension than the\noriginal image and can facilitate the actor to generate a tractable high\ndimensional action. In the RL-I2IT framework, we also employ a task-specific\nauxiliary learning strategy to stabilize the training process and improve the\nperformance of the corresponding task. Experiments on several I2IT tasks\ndemonstrate the effectiveness and robustness of the proposed method when facing\nhigh-dimensional continuous action space problems. Our implementation of the\nRL-I2IT framework is available at\nhttps://github.com/Algolzw/SPAC-Deformable-Registration.\n","authors":["Jing Hu","Chengming Feng","Shu Hu","Ming-Ching Chang","Xin Li","Xi Wu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2309.13672v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23118v3","updated":"2025-10-29T15:24:05Z","published":"2025-10-27T08:38:52Z","title":"Quantizing Space and Time: Fusing Time Series and Images for Earth\n  Observation","summary":"  We propose a task-agnostic framework for multimodal fusion of time series and\nsingle timestamp images, enabling cross-modal generation and robust downstream\nperformance. Our approach explores deterministic and learned strategies for\ntime series quantization and then leverages a masked correlation learning\nobjective, aligning discrete image and time series tokens in a unified\nrepresentation space. Instantiated in the Earth observation domain, the\npretrained model generates consistent global temperature profiles from\nsatellite imagery and is validated through counterfactual experiments. Across\ndownstream tasks, our task-agnostic pretraining outperforms task-specific\nfusion by 6% in R^2 and 2% in RMSE on average, and exceeds baseline methods by\n50% in R^2 and 12% in RMSE. Finally, we analyze gradient sensitivity across\nmodalities, providing insights into model robustness. Code, data, and weights\nwill be released under a permissive license.\n","authors":["Gianfranco Basile","Johannes Jakubik","Benedikt Blumenstiel","Thomas Brunschwiler","Juan Bernabe Moreno"],"pdf_url":"https://arxiv.org/pdf/2510.23118v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25594v1","updated":"2025-10-29T15:03:46Z","published":"2025-10-29T15:03:46Z","title":"Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for\n  Local Learning","summary":"  Training deep neural networks (DNNs) with backpropagation (BP) achieves\nstate-of-the-art accuracy but requires global error propagation and full\nparameterization, leading to substantial memory and computational overhead.\nDirect Feedback Alignment (DFA) enables local, parallelizable updates with\nlower memory requirements but is limited by unstructured feedback and poor\nscalability in deeper architectures, specially convolutional neural networks.\nTo address these limitations, we propose a structured local learning framework\nthat operates directly on low-rank manifolds defined by the Singular Value\nDecomposition (SVD) of weight matrices. Each layer is trained in its decomposed\nform, with updates applied to the SVD components using a composite loss that\nintegrates cross-entropy, subspace alignment, and orthogonality regularization.\nFeedback matrices are constructed to match the SVD structure, ensuring\nconsistent alignment between forward and feedback pathways. Our method reduces\nthe number of trainable parameters relative to the original DFA model, without\nrelying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,\nand ImageNet show that our method achieves accuracy comparable to that of BP.\nAblation studies confirm the importance of each loss term in the low-rank\nsetting. These results establish local learning on low-rank manifolds as a\nprincipled and scalable alternative to full-rank gradient-based training.\n","authors":["Arani Roy","Marco P. Apolinario","Shristi Das Biswas","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2510.25594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25590v1","updated":"2025-10-29T14:58:37Z","published":"2025-10-29T14:58:37Z","title":"RegionE: Adaptive Region-Aware Generation for Efficient Image Editing","summary":"  Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.\n","authors":["Pengtao Chen","Xianfang Zeng","Maosen Zhao","Mingzhu Shen","Peng Ye","Bangyin Xiang","Zhibo Wang","Wei Cheng","Gang Yu","Tao Chen"],"pdf_url":"https://arxiv.org/pdf/2510.25590v1.pdf","comment":"26 pages, 10 figures, 18 tables"},{"id":"http://arxiv.org/abs/2506.21710v2","updated":"2025-10-29T14:46:17Z","published":"2025-06-26T18:51:04Z","title":"FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering","summary":"  While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.\n","authors":["Liangyu Zhong","Fabio Rosenthal","Joachim Sicking","Fabian Hüger","Thorsten Bagdonat","Hanno Gottschalk","Leo Schwinn"],"pdf_url":"https://arxiv.org/pdf/2506.21710v2.pdf","comment":"Accepted by NeurIPS 2025 - main track. Project page:\n  https://focus-mllm-vqa.github.io/"},{"id":"http://arxiv.org/abs/2510.25522v1","updated":"2025-10-29T13:46:19Z","published":"2025-10-29T13:46:19Z","title":"Comparative Study of UNet-based Architectures for Liver Tumor\n  Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography","summary":"  Segmentation of liver structures in multi-phase contrast-enhanced computed\ntomography (CECT) plays a crucial role in computer-aided diagnosis and\ntreatment planning for liver diseases, including tumor detection. In this\nstudy, we investigate the performance of UNet-based architectures for liver\ntumor segmentation, starting from the original UNet and extending to UNet3+\nwith various backbone networks. We evaluate ResNet, Transformer-based, and\nState-space (Mamba) backbones, all initialized with pretrained weights.\nSurprisingly, despite the advances in modern architecture, ResNet-based models\nconsistently outperform Transformer- and Mamba-based alternatives across\nmultiple evaluation metrics. To further improve segmentation quality, we\nintroduce attention mechanisms into the backbone and observe that incorporating\nthe Convolutional Block Attention Module (CBAM) yields the best performance.\nResNetUNet3+ with CBAM module not only produced the best overlap metrics with a\nDice score of 0.755 and IoU of 0.662, but also achieved the most precise\nboundary delineation, evidenced by the lowest HD95 distance of 77.911. The\nmodel's superiority was further cemented by its leading overall accuracy of\n0.925 and specificity of 0.926, showcasing its robust capability in accurately\nidentifying both lesion and healthy tissue. To further enhance\ninterpretability, Grad-CAM visualizations were employed to highlight the\nregion's most influential predictions, providing insights into its\ndecision-making process. These findings demonstrate that classical ResNet\narchitecture, when combined with modern attention modules, remain highly\ncompetitive for medical image segmentation tasks, offering a promising\ndirection for liver tumor detection in clinical practice.\n","authors":["Doan-Van-Anh Ly","Thi-Thu-Hien Pham","Thanh-Hai Le"],"pdf_url":"https://arxiv.org/pdf/2510.25522v1.pdf","comment":"27 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.23763v2","updated":"2025-10-29T13:37:19Z","published":"2025-10-27T18:49:03Z","title":"RoboOmni: Proactive Robot Manipulation in Omni-modal Context","summary":"  Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.\n","authors":["Siyin Wang","Jinlan Fu","Feihong Liu","Xinzhe He","Huangxuan Wu","Junhao Shi","Kexin Huang","Zhaoye Fei","Jingjing Gong","Zuxuan Wu","Yugang Jiang","See-Kiong Ng","Tat-Seng Chua","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2510.23763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25512v1","updated":"2025-10-29T13:35:46Z","published":"2025-10-29T13:35:46Z","title":"FaCT: Faithful Concept Traces for Explaining Neural Network Decisions","summary":"  Deep networks have shown remarkable performance across a wide range of tasks,\nyet getting a global concept-level understanding of how they function remains a\nkey challenge. Many post-hoc concept-based approaches have been introduced to\nunderstand their workings, yet they are not always faithful to the model.\nFurther, they make restrictive assumptions on the concepts a model learns, such\nas class-specificity, small spatial extent, or alignment to human expectations.\nIn this work, we put emphasis on the faithfulness of such concept-based\nexplanations and propose a new model with model-inherent mechanistic\nconcept-explanations. Our concepts are shared across classes and, from any\nlayer, their contribution to the logit and their input-visualization can be\nfaithfully traced. We also leverage foundation models to propose a new\nconcept-consistency metric, C$^2$-Score, that can be used to evaluate\nconcept-based methods. We show that, compared to prior work, our concepts are\nquantitatively more consistent and users find our concepts to be more\ninterpretable, all while retaining competitive ImageNet performance.\n","authors":["Amin Parchami-Araghi","Sukrut Rao","Jonas Fischer","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2510.25512v1.pdf","comment":"Accepted to NeurIPS 2025; Code is available at\n  https://github.com/m-parchami/FaCT"},{"id":"http://arxiv.org/abs/2411.10237v2","updated":"2025-10-29T13:22:04Z","published":"2024-11-15T14:51:30Z","title":"ScribbleVS: Scribble-Supervised Medical Image Segmentation via Dynamic\n  Competitive Pseudo Label Selection","summary":"  In clinical medicine, precise image segmentation can provide substantial\nsupport to clinicians. However, obtaining high-quality segmentation typically\ndemands extensive pixel-level annotations, which are labor-intensive and\nexpensive. Scribble annotations offer a more cost-effective alternative by\nimproving labeling efficiency. Nonetheless, using such sparse supervision for\ntraining reliable medical image segmentation models remains a significant\nchallenge. Some studies employ pseudo-labeling to enhance supervision, but\nthese methods are susceptible to noise interference. To address these\nchallenges, we introduce ScribbleVS, a framework designed to learn from\nscribble annotations. We introduce a Regional Pseudo Labels Diffusion Module to\nexpand the scope of supervision and reduce the impact of noise present in\npseudo labels. Additionally, we introduce a Dynamic Competitive Selection\nmodule for enhanced refinement in selecting pseudo labels. Experiments\nconducted on the ACDC, MSCMRseg, WORD, and BraTS2020 datasets demonstrate\npromising results, achieving segmentation precision comparable to fully\nsupervised models. The codes of this study are available at\nhttps://github.com/ortonwang/ScribbleVS.\n","authors":["Tao Wang","Xinlin Zhang","Zhenxuan Zhang","Yuanbo Zhou","Yuanbin Chen","Longxuan Zhao","Chaohui Xu","Shun Chen","Guang Yang","Tong Tong"],"pdf_url":"https://arxiv.org/pdf/2411.10237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09518v2","updated":"2025-10-29T13:12:44Z","published":"2025-06-11T08:45:08Z","title":"HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for\n  Dynamic Scene","summary":"  Reconstructing dynamic 3D scenes from monocular videos remains a fundamental\nchallenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time\nrendering in static settings, extending it to dynamic scenes is challenging due\nto the difficulty of learning structured and temporally consistent motion\nrepresentations. This challenge often manifests as three limitations in\nexisting methods: redundant Gaussian updates, insufficient motion supervision,\nand weak modeling of complex non-rigid deformations. These issues collectively\nhinder coherent and efficient dynamic reconstruction. To address these\nlimitations, we propose HAIF-GS, a unified framework that enables structured\nand consistent dynamic modeling through sparse anchor-driven deformation. It\nfirst identifies motion-relevant regions via an Anchor Filter to suppress\nredundant updates in static areas. A self-supervised Induced Flow-Guided\nDeformation module induces anchor motion using multi-frame feature aggregation,\neliminating the need for explicit flow labels. To further handle fine-grained\ndeformations, a Hierarchical Anchor Propagation mechanism increases anchor\nresolution based on motion complexity and propagates multi-level\ntransformations. Extensive experiments on synthetic and real-world benchmarks\nvalidate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in\nrendering quality, temporal coherence, and reconstruction efficiency.\n","authors":["Jianing Chen","Zehao Li","Yujun Cai","Hao Jiang","Chengxuan Qian","Juyuan Kang","Shuqin Gao","Honglong Zhao","Tianlu Mao","Yucheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.09518v2.pdf","comment":"Accepted to NeurIPS 2025. Project page:\n  https://echopickle.github.io/HAIF-GS.github.io/"},{"id":"http://arxiv.org/abs/2503.08068v2","updated":"2025-10-29T12:57:01Z","published":"2025-03-11T05:59:43Z","title":"Simulating Automotive Radar with Lidar and Camera Inputs","summary":"  Low-cost millimeter automotive radar has received more and more attention due\nto its ability to handle adverse weather and lighting conditions in autonomous\ndriving. However, the lack of quality datasets hinders research and\ndevelopment. We report a new method that is able to simulate 4D millimeter wave\nradar signals including pitch, yaw, range, and Doppler velocity along with\nradar signal strength (RSS) using camera image, light detection and ranging\n(lidar) point cloud, and ego-velocity. The method is based on two new neural\nnetworks: 1) DIS-Net, which estimates the spatial distribution and number of\nradar signals, and 2) RSS-Net, which predicts the RSS of the signal based on\nappearance and geometric information. We have implemented and tested our method\nusing open datasets from 3 different models of commercial automotive radar. The\nexperimental results show that our method can successfully generate\nhigh-fidelity radar signals. Moreover, we have trained a popular object\ndetection neural network with data augmented by our synthesized radar. The\nnetwork outperforms the counterpart trained only on raw radar data, a promising\nresult to facilitate future radar-based research and development.\n","authors":["Peili Song","Dezhen Song","Yifan Yang","Enfan Lan","Jingtai Liu"],"pdf_url":"https://arxiv.org/pdf/2503.08068v2.pdf","comment":"Accepted by IROS 2025"},{"id":"http://arxiv.org/abs/2505.17685v2","updated":"2025-10-29T12:46:23Z","published":"2025-05-23T09:55:32Z","title":"FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for\n  Autonomous Driving","summary":"  Vision-Language-Action (VLA) models are increasingly used for end-to-end\ndriving due to their world knowledge and reasoning ability. Most prior work,\nhowever, inserts textual chains-of-thought (CoT) as intermediate steps tailored\nto the current scene. Such symbolic compressions can blur spatio-temporal\nrelations and discard fine visual cues, creating a cross-modal gap between\nperception and planning. We propose FSDrive, a visual spatio-temporal CoT\nframework that enables VLAs to think in images. The model first acts as a world\nmodel to generate a unified future frame that overlays coarse but\nphysically-plausible priors-future lane dividers and 3D boxes-on the predicted\nfuture image. This unified frame serves as the visual CoT, capturing both\nspatial structure and temporal evolution. The same VLA then functions as an\ninverse-dynamics model, planning trajectories from current observations and the\nvisual CoT. To equip VLAs with image generation while preserving understanding,\nwe introduce a unified pre-training paradigm that expands the vocabulary to\ninclude visual tokens and jointly optimizes VQA (for semantics) and\nfuture-frame prediction (for dynamics). A progressive easy-to-hard scheme first\npredicts lane/box priors to enforce physical constraints, then completes full\nfuture frames for fine details. On nuScenes and NAVSIM, FSDrive improves\ntrajectory accuracy and reduces collisions under both ST-P3 and UniAD metrics,\nand attains competitive FID for future-frame generation despite using\nlightweight autoregression. It also advances scene understanding on DriveLM.\nTogether, these results indicate that visual CoT narrows the cross-modal gap\nand yields safer, more anticipatory planning. Code is available at\nhttps://github.com/MIV-XJTU/FSDrive.\n","authors":["Shuang Zeng","Xinyuan Chang","Mengwei Xie","Xinran Liu","Yifan Bai","Zheng Pan","Mu Xu","Xing Wei"],"pdf_url":"https://arxiv.org/pdf/2505.17685v2.pdf","comment":"Accepted to NeurIPS 2025 as Spotlight Presentation. Code:\n  https://github.com/MIV-XJTU/FSDrive"},{"id":"http://arxiv.org/abs/2510.25463v1","updated":"2025-10-29T12:37:34Z","published":"2025-10-29T12:37:34Z","title":"SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time,\n  Monocular Depth Estimation in Underwater Environments","summary":"  Underwater infrastructure requires frequent inspection and maintenance due to\nharsh marine conditions. Current reliance on human divers or remotely operated\nvehicles is limited by perceptual and operational challenges, especially around\ncomplex structures or in turbid water. Enhancing the spatial awareness of\nunderwater vehicles is key to reducing piloting risks and enabling greater\nautonomy. To address these challenges, we present SPADE: SParsity Adaptive\nDepth Estimator, a monocular depth estimation pipeline that combines\npre-trained relative depth estimator with sparse depth priors to produce dense,\nmetric scale depth maps. Our two-stage approach first scales the relative depth\nmap with the sparse depth points, then refines the final metric prediction with\nour proposed Cascade Conv-Deformable Transformer blocks. Our approach achieves\nimproved accuracy and generalisation over state-of-the-art baselines and runs\nefficiently at over 15 FPS on embedded hardware, promising to support practical\nunderwater inspection and intervention. This work has been submitted to IEEE\nJournal of Oceanic Engineering Special Issue of AUV 2026.\n","authors":["Hongjie Zhang","Gideon Billings","Stefan B. Williams"],"pdf_url":"https://arxiv.org/pdf/2510.25463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17897v4","updated":"2025-10-29T12:15:39Z","published":"2025-07-23T19:48:27Z","title":"Multimodal Recurrent Ensembles for Predicting Brain Responses to\n  Naturalistic Movies (Algonauts 2025)","summary":"  Accurately predicting distributed cortical responses to naturalistic stimuli\nrequires models that integrate visual, auditory and semantic information over\ntime. We present a hierarchical multimodal recurrent ensemble that maps\npretrained video, audio, and language embeddings to fMRI time series recorded\nwhile four subjects watched almost 80 hours of movies provided by the Algonauts\n2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;\ntheir hidden states are fused and passed to a second recurrent layer, and\nlightweight subject-specific heads output responses for 1000 cortical parcels.\nTraining relies on a composite MSE-correlation loss and a curriculum that\ngradually shifts emphasis from early sensory to late association regions.\nAveraging 100 model variants further boosts robustness. The resulting system\nranked third on the competition leaderboard, achieving an overall Pearson r =\n0.2094 and the highest single-parcel peak score (mean r = 0.63) among all\nparticipants, with particularly strong gains for the most challenging subject\n(Subject 5). The approach establishes a simple, extensible baseline for future\nmultimodal brain-encoding benchmarks.\n","authors":["Semih Eren","Deniz Kucukahmetler","Nico Scherf"],"pdf_url":"https://arxiv.org/pdf/2507.17897v4.pdf","comment":"8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts\n  Project session (3rd-place team). Code:\n  https://github.com/erensemih/Algonauts2025_ModalityRNN v3: Added equal\n  contribution footnote to author list. Corrected reference list"},{"id":"http://arxiv.org/abs/2509.09349v2","updated":"2025-10-29T12:14:41Z","published":"2025-09-11T11:05:14Z","title":"Classification of Driver Behaviour Using External Observation Techniques\n  for Autonomous Vehicles","summary":"  Road traffic accidents remain a significant global concern, with human error,\nparticularly distracted and impaired driving, among the leading causes. This\nstudy introduces a novel driver behaviour classification system that uses\nexternal observation techniques to detect indicators of distraction and\nimpairment. The proposed framework employs advanced computer vision\nmethodologies, including real-time object tracking, lateral displacement\nanalysis, and lane position monitoring. The system identifies unsafe driving\nbehaviours such as excessive lateral movement and erratic trajectory patterns\nby implementing the YOLO object detection model and custom lane estimation\nalgorithms. Unlike systems reliant on inter-vehicular communication, this\nvision-based approach enables behavioural analysis of non-connected vehicles.\nExperimental evaluations on diverse video datasets demonstrate the framework's\nreliability and adaptability across varying road and environmental conditions.\n","authors":["Ian Nell","Shane Gilroy"],"pdf_url":"https://arxiv.org/pdf/2509.09349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25440v1","updated":"2025-10-29T12:06:42Z","published":"2025-10-29T12:06:42Z","title":"More than a Moment: Towards Coherent Sequences of Audio Descriptions","summary":"  Audio Descriptions (ADs) convey essential on-screen information, allowing\nvisually impaired audiences to follow videos. To be effective, ADs must form a\ncoherent sequence that helps listeners to visualise the unfolding scene, rather\nthan describing isolated moments. However, most automatic methods generate each\nAD independently, often resulting in repetitive, incoherent descriptions. To\naddress this, we propose a training-free method, CoherentAD, that first\ngenerates multiple candidate descriptions for each AD time interval, and then\nperforms auto-regressive selection across the sequence to form a coherent and\ninformative narrative. To evaluate AD sequences holistically, we introduce a\nsequence-level metric, StoryRecall, which measures how well the predicted ADs\nconvey the ground truth narrative, alongside repetition metrics that capture\nthe redundancy across consecutive AD outputs. Our method produces coherent AD\nsequences with enhanced narrative understanding, outperforming prior approaches\nthat rely on independent generations.\n","authors":["Eshika Khandelwal","Junyu Xie","Tengda Han","Max Bain","Arsha Nagrani","Andrew Zisserman","Gül Varol","Makarand Tapaswi"],"pdf_url":"https://arxiv.org/pdf/2510.25440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05780v2","updated":"2025-10-29T10:59:02Z","published":"2024-08-11T14:11:45Z","title":"U-DECN: End-to-End Underwater Object Detection ConvNet with Improved\n  DeNoising Training","summary":"  Underwater object detection has higher requirements of running speed and\ndeployment efficiency for the detector due to its specific environmental\nchallenges. NMS of two- or one-stage object detectors and transformer\narchitecture of query-based end-to-end object detectors are not conducive to\ndeployment on underwater embedded devices with limited processing power. As for\nthe detrimental effect of underwater color cast noise, recent underwater object\ndetectors make network architecture or training complex, which also hinders\ntheir application and deployment on unmanned underwater vehicles. In this\npaper, we propose the Underwater DECO with improved deNoising training\n(U-DECN), the query-based end-to-end object detector (with ConvNet\nencoder-decoder architecture) for underwater color cast noise that addresses\nthe above problems. We integrate advanced technologies from DETR variants into\nDECO and design optimization methods specifically for the ConvNet architecture,\nincluding Deformable Convolution in SIM and Separate Contrastive DeNoising\nForward methods. To address the underwater color cast noise issue, we propose\nan Underwater Color DeNoising Query method to improve the generalization of the\nmodel for the biased object feature information by different color cast noise.\nOur U-DECN, with ResNet-50 backbone, achieves the best 64.0 AP on DUO and the\nbest 58.1 AP on RUOD, and 21 FPS (5 times faster than Deformable DETR and DINO\n4 FPS) on NVIDIA AGX Orin by TensorRT FP16, outperforming the other\nstate-of-the-art query-based end-to-end object detectors. The code is available\nat https://github.com/LEFTeyex/U-DECN.\n","authors":["Zhuoyan Liu","Bo Wang","Bing Wang","Ye Li"],"pdf_url":"https://arxiv.org/pdf/2408.05780v2.pdf","comment":"10 pages, 6 figures, 7 tables, accepted by IEEE TGRS"},{"id":"http://arxiv.org/abs/2510.25387v1","updated":"2025-10-29T10:57:59Z","published":"2025-10-29T10:57:59Z","title":"Instance-Level Composed Image Retrieval","summary":"  The progress of composed image retrieval (CIR), a popular research direction\nin image retrieval, where a combined visual and textual query is used, is held\nback by the absence of high-quality training and evaluation data. We introduce\na new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an\ninstance-level class definition. The goal is to retrieve images that contain\nthe same particular object as the visual query, presented under a variety of\nmodifications defined by textual queries. Its design and curation process keep\nthe dataset compact to facilitate future research, while maintaining its\nchallenge-comparable to retrieval among more than 40M random\ndistractors-through a semi-automated selection of hard negatives.\n  To overcome the challenge of obtaining clean, diverse, and suitable training\ndata, we leverage pre-trained vision-and-language models (VLMs) in a\ntraining-free approach called BASIC. The method separately estimates\nquery-image-to-image and query-text-to-image similarities, performing late\nfusion to upweight images that satisfy both queries, while down-weighting those\nthat exhibit high similarity with only one of the two. Each individual\nsimilarity is further improved by a set of components that are simple and\nintuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR\ndatasets that follow a semantic-level class definition. Project page:\nhttps://vrg.fel.cvut.cz/icir/.\n","authors":["Bill Psomas","George Retsinas","Nikos Efthymiadis","Panagiotis Filntisis","Yannis Avrithis","Petros Maragos","Ondrej Chum","Giorgos Tolias"],"pdf_url":"https://arxiv.org/pdf/2510.25387v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.20274v2","updated":"2025-10-29T10:47:16Z","published":"2025-05-26T17:53:28Z","title":"Probabilistic Kernel Function for Fast Angle Testing","summary":"  In this paper, we study the angle testing problem in the context of\nsimilarity search in high-dimensional Euclidean spaces and propose two\nprojection-based probabilistic kernel functions, one designed for angle\ncomparison and the other for angle thresholding. Unlike existing approaches\nthat rely on random projection vectors drawn from Gaussian distributions, our\napproach leverages reference angles and employs a deterministic structure for\nthe projection vectors. Notably, our kernel functions do not require asymptotic\nassumptions, such as the number of projection vectors tending to infinity, and\ncan be both theoretically and experimentally shown to outperform\nGaussian-distribution-based kernel functions. We apply the proposed kernel\nfunction to Approximate Nearest Neighbor Search (ANNS) and demonstrate that our\napproach achieves a 2.5X ~ 3X higher query-per-second (QPS) throughput compared\nto the widely-used graph-based search algorithm HNSW.\n","authors":["Kejing Lu","Chuan Xiao","Yoshiharu Ishikawa"],"pdf_url":"https://arxiv.org/pdf/2505.20274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25372v1","updated":"2025-10-29T10:42:56Z","published":"2025-10-29T10:42:56Z","title":"Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision\n  Transformers","summary":"  Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has\nproven highly effective as a parameter-efficient fine-tuning technique for\nadapting large models to downstream tasks with limited data. Its parameter\nefficiency makes it particularly suitable for Federated Learning (FL), where\nboth communication and computation budgets are often constrained. However,\nglobal prompt tuning struggles to generalize across heterogeneous clients,\nwhile personalized tuning overfits to local data and lacks generalization. We\npropose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt\nTuning), a unified framework designed to achieve both generalization and\npersonalization in federated prompt tuning of ViTs. Within this framework, we\nintroduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on\nclass-specific prompts maintained alongside a globally shared prompt. For each\ninput, CCMP adaptively combines class-specific prompts using weights derived\nfrom global class prototypes and client class priors. This approach enables\nper-sample prompt personalization without storing client-dependent trainable\nparameters. The prompts are collaboratively optimized via traditional federated\naveraging technique on the same. Comprehensive evaluations on CIFAR-100,\nTinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT\nconsistently surpasses the state-of-the-art baselines under diverse data\nheterogeneity scenarios, establishing a strong foundation for efficient and\ngeneralizable federated prompt tuning of Vision Transformers.\n","authors":["M Yashwanth","Sharannya Ghosh","Aditay Tripathi","Anirban Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2510.25372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22842v2","updated":"2025-10-29T10:18:17Z","published":"2025-10-26T21:21:27Z","title":"FastJAM: a Fast Joint Alignment Model for Images","summary":"  Joint Alignment (JA) of images aims to align a collection of images into a\nunified coordinate frame, such that semantically-similar features appear at\ncorresponding spatial locations. Most existing approaches often require long\ntraining times, large-capacity models, and extensive hyperparameter tuning. We\nintroduce FastJAM, a rapid, graph-based method that drastically reduces the\ncomputational complexity of joint alignment tasks. FastJAM leverages pairwise\nmatches computed by an off-the-shelf image matcher, together with a rapid\nnonparametric clustering, to construct a graph representing intra- and\ninter-image keypoint relations. A graph neural network propagates and\naggregates these correspondences, efficiently predicting per-image homography\nparameters via image-level pooling. Utilizing an inverse-compositional loss,\nthat eliminates the need for a regularization term over the predicted\ntransformations (and thus also obviates the hyperparameter tuning associated\nwith such terms), FastJAM performs image JA quickly and effectively.\nExperimental results on several benchmarks demonstrate that FastJAM achieves\nresults better than existing modern JA methods in terms of alignment quality,\nwhile reducing computation time from hours or minutes to mere seconds. Our code\nis available at our project webpage, https://bgu-cs-vil.github.io/FastJAM/\n","authors":["Omri Hirsch","Ron Shapira Weber","Shira Ifergane","Oren Freifeld"],"pdf_url":"https://arxiv.org/pdf/2510.22842v2.pdf","comment":"Accepted to NeurIPS 2025. Pages 1-10 are the Main Paper. Pages 23-31\n  are Supplemental Material. FastJAM website -\n  https://bgu-cs-vil.github.io/FastJAM/"},{"id":"http://arxiv.org/abs/2510.25347v1","updated":"2025-10-29T10:04:47Z","published":"2025-10-29T10:04:47Z","title":"3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine\n  Learning Framework","summary":"  Coronary artery calcium (CAC) scoring plays a crucial role in the early\ndetection and risk stratification of coronary artery disease (CAD). In this\nstudy, we focus on non-contrast coronary computed tomography angiography (CCTA)\nscans, which are commonly used for early calcification detection in clinical\nsettings. To address the challenge of limited annotated data, we propose a\nradiomics-based pipeline that leverages pseudo-labeling to generate training\nlabels, thereby eliminating the need for expert-defined segmentations.\nAdditionally, we explore the use of pretrained foundation models, specifically\nCT-FM and RadImageNet, to extract image features, which are then used with\ntraditional classifiers. We compare the performance of these deep learning\nfeatures with that of radiomics features. Evaluation is conducted on a clinical\nCCTA dataset comprising 182 patients, where individuals are classified into two\ngroups: zero versus non-zero calcium scores. We further investigate the impact\nof training on non-contrast datasets versus combined contrast and non-contrast\ndatasets, with testing performed only on non contrast scans. Results show that\nradiomics-based models significantly outperform CNN-derived embeddings from\nfoundation models (achieving 84% accuracy and p<0.05), despite the\nunavailability of expert annotations.\n","authors":["Ayman Abaid","Gianpiero Guidone","Sara Alsubai","Foziyah Alquahtani","Talha Iqbal","Ruth Sharif","Hesham Elzomor","Emiliano Bianchini","Naeif Almagal","Michael G. Madden","Faisal Sharif","Ihsan Ullah"],"pdf_url":"https://arxiv.org/pdf/2510.25347v1.pdf","comment":"11 pages, 2 Figures, MICCAI AMAI 2025 workshop, to be published in\n  Volume 16206 of the Lecture Notes in Computer Science series"},{"id":"http://arxiv.org/abs/2510.25345v1","updated":"2025-10-29T10:03:33Z","published":"2025-10-29T10:03:33Z","title":"Informative Sample Selection Model for Skeleton-based Action Recognition\n  with Limited Training Samples","summary":"  Skeleton-based human action recognition aims to classify human skeletal\nsequences, which are spatiotemporal representations of actions, into predefined\ncategories. To reduce the reliance on costly annotations of skeletal sequences\nwhile maintaining competitive recognition accuracy, the task of 3D Action\nRecognition with Limited Training Samples, also known as semi-supervised 3D\nAction Recognition, has been proposed. In addition, active learning, which aims\nto proactively select the most informative unlabeled samples for annotation,\nhas been explored in semi-supervised 3D Action Recognition for training sample\nselection. Specifically, researchers adopt an encoder-decoder framework to\nembed skeleton sequences into a latent space, where clustering information,\ncombined with a margin-based selection strategy using a multi-head mechanism,\nis utilized to identify the most informative sequences in the unlabeled set for\nannotation. However, the most representative skeleton sequences may not\nnecessarily be the most informative for the action recognizer, as the model may\nhave already acquired similar knowledge from previously seen skeleton samples.\nTo solve it, we reformulate Semi-supervised 3D action recognition via active\nlearning from a novel perspective by casting it as a Markov Decision Process\n(MDP). Built upon the MDP framework and its training paradigm, we train an\ninformative sample selection model to intelligently guide the selection of\nskeleton sequences for annotation. To enhance the representational capacity of\nthe factors in the state-action pairs within our method, we project them from\nEuclidean space to hyperbolic space. Furthermore, we introduce a meta tuning\nstrategy to accelerate the deployment of our method in real-world scenarios.\nExtensive experiments on three 3D action recognition benchmarks demonstrate the\neffectiveness of our method.\n","authors":["Zhigang Tu","Zhengbo Zhang","Jia Gong","Junsong Yuan","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2510.25345v1.pdf","comment":"Accepted by IEEE Transactions on Image Processing (TIP), 2025"},{"id":"http://arxiv.org/abs/2503.11094v3","updated":"2025-10-29T09:54:24Z","published":"2025-03-14T05:35:38Z","title":"Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with\n  Multimodal Large Language Model in Open Space","summary":"  Spatial reasoning is a fundamental capability of multimodal large language\nmodels (MLLMs), yet their performance in open aerial environments remains\nunderexplored. In this work, we present Open3D-VQA, a novel benchmark for\nevaluating MLLMs' ability to reason about complex spatial relationships from an\naerial perspective. The benchmark comprises 73k QA pairs spanning 7 general\nspatial reasoning tasks, including multiple-choice, true/false, and\nshort-answer formats, and supports both visual and point cloud modalities. The\nquestions are automatically generated from spatial relations extracted from\nboth real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs\nreveals that: 1) Models are generally better at answering questions about\nrelative spatial relations than absolute distances, 2) 3D LLMs fail to\ndemonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on\nthe simulated dataset can significantly improve the model's spatial reasoning\nperformance in real-world scenarios. We release our benchmark, data generation\npipeline, and evaluation toolkit to support further research:\nhttps://github.com/EmbodiedCity/Open3D-VQA.code.\n","authors":["Weichen Zhang","Zile Zhou","Xin Zeng","Xuchen Liu","Jianjie Fang","Chen Gao","Yong Li","Jinqiang Cui","Xinlei Chen","Xiao-Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11094v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25332v1","updated":"2025-10-29T09:47:38Z","published":"2025-10-29T09:47:38Z","title":"StreamingCoT: A Dataset for Temporal Dynamics and Multimodal\n  Chain-of-Thought Reasoning in Streaming VideoQA","summary":"  The rapid growth of streaming video applications demands multimodal models\nwith enhanced capabilities for temporal dynamics understanding and complex\nreasoning. However, current Video Question Answering (VideoQA) datasets suffer\nfrom two critical limitations: 1) Static annotation mechanisms fail to capture\nthe evolving nature of answers in temporal video streams, and 2) The absence of\nexplicit reasoning process annotations restricts model interpretability and\nlogical deduction capabilities. To address these challenges, We introduce\nStreamingCoT, the first dataset explicitly designed for temporally evolving\nreasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our\nframework first establishes a dynamic hierarchical annotation architecture that\ngenerates per-second dense descriptions and constructs temporally-dependent\nsemantic segments through similarity fusion, paired with question-answer sets\nconstrained by temporal evolution patterns. We further propose an explicit\nreasoning chain generation paradigm that extracts spatiotemporal objects via\nkeyframe semantic alignment, derives object state transition-based reasoning\npaths using large language models, and ensures logical coherence through\nhuman-verified validation. This dataset establishes a foundation for advancing\nresearch in streaming video understanding, complex temporal reasoning, and\nmultimodal inference. Our StreamingCoT and its construction toolkit can be\naccessed at https://github.com/Fleeting-hyh/StreamingCoT.\n","authors":["Yuhang Hu","Zhenyu Yang","Shihan Wang","Shengsheng Qian","Bin Wen","Fan Yang","Tingting Gao","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25327v1","updated":"2025-10-29T09:41:03Z","published":"2025-10-29T09:41:03Z","title":"MMEdge: Accelerating On-device Multimodal Inference via Pipelined\n  Sensing and Encoding","summary":"  Real-time multimodal inference on resource-constrained edge devices is\nessential for applications such as autonomous driving, human-computer\ninteraction, and mobile health. However, prior work often overlooks the tight\ncoupling between sensing dynamics and model execution, as well as the complex\ninter-modality dependencies. In this paper, we propose MMEdge, an new on-device\nmulti-modal inference framework based on pipelined sensing and encoding.\nInstead of waiting for complete sensor inputs, MMEdge decomposes the entire\ninference process into a sequence of fine-grained sensing and encoding units,\nallowing computation to proceed incrementally as data arrive. MMEdge also\nintroduces a lightweight but effective temporal aggregation module that\ncaptures rich temporal dynamics across different pipelined units to maintain\naccuracy performance. Such pipelined design also opens up opportunities for\nfine-grained cross-modal optimization and early decision-making during\ninference. To further enhance system performance under resource variability and\ninput data complexity, MMEdge incorporates an adaptive multimodal configuration\noptimizer that dynamically selects optimal sensing and model configurations for\neach modality under latency constraints, and a cross-modal speculative skipping\nmechanism that bypasses future units of slower modalities when early\npredictions reach sufficient confidence. We evaluate MMEdge using two public\nmultimodal datasets and deploy it on a real-world unmanned aerial vehicle\n(UAV)-based multimodal testbed. The results show that MMEdge significantly\nreduces end-to-end latency while maintaining high task accuracy across various\nsystem and data dynamics.\n","authors":["Runxi Huang","Mingxuan Yu","Mingyu Tsoi","Xiaomin Ouyang"],"pdf_url":"https://arxiv.org/pdf/2510.25327v1.pdf","comment":"Accepted by SenSys 2026"},{"id":"http://arxiv.org/abs/2510.25318v1","updated":"2025-10-29T09:32:42Z","published":"2025-10-29T09:32:42Z","title":"Prototype-Driven Adaptation for Few-Shot Object Detection","summary":"  Few-shot object detection (FSOD) often suffers from base-class bias and\nunstable calibration when only a few novel samples are available. We propose\nPrototype-Driven Alignment (PDA), a lightweight, plug-in metric head for DeFRCN\nthat provides a prototype-based \"second opinion\" complementary to the linear\nclassifier. PDA maintains support-only prototypes in a learnable\nidentity-initialized projection space and optionally applies\nprototype-conditioned RoI alignment to reduce geometric mismatch. During\nfine-tuning, prototypes can be adapted via exponential moving average(EMA)\nupdates on labeled foreground RoIs-without introducing class-specific\nparameters-and are frozen at inference to ensure strict protocol compliance.\nPDA employs a best-of-K matching scheme to capture intra-class multi-modality\nand temperature-scaled fusion to combine metric similarities with detector\nlogits. Experiments on VOC FSOD and GFSOD benchmarks show that PDA consistently\nimproves novel-class performance with minimal impact on base classes and\nnegligible computational overhead.\n","authors":["Yushen Huang","Zhiming Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25318v1.pdf","comment":"7 pages,1 figure,2 tables,Preprint"},{"id":"http://arxiv.org/abs/2510.25314v1","updated":"2025-10-29T09:27:38Z","published":"2025-10-29T09:27:38Z","title":"Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired\n  Monocentric Design","summary":"  Achieving high-fidelity, compact RGBD imaging presents a dual challenge:\nconventional compact optics struggle with RGB sharpness across the entire\ndepth-of-field, while software-only Monocular Depth Estimation (MDE) is an\nill-posed problem reliant on unreliable semantic priors. While deep optics with\nelements like DOEs can encode depth, they introduce trade-offs in fabrication\ncomplexity and chromatic aberrations, compromising simplicity. To address this,\nwe first introduce a novel bio-inspired all-spherical monocentric lens, around\nwhich we build the Bionic Monocentric Imaging (BMI) framework, a holistic\nco-design. This optical design naturally encodes depth into its depth-varying\nPoint Spread Functions (PSFs) without requiring complex diffractive or freeform\nelements. We establish a rigorous physically-based forward model to generate a\nsynthetic dataset by precisely simulating the optical degradation process. This\nsimulation pipeline is co-designed with a dual-head, multi-scale reconstruction\nnetwork that employs a shared encoder to jointly recover a high-fidelity\nAll-in-Focus (AiF) image and a precise depth map from a single coded capture.\nExtensive experiments validate the state-of-the-art performance of the proposed\nframework. In depth estimation, the method attains an Abs Rel of 0.026 and an\nRMSE of 0.130, markedly outperforming leading software-only approaches and\nother deep optics systems. For image restoration, the system achieves an SSIM\nof 0.960 and a perceptual LPIPS score of 0.082, thereby confirming a superior\nbalance between image fidelity and depth accuracy. This study illustrates that\nthe integration of bio-inspired, fully spherical optics with a joint\nreconstruction algorithm constitutes an effective strategy for addressing the\nintrinsic challenges in high-performance compact RGBD imaging. Source code will\nbe publicly available at https://github.com/ZongxiYu-ZJU/BMI.\n","authors":["Zongxi Yu","Xiaolong Qian","Shaohua Gao","Qi Jiang","Yao Gao","Kailun Yang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25314v1.pdf","comment":"The source code will be publicly available at\n  https://github.com/ZongxiYu-ZJU/BMI"},{"id":"http://arxiv.org/abs/2510.25301v1","updated":"2025-10-29T09:14:07Z","published":"2025-10-29T09:14:07Z","title":"GaTector+: A Unified Head-free Framework for Gaze Object and Gaze\n  Following Prediction","summary":"  Gaze object detection and gaze following are fundamental tasks for\ninterpreting human gaze behavior or intent. However, most previous methods\nusually solve these two tasks separately, and their prediction of gaze objects\nand gaze following typically depend on head-related prior knowledge during both\nthe training phase and real-world deployment. This dependency necessitates an\nauxiliary network to extract head location, thus precluding joint optimization\nacross the entire system and constraining the practical applicability. To this\nend, we propose GaTector+, a unified framework for gaze object detection and\ngaze following, which eliminates the dependence on the head-related priors\nduring inference. Specifically, GaTector+ uses an expanded\nspecific-general-specific feature extractor that leverages a shared backbone,\nwhich extracts general features for gaze following and object detection using\nthe shared backbone while using specific blocks before and after the shared\nbackbone to better consider the specificity of each sub-task. To obtain\nhead-related knowledge without prior information, we first embed a head\ndetection branch to predict the head of each person. Then, before regressing\nthe gaze point, a head-based attention mechanism is proposed to fuse the sense\nfeature and gaze feature with the help of head location. Since the\nsuboptimization of the gaze point heatmap leads to the performance bottleneck,\nwe propose an attention supervision mechanism to accelerate the learning of the\ngaze heatmap. Finally, we propose a novel evaluation metric, mean Similarity\nover Candidates (mSoC), for gaze object detection, which is more sensitive to\nvariations between bounding boxes. The experimental results on multiple\nbenchmark datasets demonstrate the effectiveness of our model in both gaze\nobject detection and gaze following tasks.\n","authors":["Yang Jin","Guangyu Guo","Binglu Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.23051v2","updated":"2025-10-29T09:09:07Z","published":"2025-09-27T02:12:09Z","title":"Activation Matching for Explanation Generation","summary":"  In this paper we introduce an activation-matching--based approach to generate\nminimal, faithful explanations for the decision-making of a pretrained\nclassifier on any given image. Given an input image $x$ and a frozen model $f$,\nwe train a lightweight autoencoder to output a binary mask $m$ such that the\nexplanation $e = m \\odot x$ preserves both the model's prediction and the\nintermediate activations of \\(x\\). Our objective combines: (i) multi-layer\nactivation matching with KL divergence to align distributions and cross-entropy\nto retain the top-1 label for both the image and the explanation; (ii) mask\npriors -- L1 area for minimality, a binarization penalty for crisp 0/1 masks,\nand total variation for compactness; and (iii) abductive constraints for\nfaithfulness and necessity. Together, these objectives yield small,\nhuman-interpretable masks that retain classifier behavior while discarding\nirrelevant input regions, providing practical and faithful minimalist\nexplanations for the decision making of the underlying model.\n","authors":["Pirzada Suhail","Aditya Anand","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2509.23051v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19162v2","updated":"2025-10-29T08:59:36Z","published":"2024-06-27T13:29:25Z","title":"Single Image Estimation of Cell Migration Direction by Deep Circular\n  Regression","summary":"  In this paper, we address the problem of estimating the migration direction\nof cells based on a single image. A solution to this problem lays the\nfoundation for a variety of applications that were previously not possible. To\nour knowledge, there is only one related work that employs a classification CNN\nwith four classes (quadrants). However, this approach does not allow for\ndetailed directional resolution. We tackle the single image estimation problem\nusing deep circular regression, with a particular focus on cycle-sensitive\nmethods. On two common datasets, we achieve a mean estimation error of\n$\\sim\\!17^\\circ$, representing a significant improvement over previous work,\nwhich reported estimation error of $30^\\circ$ and $34^\\circ$, respectively.\n","authors":["Lennart Bruns","Lucas Lamparter","Milos Galic","Xiaoyi Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.19162v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24385v2","updated":"2025-10-29T08:55:13Z","published":"2025-10-28T13:01:42Z","title":"When are radiology reports useful for training medical image\n  classifiers?","summary":"  Medical images used to train machine learning models are often accompanied by\nradiology reports containing rich expert annotations. However, relying on these\nreports as inputs for clinical prediction requires the timely manual work of a\ntrained radiologist. This raises a natural question: when can radiology reports\nbe leveraged during training to improve image-only classification? Prior works\nare limited to evaluating pre-trained image representations by fine-tuning them\nto predict diagnostic labels, often extracted from reports, ignoring tasks with\nlabels that are weakly associated with the text. To address this gap, we\nconduct a systematic study of how radiology reports can be used during both\npre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,\n12-month readmission), and under varying training set sizes. Our findings\nreveal that: (1) Leveraging reports during pre-training is beneficial for\ndownstream classification tasks where the label is well-represented in the\ntext; however, pre-training through explicit image-text alignment can be\ndetrimental in settings where it's not; (2) Fine-tuning with reports can lead\nto significant improvements and even have a larger impact than the pre-training\nmethod in certain settings. These results provide actionable insights into when\nand how to leverage privileged text data to train medical image classifiers\nwhile highlighting gaps in current research.\n","authors":["Herman Bergström","Zhongqi Yue","Fredrik D. Johansson"],"pdf_url":"https://arxiv.org/pdf/2510.24385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25279v1","updated":"2025-10-29T08:38:03Z","published":"2025-10-29T08:38:03Z","title":"Diffusion-Driven Progressive Target Manipulation for Source-Free Domain\n  Adaptation","summary":"  Source-free domain adaptation (SFDA) is a challenging task that tackles\ndomain shifts using only a pre-trained source model and unlabeled target data.\nExisting SFDA methods are restricted by the fundamental limitation of\nsource-target domain discrepancy. Non-generation SFDA methods suffer from\nunreliable pseudo-labels in challenging scenarios with large domain\ndiscrepancies, while generation-based SFDA methods are evidently degraded due\nto enlarged domain discrepancies in creating pseudo-source data. To address\nthis limitation, we propose a novel generation-based framework named\nDiffusion-Driven Progressive Target Manipulation (DPTM) that leverages\nunlabeled target data as references to reliably generate and progressively\nrefine a pseudo-target domain for SFDA. Specifically, we divide the target\nsamples into a trust set and a non-trust set based on the reliability of\npseudo-labels to sufficiently and reliably exploit their information. For\nsamples from the non-trust set, we develop a manipulation strategy to\nsemantically transform them into the newly assigned categories, while\nsimultaneously maintaining them in the target distribution via a latent\ndiffusion model. Furthermore, we design a progressive refinement mechanism that\nprogressively reduces the domain discrepancy between the pseudo-target domain\nand the real target domain via iterative refinement. Experimental results\ndemonstrate that DPTM outperforms existing methods by a large margin and\nachieves state-of-the-art performance on four prevailing SFDA benchmark\ndatasets with different scales. Remarkably, DPTM can significantly enhance the\nperformance by up to 18.6% in scenarios with large source-target gaps.\n","authors":["Yuyang Huang","Yabo Chen","Junyu Zhou","Wenrui Dai","Xiaopeng Zhang","Junni Zou","Hongkai Xiong","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2510.25279v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2504.21497v3","updated":"2025-10-29T08:32:29Z","published":"2025-04-30T10:30:46Z","title":"MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric\n  Guidance","summary":"  In this study, we propose a method for video face reenactment that integrates\na 3D face parametric model into a latent diffusion framework, aiming to improve\nshape consistency and motion control in existing video-based face generation\napproaches. Our approach employs the FLAME (Faces Learned with an Articulated\nModel and Expressions) model as the 3D face parametric representation,\nproviding a unified framework for modeling face expressions and head pose. This\nnot only enables precise extraction of motion features from driving videos, but\nalso contributes to the faithful preservation of face shape and geometry.\nSpecifically, we enhance the latent diffusion model with rich 3D expression and\ndetailed pose information by incorporating depth maps, normal maps, and\nrendering maps derived from FLAME sequences. These maps serve as motion\nguidance and are encoded into the denoising UNet through a specifically\ndesigned Geometric Guidance Encoder (GGE). A multi-layer feature fusion module\nwith integrated self-attention mechanisms is used to combine facial appearance\nand motion latent features within the spatial domain. By utilizing the 3D face\nparametric model as motion guidance, our method enables parametric alignment of\nface identity between the reference image and the motion captured from the\ndriving video. Experimental results on benchmark datasets show that our method\nexcels at generating high-quality face animations with precise expression and\nhead pose variation modeling. In addition, it demonstrates strong\ngeneralization performance on out-of-domain images. Code is publicly available\nat https://github.com/weimengting/MagicPortrait.\n","authors":["Mengting Wei","Yante Li","Tuomas Varanka","Yan Jiang","Guoying Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.21497v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25268v1","updated":"2025-10-29T08:27:00Z","published":"2025-10-29T08:27:00Z","title":"SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object\n  with Discrete Human Object Interaction Representation","summary":"  Generating hand grasps with language instructions is a widely studied topic\nthat benefits from embodied AI and VR/AR applications. While transferring into\nhand articulatied object interaction (HAOI), the hand grasps synthesis requires\nnot only object functionality but also long-term manipulation sequence along\nthe object deformation. This paper proposes a novel HAOI sequence generation\nframework SynHLMA, to synthesize hand language manipulation for articulated\nobjects. Given a complete point cloud of an articulated object, we utilize a\ndiscrete HAOI representation to model each hand object interaction frame. Along\nwith the natural language embeddings, the representations are trained by an\nHAOI manipulation language model to align the grasping process with its\nlanguage description in a shared representation space. A joint-aware loss is\nemployed to ensure hand grasps follow the dynamic variations of articulated\nobject joints. In this way, our SynHLMA achieves three typical hand\nmanipulation tasks for articulated objects of HAOI generation, HAOI prediction\nand HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset and\nexperimental results demonstrate the superior hand grasp sequence generation\nperformance comparing with state-of-the-art. We also show a robotics grasp\napplication that enables dexterous grasps execution from imitation learning\nusing the manipulation sequence provided by our SynHLMA. Our codes and datasets\nwill be made publicly available.\n","authors":["Wang zhi","Yuyan Liu","Liu Liu","Li Zhang","Ruixuan Lu","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2510.25268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20322v2","updated":"2025-10-29T08:24:40Z","published":"2025-10-23T08:16:44Z","title":"HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large\n  Language Models","summary":"  Multi-modal large language models (MLLMs) have emerged as a transformative\napproach for aligning visual and textual understanding. They typically require\nextremely high computational resources (e.g., thousands of GPUs) for training\nto achieve cross-modal alignment at multi-granularity levels. We argue that a\nkey source of this inefficiency lies in the vision encoders they widely equip\nwith, e.g., CLIP and SAM, which lack the alignment with language at\nmulti-granularity levels. To address this issue, in this paper, we leverage\nhyperbolic space, which inherently models hierarchical levels and thus provides\na principled framework for bridging the granularity gap between visual and\ntextual modalities at an arbitrary granularity level. Concretely, we propose an\nefficient training paradigm for MLLMs, dubbed as HyperET, which can optimize\nvisual representations to align with their textual counterparts at an arbitrary\ngranularity level through dynamic hyperbolic radius adjustment in hyperbolic\nspace. HyperET employs learnable matrices with M\\\"{o}bius multiplication\noperations, implemented via three effective configurations: diagonal scaling\nmatrices, block-diagonal matrices, and banded matrices, providing a flexible\nyet efficient parametrization strategy. Comprehensive experiments across\nmultiple MLLM benchmarks demonstrate that HyperET consistently improves both\nexisting pre-training and fine-tuning MLLMs clearly with less than 1\\%\nadditional parameters.\n","authors":["Zelin Peng","Zhengqin Xu","Qingyang Liu","Xiaokang Yang","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2510.20322v2.pdf","comment":"Accepted by NeurIPS2025 (Oral)"},{"id":"http://arxiv.org/abs/2510.25263v1","updated":"2025-10-29T08:21:59Z","published":"2025-10-29T08:21:59Z","title":"LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part\n  Segmentation","summary":"  We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based\nframework for open-vocabulary object-part instance segmentation. Given an\nimage, LangHOPS can jointly detect and segment hierarchical object and part\ninstances from open-vocabulary candidate categories. Unlike prior approaches\nthat rely on heuristic or learnable visual grouping, our approach grounds\nobject-part hierarchies in language space. It integrates the MLLM into the\nobject-part parsing pipeline to leverage its rich knowledge and reasoning\ncapabilities, and link multi-granularity concepts within the hierarchies. We\nevaluate LangHOPS across multiple challenging scenarios, including in-domain\nand cross-dataset object-part instance segmentation, and zero-shot semantic\nsegmentation. LangHOPS achieves state-of-the-art results, surpassing previous\nmethods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on\nthe PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K\n(zero-shot). Ablation studies further validate the effectiveness of the\nlanguage-grounded hierarchy and MLLM driven part query refinement strategy. The\ncode will be released here.\n","authors":["Yang Miao","Jan-Nico Zaech","Xi Wang","Fabien Despinoy","Danda Pani Paudel","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2510.25263v1.pdf","comment":"10 pages, 5 figures, 14 tables, Neurips 2025"},{"id":"http://arxiv.org/abs/2503.19311v2","updated":"2025-10-29T08:14:38Z","published":"2025-03-25T03:17:42Z","title":"DGTRSD & DGTRS-CLIP: A Dual-Granularity Remote Sensing Image-Text\n  Dataset and Vision Language Foundation Model for Alignment","summary":"  Vision Language Foundation Models based on CLIP architecture for remote\nsensing primarily rely on short text captions, which often result in incomplete\nsemantic representations. Although longer captions convey richer information,\nexisting models struggle to process them effectively because of limited\ntext-encoding capacity, and there remains a shortage of resources that align\nremote sensing images with both short text and long text captions. To address\nthis gap, we introduce DGTRSD, a dual-granularity remote sensing image-text\ndataset, where each image is paired with both a short text caption and a long\ntext description, providing a solid foundation for dual-granularity semantic\nmodeling. Based on this, we further propose DGTRS-CLIP, a dual-granularity\ncurriculum learning framework that combines short text and long text\nsupervision to achieve dual-granularity semantic alignment. Extensive\nexperiments on four typical zero-shot tasks: long text cross-modal retrieval,\nshort text cross-modal retrieval, image classification, and semantic\nlocalization demonstrate that DGTRS-CLIP consistently outperforms existing\nmethods across all tasks. The code has been open-sourced and is available at\nhttps://github.com/MitsuiChen14/DGTRS.\n","authors":["Weizhi Chen","Yupeng Deng","Jin Wei","Jingbo Chen","Jiansheng Chen","Yuman Feng","Zhihao Xi","Diyou Liu","Kai Li","Yu Meng"],"pdf_url":"https://arxiv.org/pdf/2503.19311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25257v1","updated":"2025-10-29T08:13:17Z","published":"2025-10-29T08:13:17Z","title":"RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision\n  Foundation Models","summary":"  Real-time object detection has achieved substantial progress through\nmeticulously designed architectures and optimization strategies. However, the\npursuit of high-speed inference via lightweight network designs often leads to\ndegraded feature representation, which hinders further performance improvements\nand practical on-device deployment. In this paper, we propose a cost-effective\nand highly adaptable distillation framework that harnesses the rapidly evolving\ncapabilities of Vision Foundation Models (VFMs) to enhance lightweight object\ndetectors. Given the significant architectural and learning objective\ndisparities between VFMs and resource-constrained detectors, achieving stable\nand task-aligned semantic transfer is challenging. To address this, on one\nhand, we introduce a Deep Semantic Injector (DSI) module that facilitates the\nintegration of high-level representations from VFMs into the deep layers of the\ndetector. On the other hand, we devise a Gradient-guided Adaptive Modulation\n(GAM) strategy, which dynamically adjusts the intensity of semantic transfer\nbased on gradient norm ratios. Without increasing deployment and inference\noverhead, our approach painlessly delivers striking and consistent performance\ngains across diverse DETR-based models, underscoring its practical utility for\nreal-time detection. Our new model family, RT-DETRv4, achieves state-of-the-art\nresults on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at corresponding\nspeeds of 273/169/124/78 FPS.\n","authors":["Zijun Liao","Yian Zhao","Xin Shan","Yu Yan","Chang Liu","Lei Lu","Xiangyang Ji","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2510.25257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10258v2","updated":"2025-10-29T07:51:00Z","published":"2025-04-14T14:19:57Z","title":"XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a\n  Novel Benchmark","summary":"  Document Reading Order Recovery is a fundamental task in document image\nunderstanding, playing a pivotal role in enhancing Retrieval-Augmented\nGeneration (RAG) and serving as a critical preprocessing step for large\nlanguage models (LLMs). Existing methods often struggle with complex\nlayouts(e.g., multi-column newspapers), high-overhead interactions between\ncross-modal elements (visual regions and textual semantics), and a lack of\nrobust evaluation benchmarks. We introduce XY-Cut++, an advanced layout\nordering method that integrates pre-mask processing, multi-granularity\nsegmentation, and cross-modal matching to address these challenges. Our method\nsignificantly enhances layout ordering accuracy compared to traditional XY-Cut\ntechniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8\nBLEU overall) while maintaining simplicity and efficiency. It outperforms\nexisting baselines by up to 24\\% and demonstrates consistent accuracy across\nsimple and complex layouts on the newly introduced DocBench-100 dataset. This\nadvancement establishes a reliable foundation for document structure recovery,\nsetting a new standard for layout ordering tasks and facilitating more\neffective RAG and LLM preprocessing.\n","authors":["Shuai Liu","Youmeng Li","Jizeng Wei"],"pdf_url":"https://arxiv.org/pdf/2504.10258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25239v1","updated":"2025-10-29T07:37:19Z","published":"2025-10-29T07:37:19Z","title":"Mapping and Classification of Trees Outside Forests using Deep Learning","summary":"  Trees Outside Forests (TOF) play an important role in agricultural landscapes\nby supporting biodiversity, sequestering carbon, and regulating microclimates.\nYet, most studies have treated TOF as a single class or relied on rigid\nrule-based thresholds, limiting ecological interpretation and adaptability\nacross regions. To address this, we evaluate deep learning for TOF\nclassification using a newly generated dataset and high-resolution aerial\nimagery from four agricultural landscapes in Germany. Specifically, we compare\nconvolutional neural networks (CNNs), vision transformers, and hybrid\nCNN-transformer models across six semantic segmentation architectures (ABCNet,\nLSKNet, FT-UNetFormer, DC-Swin, BANet, and U-Net) to map four categories of\nwoody vegetation: Forest, Patch, Linear, and Tree, derived from previous\nstudies and governmental products. Overall, the models achieved good\nclassification accuracy across the four landscapes, with the FT-UNetFormer\nperforming best (mean Intersection-over-Union 0.74; mean F1 score 0.84),\nunderscoring the importance of spatial context understanding in TOF mapping and\nclassification. Our results show good results for Forest and Linear class and\nreveal challenges particularly in classifying complex structures with high edge\ndensity, notably the Patch and Tree class. Our generalization experiments\nhighlight the need for regionally diverse training data to ensure reliable\nlarge-scale mapping. The dataset and code are openly available at\nhttps://github.com/Moerizzy/TOFMapper\n","authors":["Moritz Lucas","Hamid Ebrahimy","Viacheslav Barkov","Ralf Pecenka","Kai-Uwe Kühnberger","Björn Waske"],"pdf_url":"https://arxiv.org/pdf/2510.25239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25238v1","updated":"2025-10-29T07:37:08Z","published":"2025-10-29T07:37:08Z","title":"VADB: A Large-Scale Video Aesthetic Database with Professional and\n  Multi-Dimensional Annotations","summary":"  Video aesthetic assessment, a vital area in multimedia computing, integrates\ncomputer vision with human cognition. Its progress is limited by the lack of\nstandardized datasets and robust models, as the temporal dynamics of video and\nmultimodal fusion challenges hinder direct application of image-based methods.\nThis study introduces VADB, the largest video aesthetic database with 10,490\ndiverse videos annotated by 37 professionals across multiple aesthetic\ndimensions, including overall and attribute-specific aesthetic scores, rich\nlanguage comments and objective tags. We propose VADB-Net, a dual-modal\npre-training framework with a two-stage training strategy, which outperforms\nexisting video quality assessment models in scoring tasks and supports\ndownstream video aesthetic assessment tasks. The dataset and source code are\navailable at https://github.com/BestiVictory/VADB.\n","authors":["Qianqian Qiao","DanDan Zheng","Yihang Bo","Bao Peng","Heng Huang","Longteng Jiang","Huaye Wang","Jingdong Chen","Jun Zhou","Xin Jin"],"pdf_url":"https://arxiv.org/pdf/2510.25238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25237v1","updated":"2025-10-29T07:35:29Z","published":"2025-10-29T07:35:29Z","title":"DeepShield: Fortifying Deepfake Video Detection with Local and Global\n  Forgery Analysis","summary":"  Recent advances in deep generative models have made it easier to manipulate\nface videos, raising significant concerns about their potential misuse for\nfraud and misinformation. Existing detectors often perform well in in-domain\nscenarios but fail to generalize across diverse manipulation techniques due to\ntheir reliance on forgery-specific artifacts. In this work, we introduce\nDeepShield, a novel deepfake detection framework that balances local\nsensitivity and global generalization to improve robustness across unseen\nforgeries. DeepShield enhances the CLIP-ViT encoder through two key components:\nLocal Patch Guidance (LPG) and Global Forgery Diversification (GFD). LPG\napplies spatiotemporal artifact modeling and patch-wise supervision to capture\nfine-grained inconsistencies often overlooked by global models. GFD introduces\ndomain feature augmentation, leveraging domain-bridging and boundary-expanding\nfeature generation to synthesize diverse forgeries, mitigating overfitting and\nenhancing cross-domain adaptability. Through the integration of novel local and\nglobal analysis for deepfake detection, DeepShield outperforms state-of-the-art\nmethods in cross-dataset and cross-manipulation evaluations, achieving superior\nrobustness against unseen deepfake attacks.\n","authors":["Yinqi Cai","Jichang Li","Zhaolun Li","Weikai Chen","Rushi Lan","Xi Xie","Xiaonan Luo","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2510.25237v1.pdf","comment":"ICCV 2025"},{"id":"http://arxiv.org/abs/2510.25234v1","updated":"2025-10-29T07:29:21Z","published":"2025-10-29T07:29:21Z","title":"Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D\n  Talking Face Animation","summary":"  Expressions are fundamental to conveying human emotions. With the rapid\nadvancement of AI-generated content (AIGC), realistic and expressive 3D facial\nanimation has become increasingly crucial. Despite recent progress in\nspeech-driven lip-sync for talking-face animation, generating emotionally\nexpressive talking faces remains underexplored. A major obstacle is the\nscarcity of real emotional 3D talking-face datasets due to the high cost of\ndata capture. To address this, we model facial animation driven by both speech\nand emotion as a linear additive problem. Leveraging a 3D talking-face dataset\nwith neutral expressions (VOCAset) and a dataset of 3D expression sequences\n(Florence4D), we jointly learn a set of blendshapes driven by speech and\nemotion. We introduce a sparsity constraint loss to encourage disentanglement\nbetween the two types of blendshapes while allowing the model to capture\ninherent secondary cross-domain deformations present in the training data. The\nlearned blendshapes can be further mapped to the expression and jaw pose\nparameters of the FLAME model, enabling the animation of 3D Gaussian avatars.\nQualitative and quantitative experiments demonstrate that our method naturally\ngenerates talking faces with specified expressions while maintaining accurate\nlip synchronization. Perceptual studies further show that our approach achieves\nsuperior emotional expressivity compared to existing methods, without\ncompromising lip-sync quality.\n","authors":["Yuxiang Mao","Zhijie Zhang","Zhiheng Zhang","Jiawei Liu","Chen Zeng","Shihong Xia"],"pdf_url":"https://arxiv.org/pdf/2510.25234v1.pdf","comment":"18 pages, 6 figures, accepted to ICXR 2025 conference"},{"id":"http://arxiv.org/abs/2510.21122v2","updated":"2025-10-29T07:06:34Z","published":"2025-10-24T03:23:34Z","title":"NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection\n  and Bayesian Estimation","summary":"  Reinforcement learning (RL) has shown promise in enhancing the general\nChain-of-Thought (CoT) reasoning capabilities of multimodal large language\nmodels (MLLMs). However, when applied to improve general CoT reasoning,\nexisting RL frameworks often struggle to generalize beyond the training\ndistribution. To address this, we propose NoisyGRPO, a systematic multimodal RL\nframework that introduces controllable noise into visual inputs for enhanced\nexploration and explicitly models the advantage estimation process via a\nBayesian framework. Specifically, NoisyGRPO improves RL training by: (1)\nNoise-Injected Exploration Policy: Perturbing visual inputs with Gaussian noise\nto encourage exploration across a wider range of visual scenarios; and (2)\nBayesian Advantage Estimation: Formulating advantage estimation as a principled\nBayesian inference problem, where the injected noise level serves as a prior\nand the observed trajectory reward as the likelihood. This Bayesian modeling\nfuses both sources of information to compute a robust posterior estimate of\ntrajectory advantage, effectively guiding MLLMs to prefer visually grounded\ntrajectories over noisy ones. Experiments on standard CoT quality, general\ncapability, and hallucination benchmarks demonstrate that NoisyGRPO\nsubstantially improves generalization and robustness, especially in RL settings\nwith small-scale MLLMs such as Qwen2.5-VL 3B. The project page is available at\nhttps://artanic30.github.io/project_pages/NoisyGRPO/.\n","authors":["Longtian Qiu","Shan Ning","Jiaxuan Sun","Xuming He"],"pdf_url":"https://arxiv.org/pdf/2510.21122v2.pdf","comment":"Accepted by Neurips2025, Project page at at\n  https://artanic30.github.io/project_pages/NoisyGRPO/"},{"id":"http://arxiv.org/abs/2510.25229v1","updated":"2025-10-29T07:06:01Z","published":"2025-10-29T07:06:01Z","title":"Balanced conic rectified flow","summary":"  Rectified flow is a generative model that learns smooth transport mappings\nbetween two distributions through an ordinary differential equation (ODE).\nUnlike diffusion-based generative models, which require costly numerical\nintegration of a generative ODE to sample images with state-of-the-art quality,\nrectified flow uses an iterative process called reflow to learn smooth and\nstraight ODE paths. This allows for relatively simple and efficient generation\nof high-quality images. However, rectified flow still faces several challenges.\n1) The reflow process requires a large number of generative pairs to preserve\nthe target distribution, leading to significant computational costs. 2) Since\nthe model is typically trained using only generated image pairs, its\nperformance heavily depends on the 1-rectified flow model, causing it to become\nbiased towards the generated data.\n  In this work, we experimentally expose the limitations of the original\nrectified flow and propose a novel approach that incorporates real images into\nthe training process. By preserving the ODE paths for real images, our method\neffectively reduces reliance on large amounts of generated data. Instead, we\ndemonstrate that the reflow process can be conducted efficiently using a much\nsmaller set of generated and real images. In CIFAR-10, we achieved\nsignificantly better FID scores, not only in one-step generation but also in\nfull-step simulations, while using only of the generative pairs compared to the\noriginal method. Furthermore, our approach induces straighter paths and avoids\nsaturation on generated images during reflow, leading to more robust ODE\nlearning while preserving the distribution of real images.\n","authors":["Kim Shin Seong","Mingi Kwon","Jaeseok Jeong","Youngjung Uh"],"pdf_url":"https://arxiv.org/pdf/2510.25229v1.pdf","comment":"Main paper: 10 pages (total 40 pages including appendix), 5 figures.\n  Accepted at NeurIPS 2025 (Poster). Acknowledgment: Supported by the NRF of\n  Korea (RS-2023-00223062) and IITP grants (RS-2020-II201361, RS-2024-00439762)\n  funded by the Korean government (MSIT)"},{"id":"http://arxiv.org/abs/2510.25227v1","updated":"2025-10-29T07:05:26Z","published":"2025-10-29T07:05:26Z","title":"Aligning What You Separate: Denoised Patch Mixing for Source-Free Domain\n  Adaptation in Medical Image Segmentation","summary":"  Source-Free Domain Adaptation (SFDA) is emerging as a compelling solution for\nmedical image segmentation under privacy constraints, yet current approaches\noften ignore sample difficulty and struggle with noisy supervision under domain\nshift. We present a new SFDA framework that leverages Hard Sample Selection and\nDenoised Patch Mixing to progressively align target distributions. First,\nunlabeled images are partitioned into reliable and unreliable subsets through\nentropy-similarity analysis, allowing adaptation to start from easy samples and\ngradually incorporate harder ones. Next, pseudo-labels are refined via Monte\nCarlo-based denoising masks, which suppress unreliable pixels and stabilize\ntraining. Finally, intra- and inter-domain objectives mix patches between\nsubsets, transferring reliable semantics while mitigating noise. Experiments on\nbenchmark datasets show consistent gains over prior SFDA and UDA methods,\ndelivering more accurate boundary delineation and achieving state-of-the-art\nDice and ASSD scores. Our study highlights the importance of progressive\nadaptation and denoised supervision for robust segmentation under domain shift.\n","authors":["Quang-Khai Bui-Tran","Thanh-Huy Nguyen","Hoang-Thien Nguyen","Ba-Thinh Lam","Nguyen Lan Vi Vu","Phat K. Huynh","Ulas Bagci","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25227v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2508.12015v2","updated":"2025-10-29T07:05:00Z","published":"2025-08-16T11:17:31Z","title":"InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes","summary":"  Reconstructing dynamic driving scenes from dashcam videos has attracted\nincreasing attention due to its significance in autonomous driving and scene\nunderstanding. While recent advances have made impressive progress, most\nmethods still unify all background elements into a single representation,\nhindering both instance-level understanding and flexible scene editing. Some\napproaches attempt to lift 2D segmentation into 3D space, but often rely on\npre-processed instance IDs or complex pipelines to map continuous features to\ndiscrete identities. Moreover, these methods are typically designed for indoor\nscenes with rich viewpoints, making them less applicable to outdoor driving\nscenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian\nSplatting framework tailored for the interactive reconstruction of dynamic\ndriving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D\nfeature learning via contrastive loss and pseudo-supervised objectives. At the\n3D level, we introduce regularization to implicitly encode instance identities\nand enforce consistency through a voxel-based loss. A lightweight static\ncodebook further bridges continuous features and discrete identities without\nrequiring data pre-processing or complex optimization. Quantitative and\nqualitative experiments demonstrate the effectiveness of InstDrive, and to the\nbest of our knowledge, it is the first framework to achieve 3D instance\nsegmentation in dynamic, open-world driving scenes.More visualizations are\navailable at our project page.\n","authors":["Hongyuan Liu","Haochen Yu","Bochao Zou","Jianfei Jiang","Qiankun Liu","Jiansheng Chen","Huimin Ma"],"pdf_url":"https://arxiv.org/pdf/2508.12015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.17732v2","updated":"2025-10-29T07:04:04Z","published":"2025-04-24T16:46:32Z","title":"DPMambaIR: All-in-One Image Restoration via Degradation-Aware Prompt\n  State Space Model","summary":"  All-in-One image restoration aims to address multiple image degradation\nproblems using a single model, offering a more practical and versatile solution\ncompared to designing dedicated models for each degradation type. Existing\napproaches typically rely on Degradation-specific models or coarse-grained\ndegradation prompts to guide image restoration. However, they lack fine-grained\nmodeling of degradation information and face limitations in balancing\nmulti-task conflicts. To overcome these limitations, we propose DPMambaIR, a\nnovel All-in-One image restoration framework that introduces a fine-grained\ndegradation extractor and a Degradation-Aware Prompt State Space Model\n(DP-SSM). The DP-SSM leverages the fine-grained degradation features captured\nby the extractor as dynamic prompts, which are then incorporated into the state\nspace modeling process. This enhances the model's adaptability to diverse\ndegradation types, while a complementary High-Frequency Enhancement Block (HEB)\nrecovers local high-frequency details. Extensive experiments on a mixed dataset\ncontaining seven degradation types show that DPMambaIR achieves the best\nperformance, with 27.69dB and 0.893 in PSNR and SSIM, respectively. These\nresults highlight the potential and superiority of DPMambaIR as a unified\nsolution for All-in-One image restoration.\n","authors":["Zhanwen Liu","Sai Zhou","Yuchao Dai","Yang Wang","Yisheng An","Xiangmo Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.17732v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25221v1","updated":"2025-10-29T06:56:30Z","published":"2025-10-29T06:56:30Z","title":"MSF-Net: Multi-Stage Feature Extraction and Fusion for Robust\n  Photometric Stereo","summary":"  Photometric stereo is a technique aimed at determining surface normals\nthrough the utilization of shading cues derived from images taken under\ndifferent lighting conditions. However, existing learning-based approaches\noften fail to accurately capture features at multiple stages and do not\nadequately promote interaction between these features. Consequently, these\nmodels tend to extract redundant features, especially in areas with intricate\ndetails such as wrinkles and edges. To tackle these issues, we propose MSF-Net,\na novel framework for extracting information at multiple stages, paired with\nselective update strategy, aiming to extract high-quality feature information,\nwhich is critical for accurate normal construction. Additionally, we have\ndeveloped a feature fusion module to improve the interplay among different\nfeatures. Experimental results on the DiLiGenT benchmark show that our proposed\nMSF-Net significantly surpasses previous state-of-the-art methods in the\naccuracy of surface normal estimation.\n","authors":["Shiyu Qin","Zhihao Cai","Kaixuan Wang","Lin Qi","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2510.25221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.16729v2","updated":"2025-10-29T06:53:04Z","published":"2025-10-19T06:45:37Z","title":"Vision-Centric 4D Occupancy Forecasting and Planning via Implicit\n  Residual World Models","summary":"  End-to-end autonomous driving systems increasingly rely on vision-centric\nworld models to understand and predict their environment. However, a common\nineffectiveness in these models is the full reconstruction of future scenes,\nwhich expends significant capacity on redundantly modeling static backgrounds.\nTo address this, we propose IR-WM, an Implicit Residual World Model that\nfocuses on modeling the current state and evolution of the world. IR-WM first\nestablishes a robust bird's-eye-view representation of the current state from\nthe visual observation. It then leverages the BEV features from the previous\ntimestep as a strong temporal prior and predicts only the \"residual\", i.e., the\nchanges conditioned on the ego-vehicle's actions and scene context. To\nalleviate error accumulation over time, we further apply an alignment module to\ncalibrate semantic and dynamic misalignments. Moreover, we investigate\ndifferent forecasting-planning coupling schemes and demonstrate that the\nimplicit future state generated by world models substantially improves planning\naccuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D\noccupancy forecasting and trajectory planning.\n","authors":["Jianbiao Mei","Yu Yang","Xuemeng Yang","Licheng Wen","Jiajun Lv","Botian Shi","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.16729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25210v1","updated":"2025-10-29T06:20:21Z","published":"2025-10-29T06:20:21Z","title":"U-CAN: Unsupervised Point Cloud Denoising with Consistency-Aware\n  Noise2Noise Matching","summary":"  Point clouds captured by scanning sensors are often perturbed by noise, which\nhave a highly negative impact on downstream tasks (e.g. surface reconstruction\nand shape understanding). Previous works mostly focus on training neural\nnetworks with noisy-clean point cloud pairs for learning denoising priors,\nwhich requires extensively manual efforts. In this work, we introduce U-CAN, an\nUnsupervised framework for point cloud denoising with Consistency-Aware\nNoise2Noise matching. Specifically, we leverage a neural network to infer a\nmulti-step denoising path for each point of a shape or scene with a noise to\nnoise matching scheme. We achieve this by a novel loss which enables\nstatistical reasoning on multiple noisy point cloud observations. We further\nintroduce a novel constraint on the denoised geometry consistency for learning\nconsistency-aware denoising patterns. We justify that the proposed constraint\nis a general term which is not limited to 3D domain and can also contribute to\nthe area of 2D image denoising. Our evaluations under the widely used\nbenchmarks in point cloud denoising, upsampling and image denoising show\nsignificant improvement over the state-of-the-art unsupervised methods, where\nU-CAN also produces comparable results with the supervised methods.\n","authors":["Junsheng Zhou","Xingyu Shi","Haichuan Song","Yi Fang","Yu-Shen Liu","Zhizhong Han"],"pdf_url":"https://arxiv.org/pdf/2510.25210v1.pdf","comment":"Accepted by NeurIPS 2025. Project page:\n  https://gloriasze.github.io/U-CAN/"},{"id":"http://arxiv.org/abs/2505.19028v4","updated":"2025-10-29T06:12:40Z","published":"2025-05-25T08:28:03Z","title":"InfoChartQA: A Benchmark for Multimodal Question Answering on\n  Infographic Charts","summary":"  Understanding infographic charts with design-driven visual elements (e.g.,\npictograms, icons) requires both visual recognition and reasoning, posing\nchallenges for multimodal large language models (MLLMs). However, existing\nvisual-question answering benchmarks fall short in evaluating these\ncapabilities of MLLMs due to the lack of paired plain charts and\nvisual-element-based questions. To bridge this gap, we introduce InfoChartQA, a\nbenchmark for evaluating MLLMs on infographic chart understanding. It includes\n5,642 pairs of infographic and plain charts, each sharing the same underlying\ndata but differing in visual presentations. We further design\nvisual-element-based questions to capture their unique visual designs and\ncommunicative intent. Evaluation of 20 MLLMs reveals a substantial performance\ndecline on infographic charts, particularly for visual-element-based questions\nrelated to metaphors. The paired infographic and plain charts enable\nfine-grained error analysis and ablation studies, which highlight new\nopportunities for advancing MLLMs in infographic chart understanding. We\nrelease InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA.\n","authors":["Tianchi Xie","Minzhi Lin","Mengchen Liu","Yilin Ye","Changjian Chen","Shixia Liu"],"pdf_url":"https://arxiv.org/pdf/2505.19028v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25199v1","updated":"2025-10-29T06:09:17Z","published":"2025-10-29T06:09:17Z","title":"AI-Powered Early Detection of Critical Diseases using Image Processing\n  and Audio Analysis","summary":"  Early diagnosis of critical diseases can significantly improve patient\nsurvival and reduce treatment costs. However, existing diagnostic techniques\nare often costly, invasive, and inaccessible in low-resource regions. This\npaper presents a multimodal artificial intelligence (AI) diagnostic framework\nintegrating image analysis, thermal imaging, and audio signal processing for\nearly detection of three major health conditions: skin cancer, vascular blood\nclots, and cardiopulmonary abnormalities. A fine-tuned MobileNetV2\nconvolutional neural network was trained on the ISIC 2019 dataset for skin\nlesion classification, achieving 89.3% accuracy, 91.6% sensitivity, and 88.2%\nspecificity. A support vector machine (SVM) with handcrafted features was\nemployed for thermal clot detection, achieving 86.4% accuracy (AUC = 0.89) on\nsynthetic and clinical data. For cardiopulmonary analysis, lung and heart sound\ndatasets from PhysioNet and Pascal were processed using Mel-Frequency Cepstral\nCoefficients (MFCC) and classified via Random Forest, reaching 87.2% accuracy\nand 85.7% sensitivity. Comparative evaluation against state-of-the-art models\ndemonstrates that the proposed system achieves competitive results while\nremaining lightweight and deployable on low-cost devices. The framework\nprovides a promising step toward scalable, real-time, and accessible AI-based\npre-diagnostic healthcare solutions.\n","authors":["Manisha More","Kavya Bhand","Kaustubh Mukdam","Kavya Sharma","Manas Kawtikwar","Hridayansh Kaware","Prajwal Kavhar"],"pdf_url":"https://arxiv.org/pdf/2510.25199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19638v3","updated":"2025-10-29T06:04:58Z","published":"2025-05-26T07:55:49Z","title":"HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and\n  Semantic Alignment","summary":"  Virtual try-on technology has become increasingly important in the fashion\nand retail industries, enabling the generation of high-fidelity garment images\nthat adapt seamlessly to target human models. While existing methods have\nachieved notable progress, they still face significant challenges in\nmaintaining consistency across different poses. Specifically, geometric\ndistortions lead to a lack of spatial consistency, mismatches in garment\nstructure and texture across poses result in semantic inconsistency, and the\nloss or distortion of fine-grained details diminishes visual fidelity. To\naddress these challenges, we propose HF-VTON, a novel framework that ensures\nhigh-fidelity virtual try-on performance across diverse poses. HF-VTON consists\nof three key modules: (1) the Appearance-Preserving Warp Alignment Module\n(APWAM), which aligns garments to human poses, addressing geometric\ndeformations and ensuring spatial consistency; (2) the Semantic Representation\nand Comprehension Module (SRCM), which captures fine-grained garment attributes\nand multi-pose data to enhance semantic representation, maintaining structural,\ntextural, and pattern consistency; and (3) the Multimodal Prior-Guided\nAppearance Generation Module (MPAGM), which integrates multimodal features and\nprior knowledge from pre-trained models to optimize appearance generation,\nensuring both semantic and geometric consistency. Additionally, to overcome\ndata limitations in existing benchmarks, we introduce the SAMP-VTONS dataset,\nfeaturing multi-pose pairs and rich textual annotations for a more\ncomprehensive evaluation. Experimental results demonstrate that HF-VTON\noutperforms state-of-the-art methods on both VITON-HD and SAMP-VTONS, excelling\nin visual fidelity, semantic consistency, and detail preservation.\n","authors":["Ming Meng","Qi Dong","Jiajie Li","Zhe Zhu","Xingyu Wang","Zhaoxin Fan","Wei Zhao","Wenjun Wu"],"pdf_url":"https://arxiv.org/pdf/2505.19638v3.pdf","comment":"After the publication of the paper, we discovered some significant\n  errors/omissions that need to be corrected and improved"},{"id":"http://arxiv.org/abs/2508.08549v3","updated":"2025-10-29T05:58:05Z","published":"2025-08-12T01:33:30Z","title":"Diverse Teaching and Label Propagation for Generic Semi-Supervised\n  Medical Image Segmentation","summary":"  Both limited annotation and domain shift are significant challenges\nfrequently encountered in medical image segmentation, leading to derivative\nscenarios like semi-supervised medical (SSMIS), semi-supervised medical domain\ngeneralization (Semi-MDG) and unsupervised medical domain adaptation (UMDA).\nConventional methods are generally tailored to specific tasks in isolation, the\nerror accumulation hinders the effective utilization of unlabeled data and\nlimits further improvements, resulting in suboptimal performance when these\nissues occur. In this paper, we aim to develop a generic framework that masters\nall three tasks. We found that the key to solving the problem lies in how to\ngenerate reliable pseudo labels for the unlabeled data in the presence of\ndomain shift with labeled data and increasing the diversity of the model. To\ntackle this issue, we employ a Diverse Teaching and Label Propagation Network\n(DTLP-Net) to boosting the Generic Semi-Supervised Medical Image Segmentation.\nOur DTLP-Net involves a single student model and two diverse teacher models,\nwhich can generate reliable pseudo-labels for the student model. The first\nteacher model decouple the training process with labeled and unlabeled data,\nThe second teacher is momentum-updated periodically, thus generating reliable\nyet divers pseudo-labels. To fully utilize the information within the data, we\nadopt inter-sample and intra-sample data augmentation to learn the global and\nlocal knowledge. In addition, to further capture the voxel-level correlations,\nwe propose label propagation to enhance the model robust. We evaluate our\nproposed framework on five benchmark datasets for SSMIS, UMDA, and Semi-MDG\ntasks. The results showcase notable improvements compared to state-of-the-art\nmethods across all five settings, indicating the potential of our framework to\ntackle more challenging SSL scenarios.\n","authors":["Wei Li","Pengcheng Zhou","Linye Ma","Wenyi Zhao","Huihua Yang","Yuchen Guo"],"pdf_url":"https://arxiv.org/pdf/2508.08549v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2510.25184v1","updated":"2025-10-29T05:30:16Z","published":"2025-10-29T05:30:16Z","title":"Mask-Robust Face Verification for Online Learning via YOLOv5 and\n  Residual Networks","summary":"  In the contemporary landscape, the fusion of information technology and the\nrapid advancement of artificial intelligence have ushered school education into\na transformative phase characterized by digitization and heightened\nintelligence. Concurrently, the global paradigm shift caused by the Covid-19\npandemic has catalyzed the evolution of e-learning, accentuating its\nsignificance. Amidst these developments, one pivotal facet of the online\neducation paradigm that warrants attention is the authentication of identities\nwithin the digital learning sphere. Within this context, our study delves into\na solution for online learning authentication, utilizing an enhanced\nconvolutional neural network architecture, specifically the residual network\nmodel. By harnessing the power of deep learning, this technological approach\naims to galvanize the ongoing progress of online education, while concurrently\nbolstering its security and stability. Such fortification is imperative in\nenabling online education to seamlessly align with the swift evolution of the\neducational landscape. This paper's focal proposition involves the deployment\nof the YOLOv5 network, meticulously trained on our proprietary dataset. This\nnetwork is tasked with identifying individuals' faces culled from images\ncaptured by students' open online cameras. The resultant facial information is\nthen channeled into the residual network to extract intricate features at a\ndeeper level. Subsequently, a comparative analysis of Euclidean distances\nagainst students' face databases is performed, effectively ascertaining the\nidentity of each student.\n","authors":["Zhifeng Wang","Minghui Wang","Chunyan Zeng","Jialong Yao","Yang Yang","Hongmin Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25184v1.pdf","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2510.25175v1","updated":"2025-10-29T05:19:38Z","published":"2025-10-29T05:19:38Z","title":"Test-Time Adaptive Object Detection with Foundation Model","summary":"  In recent years, test-time adaptive object detection has attracted increasing\nattention due to its unique advantages in online domain adaptation, which\naligns more closely with real-world application scenarios. However, existing\napproaches heavily rely on source-derived statistical characteristics while\nmaking the strong assumption that the source and target domains share an\nidentical category space. In this paper, we propose the first foundation\nmodel-powered test-time adaptive object detection method that eliminates the\nneed for source data entirely and overcomes traditional closed-set limitations.\nSpecifically, we design a Multi-modal Prompt-based Mean-Teacher framework for\nvision-language detector-driven test-time adaptation, which incorporates text\nand visual prompt tuning to adapt both language and vision representation\nspaces on the test data in a parameter-efficient manner. Correspondingly, we\npropose a Test-time Warm-start strategy tailored for the visual prompts to\neffectively preserve the representation capability of the vision branch.\nFurthermore, to guarantee high-quality pseudo-labels in every test batch, we\nmaintain an Instance Dynamic Memory (IDM) module that stores high-quality\npseudo-labels from previous test samples, and propose two novel\nstrategies-Memory Enhancement and Memory Hallucination-to leverage IDM's\nhigh-quality instances for enhancing original predictions and hallucinating\nimages without available pseudo-labels, respectively. Extensive experiments on\ncross-corruption and cross-dataset benchmarks demonstrate that our method\nconsistently outperforms previous state-of-the-art methods, and can adapt to\narbitrary cross-domain and cross-category target data. Code is available at\nhttps://github.com/gaoyingjay/ttaod_foundation.\n","authors":["Yingjie Gao","Yanan Zhang","Zhi Cai","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2510.25175v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25174v1","updated":"2025-10-29T05:17:13Z","published":"2025-10-29T05:17:13Z","title":"Classifier Enhancement Using Extended Context and Domain Experts for\n  Semantic Segmentation","summary":"  Prevalent semantic segmentation methods generally adopt a vanilla classifier\nto categorize each pixel into specific classes.\n  Although such a classifier learns global information from the training data,\nthis information is represented by a set of fixed parameters (weights and\nbiases).\n  However, each image has a different class distribution, which prevents the\nclassifier from addressing the unique characteristics of individual images.\n  At the dataset level, class imbalance leads to segmentation results being\nbiased towards majority classes, limiting the model's effectiveness in\nidentifying and segmenting minority class regions.\n  In this paper, we propose an Extended Context-Aware Classifier (ECAC) that\ndynamically adjusts the classifier using global (dataset-level) and local\n(image-level) contextual information.\n  Specifically, we leverage a memory bank to learn dataset-level contextual\ninformation of each class, incorporating the class-specific contextual\ninformation from the current image to improve the classifier for precise pixel\nlabeling.\n  Additionally, a teacher-student network paradigm is adopted, where the domain\nexpert (teacher network) dynamically adjusts contextual information with ground\ntruth and transfers knowledge to the student network.\n  Comprehensive experiments illustrate that the proposed ECAC can achieve\nstate-of-the-art performance across several datasets, including ADE20K,\nCOCO-Stuff10K, and Pascal-Context.\n","authors":["Huadong Tang","Youpeng Zhao","Min Xu","Jun Wang","Qiang Wu"],"pdf_url":"https://arxiv.org/pdf/2510.25174v1.pdf","comment":"Accepted at IEEE TRANSACTIONS ON MULTIMEDIA (TMM)"},{"id":"http://arxiv.org/abs/2510.25173v1","updated":"2025-10-29T05:13:09Z","published":"2025-10-29T05:13:09Z","title":"$D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene\n  Reconstruction","summary":"  Recently, Gaussian Splatting (GS) has shown great potential for urban scene\nreconstruction in the field of autonomous driving. However, current urban scene\nreconstruction methods often depend on multimodal sensors as inputs,\n\\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR\npoint clouds can largely mitigate ill-posedness in reconstruction, acquiring\nsuch accurate LiDAR data is still challenging in practice: i) precise\nspatiotemporal calibration between LiDAR and other sensors is required, as they\nmay not capture data simultaneously; ii) reprojection errors arise from spatial\nmisalignment when LiDAR and cameras are mounted at different locations. To\navoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, a\nLiDAR-free urban scene reconstruction framework. In this work, we obtain\ngeometry priors that are as effective as LiDAR while being denser and more\naccurate. $\\textbf{First}$, we initialize a dense point cloud by\nback-projecting multi-view metric depth predictions. This point cloud is then\noptimized by a Progressive Pruning strategy to improve the global consistency.\n$\\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense\nmetric depth via a Depth Enhancer. Specifically, we leverage diffusion priors\nfrom a depth foundation model to enhance the depth maps rendered by Gaussians.\nIn turn, the enhanced depths provide stronger geometric constraints during\nGaussian training. $\\textbf{Finally}$, we improve the accuracy of ground\ngeometry by constraining the shape and normal attributes of Gaussians within\nroad regions. Extensive experiments on the Waymo dataset demonstrate that our\nmethod consistently outperforms state-of-the-art methods, producing more\naccurate geometry even when compared with those using ground-truth LiDAR data.\n","authors":["Kejing Xia","Jidong Jia","Ke Jin","Yucai Bai","Li Sun","Dacheng Tao","Youjian Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.25173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25166v1","updated":"2025-10-29T04:57:49Z","published":"2025-10-29T04:57:49Z","title":"A Study on Inference Latency for Vision Transformers on Mobile Devices","summary":"  Given the significant advances in machine learning techniques on mobile\ndevices, particularly in the domain of computer vision, in this work we\nquantitatively study the performance characteristics of 190 real-world vision\ntransformers (ViTs) on mobile devices. Through a comparison with 102 real-world\nconvolutional neural networks (CNNs), we provide insights into the factors that\ninfluence the latency of ViT architectures on mobile devices. Based on these\ninsights, we develop a dataset including measured latencies of 1000 synthetic\nViTs with representative building blocks and state-of-the-art architectures\nfrom two machine learning frameworks and six mobile platforms. Using this\ndataset, we show that inference latency of new ViTs can be predicted with\nsufficient accuracy for real-world applications.\n","authors":["Zhuojin Li","Marco Paolieri","Leana Golubchik"],"pdf_url":"https://arxiv.org/pdf/2510.25166v1.pdf","comment":"To appear in Springer LNICST, volume 663, Proceedings of VALUETOOLS\n  2024"},{"id":"http://arxiv.org/abs/2510.25164v1","updated":"2025-10-29T04:49:20Z","published":"2025-10-29T04:49:20Z","title":"Transformers in Medicine: Improving Vision-Language Alignment for\n  Medical Image Captioning","summary":"  We present a transformer-based multimodal framework for generating clinically\nrelevant captions for MRI scans. Our system combines a DEiT-Small vision\ntransformer as an image encoder, MediCareBERT for caption embedding, and a\ncustom LSTM-based decoder. The architecture is designed to semantically align\nimage and textual embeddings, using hybrid cosine-MSE loss and contrastive\ninference via vector similarity. We benchmark our method on the MultiCaRe\ndataset, comparing performance on filtered brain-only MRIs versus general MRI\nimages against state-of-the-art medical image captioning methods including\nBLIP, R2GenGPT, and recent transformer-based approaches. Results show that\nfocusing on domain-specific data improves caption accuracy and semantic\nalignment. Our work proposes a scalable, interpretable solution for automated\nmedical image reporting.\n","authors":["Yogesh Thakku Suresh","Vishwajeet Shivaji Hogale","Luca-Alexandru Zamfira","Anandavardhana Hegde"],"pdf_url":"https://arxiv.org/pdf/2510.25164v1.pdf","comment":"This work is to appear in the Proceedings of MICAD 2025, the 6th\n  International Conference on Medical Imaging and Computer-Aided Diagnosis"},{"id":"http://arxiv.org/abs/2510.25163v1","updated":"2025-10-29T04:49:15Z","published":"2025-10-29T04:49:15Z","title":"Target-Guided Bayesian Flow Networks for Quantitatively Constrained CAD\n  Generation","summary":"  Deep generative models, such as diffusion models, have shown promising\nprogress in image generation and audio generation via simplified continuity\nassumptions. However, the development of generative modeling techniques for\ngenerating multi-modal data, such as parametric CAD sequences, still lags\nbehind due to the challenges in addressing long-range constraints and parameter\nsensitivity. In this work, we propose a novel framework for quantitatively\nconstrained CAD generation, termed Target-Guided Bayesian Flow Network (TGBFN).\nFor the first time, TGBFN handles the multi-modality of CAD sequences (i.e.,\ndiscrete commands and continuous parameters) in a unified continuous and\ndifferentiable parameter space rather than in the discrete data space. In\naddition, TGBFN penetrates the parameter update kernel and introduces a guided\nBayesian flow to control the CAD properties. To evaluate TGBFN, we construct a\nnew dataset for quantitatively constrained CAD generation. Extensive\ncomparisons across single-condition and multi-condition constrained generation\ntasks demonstrate that TGBFN achieves state-of-the-art performance in\ngenerating high-fidelity, condition-aware CAD sequences. The code is available\nat https://github.com/scu-zwh/TGBFN.\n","authors":["Wenhao Zheng","Chenwei Sun","Wenbo Zhang","Jiancheng Lv","Xianggen Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06497v3","updated":"2025-10-29T04:35:35Z","published":"2025-03-09T07:53:19Z","title":"Evaluation of Safety Cognition Capability in Vision-Language Models for\n  Autonomous Driving","summary":"  Ensuring the safety of vision-language models (VLMs) in autonomous driving\nsystems is of paramount importance, yet existing research has largely focused\non conventional benchmarks rather than safety-critical evaluation. In this\nwork, we present SCD-Bench (Safety Cognition Driving Benchmark) a novel\nframework specifically designed to assess the safety cognition capabilities of\nVLMs within interactive driving scenarios. To address the scalability challenge\nof data annotation, we introduce ADA (Autonomous Driving Annotation), a\nsemi-automated labeling system, further refined through expert review by\nprofessionals with domain-specific knowledge in autonomous driving. To\nfacilitate scalable and consistent evaluation, we also propose an automated\nassessment pipeline leveraging large language models, which demonstrates over\n98% agreement with human expert judgments. In addressing the broader challenge\nof aligning VLMs with safety cognition in driving environments, we construct\nSCD-Training, the first large-scale dataset tailored for this task, comprising\n324.35K high-quality samples. Through extensive experiments, we show that\nmodels trained on SCD-Training exhibit marked improvements not only on\nSCD-Bench, but also on general and domain-specific benchmarks, offering a new\nperspective on enhancing safety-aware interactions in vision-language systems\nfor autonomous driving.\n","authors":["Enming Zhang","Peizhe Gong","Xingyuan Dai","Min Huang","Yisheng Lv","Qinghai Miao"],"pdf_url":"https://arxiv.org/pdf/2503.06497v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.05746v2","updated":"2025-10-29T04:32:16Z","published":"2025-09-06T15:35:37Z","title":"Depth-Aware Super-Resolution via Distance-Adaptive Variational\n  Formulation","summary":"  Single image super-resolution traditionally assumes spatially-invariant\ndegradation models, yet real-world imaging systems exhibit complex\ndistance-dependent effects including atmospheric scattering, depth-of-field\nvariations, and perspective distortions. This fundamental limitation\nnecessitates spatially-adaptive reconstruction strategies that explicitly\nincorporate geometric scene understanding for optimal performance. We propose a\nrigorous variational framework that characterizes super-resolution as a\nspatially-varying inverse problem, formulating the degradation operator as a\npseudodifferential operator with distance-dependent spectral characteristics\nthat enable theoretical analysis of reconstruction limits across depth ranges.\nOur neural architecture implements discrete gradient flow dynamics through\ncascaded residual blocks with depth-conditional convolution kernels, ensuring\nconvergence to stationary points of the theoretical energy functional while\nincorporating learned distance-adaptive regularization terms that dynamically\nadjust smoothness constraints based on local geometric structure. Spectral\nconstraints derived from atmospheric scattering theory prevent bandwidth\nviolations and noise amplification in far-field regions, while adaptive kernel\ngeneration networks learn continuous mappings from depth to reconstruction\nfilters. Comprehensive evaluation across five benchmark datasets demonstrates\nstate-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM\nat 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by\n0.44dB and 0.36dB respectively. This work establishes the first\ntheoretically-grounded distance-adaptive super-resolution framework and\ndemonstrates significant improvements on depth-variant scenarios while\nmaintaining competitive performance across traditional benchmarks.\n","authors":["Tianhao Guo","Bingjie Lu","Feng Wang","Zhengyang Lu"],"pdf_url":"https://arxiv.org/pdf/2509.05746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25157v1","updated":"2025-10-29T04:19:52Z","published":"2025-10-29T04:19:52Z","title":"Towards Real-Time Inference of Thin Liquid Film Thickness Profiles from\n  Interference Patterns Using Vision Transformers","summary":"  Thin film interferometry is a powerful technique for non-invasively measuring\nliquid film thickness with applications in ophthalmology, but its clinical\ntranslation is hindered by the challenges in reconstructing thickness profiles\nfrom interference patterns - an ill-posed inverse problem complicated by phase\nperiodicity, imaging noise and ambient artifacts. Traditional reconstruction\nmethods are either computationally intensive, sensitive to noise, or require\nmanual expert analysis, which is impractical for real-time diagnostics. To\naddress this challenge, here we present a vision transformer-based approach for\nreal-time inference of thin liquid film thickness profiles directly from\nisolated interferograms. Trained on a hybrid dataset combining\nphysiologically-relevant synthetic and experimental tear film data, our model\nleverages long-range spatial correlations to resolve phase ambiguities and\nreconstruct temporally coherent thickness profiles in a single forward pass\nfrom dynamic interferograms acquired in vivo and ex vivo. The network\ndemonstrates state-of-the-art performance on noisy, rapidly-evolving films with\nmotion artifacts, overcoming limitations of conventional phase-unwrapping and\niterative fitting methods. Our data-driven approach enables automated,\nconsistent thickness reconstruction at real-time speeds on consumer hardware,\nopening new possibilities for continuous monitoring of pre-lens ocular tear\nfilms and non-invasive diagnosis of conditions such as the dry eye disease.\n","authors":["Gautam A. Viruthagiri","Arnuv Tandon","Gerald G. Fuller","Vinny Chandran Suja"],"pdf_url":"https://arxiv.org/pdf/2510.25157v1.pdf","comment":"6 pages, 2 figures, will be updated"},{"id":"http://arxiv.org/abs/2510.23807v2","updated":"2025-10-29T04:15:53Z","published":"2025-10-27T19:44:52Z","title":"Why Foundation Models in Pathology Are Failing","summary":"  In non-medical domains, foundation models (FMs) have revolutionized computer\nvision and language processing through large-scale self-supervised and\nmultimodal learning. Consequently, their rapid adoption in computational\npathology was expected to deliver comparable breakthroughs in cancer diagnosis,\nprognostication, and multimodal retrieval. However, recent systematic\nevaluations reveal fundamental weaknesses: low diagnostic accuracy, poor\nrobustness, geometric instability, heavy computational demands, and concerning\nsafety vulnerabilities. This short paper examines these shortcomings and argues\nthat they stem from deeper conceptual mismatches between the assumptions\nunderlying generic foundation modeling in mainstream AI and the intrinsic\ncomplexity of human tissue. Seven interrelated causes are identified:\nbiological complexity, ineffective self-supervision, overgeneralization,\nexcessive architectural complexity, lack of domain-specific innovation,\ninsufficient data, and a fundamental design flaw related to tissue patch size.\nThese findings suggest that current pathology foundation models remain\nconceptually misaligned with the nature of tissue morphology and call for a\nfundamental rethinking of the paradigm itself.\n","authors":["Hamid R. Tizhoosh"],"pdf_url":"https://arxiv.org/pdf/2510.23807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22946v2","updated":"2025-10-29T04:07:45Z","published":"2025-10-27T02:59:57Z","title":"LightBagel: A Light-weighted, Double Fusion Framework for Unified\n  Multimodal Understanding and Generation","summary":"  Unified multimodal models have recently shown remarkable gains in both\ncapability and versatility, yet most leading systems are still trained from\nscratch and require substantial computational resources. In this paper, we show\nthat competitive performance can be obtained far more efficiently by\nstrategically fusing publicly available models specialized for either\ngeneration or understanding. Our key design is to retain the original blocks\nwhile additionally interleaving multimodal self-attention blocks throughout the\nnetworks. This double fusion mechanism (1) effectively enables rich multi-modal\nfusion while largely preserving the original strengths of the base models, and\n(2) catalyzes synergistic fusion of high-level semantic representations from\nthe understanding encoder with low-level spatial signals from the generation\nencoder. By training with only ~ 35B tokens, this approach achieves strong\nresults across multiple benchmarks: 0.91 on GenEval for compositional\ntext-to-image generation, 82.16 on DPG-Bench for complex text-to-image\ngeneration, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By\nfully releasing the entire suite of code, model weights, and datasets, we hope\nto support future research on unified multimodal modeling.\n","authors":["Zeyu Wang","Zilong Chen","Chenhui Gou","Feng Li","Chaorui Deng","Deyao Zhu","Kunchang Li","Weihao Yu","Haoqin Tu","Haoqi Fan","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2510.22946v2.pdf","comment":"Withdrawn because the submission was premature and not agreed by all\n  parties in collaboration"},{"id":"http://arxiv.org/abs/2505.03318v3","updated":"2025-10-29T04:02:02Z","published":"2025-05-06T08:46:41Z","title":"Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning","summary":"  Recent advances in multimodal Reward Models (RMs) have shown significant\npromise in delivering reward signals to align vision models with human\npreferences. However, current RMs are generally restricted to providing direct\nresponses or engaging in shallow reasoning processes with limited depth, often\nleading to inaccurate reward signals. We posit that incorporating explicit long\nchains of thought (CoT) into the reward reasoning process can significantly\nstrengthen their reliability and robustness. Furthermore, we believe that once\nRMs internalize CoT reasoning, their direct response accuracy can also be\nimproved through implicit reasoning capabilities. To this end, this paper\nproposes UnifiedReward-Think, the first unified multimodal CoT-based reward\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\nvisual understanding and generation reward tasks. Specifically, we adopt an\nexploration-driven reinforcement fine-tuning approach to elicit and incentivize\nthe model's latent complex reasoning ability: (1) We first use a small amount\nof image generation preference data to distill the reasoning process of GPT-4o,\nwhich is then used for the model's cold start to learn the format and structure\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\nand generalization capabilities, we prepare large-scale unified multimodal\npreference data to elicit the model's reasoning process across various vision\ntasks. During this phase, correct reasoning outputs are retained for rejection\nsampling to refine the model (3) while incorrect predicted samples are finally\nused for Group Relative Policy Optimization (GRPO) based reinforcement\nfine-tuning, enabling the model to explore diverse reasoning paths and optimize\nfor correct and robust solutions. Extensive experiments across various vision\nreward tasks demonstrate the superiority of our model.\n","authors":["Yibin Wang","Zhimin Li","Yuhang Zang","Chunyu Wang","Qinglin Lu","Cheng Jin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03318v3.pdf","comment":"[NeurIPS2025] Project Page:\n  https://codegoat24.github.io/UnifiedReward/think"},{"id":"http://arxiv.org/abs/2510.25146v1","updated":"2025-10-29T03:56:41Z","published":"2025-10-29T03:56:41Z","title":"EA3D: Online Open-World 3D Object Extraction from Streaming Videos","summary":"  Current 3D scene understanding methods are limited by offline-collected\nmulti-view data or pre-constructed 3D geometry. In this paper, we present\nExtractAnything3D (EA3D), a unified online framework for open-world 3D object\nextraction that enables simultaneous geometric reconstruction and holistic\nscene understanding. Given a streaming video, EA3D dynamically interprets each\nframe using vision-language and 2D vision foundation encoders to extract\nobject-level knowledge. This knowledge is integrated and embedded into a\nGaussian feature map via a feed-forward online update strategy. We then\niteratively estimate visual odometry from historical frames and incrementally\nupdate online Gaussian features with new observations. A recurrent joint\noptimization module directs the model's attention to regions of interest,\nsimultaneously enhancing both geometric reconstruction and semantic\nunderstanding. Extensive experiments across diverse benchmarks and tasks,\nincluding photo-realistic rendering, semantic and instance segmentation, 3D\nbounding box and semantic occupancy estimation, and 3D mesh generation,\ndemonstrate the effectiveness of EA3D. Our method establishes a unified and\nefficient framework for joint online 3D reconstruction and holistic scene\nunderstanding, enabling a broad range of downstream tasks.\n","authors":["Xiaoyu Zhou","Jingqi Wang","Yuang Jia","Yongtao Wang","Deqing Sun","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2510.25146v1.pdf","comment":"The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems(NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.25141v1","updated":"2025-10-29T03:45:03Z","published":"2025-10-29T03:45:03Z","title":"Revisiting Reconstruction-based AI-generated Image Detection: A\n  Geometric Perspective","summary":"  The rise of generative Artificial Intelligence (AI) has made detecting\nAI-generated images a critical challenge for ensuring authenticity. Existing\nreconstruction-based methods lack theoretical foundations and on empirical\nheuristics, limiting interpretability and reliability. In this paper, we\nintroduce the Jacobian-Spectral Lower Bound for reconstruction error from a\ngeometric perspective, showing that real images off the reconstruction manifold\nexhibit a non-trivial error lower bound, while generated images on the manifold\nhave near-zero error. Furthermore, we reveal the limitations of existing\nmethods that rely on static reconstruction error from a single pass. These\nmethods often fail when some real images exhibit lower error than generated\nones. This counterintuitive behavior reduces detection accuracy and requires\ndata-specific threshold tuning, limiting their applicability in real-world\nscenarios. To address these challenges, we propose ReGap, a training-free\nmethod that computes dynamic reconstruction error by leveraging structured\nediting operations to introduce controlled perturbations. This enables\nmeasuring error changes before and after editing, improving detection accuracy\nby enhancing error separation. Experimental results show that our method\noutperforms existing baselines, exhibits robustness to common post-processing\noperations and generalizes effectively across diverse conditions.\n","authors":["Wan Jiang","Jing Yan","Ruixuan Zhang","Xiaojing Chen","Changtao Miao","Zhe Li","Chenhao Lin","Yunfeng Diao","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2510.25141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25140v1","updated":"2025-10-29T03:40:40Z","published":"2025-10-29T03:40:40Z","title":"DINO-YOLO: Self-Supervised Pre-training for Data-Efficient Object\n  Detection in Civil Engineering Applications","summary":"  Object detection in civil engineering applications is constrained by limited\nannotated data in specialized domains. We introduce DINO-YOLO, a hybrid\narchitecture combining YOLOv12 with DINOv3 self-supervised vision transformers\nfor data-efficient detection. DINOv3 features are strategically integrated at\ntwo locations: input preprocessing (P0) and mid-backbone enhancement (P3).\nExperimental validation demonstrates substantial improvements: Tunnel Segment\nCrack detection (648 images) achieves 12.4% improvement, Construction PPE (1K\nimages) gains 13.7%, and KITTI (7K images) shows 88.6% improvement, while\nmaintaining real-time inference (30-47 FPS). Systematic ablation across five\nYOLO scales and nine DINOv3 variants reveals that Medium-scale architectures\nachieve optimal performance with DualP0P3 integration (55.77% mAP@0.5), while\nSmall-scale requires Triple Integration (53.63%). The 2-4x inference overhead\n(21-33ms versus 8-16ms baseline) remains acceptable for field deployment on\nNVIDIA RTX 5090. DINO-YOLO establishes state-of-the-art performance for civil\nengineering datasets (<10K images) while preserving computational efficiency,\nproviding practical solutions for construction safety monitoring and\ninfrastructure inspection in data-constrained environments.\n","authors":["Malaisree P","Youwai S","Kitkobsin T","Janrungautai S","Amorndechaphon D","Rojanavasu P"],"pdf_url":"https://arxiv.org/pdf/2510.25140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06677v2","updated":"2025-10-29T03:38:36Z","published":"2025-06-07T06:15:49Z","title":"RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic\n  Manipulation Evaluation","summary":"  Recent advances in vision-language models (VLMs) have enabled\ninstruction-conditioned robotic systems with improved generalization. However,\nmost existing work focuses on reactive System 1 policies, underutilizing VLMs'\nstrengths in semantic reasoning and long-horizon planning. These System 2\ncapabilities-characterized by deliberative, goal-directed thinking-remain under\nexplored due to the limited temporal scale and structural complexity of current\nbenchmarks. To address this gap, we introduce RoboCerebra, a benchmark for\nevaluating high-level reasoning in long-horizon robotic manipulation.\nRoboCerebra includes: (1) a large-scale simulation dataset with extended task\nhorizons and diverse subtask sequences in household environments; (2) a\nhierarchical framework combining a high-level VLM planner with a low-level\nvision-language-action (VLA) controller; and (3) an evaluation protocol\ntargeting planning, reflection, and memory through structured System 1-System 2\ninteraction. The dataset is constructed via a top-down pipeline, where GPT\ngenerates task instructions and decomposes them into subtask sequences. Human\noperators execute the subtasks in simulation, yielding high-quality\ntrajectories with dynamic object variations. Compared to prior benchmarks,\nRoboCerebra features significantly longer action sequences and denser\nannotations. We further benchmark state-of-the-art VLMs as System 2 modules and\nanalyze their performance across key cognitive dimensions, advancing the\ndevelopment of more capable and generalizable robotic planners.\n","authors":["Songhao Han","Boxiang Qiu","Yue Liao","Siyuan Huang","Chen Gao","Shuicheng Yan","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2506.06677v2.pdf","comment":"25 pages, 18 figures, Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25134v1","updated":"2025-10-29T03:28:18Z","published":"2025-10-29T03:28:18Z","title":"Region-CAM: Towards Accurate Object Regions in Class Activation Maps for\n  Weakly Supervised Learning Tasks","summary":"  Class Activation Mapping (CAM) methods are widely applied in weakly\nsupervised learning tasks due to their ability to highlight object regions.\nHowever, conventional CAM methods highlight only the most discriminative\nregions of the target. These highlighted regions often fail to cover the entire\nobject and are frequently misaligned with object boundaries, thereby limiting\nthe performance of downstream weakly supervised learning tasks, particularly\nWeakly Supervised Semantic Segmentation (WSSS), which demands pixel-wise\naccurate activation maps to get the best results. To alleviate the above\nproblems, we propose a novel activation method, Region-CAM. Distinct from\nnetwork feature weighting approaches, Region-CAM generates activation maps by\nextracting semantic information maps (SIMs) and performing semantic information\npropagation (SIP) by considering both gradients and features in each of the\nstages of the baseline classification model. Our approach highlights a greater\nproportion of object regions while ensuring activation maps to have precise\nboundaries that align closely with object edges. Region-CAM achieves 60.12% and\n58.43% mean intersection over union (mIoU) using the baseline model on the\nPASCAL VOC training and validation datasets, respectively, which are\nimprovements of 13.61% and 13.13% over the original CAM (46.51% and 45.30%). On\nthe MS COCO validation set, Region-CAM achieves 36.38%, a 16.23% improvement\nover the original CAM (20.15%). We also demonstrate the superiority of\nRegion-CAM in object localization tasks, using the ILSVRC2012 validation set.\nRegion-CAM achieves 51.7% in Top-1 Localization accuracy Loc1. Compared with\nLayerCAM, an activation method designed for weakly supervised object\nlocalization, Region-CAM achieves 4.5% better performance in Loc1.\n","authors":["Qingdong Cai","Charith Abhayaratne"],"pdf_url":"https://arxiv.org/pdf/2510.25134v1.pdf","comment":"Preprint for journal paper"},{"id":"http://arxiv.org/abs/2510.25129v1","updated":"2025-10-29T03:17:58Z","published":"2025-10-29T03:17:58Z","title":"AtlasGS: Atlanta-world Guided Surface Reconstruction with Implicit\n  Structured Gaussians","summary":"  3D reconstruction of indoor and urban environments is a prominent research\ntopic with various downstream applications. However, existing geometric priors\nfor addressing low-texture regions in indoor and urban settings often lack\nglobal consistency. Moreover, Gaussian Splatting and implicit SDF fields often\nsuffer from discontinuities or exhibit computational inefficiencies, resulting\nin a loss of detail. To address these issues, we propose an Atlanta-world\nguided implicit-structured Gaussian Splatting that achieves smooth indoor and\nurban scene reconstruction while preserving high-frequency details and\nrendering efficiency. By leveraging the Atlanta-world model, we ensure the\naccurate surface reconstruction for low-texture regions, while the proposed\nnovel implicit-structured GS representations provide smoothness without\nsacrificing efficiency and high-frequency details. Specifically, we propose a\nsemantic GS representation to predict the probability of all semantic regions\nand deploy a structure plane regularization with learnable plane indicators for\nglobal accurate surface reconstruction. Extensive experiments demonstrate that\nour method outperforms state-of-the-art approaches in both indoor and urban\nscenes, delivering superior surface reconstruction quality.\n","authors":["Xiyu Zhang","Chong Bao","Yipeng Chen","Hongjia Zhai","Yitong Dong","Hujun Bao","Zhaopeng Cui","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.25129v1.pdf","comment":"18 pages, 11 figures. NeurIPS 2025; Project page:\n  https://zju3dv.github.io/AtlasGS/"},{"id":"http://arxiv.org/abs/2503.11245v4","updated":"2025-10-29T03:13:22Z","published":"2025-03-14T09:52:54Z","title":"L2RSI: Cross-view LiDAR-based Place Recognition for Large-scale Urban\n  Scenes via Remote Sensing Imagery","summary":"  We tackle the challenge of LiDAR-based place recognition, which traditionally\ndepends on costly and time-consuming prior 3D maps. To overcome this, we first\nconstruct LiRSI-XA dataset, which encompasses approximately $110,000$ remote\nsensing submaps and $13,000$ LiDAR point cloud submaps captured in urban\nscenes, and propose a novel method, L2RSI, for cross-view LiDAR place\nrecognition using high-resolution Remote Sensing Imagery. This approach enables\nlarge-scale localization capabilities at a reduced cost by leveraging readily\navailable overhead images as map proxies. L2RSI addresses the dual challenges\nof cross-view and cross-modal place recognition by learning feature alignment\nbetween point cloud submaps and remote sensing submaps in the semantic domain.\nAdditionally, we introduce a novel probability propagation method based on\nparticle estimation to refine position predictions, effectively leveraging\ntemporal and spatial information. This approach enables large-scale retrieval\nand cross-scene generalization without fine-tuning. Extensive experiments on\nLiRSI-XA demonstrate that, within a $100km^2$ retrieval range, L2RSI accurately\nlocalizes $83.27\\%$ of point cloud submaps within a $30m$ radius for top-$1$\nretrieved location. Our project page is publicly available at\nhttps://shizw695.github.io/L2RSI/.\n","authors":["Ziwei Shi","Xiaoran Zhang","Wenjing Xu","Yan Xia","Yu Zang","Siqi Shen","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2503.11245v4.pdf","comment":"17 pages, 7 figures, NeurIPS 2025"},{"id":"http://arxiv.org/abs/2509.22689v2","updated":"2025-10-29T02:47:25Z","published":"2025-09-19T04:41:03Z","title":"Graph-Theoretic Consistency for Robust and Topology-Aware\n  Semi-Supervised Histopathology Segmentation","summary":"  Semi-supervised semantic segmentation (SSSS) is vital in computational\npathology, where dense annotations are costly and limited. Existing methods\noften rely on pixel-level consistency, which propagates noisy pseudo-labels and\nproduces fragmented or topologically invalid masks. We propose Topology Graph\nConsistency (TGC), a framework that integrates graph-theoretic constraints by\naligning Laplacian spectra, component counts, and adjacency statistics between\nprediction graphs and references. This enforces global topology and improves\nsegmentation accuracy. Experiments on GlaS and CRAG demonstrate that TGC\nachieves state-of-the-art performance under 5-10% supervision and significantly\nnarrows the gap to full supervision.\n","authors":["Ha-Hieu Pham","Minh Le","Han Huynh","Nguyen Quoc Khanh Le","Huy-Hieu Pham"],"pdf_url":"https://arxiv.org/pdf/2509.22689v2.pdf","comment":"Accepted to the AAAI 2026 Student Abstract and Poster Program"},{"id":"http://arxiv.org/abs/2405.04605v4","updated":"2025-10-29T02:33:21Z","published":"2024-05-07T18:36:40Z","title":"AI in Lung Health: Benchmarking Detection and Diagnostic Models Across\n  Multiple CT Scan Datasets","summary":"  Background: Development of artificial intelligence (AI) models for lung\ncancer screening requires large, well-annotated low-dose computed tomography\n(CT) datasets and rigorous performance benchmarks. Purpose: To create a\nreproducible benchmarking resource leveraging the Duke Lung Cancer Screening\n(DLCS) and multiple public datasets to develop and evaluate models for nodule\ndetection and classification. Materials & Methods: This retrospective study\nuses the DLCS dataset (1,613 patients; 2,487 nodules) and external datasets\nincluding LUNA16, LUNA25, and NLST-3D. For detection, MONAI RetinaNet models\nwere trained on DLCS (DLCS-De) and LUNA16 (LUNA16-De) and evaluated using the\nCompetition Performance Metric (CPM). For nodule-level classification, we\ncompare five strategies: pretrained models (Models Genesis, Med3D), a\nself-supervised foundation model (FMCB), and ResNet50 with random\ninitialization versus Strategic Warm-Start (ResNet50-SWS) pretrained with\ndetection-derived candidate patches stratified by confidence. Results: For\ndetection on the DLCS test set, DLCS-De achieved sensitivity 0.82 at 2 false\npositives/scan (CPM 0.63) versus LUNA16-De (0.62, CPM 0.45). For external\nvalidation on NLST-3D, DLCS-De (sensitivity 0.72, CPM 0.58) also outperformed\nLUNA16-De (sensitivity 0.64, CPM 0.49). For classification across multiple\ndatasets, ResNet50-SWS attained AUCs of 0.71 (DLCS; 95% CI, 0.61-0.81), 0.90\n(LUNA16; 0.87-0.93), 0.81 (NLST-3D; 0.79-0.82), and 0.80 (LUNA25; 0.78-0.82),\nmatching or exceeding pretrained/self-supervised baselines. Performance\ndifferences reflected dataset label standards. Conclusion: This work\nestablishes a standardized benchmarking resource for lung cancer AI research,\nsupporting model development, validation, and translation. All code, models,\nand data are publicly released to promote reproducibility.\n","authors":["Fakrul Islam Tushar","Avivah Wang","Lavsen Dahal","Ehsan Samei","Michael R. Harowicz","Jayashree Kalpathy-Cramer","Kyle J. Lafata","Tina D. Tailor","Cynthia Rudin","Joseph Y. Lo"],"pdf_url":"https://arxiv.org/pdf/2405.04605v4.pdf","comment":"2 tables, 5 figures"},{"id":"http://arxiv.org/abs/2510.22035v3","updated":"2025-10-29T02:18:32Z","published":"2025-10-24T21:41:32Z","title":"Caption-Driven Explainability: Probing CNNs for Bias via CLIP","summary":"  Robustness has become one of the most critical problems in machine learning\n(ML). The science of interpreting ML models to understand their behavior and\nimprove their robustness is referred to as explainable artificial intelligence\n(XAI). One of the state-of-the-art XAI methods for computer vision problems is\nto generate saliency maps. A saliency map highlights the pixel space of an\nimage that excites the ML model the most. However, this property could be\nmisleading if spurious and salient features are present in overlapping pixel\nspaces. In this paper, we propose a caption-based XAI method, which integrates\na standalone model to be explained into the contrastive language-image\npre-training (CLIP) model using a novel network surgery approach. The resulting\ncaption-based XAI model identifies the dominant concept that contributes the\nmost to the models prediction. This explanation minimizes the risk of the\nstandalone model falling for a covariate shift and contributes significantly\ntowards developing robust ML models. Our code is available at\nhttps://github.com/patch0816/caption-driven-xai\n","authors":["Patrick Koller","Amil V. Dravid","Guido M. Schuster","Aggelos K. Katsaggelos"],"pdf_url":"https://arxiv.org/pdf/2510.22035v3.pdf","comment":"Accepted and presented at the IEEE ICIP 2025 Satellite Workshop\n  \"Generative AI for World Simulations and Communications & Celebrating 40\n  Years of Excellence in Education: Honoring Professor Aggelos Katsaggelos\",\n  Anchorage, Alaska, USA, September 14, 2025. Camera-ready preprint; the\n  official IEEE Xplore publication will follow. Code:\n  https://github.com/patch0816/caption-driven-xai"},{"id":"http://arxiv.org/abs/2510.07316v2","updated":"2025-10-29T02:15:20Z","published":"2025-10-08T17:59:33Z","title":"Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers","summary":"  This paper presents Pixel-Perfect Depth, a monocular depth estimation model\nbased on pixel-space diffusion generation that produces high-quality,\nflying-pixel-free point clouds from estimated depth maps. Current generative\ndepth estimation models fine-tune Stable Diffusion and achieve impressive\nperformance. However, they require a VAE to compress depth maps into latent\nspace, which inevitably introduces \\textit{flying pixels} at edges and details.\nOur model addresses this challenge by directly performing diffusion generation\nin the pixel space, avoiding VAE-induced artifacts. To overcome the high\ncomplexity associated with pixel-space generation, we introduce two novel\ndesigns: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which\nincorporate semantic representations from vision foundation models into DiT to\nprompt the diffusion process, thereby preserving global semantic consistency\nwhile enhancing fine-grained visual details; and 2) Cascade DiT Design that\nprogressively increases the number of tokens to further enhance efficiency and\naccuracy. Our model achieves the best performance among all published\ngenerative models across five benchmarks, and significantly outperforms all\nother models in edge-aware point cloud evaluation.\n","authors":["Gangwei Xu","Haotong Lin","Hongcheng Luo","Xianqi Wang","Jingfeng Yao","Lianghui Zhu","Yuechuan Pu","Cheng Chi","Haiyang Sun","Bing Wang","Guang Chen","Hangjun Ye","Sida Peng","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2510.07316v2.pdf","comment":"NeurIPS 2025. Project page: https://pixel-perfect-depth.github.io/"},{"id":"http://arxiv.org/abs/2510.16765v2","updated":"2025-10-29T02:07:16Z","published":"2025-10-19T09:11:58Z","title":"WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and\n  Mamba-based Channel Modeling with Texture Enhancement","summary":"  Image restoration is a fundamental and challenging task in computer vision,\nwhere CNN-based frameworks demonstrate significant computational efficiency.\nHowever, previous CNN-based methods often face challenges in adequately\nrestoring fine texture details, which are limited by the small receptive field\nof CNN structures and the lack of channel feature modeling. In this paper, we\npropose WaMaIR, which is a novel framework with a large receptive field for\nimage perception and improves the reconstruction of texture details in restored\nimages. Specifically, we introduce the Global Multiscale Wavelet Transform\nConvolutions (GMWTConvs) for expandding the receptive field to extract image\nfeatures, preserving and enriching texture features in model inputs. Meanwhile,\nwe propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to\ncapture long-range dependencies within feature channels, which enhancing the\nmodel sensitivity to color, edges, and texture information. Additionally, we\npropose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to\nguide the model in preserving detailed texture structures effectively.\nExtensive experiments confirm that WaMaIR outperforms state-of-the-art methods,\nachieving better image restoration and efficient computational performance of\nthe model.\n","authors":["Shengyu Zhu","Congyi Fan","Fuxuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.16765v2.pdf","comment":"Chinese Conference on Pattern Recognition and Computer Vision (PRCV),\n  Oral"},{"id":"http://arxiv.org/abs/2510.25094v1","updated":"2025-10-29T01:58:35Z","published":"2025-10-29T01:58:35Z","title":"Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI\n  Detection","summary":"  Zero-shot Human-Object Interaction detection aims to localize humans and\nobjects in an image and recognize their interaction, even when specific\nverb-object pairs are unseen during training. Recent works have shown promising\nresults using prompt learning with pretrained vision-language models such as\nCLIP, which align natural language prompts with visual features in a shared\nembedding space. However, existing approaches still fail to handle the visual\ncomplexity of interaction, including (1) intra-class visual diversity, where\ninstances of the same verb appear in diverse poses and contexts, and (2)\ninter-class visual entanglement, where distinct verbs yield visually similar\npatterns. To address these challenges, we propose VDRP, a framework for Visual\nDiversity and Region-aware Prompt learning. First, we introduce a visual\ndiversity-aware prompt learning strategy that injects group-wise visual\nvariance into the context embedding. We further apply Gaussian perturbation to\nencourage the prompts to capture diverse visual variations of a verb. Second,\nwe retrieve region-specific concepts from the human, object, and union regions.\nThese are used to augment the diversity-aware prompt embeddings, yielding\nregion-aware prompts that enhance verb-level discrimination. Experiments on the\nHICO-DET benchmark demonstrate that our method achieves state-of-the-art\nperformance under four zero-shot evaluation settings, effectively addressing\nboth intra-class diversity and inter-class visual entanglement. Code is\navailable at https://github.com/mlvlab/VDRP.\n","authors":["Chanhyeong Yang","Taehoon Song","Jihwan Park","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2510.25094v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25084v1","updated":"2025-10-29T01:42:23Z","published":"2025-10-29T01:42:23Z","title":"PSTF-AttControl: Per-Subject-Tuning-Free Personalized Image Generation\n  with Controllable Face Attributes","summary":"  Recent advancements in personalized image generation have significantly\nimproved facial identity preservation, particularly in fields such as\nentertainment and social media. However, existing methods still struggle to\nachieve precise control over facial attributes in a per-subject-tuning-free\n(PSTF) way. Tuning-based techniques like PreciseControl have shown promise by\nproviding fine-grained control over facial features, but they often require\nextensive technical expertise and additional training data, limiting their\naccessibility. In contrast, PSTF approaches simplify the process by enabling\nimage generation from a single facial input, but they lack precise control over\nfacial attributes. In this paper, we introduce a novel, PSTF method that\nenables both precise control over facial attributes and high-fidelity\npreservation of facial identity. Our approach utilizes a face recognition model\nto extract facial identity features, which are then mapped into the $W^+$\nlatent space of StyleGAN2 using the e4e encoder. We further enhance the model\nwith a Triplet-Decoupled Cross-Attention module, which integrates facial\nidentity, attribute features, and text embeddings into the UNet architecture,\nensuring clean separation of identity and attribute information. Trained on the\nFFHQ dataset, our method allows for the generation of personalized images with\nfine-grained control over facial attributes, while without requiring additional\nfine-tuning or training data for individual identities. We demonstrate that our\napproach successfully balances personalization with precise facial attribute\ncontrol, offering a more efficient and user-friendly solution for high-quality,\nadaptable facial image synthesis. The code is publicly available at\nhttps://github.com/UnicomAI/PSTF-AttControl.\n","authors":["Xiang liu","Zhaoxiang Liu","Huan Hu","Zipeng Wang","Ping Chen","Zezhou Chen","Kai Wang","Shiguo Lian"],"pdf_url":"https://arxiv.org/pdf/2510.25084v1.pdf","comment":"Accepted by Image and Vision Computing (18 pages, 8 figures)"},{"id":"http://arxiv.org/abs/2510.25077v1","updated":"2025-10-29T01:24:49Z","published":"2025-10-29T01:24:49Z","title":"Neighborhood Feature Pooling for Remote Sensing Image Classification","summary":"  In this work, we propose neighborhood feature pooling (NFP) as a novel\ntexture feature extraction method for remote sensing image classification. The\nNFP layer captures relationships between neighboring inputs and efficiently\naggregates local similarities across feature dimensions. Implemented using\nconvolutional layers, NFP can be seamlessly integrated into any network.\nResults comparing the baseline models and the NFP method indicate that NFP\nconsistently improves performance across diverse datasets and architectures\nwhile maintaining minimal parameter overhead.\n","authors":["Fahimeh Orvati Nia","Amirmohammad Mohammadi","Salim Al Kharsa","Pragati Naikare","Zigfried Hampel-Arias","Joshua Peeples"],"pdf_url":"https://arxiv.org/pdf/2510.25077v1.pdf","comment":"9 pages, 5 figures. Accepted to WACV 2026 (Winter Conference on\n  Applications of Computer Vision)"},{"id":"http://arxiv.org/abs/2505.16854v3","updated":"2025-10-29T01:19:12Z","published":"2025-05-22T16:13:29Z","title":"Think or Not? Selective Reasoning via Reinforcement Learning for\n  Vision-Language Models","summary":"  Reinforcement Learning (RL) has proven to be an effective post-training\nstrategy for enhancing reasoning in vision-language models (VLMs). Group\nRelative Policy Optimization (GRPO) is a recent prominent method that\nencourages models to generate complete reasoning traces before answering,\nleading to increased token usage and computational cost. Inspired by the\nhuman-like thinking process-where people skip reasoning for easy questions but\nthink carefully when needed-we explore how to enable VLMs to first decide when\nreasoning is necessary. To realize this, we propose TON, a two-stage training\nstrategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective\n'thought dropout' operation, where reasoning traces are randomly replaced with\nempty thoughts. This introduces a think-or-not format that serves as a cold\nstart for selective reasoning; (ii) a GRPO stage that enables the model to\nfreely explore when to think or not, while maximizing task-aware outcome\nrewards. Experimental results show that TON can reduce the completion length by\nup to 90% compared to vanilla GRPO, without sacrificing performance or even\nimproving it. Further evaluations across LLM (GSM8K), VLM (CLEVR, Super-CLEVR,\nGeoQA), and Agentic (AITZ) tasks-covering a range of reasoning difficulties\nunder both 3B and 7B models-consistently reveal that the model progressively\nlearns to bypass unnecessary reasoning steps as training advances. These\nfindings shed light on the path toward human-like reasoning patterns in RL\napproaches. Our code is available at https://github.com/kokolerk/TON.\n","authors":["Jiaqi Wang","Kevin Qinghong Lin","James Cheng","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2505.16854v3.pdf","comment":"camera ready revision"},{"id":"http://arxiv.org/abs/2510.25070v1","updated":"2025-10-29T01:16:21Z","published":"2025-10-29T01:16:21Z","title":"Vision-Language Integration for Zero-Shot Scene Understanding in\n  Real-World Environments","summary":"  Zero-shot scene understanding in real-world settings presents major\nchallenges due to the complexity and variability of natural scenes, where\nmodels must recognize new objects, actions, and contexts without prior labeled\nexamples. This work proposes a vision-language integration framework that\nunifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models\n(e.g., GPT-based architectures) to achieve semantic alignment between visual\nand textual modalities. The goal is to enable robust zero-shot comprehension of\nscenes by leveraging natural language as a bridge to generalize over unseen\ncategories and contexts. Our approach develops a unified model that embeds\nvisual inputs and textual prompts into a shared space, followed by multimodal\nfusion and reasoning layers for contextual interpretation. Experiments on\nVisual Genome, COCO, ADE20K, and custom real-world datasets demonstrate\nsignificant gains over state-of-the-art zero-shot models in object recognition,\nactivity detection, and scene captioning. The proposed system achieves up to\n18% improvement in top-1 accuracy and notable gains in semantic coherence\nmetrics, highlighting the effectiveness of cross-modal alignment and language\ngrounding in enhancing generalization for real-world scene understanding.\n","authors":["Manjunath Prasad Holenarasipura Rajiv","B. M. Vidyavathi"],"pdf_url":"https://arxiv.org/pdf/2510.25070v1.pdf","comment":"Preprint under review at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI), 2025"},{"id":"http://arxiv.org/abs/2510.25067v1","updated":"2025-10-29T01:10:28Z","published":"2025-10-29T01:10:28Z","title":"DRIP: Dynamic patch Reduction via Interpretable Pooling","summary":"  Recently, the advances in vision-language models, including contrastive\npretraining and instruction tuning, have greatly pushed the frontier of\nmultimodal AI. However, owing to the large-scale and hence expensive\npretraining, the efficiency concern has discouraged researchers from attempting\nto pretrain a vision language model from scratch. In this work, we propose\nDynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the\ninput images and dynamically merges tokens in the deeper layers of a visual\nencoder. Our results on both ImageNet training from scratch and CLIP\ncontrastive pretraining demonstrate a significant GFLOP reduction while\nmaintaining comparable classification/zero-shot performance. To further\nvalidate our proposed method, we conduct continual pretraining on a large\nbiology dataset, extending its impact into scientific domains.\n","authors":["Yusen Peng","Sachin Kumar"],"pdf_url":"https://arxiv.org/pdf/2510.25067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25058v1","updated":"2025-10-29T00:49:33Z","published":"2025-10-29T00:49:33Z","title":"Auto3DSeg for Brain Tumor Segmentation from 3D MRI in BraTS 2023\n  Challenge","summary":"  In this work, we describe our solution to the BraTS 2023 cluster of\nchallenges using Auto3DSeg from MONAI. We participated in all 5 segmentation\nchallenges, and achieved the 1st place results in three of them: Brain\nMetastasis, Brain Meningioma, BraTS-Africa challenges, and the 2nd place\nresults in the remaining two: Adult and Pediatic Glioma challenges.\n","authors":["Andriy Myronenko","Dong Yang","Yufan He","Daguang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25058v1.pdf","comment":"BraTS23 winner"},{"id":"http://arxiv.org/abs/2510.25051v1","updated":"2025-10-29T00:37:18Z","published":"2025-10-29T00:37:18Z","title":"Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference\n  Models","summary":"  Breast cancer remains the most commonly diagnosed malignancy among women in\nthe developed world. Early detection through mammography screening plays a\npivotal role in reducing mortality rates. While computer-aided diagnosis (CAD)\nsystems have shown promise in assisting radiologists, existing approaches face\ncritical limitations in clinical deployment - particularly in handling the\nnuanced interpretation of multi-modal data and feasibility due to the\nrequirement of prior clinical history. This study introduces a novel framework\nthat synergistically combines visual features from 2D mammograms with\nstructured textual descriptors derived from easily accessible clinical metadata\nand synthesized radiological reports through innovative tokenization modules.\nOur proposed methods in this study demonstrate that strategic integration of\nconvolutional neural networks (ConvNets) with language representations achieves\nsuperior performance to vision transformer-based models while handling\nhigh-resolution images and enabling practical deployment across diverse\npopulations. By evaluating it on multi-national cohort screening mammograms,\nour multi-modal approach achieves superior performance in cancer detection and\ncalcification identification compared to unimodal baselines, with particular\nimprovements. The proposed method establishes a new paradigm for developing\nclinically viable VLM-based CAD systems that effectively leverage imaging data\nand contextual patient information through effective fusion mechanisms.\n","authors":["Shunjie-Fabian Zheng","Hyeonjun Lee","Thijs Kooi","Ali Diba"],"pdf_url":"https://arxiv.org/pdf/2510.25051v1.pdf","comment":"Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)\n  Workshop at ICCV 2025"},{"id":"http://arxiv.org/abs/2510.26027v1","updated":"2025-10-29T23:50:57Z","published":"2025-10-29T23:50:57Z","title":"Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal\n  Attention in Vision Encoders","summary":"  Despite significant advances in Multimodal Large Language Models (MLLMs),\nunderstanding complex temporal dynamics in videos remains a major challenge.\nOur experiments show that current Video Large Language Model (Video-LLM)\narchitectures have critical limitations in temporal understanding, struggling\nwith tasks that require detailed comprehension of action sequences and temporal\nprogression. In this work, we propose a Video-LLM architecture that introduces\nstacked temporal attention modules directly within the vision encoder. This\ndesign incorporates a temporal attention in vision encoder, enabling the model\nto better capture the progression of actions and the relationships between\nframes before passing visual tokens to the LLM. Our results show that this\napproach significantly improves temporal reasoning and outperforms existing\nmodels in video question answering tasks, specifically in action recognition.\nWe improve on benchmarks including VITATECS, MVBench, and Video-MME by up to\n+5.5%. By enhancing the vision encoder with temporal structure, we address a\ncritical gap in video understanding for Video-LLMs. Project page and code are\navailable at: https://alirasekh.github.io/STAVEQ2/.\n","authors":["Ali Rasekh","Erfan Bagheri Soula","Omid Daliran","Simon Gottschalk","Mohsen Fayyaz"],"pdf_url":"https://arxiv.org/pdf/2510.26027v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26022v1","updated":"2025-10-29T23:30:09Z","published":"2025-10-29T23:30:09Z","title":"Groupwise Registration with Physics-Informed Test-Time Adaptation on\n  Multi-parametric Cardiac MRI","summary":"  Multiparametric mapping MRI has become a viable tool for myocardial tissue\ncharacterization. However, misalignment between multiparametric maps makes\npixel-wise analysis challenging. To address this challenge, we developed a\ngeneralizable physics-informed deep-learning model using test-time adaptation\nto enable group image registration across contrast weighted images acquired\nfrom multiple physical models (e.g., a T1 mapping model and T2 mapping model).\nThe physics-informed adaptation utilized the synthetic images from specific\nphysics model as registration reference, allows for transductive learning for\nvarious tissue contrast. We validated the model in healthy volunteers with\nvarious MRI sequences, demonstrating its improvement for multi-modal\nregistration with a wide range of image contrast variability.\n","authors":["Xinqi Li","Yi Zhang","Li-Ting Huang","Hsiao-Huang Chang","Thoralf Niendorf","Min-Chi Ku","Qian Tao","Hsin-Jung Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26017v1","updated":"2025-10-29T23:23:11Z","published":"2025-10-29T23:23:11Z","title":"Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep\n  Learning","summary":"  Climate change and sea-level rise (SLR) pose escalating threats to coastal\ncities, intensifying the need for efficient and accurate methods to predict\npotential flood hazards. Traditional physics-based hydrodynamic simulators,\nalthough precise, are computationally expensive and impractical for city-scale\ncoastal planning applications. Deep Learning (DL) techniques offer promising\nalternatives, however, they are often constrained by challenges such as data\nscarcity and high-dimensional output requirements. Leveraging a recently\nproposed vision-based, low-resource DL framework, we develop a novel,\nlightweight Convolutional Neural Network (CNN)-based model designed to predict\ncoastal flooding under variable SLR projections and shoreline adaptation\nscenarios. Furthermore, we demonstrate the ability of the model to generalize\nacross diverse geographical contexts by utilizing datasets from two distinct\nregions: Abu Dhabi and San Francisco. Our findings demonstrate that the\nproposed model significantly outperforms state-of-the-art methods, reducing the\nmean absolute error (MAE) in predicted flood depth maps on average by nearly\n20%. These results highlight the potential of our approach to serve as a\nscalable and practical tool for coastal flood management, empowering\ndecision-makers to develop effective mitigation strategies in response to the\ngrowing impacts of climate change. Project Page: https://caspiannet.github.io/\n","authors":["Bilal Hassan","Areg Karapetyan","Aaron Chung Hin Chow","Samer Madanat"],"pdf_url":"https://arxiv.org/pdf/2510.26017v1.pdf","comment":"Submitted to Hydrology and Earth System Sciences"},{"id":"http://arxiv.org/abs/2505.12191v2","updated":"2025-10-29T23:02:31Z","published":"2025-05-18T01:37:58Z","title":"Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised\n  Learning from Data Curriculum","summary":"  Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2.\n","authors":["Wenquan Lu","Jiaqi Zhang","Hugues Van Assel","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2505.12191v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2401.13267v4","updated":"2025-10-29T23:00:50Z","published":"2024-01-24T07:13:06Z","title":"Dynamic Traceback Learning for Medical Report Generation","summary":"  Automated medical report generation has demonstrated the potential to\nsignificantly reduce the workload associated with time-consuming medical\nreporting. Recent generative representation learning methods have shown promise\nin integrating vision and language modalities for medical report generation.\nHowever, when trained end-to-end and applied directly to medical image-to-text\ngeneration, they face two significant challenges: i) difficulty in accurately\ncapturing subtle yet crucial pathological details, and ii) reliance on both\nvisual and textual inputs during inference, leading to performance degradation\nin zero-shot inference when only images are available. To address these\nchallenges, this study proposes a novel multimodal dynamic traceback learning\nframework (DTrace). Specifically, we introduce a traceback mechanism to\nsupervise the semantic validity of generated content and a dynamic learning\nstrategy to adapt to various proportions of image and text input, enabling text\ngeneration without strong reliance on the input from both modalities during\ninference. The learning of cross-modal knowledge is enhanced by supervising the\nmodel to recover masked semantic information from a complementary counterpart.\nExtensive experiments conducted on two benchmark datasets, IU-Xray and\nMIMIC-CXR, demonstrate that the proposed DTrace framework outperforms\nstate-of-the-art methods for medical report generation.\n","authors":["Shuchang Ye","Mingyuan Meng","Mingjian Li","Dagan Feng","Usman Naseem","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2401.13267v4.pdf","comment":"Accepted to IEEE Transactions on Multimedia (TMM)"},{"id":"http://arxiv.org/abs/2505.21996v2","updated":"2025-10-29T22:39:29Z","published":"2025-05-28T05:55:44Z","title":"Learning World Models for Interactive Video Generation","summary":"  Foundational world models must be both interactive and preserve\nspatiotemporal coherence for effective future planning with action choices.\nHowever, present models for long video generation have limited inherent world\nmodeling capabilities due to two main challenges: compounding errors and\ninsufficient memory mechanisms. We enhance image-to-video models with\ninteractive capabilities through additional action conditioning and\nautoregressive framework, and reveal that compounding error is inherently\nirreducible in autoregressive video generation, while insufficient memory\nmechanism leads to incoherence of world models. We propose video retrieval\naugmented generation (VRAG) with explicit global state conditioning, which\nsignificantly reduces long-term compounding errors and increases spatiotemporal\nconsistency of world models. In contrast, naive autoregressive generation with\nextended context windows and retrieval-augmented generation prove less\neffective for video generation, primarily due to the limited in-context\nlearning capabilities of current video models. Our work illuminates the\nfundamental challenges in video world models and establishes a comprehensive\nbenchmark for improving video generation models with internal world modeling\ncapabilities.\n","authors":["Taiye Chen","Xun Hu","Zihan Ding","Chi Jin"],"pdf_url":"https://arxiv.org/pdf/2505.21996v2.pdf","comment":"Project page: https://sites.google.com/view/vrag"},{"id":"http://arxiv.org/abs/2510.26006v1","updated":"2025-10-29T22:34:26Z","published":"2025-10-29T22:34:26Z","title":"CAVE: Detecting and Explaining Commonsense Anomalies in Visual\n  Environments","summary":"  Humans can naturally identify, reason about, and explain anomalies in their\nenvironment. In computer vision, this long-standing challenge remains limited\nto industrial defects or unrealistic, synthetically generated anomalies,\nfailing to capture the richness and unpredictability of real-world anomalies.\nIn this work, we introduce CAVE, the first benchmark of real-world visual\nanomalies. CAVE supports three open-ended tasks: anomaly description,\nexplanation, and justification; with fine-grained annotations for visual\ngrounding and categorizing anomalies based on their visual manifestations,\ntheir complexity, severity, and commonness. These annotations draw inspiration\nfrom cognitive science research on how humans identify and resolve anomalies,\nproviding a comprehensive framework for evaluating Vision-Language Models\n(VLMs) in detecting and understanding anomalies. We show that state-of-the-art\nVLMs struggle with visual anomaly perception and commonsense reasoning, even\nwith advanced prompting strategies. By offering a realistic and cognitively\ngrounded benchmark, CAVE serves as a valuable resource for advancing research\nin anomaly detection and commonsense reasoning in VLMs.\n","authors":["Rishika Bhagwatkar","Syrielle Montariol","Angelika Romanou","Beatriz Borges","Irina Rish","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2510.26006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10501v2","updated":"2025-10-29T22:32:43Z","published":"2024-11-15T11:19:25Z","title":"OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion\n  Models","summary":"  We consider the problem of text-to-video generation tasks with precise\ncontrol for various applications such as camera movement control and\nvideo-to-video editing. Most methods tacking this problem rely on providing\nuser-defined controls, such as binary masks or camera movement embeddings. In\nour approach we propose OnlyFlow, an approach leveraging the optical flow\nfirstly extracted from an input video to condition the motion of generated\nvideos. Using a text prompt and an input video, OnlyFlow allows the user to\ngenerate videos that respect the motion of the input video as well as the text\nprompt. This is implemented through an optical flow estimation model applied on\nthe input video, which is then fed to a trainable optical flow encoder. The\noutput feature maps are then injected into the text-to-video backbone model. We\nperform quantitative, qualitative and user preference studies to show that\nOnlyFlow positively compares to state-of-the-art methods on a wide range of\ntasks, even though OnlyFlow was not specifically trained for such tasks.\nOnlyFlow thus constitutes a versatile, lightweight yet efficient method for\ncontrolling motion in text-to-video generation. Models and code will be made\navailable on GitHub and HuggingFace.\n","authors":["Mathis Koroglu","Hugo Caselles-Dupré","Guillaume Jeanneret Sanmiguel","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2411.10501v2.pdf","comment":"8 pages, 1 supplementary page, 9 figures"},{"id":"http://arxiv.org/abs/2311.07734v2","updated":"2025-10-29T22:32:42Z","published":"2023-11-13T20:36:54Z","title":"Quality-Aware Prototype Memory for Face Representation Learning","summary":"  Prototype Memory is a powerful model for face representation learning. It\nenables training face recognition models on datasets of any size by generating\nprototypes (classifier weights) on the fly and efficiently utilizing them.\nPrototype Memory demonstrated strong results in many face recognition\nbenchmarks. However, the algorithm of prototype generation, used in it, is\nprone to the problems of imperfectly calculated prototypes in case of\nlow-quality or poorly recognizable faces in the images, selected for the\nprototype creation. All images of the same person presented in the mini-batch\nare used with equal weights, and the resulting averaged prototype can be\ncontaminated by imperfect embeddings of low-quality face images. This may lead\nto misleading training signals and degrade the performance of the trained\nmodels. In this paper, we propose a simple and effective way to improve\nPrototype Memory with quality-aware prototype generation. Quality-Aware\nPrototype Memory uses different weights for images of different quality in the\nprocess of prototype generation. With this improvement, prototypes receive more\ninformative signals from high-quality images and are less affected by\nlow-quality ones. We propose and compare several methods of quality estimation\nand usage, perform extensive experiments on the different face recognition\nbenchmarks and demonstrate the advantages of the proposed model compared to the\nbasic version of Prototype Memory.\n","authors":["Evgeny Smirnov","Vasiliy Galyuk","Evgeny Lukyanets"],"pdf_url":"https://arxiv.org/pdf/2311.07734v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2510.26004v1","updated":"2025-10-29T22:32:16Z","published":"2025-10-29T22:32:16Z","title":"DARTS: A Drone-Based AI-Powered Real-Time Traffic Incident Detection\n  System","summary":"  Rapid and reliable incident detection is critical for reducing crash-related\nfatalities, injuries, and congestion. However, conventional methods, such as\nclosed-circuit television, dashcam footage, and sensor-based detection,\nseparate detection from verification, suffer from limited flexibility, and\nrequire dense infrastructure or high penetration rates, restricting\nadaptability and scalability to shifting incident hotspots. To overcome these\nchallenges, we developed DARTS, a drone-based, AI-powered real-time traffic\nincident detection system. DARTS integrates drones' high mobility and aerial\nperspective for adaptive surveillance, thermal imaging for better\nlow-visibility performance and privacy protection, and a lightweight deep\nlearning framework for real-time vehicle trajectory extraction and incident\ndetection. The system achieved 99% detection accuracy on a self-collected\ndataset and supports simultaneous online visual verification, severity\nassessment, and incident-induced congestion propagation monitoring via a\nweb-based interface. In a field test on Interstate 75 in Florida, DARTS\ndetected and verified a rear-end collision 12 minutes earlier than the local\ntransportation management center and monitored incident-induced congestion\npropagation, suggesting potential to support faster emergency response and\nenable proactive traffic control to reduce congestion and secondary crash risk.\nCrucially, DARTS's flexible deployment architecture reduces dependence on\nfrequent physical patrols, indicating potential scalability and\ncost-effectiveness for use in remote areas and resource-constrained settings.\nThis study presents a promising step toward a more flexible and integrated\nreal-time traffic incident detection system, with significant implications for\nthe operational efficiency and responsiveness of modern transportation\nmanagement.\n","authors":["Bai Li","Achilleas Kourtellis","Rong Cao","Joseph Post","Brian Porter","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26004v1.pdf","comment":"Preprint version. This manuscript is currently under review at\n  Transportation Research Part C: Emerging Technologies. The PDF corresponds to\n  the version submitted in June 2025. The main findings of this work were\n  recognized with the Best Intelligent Transportation Systems Paper Award at\n  the 2025 TRB Annual Meeting"},{"id":"http://arxiv.org/abs/2510.26001v1","updated":"2025-10-29T22:25:48Z","published":"2025-10-29T22:25:48Z","title":"Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based\n  Methods in Low-Light Image Enhancement","summary":"  We propose an innovative enhancement to the Mamba framework by increasing the\nHausdorff dimension of its scanning pattern through a novel Hilbert Selective\nScan mechanism. This mechanism explores the feature space more effectively,\ncapturing intricate fine-scale details and improving overall coverage. As a\nresult, it mitigates information inconsistencies while refining spatial\nlocality to better capture subtle local interactions without sacrificing the\nmodel's ability to handle long-range dependencies. Extensive experiments on\npublicly available benchmarks demonstrate that our approach significantly\nimproves both the quantitative metrics and qualitative visual fidelity of\nexisting Mamba-based low-light image enhancement methods, all while reducing\ncomputational resource consumption and shortening inference time. We believe\nthat this refined strategy not only advances the state-of-the-art in low-light\nimage enhancement but also holds promise for broader applications in fields\nthat leverage Mamba-based techniques.\n","authors":["Xinhua Wang","Caibo Feng","Xiangjun Fu","Chunxiao Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06220v2","updated":"2025-10-29T22:25:02Z","published":"2025-06-06T16:28:03Z","title":"GenIR: Generative Visual Feedback for Mental Image Retrieval","summary":"  Vision-language models (VLMs) have shown strong performance on text-to-image\nretrieval benchmarks. However, bridging this success to real-world applications\nremains a challenge. In practice, human search behavior is rarely a one-shot\naction. Instead, it is often a multi-round process guided by clues in mind.\nThat is, a mental image ranging from vague recollections to vivid mental\nrepresentations of the target image. Motivated by this gap, we study the task\nof Mental Image Retrieval (MIR), which targets the realistic yet underexplored\nsetting where users refine their search for a mentally envisioned image through\nmulti-round interactions with an image search engine. Central to successful\ninteractive retrieval is the capability of machines to provide users with\nclear, actionable feedback; however, existing methods rely on indirect or\nabstract verbal feedback, which can be ambiguous, misleading, or ineffective\nfor users to refine the query. To overcome this, we propose GenIR, a generative\nmulti-round retrieval paradigm leveraging diffusion-based image generation to\nexplicitly reify the AI system's understanding at each round. These synthetic\nvisual representations provide clear, interpretable feedback, enabling users to\nrefine their queries intuitively and effectively. We further introduce a fully\nautomated pipeline to generate a high-quality multi-round MIR dataset.\nExperimental results demonstrate that GenIR significantly outperforms existing\ninteractive methods in the MIR scenario. This work establishes a new task with\na dataset and an effective generative retrieval method, providing a foundation\nfor future research in this direction\n","authors":["Diji Yang","Minghao Liu","Chung-Hsiang Lo","Yi Zhang","James Davis"],"pdf_url":"https://arxiv.org/pdf/2506.06220v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25990v1","updated":"2025-10-29T21:57:12Z","published":"2025-10-29T21:57:12Z","title":"Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI","summary":"  In this work, we address the TrackRAD2025 challenge of real-time tumor\ntracking in cine-MRI sequences of the thoracic and abdominal regions under\nstrong data scarcity constraints. Two complementary strategies were explored:\n(i) unsupervised registration with the IMPACT similarity metric and (ii)\nfoundation model-based segmentation leveraging SAM 2.1 and its recent variants\nthrough prompt-based interaction. Due to the one-second runtime constraint, the\nSAM-based method was ultimately selected. The final configuration used SAM2.1\nb+ with mask-based prompts from the first annotated slice, fine-tuned solely on\nthe small labeled subset from TrackRAD2025. Training was configured to minimize\noverfitting, using 1024x1024 patches (batch size 1), standard augmentations,\nand a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was\napplied to all modules (prompt encoder, decoder, Hiera backbone) to preserve\ngeneralization while adapting to annotator-specific styles. Training lasted 300\nepochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently\napplied across all anatomical sites and MRI field strengths. Test-time\naugmentation was considered but ultimately discarded due to negligible\nperformance gains. The final model was selected based on the highest Dice\nSimilarity Coefficient achieved on the validation set after fine-tuning. On the\nhidden test set, the model reached a Dice score of 0.8794, ranking 6th overall\nin the TrackRAD2025 challenge. These results highlight the strong potential of\nfoundation models for accurate and real-time tumor tracking in MRI-guided\nradiotherapy.\n","authors":["Valentin Boussot","Cédric Hémon","Jean-Claude Nunes","Jean-Louis Dillenseger"],"pdf_url":"https://arxiv.org/pdf/2510.25990v1.pdf","comment":"Paper for the Trackrad2025 challenge, Team BreizhTrack"},{"id":"http://arxiv.org/abs/2506.05696v2","updated":"2025-10-29T21:34:31Z","published":"2025-06-06T02:52:13Z","title":"MoralCLIP: Contrastive Alignment of Vision-and-Language Representations\n  with Moral Foundations Theory","summary":"  Recent advances in vision-language models have enabled rich semantic\nunderstanding across modalities. However, these encoding methods lack the\nability to interpret or reason about the moral dimensions of content-a crucial\naspect of human cognition. In this paper, we address this gap by introducing\nMoralCLIP, a novel embedding representation method that extends multimodal\nlearning with explicit moral grounding based on Moral Foundations Theory (MFT).\nOur approach integrates visual and textual moral cues into a unified embedding\nspace, enabling cross-modal moral alignment. MoralCLIP is grounded on the\nmulti-label dataset Social-Moral Image Database to identify co-occurring moral\nfoundations in visual content. For MoralCLIP training, we design a moral data\naugmentation strategy to scale our annotated dataset to 15,000 image-text pairs\nlabeled with MFT-aligned dimensions. Our results demonstrate that explicit\nmoral supervision improves both unimodal and multimodal understanding of moral\ncontent, establishing a foundation for morally-aware AI systems capable of\nrecognizing and aligning with human moral values.\n","authors":["Ana Carolina Condez","Diogo Tavares","João Magalhães"],"pdf_url":"https://arxiv.org/pdf/2506.05696v2.pdf","comment":"Updated version: corresponds to the ACM MM '25 published paper and\n  includes full appendix material"},{"id":"http://arxiv.org/abs/2510.25976v1","updated":"2025-10-29T21:21:54Z","published":"2025-10-29T21:21:54Z","title":"Brain-IT: Image Reconstruction from fMRI via Brain-Interaction\n  Transformer","summary":"  Reconstructing images seen by people from their fMRI brain recordings\nprovides a non-invasive window into the human brain. Despite recent progress\nenabled by diffusion models, current methods often lack faithfulness to the\nactual seen images. We present \"Brain-IT\", a brain-inspired approach that\naddresses this challenge through a Brain Interaction Transformer (BIT),\nallowing effective interactions between clusters of functionally-similar\nbrain-voxels. These functional-clusters are shared by all subjects, serving as\nbuilding blocks for integrating information both within and across brains. All\nmodel components are shared by all clusters & subjects, allowing efficient\ntraining with a limited amount of data. To guide the image reconstruction, BIT\npredicts two complementary localized patch-level image features: (i)high-level\nsemantic features which steer the diffusion model toward the correct semantic\ncontent of the image; and (ii)low-level structural features which help to\ninitialize the diffusion process with the correct coarse layout of the image.\nBIT's design enables direct flow of information from brain-voxel clusters to\nlocalized image features. Through these principles, our method achieves image\nreconstructions from fMRI that faithfully reconstruct the seen images, and\nsurpass current SotA approaches both visually and by standard objective\nmetrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve\nresults comparable to current methods trained on full 40-hour recordings.\n","authors":["Roman Beliy","Amit Zalcher","Jonathan Kogman","Navve Wasserman","Michal Irani"],"pdf_url":"https://arxiv.org/pdf/2510.25976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25970v1","updated":"2025-10-29T21:12:58Z","published":"2025-10-29T21:12:58Z","title":"SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing","summary":"  Rectified flow models have become a de facto standard in image generation due\nto their stable sampling trajectories and high-fidelity outputs. Despite their\nstrong generative capabilities, they face critical limitations in image editing\ntasks: inaccurate inversion processes for mapping real images back into the\nlatent space, and gradient entanglement issues during editing often result in\noutputs that do not faithfully reflect the target prompt. Recent efforts have\nattempted to directly map source and target distributions via ODE-based\napproaches without inversion; however,these methods still yield suboptimal\nediting quality. In this work, we propose a flow decomposition-and-aggregation\nframework built upon an inversion-free formulation to address these\nlimitations. Specifically, we semantically decompose the target prompt into\nmultiple sub-prompts, compute an independent flow for each, and aggregate them\nto form a unified editing trajectory. While we empirically observe that\ndecomposing the original flow enhances diversity in the target space,\ngenerating semantically aligned outputs still requires consistent guidance\ntoward the full target prompt. To this end, we design a projection and\nsoft-aggregation mechanism for flow, inspired by gradient conflict resolution\nin multi-task learning. This approach adaptively weights the sub-target\nvelocity fields, suppressing semantic redundancy while emphasizing distinct\ndirections, thereby preserving both diversity and consistency in the final\nedited output. Experimental results demonstrate that our method outperforms\nexisting zero-shot editing approaches in terms of semantic fidelity and\nattribute disentanglement. The code is available at\nhttps://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.\n","authors":["Sung-Hoon Yoon","Minghan Li","Gaspard Beaudouin","Congcong Wen","Muhammad Rafay Azhar","Mengyu Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25970v1.pdf","comment":"Camera-ready version for NeurIPS 2025, 10 pages (main paper)"},{"id":"http://arxiv.org/abs/2503.04852v3","updated":"2025-10-29T20:44:13Z","published":"2025-03-06T03:40:01Z","title":"CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data","summary":"  True intelligence hinges on the ability to uncover and leverage hidden causal\nrelations. Despite significant progress in AI and computer vision (CV), there\nremains a lack of benchmarks for assessing models' abilities to infer latent\ncausality from complex visual data. In this paper, we introduce\n\\textsc{\\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates\nstructured data (tables) with corresponding visual representations (images) to\nevaluate causal reasoning. Designed within a systematic framework, Causal3D\ncomprises 19 3D-scene datasets capturing diverse causal relations, views, and\nbackgrounds, enabling evaluations across scenes of varying complexity. We\nassess multiple state-of-the-art methods, including classical causal discovery,\ncausal representation learning, and large/vision-language models (LLMs/VLMs).\nOur experiments show that as causal structures grow more complex without prior\nknowledge, performance declines significantly, highlighting the challenges even\nadvanced methods face in complex causal scenarios. Causal3D serves as a vital\nresource for advancing causal reasoning in CV and fostering trustworthy AI in\ncritical domains.\n","authors":["Disheng Liu","Yiran Qiao","Wuche Liu","Yiren Lu","Yunlai Zhou","Tuo Liang","Yu Yin","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2503.04852v3.pdf","comment":"Datasets link:\n  https://huggingface.co/datasets/LLDDSS/Causal3D_Dataset"},{"id":"http://arxiv.org/abs/2505.23158v2","updated":"2025-10-29T19:53:57Z","published":"2025-05-29T06:50:57Z","title":"LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient\n  Rendering","summary":"  In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian\nSplatting that enables real-time rendering of large-scale scenes on\nmemory-constrained devices. Our approach introduces a hierarchical LOD\nrepresentation that iteratively selects optimal subsets of Gaussians based on\ncamera distance, thus largely reducing both rendering time and GPU memory\nusage. We construct each LOD level by applying a depth-aware 3D smoothing\nfilter, followed by importance-based pruning and fine-tuning to maintain visual\nfidelity. To further reduce memory overhead, we partition the scene into\nspatial chunks and dynamically load only relevant Gaussians during rendering,\nemploying an opacity-blending mechanism to avoid visual artifacts at chunk\nboundaries. Our method achieves state-of-the-art performance on both outdoor\n(Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality\nrenderings with reduced latency and memory requirements.\n","authors":["Jonas Kulhanek","Marie-Julie Rakotosaona","Fabian Manhardt","Christina Tsalicoglou","Michael Niemeyer","Torsten Sattler","Songyou Peng","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2505.23158v2.pdf","comment":"NeurIPS 2025; Web: https://lodge-gs.github.io/"},{"id":"http://arxiv.org/abs/2510.25921v1","updated":"2025-10-29T19:50:34Z","published":"2025-10-29T19:50:34Z","title":"Generative Image Restoration and Super-Resolution using Physics-Informed\n  Synthetic Data for Scanning Tunneling Microscopy","summary":"  Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and\natom manipulation, but its utility is often limited by tip degradation and slow\nserial data acquisition. Fabrication adds another layer of complexity since the\ntip is often subjected to large voltages, which may alter the shape of its\napex, requiring it to be conditioned. Here, we propose a machine learning (ML)\napproach for image repair and super-resolution to alleviate both challenges.\nUsing a dataset of only 36 pristine experimental images of Si(001):H, we\ndemonstrate that a physics-informed synthetic data generation pipeline can be\nused to train several state-of-the-art flow-matching and diffusion models.\nQuantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy\n(CMMD) score and structural similarity demonstrates that our models are able to\neffectively restore images and offer a two- to fourfold reduction in image\nacquisition time by accurately reconstructing images from sparsely sampled\ndata. Our framework has the potential to significantly increase STM\nexperimental throughput by offering a route to reducing the frequency of\ntip-conditioning procedures and to enhancing frame rates in existing high-speed\nSTM systems.\n","authors":["Nikola L. Kolev","Tommaso Rodani","Neil J. Curson","Taylor J. Z. Stock","Alberto Cazzaniga"],"pdf_url":"https://arxiv.org/pdf/2510.25921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17345v2","updated":"2025-10-29T19:47:47Z","published":"2024-06-25T07:58:47Z","title":"NerfBaselines: Consistent and Reproducible Evaluation of Novel View\n  Synthesis Methods","summary":"  Novel view synthesis is an important problem with many applications,\nincluding AR/VR, gaming, and robotic simulations. With the recent rapid\ndevelopment of Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS)\nmethods, it is becoming difficult to keep track of the current state of the art\n(SoTA) due to methods using different evaluation protocols, codebases being\ndifficult to install and use, and methods not generalizing well to novel 3D\nscenes. In our experiments, we show that even tiny differences in the\nevaluation protocols of various methods can artificially boost the performance\nof these methods. This raises questions about the validity of quantitative\ncomparisons performed in the literature. To address these questions, we propose\nNerfBaselines, an evaluation framework which provides consistent benchmarking\ntools, ensures reproducibility, and simplifies the installation and use of\nvarious methods. We validate our implementation experimentally by reproducing\nthe numbers reported in the original papers. For improved accessibility, we\nrelease a web platform that compares commonly used methods on standard\nbenchmarks. We strongly believe NerfBaselines is a valuable contribution to the\ncommunity as it ensures that quantitative results are comparable and thus truly\nmeasure progress in the field of novel view synthesis.\n","authors":["Jonas Kulhanek","Torsten Sattler"],"pdf_url":"https://arxiv.org/pdf/2406.17345v2.pdf","comment":"NeurIPS 2025 D&B; Web: https://jkulhanek.com/nerfbaselines"},{"id":"http://arxiv.org/abs/2510.24134v2","updated":"2025-10-29T19:17:39Z","published":"2025-10-28T07:19:01Z","title":"VC4VG: Optimizing Video Captions for Text-to-Video Generation","summary":"  Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels. We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements. Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/alimama-creative/VC4VG to\nsupport further research.\n","authors":["Yang Du","Zhuoran Lin","Kaiqiang Song","Biao Wang","Zhicheng Zheng","Tiezheng Ge","Bo Zheng","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2510.24134v2.pdf","comment":"Accepted by EMNLP 2025"},{"id":"http://arxiv.org/abs/2508.08186v2","updated":"2025-10-29T19:12:08Z","published":"2025-08-11T17:06:55Z","title":"KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold\n  Representation Learning","summary":"  Semantic segmentation of structural defects in civil infrastructure remains\nchallenging due to variable defect appearances, harsh imaging conditions, and\nsignificant class imbalance. Current deep learning methods, despite their\neffectiveness, typically require millions of parameters, rendering them\nimpractical for real-time inspection systems. We introduce KARMA\n(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient\nsemantic segmentation framework that models complex defect patterns through\ncompositions of one-dimensional functions rather than conventional\nconvolutions. KARMA features three technical innovations: (1) a\nparameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging\nlow-rank factorization for KAN-based feature transformation; (2) an optimized\nfeature pyramid structure with separable convolutions for multi-scale defect\nanalysis; and (3) a static-dynamic prototype mechanism that enhances feature\nrepresentation for imbalanced classes. Extensive experiments on benchmark\ninfrastructure inspection datasets demonstrate that KARMA achieves competitive\nor superior mean IoU performance compared to state-of-the-art approaches, while\nusing significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).\nOperating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for\nreal-time deployment, enabling practical automated infrastructure inspection\nsystems without compromising accuracy. The source code can be accessed at the\nfollowing URL: https://github.com/faeyelab/karma.\n","authors":["Md Meftahul Ferdaus","Mahdi Abdelguerfi","Elias Ioup","Steven Sloan","Kendall N. Niles","Ken Pathak"],"pdf_url":"https://arxiv.org/pdf/2508.08186v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2510.25901v1","updated":"2025-10-29T19:07:39Z","published":"2025-10-29T19:07:39Z","title":"BikeScenes: Online LiDAR Semantic Segmentation for Bicycles","summary":"  The vulnerability of cyclists, exacerbated by the rising popularity of faster\ne-bikes, motivates adapting automotive perception technologies for bicycle\nsafety. We use our multi-sensor 'SenseBike' research platform to develop and\nevaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the\nautomotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg\nDataset, comprising 3021 consecutive LiDAR scans around the university campus\nof the TU Delft, semantically annotated for 29 dynamic and static classes. By\nevaluating model performance, we demonstrate that fine-tuning on our BikeScenes\ndataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly\noutperforming the 13.8% obtained with SemanticKITTI pre-training alone. This\nresult underscores the necessity and effectiveness of domain-specific training.\nWe highlight key challenges specific to bicycle-mounted, hardware-constrained\nperception systems and contribute the BikeScenes dataset as a resource for\nadvancing research in cyclist-centric LiDAR segmentation.\n","authors":["Denniz Goren","Holger Caesar"],"pdf_url":"https://arxiv.org/pdf/2510.25901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25897v1","updated":"2025-10-29T18:59:17Z","published":"2025-10-29T18:59:17Z","title":"MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and\n  efficiency","summary":"  Current text-to-image generative models are trained on large uncurated\ndatasets to enable diverse generation capabilities. However, this does not\nalign well with user preferences. Recently, reward models have been\nspecifically designed to perform post-hoc selection of generated images and\nalign them to a reward, typically user preference. This discarding of\ninformative data together with the optimizing for a single reward tend to harm\ndiversity, semantic fidelity and efficiency. Instead of this post-processing,\nwe propose to condition the model on multiple reward models during training to\nlet the model learn user preferences directly. We show that this not only\ndramatically improves the visual quality of the generated images but it also\nsignificantly speeds up the training. Our proposed method, called MIRO,\nachieves state-of-the-art performances on the GenEval compositional benchmark\nand user-preference scores (PickAScore, ImageReward, HPSv2).\n","authors":["Nicolas Dufour","Lucas Degeorge","Arijit Ghosh","Vicky Kalogeiton","David Picard"],"pdf_url":"https://arxiv.org/pdf/2510.25897v1.pdf","comment":"Project page: https://nicolas-dufour.github.io/miro"},{"id":"http://arxiv.org/abs/2509.16336v2","updated":"2025-10-29T18:17:28Z","published":"2025-09-19T18:24:41Z","title":"Neural Atlas Graphs for Dynamic Scene Decomposition and Editing","summary":"  Learning editable high-resolution scene representations for dynamic scenes is\nan open problem with applications across the domains from autonomous driving to\ncreative editing - the most successful approaches today make a trade-off\nbetween editability and supporting scene complexity: neural atlases represent\ndynamic scenes as two deforming image layers, foreground and background, which\nare editable in 2D, but break down when multiple objects occlude and interact.\nIn contrast, scene graph models make use of annotated data such as masks and\nbounding boxes from autonomous-driving datasets to capture complex 3D spatial\nrelationships, but their implicit volumetric node representations are\nchallenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a\nhybrid high-resolution scene representation, where every graph node is a\nview-dependent neural atlas, facilitating both 2D appearance editing and 3D\nordering and positioning of scene elements. Fit at test-time, NAGs achieve\nstate-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR\nincrease compared to existing methods - and make environmental editing possible\nin high resolution and visual quality - creating counterfactual driving\nscenarios with new backgrounds and edited vehicle appearance. We find that the\nmethod also generalizes beyond driving scenes and compares favorably - by more\nthan 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS\nvideo dataset with a diverse set of human and animal-centric scenes.\n  Project Page: https://princeton-computational-imaging.github.io/nag/\n","authors":["Jan Philipp Schneider","Pratik Singh Bisht","Ilya Chugunov","Andreas Kolb","Michael Moeller","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2509.16336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25801v1","updated":"2025-10-29T03:42:23Z","published":"2025-10-29T03:42:23Z","title":"Metis-SPECS: Decoupling Multimodal Learning via Self-distilled\n  Preference-based Cold Start","summary":"  Reinforcement learning (RL) with verifiable rewards has recently catalyzed a\nwave of \"MLLM-r1\" approaches that bring RL to vision language models. Most\nrepresentative paradigms begin with a cold start, typically employing\nsupervised fine-tuning (SFT), to initialize the policy before RL. However,\nSFT-based cold start adopts the reasoning paradigm intertwined with task\nsolution and output format, which may induce instruction-style overfitting,\nweakens out-of-distribution generalization, and ultimately affects downstream\nRL. We revisit the cold start along two views, its training method and data\nconstruction, and introduce the Generalization Factor (GF) coefficient to\nquantify the generalization capability under different methods. Our empirical\nstudy finds that preference-based training methods (e.g. DPO) generalizes\nbetter than SFT-based methods in cold start. Motivated by this, we propose\nSPECS-a Self-distilled, Preference-based Cold Start framework that decouples\nmultimodal learning: (1) generates introspective preference data pairs via\nself-distillation, avoiding reliance on larger teachers or manual annotation;\n(2) performs preference-based training to learn, focusing on shallow,\ntransferable surface-form criteria (format, structure, style) rather than\nmemorizing content; and (3) hands off to RL with verifiable rewards for deep\nreasoning results. Experimental results across multiple multimodal benchmarks\nshow that our decoupling learning framework yields consistent performance gains\nover strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%.\nAdditional experiments indicate that SPECS contributes to reducing\nin-distribution \"stuckness,\" improving exploration, stabilizing training, and\nraising the performance ceiling.\n","authors":["Kun Chen","Peng Shi","Haibo Qiu","Zhixiong Zeng","Siqi Yang","Wenji Mao","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2510.25801v1.pdf","comment":"Project Page: https://github.com/Kwen-Chen/SPECS-VL"},{"id":"http://arxiv.org/abs/2510.25797v1","updated":"2025-10-29T01:22:42Z","published":"2025-10-29T01:22:42Z","title":"Enhancing Underwater Object Detection through Spatio-Temporal Analysis\n  and Spatial Attention Networks","summary":"  This study examines the effectiveness of spatio-temporal modeling and the\nintegration of spatial attention mechanisms in deep learning models for\nunderwater object detection. Specifically, in the first phase, the performance\nof temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with\nthe standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is\ndeveloped, through the addition of a Convolutional Block Attention Module\n(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and\nT-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the\nresearch highlights how temporal modeling improves detection accuracy in\ndynamic marine environments, particularly under conditions of sudden movements,\npartial occlusions, and gradual motion. The testing results showed that YOLOv5\nachieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM\noutperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,\nhighlighting their superior accuracy and generalization in detecting complex\nobjects. The findings demonstrate that T-YOLOv5 significantly enhances\ndetection reliability compared to the standard model, while T-YOLOv5 with CBAM\nfurther improves performance in challenging scenarios, although there is a loss\nof accuracy when it comes to simpler scenarios.\n","authors":["Sai Likhith Karri","Ansh Saxena"],"pdf_url":"https://arxiv.org/pdf/2510.25797v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2510.25770v1","updated":"2025-10-29T17:59:16Z","published":"2025-10-29T17:59:16Z","title":"E-Scores for (In)Correctness Assessment of Generative Model Outputs","summary":"  While generative models, especially large language models (LLMs), are\nubiquitous in today's world, principled mechanisms to assess their\n(in)correctness are limited. Using the conformal prediction framework, previous\nworks construct sets of LLM responses where the probability of including an\nincorrect response, or error, is capped at a desired user-defined tolerance\nlevel. However, since these methods are based on p-values, they are susceptible\nto p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the\nguarantees. We therefore leverage e-values to complement generative model\noutputs with e-scores as a measure of incorrectness. In addition to achieving\nthe same statistical guarantees as before, e-scores provide users flexibility\nin adaptively choosing tolerance levels after observing the e-scores\nthemselves, by upper bounding a post-hoc notion of error called size\ndistortion. We experimentally demonstrate their efficacy in assessing LLM\noutputs for different correctness types: mathematical factuality and property\nconstraints satisfaction.\n","authors":["Guneet S. Dhillon","Javier González","Teodora Pandeva","Alicia Curth"],"pdf_url":"https://arxiv.org/pdf/2510.25770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25769v1","updated":"2025-10-29T17:59:06Z","published":"2025-10-29T17:59:06Z","title":"Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE\n  Solutions","summary":"  Stochastic differential equations (SDEs) are well suited to modelling noisy\nand irregularly sampled time series found in finance, physics, and machine\nlearning. Traditional approaches require costly numerical solvers to sample\nbetween arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and\ntheir latent variants, which directly learn (latent) SDE transition laws using\nconditional normalising flows with architectural constraints that preserve\nproperties inherited from stochastic flows. This enables one-shot sampling\nbetween arbitrary states and yields up to two orders of magnitude speed-ups at\nlarge time gaps. Experiments on synthetic SDE simulations and on real-world\ntracking and video data show that NSFs maintain distributional accuracy\ncomparable to numerical approaches while dramatically reducing computation for\narbitrary time-point sampling.\n","authors":["Naoki Kiyohara","Edward Johns","Yingzhen Li"],"pdf_url":"https://arxiv.org/pdf/2510.25769v1.pdf","comment":"NeurIPS 2025 (poster). Project page:\n  https://nkiyohara.github.io/nsf-neurips2025/"},{"id":"http://arxiv.org/abs/2510.18905v2","updated":"2025-10-29T17:57:23Z","published":"2025-10-21T01:03:46Z","title":"3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and\n  Latency","summary":"  AI inference scaling is often tuned through 1D heuristics (a fixed reasoning\npasses) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail\nto consider cost and latency constraints. We introduce a 3D optimization\nframework that jointly calibrates accuracy, cost, and latency within a unified\ndecision space, enabling constraints-aware inference scaling. Using Monte Carlo\nsimulations across three representative scenarios and nine simulated large\nlanguage models, we evaluate four optimization methods to address the 3D\nmulti-objective optimization (MOO) problem. Framing inference scaling in MOO\nshapes a feasible space that 1D and 2D optimizations fail to capture, enabling\nenvironmentadaptive selection of the inference scaling k. Results show that\nknee-point optimization achieves the best balance, while accuracy-maximization\nremains favorable when precision is prioritized. The framework establishes a\ntheoretical foundation for deployment-aware inference scaling across diverse\noperational contexts.\n","authors":["Minseok Jung","Abhas Ricky","Muhammad Rameez Chatni"],"pdf_url":"https://arxiv.org/pdf/2510.18905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01939v3","updated":"2025-10-29T17:57:03Z","published":"2025-07-02T17:49:52Z","title":"SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars","summary":"  In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.\n","authors":["Xiaosheng Zhao","Yang Huang","Guirong Xue","Xiao Kong","Jifeng Liu","Xiaoyu Tang","Timothy C. Beers","Yuan-Sen Ting","A-Li Luo"],"pdf_url":"https://arxiv.org/pdf/2507.01939v3.pdf","comment":"27 pages, 8 figures, 5 tables. Minor update: added corrected\n  acknowledgments and corrected a misstated hyperparameter value (noted in\n  footnote) for reproducibility. Submitted to AAS Journals. Comments welcome"},{"id":"http://arxiv.org/abs/2510.25759v1","updated":"2025-10-29T17:55:17Z","published":"2025-10-29T17:55:17Z","title":"Synthetic Data Reveals Generalization Gaps in Correlated Multiple\n  Instance Learning","summary":"  Multiple instance learning (MIL) is often used in medical imaging to classify\nhigh-resolution 2D images by processing patches or classify 3D volumes by\nprocessing slices. However, conventional MIL approaches treat instances\nseparately, ignoring contextual relationships such as the appearance of nearby\npatches or slices that can be essential in real applications. We design a\nsynthetic classification task where accounting for adjacent instance features\nis crucial for accurate prediction. We demonstrate the limitations of\noff-the-shelf MIL approaches by quantifying their performance compared to the\noptimal Bayes estimator for this task, which is available in closed-form. We\nempirically show that newer correlated MIL methods still struggle to generalize\nas well as possible when trained from scratch on tens of thousands of\ninstances.\n","authors":["Ethan Harvey","Dennis Johan Loevlie","Michael C. Hughes"],"pdf_url":"https://arxiv.org/pdf/2510.25759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25755v1","updated":"2025-10-29T17:52:39Z","published":"2025-10-29T17:52:39Z","title":"MLPrE -- A tool for preprocessing and exploratory data analysis prior to\n  machine learning model construction","summary":"  With the recent growth of Deep Learning for AI, there is a need for tools to\nmeet the demand of data flowing into those models. In some cases, source data\nmay exist in multiple formats, and therefore the source data must be\ninvestigated and properly engineered for a Machine Learning model or graph\ndatabase. Overhead and lack of scalability with existing workflows limit\nintegration within a larger processing pipeline such as Apache Airflow, driving\nthe need for a robust, extensible, and lightweight tool to preprocess arbitrary\ndatasets that scales with data type and size. To address this, we present\nMachine Learning Preprocessing and Exploratory Data Analysis, MLPrE, in which\nSparkDataFrames were utilized to hold data during processing and ensure\nscalability. A generalizable JSON input file format was utilized to describe\nstepwise changes to that DataFrame. Stages were implemented for input and\noutput, filtering, basic statistics, feature engineering, and exploratory data\nanalysis. A total of 69 stages were implemented into MLPrE, of which we\nhighlight and demonstrate key stages using six diverse datasets. We further\nhighlight MLPrE's ability to independently process multiple fields in flat\nfiles and recombine them, otherwise requiring an additional pipeline, using a\nUniProt glossary term dataset. Building on this advantage, we demonstrated the\nclustering stage with available wine quality data. Lastly, we demonstrate the\npreparation of data for a graph database in the final stages of MLPrE using\nphosphosite kinase data. Overall, our MLPrE tool offers a generalizable and\nscalable tool for preprocessing and early data analysis, filling a critical\nneed for such a tool given the ever expanding use of machine learning. This\ntool serves to accelerate and simplify early stage development in larger\nworkflows.\n","authors":["David S Maxwell","Michael Darkoh","Sidharth R Samudrala","Caroline Chung","Stephanie T Schmidt","Bissan Al-Lazikani"],"pdf_url":"https://arxiv.org/pdf/2510.25755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17022v2","updated":"2025-10-29T17:52:01Z","published":"2025-10-19T22:04:57Z","title":"Curiosity-driven RL for symbolic equation solving","summary":"  We explore if RL can be useful for symbolic mathematics. Previous work showed\ncontrastive learning can solve linear equations in one variable. We show\nmodel-free PPO \\cite{schulman2017proximal} augmented with curiosity-based\nexploration and graph-based actions can solve nonlinear equations such as those\ninvolving radicals, exponentials, and trig functions. Our work suggests\ncuriosity-based exploration may be useful for general symbolic reasoning tasks.\n","authors":["Kevin P. O'Keeffe"],"pdf_url":"https://arxiv.org/pdf/2510.17022v2.pdf","comment":"Accepted at the NeurIPS 2025 MATH-AI Workshop"},{"id":"http://arxiv.org/abs/2510.25753v1","updated":"2025-10-29T17:51:57Z","published":"2025-10-29T17:51:57Z","title":"How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for\n  Transformers with MLPs","summary":"  Pretrained Transformers demonstrate remarkable in-context learning (ICL)\ncapabilities, enabling them to adapt to new tasks from demonstrations without\nparameter updates. However, theoretical studies often rely on simplified\narchitectures (e.g., omitting MLPs), data models (e.g., linear regression with\nisotropic inputs), and single-source training, limiting their relevance to\nrealistic settings. In this work, we study ICL in pretrained Transformers with\nnonlinear MLP heads on nonlinear tasks drawn from multiple data sources with\nheterogeneous input, task, and noise distributions. We analyze a model where\nthe MLP comprises two layers, with the first layer trained via a single\ngradient step and the second layer fully optimized. Under high-dimensional\nasymptotics, we prove that such models are equivalent in ICL error to\nstructured polynomial predictors, leveraging results from the theory of\nGaussian universality and orthogonal polynomials. This equivalence reveals that\nnonlinear MLPs meaningfully enhance ICL performance, particularly on nonlinear\ntasks, compared to linear baselines. It also enables a precise analysis of data\nmixing effects: we identify key properties of high-quality data sources (low\nnoise, structured covariances) and show that feature learning emerges only when\nthe task covariance exhibits sufficient structure. These results are validated\nempirically across various activation functions, model sizes, and data\ndistributions. Finally, we experiment with a real-world scenario involving\nmultilingual sentiment analysis where each language is treated as a different\nsource. Our experimental results for this case exemplify how our findings\nextend to real-world cases. Overall, our work advances the theoretical\nfoundations of ICL in Transformers and provides actionable insight into the\nrole of architecture and data in ICL.\n","authors":["Samet Demir","Zafer Dogan"],"pdf_url":"https://arxiv.org/pdf/2510.25753v1.pdf","comment":"NeurIPS 2025, 24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.18549v3","updated":"2025-10-29T17:50:44Z","published":"2025-07-24T16:13:56Z","title":"The Price equation reveals a universal force-metric-bias law of\n  algorithmic learning and natural selection","summary":"  Diverse learning algorithms, optimization methods, and natural selection\nshare a common mathematical structure, despite their apparent differences. Here\nI show that a simple notational partitioning of change by the Price equation\nreveals a universal force-metric-bias (FMB) law: $\\Delta\\mathbf{\\theta} =\n\\mathbf{M}\\,\\mathbf{f} + \\mathbf{b} + \\mathbf{\\xi}$. The force $\\mathbf{f}$\ndrives improvement in parameters, $\\Delta\\mathbf{\\theta}$, in proportion to the\nslope of performance with respect to the parameters. The metric $\\mathbf{M}$\nrescales movement by inverse curvature. The bias $\\mathbf{b}$ adds momentum or\nchanges in the frame of reference. The noise $\\mathbf{\\xi}$ enables\nexploration. This framework unifies natural selection, Bayesian updating,\nNewton's method, stochastic gradient descent, stochastic Langevin dynamics,\nAdam optimization, and most other algorithms as special cases of the same\nunderlying process. The Price equation also reveals why Fisher information,\nKullback-Leibler divergence, and d'Alembert's principle arise naturally in\nlearning dynamics. By exposing this common structure, the FMB law provides a\nprincipled foundation for understanding, comparing, and designing learning\nalgorithms across disciplines.\n","authors":["Steven A. Frank"],"pdf_url":"https://arxiv.org/pdf/2507.18549v3.pdf","comment":"Version 2: fixed definition of force in abstract; Version 3: added\n  citations and some minor editing"},{"id":"http://arxiv.org/abs/2510.25752v1","updated":"2025-10-29T17:49:40Z","published":"2025-10-29T17:49:40Z","title":"Meshless solutions of PDE inverse problems on irregular geometries","summary":"  Solving inverse and optimization problems over solutions of nonlinear partial\ndifferential equations (PDEs) on complex spatial domains is a long-standing\nchallenge. Here we introduce a method that parameterizes the solution using\nspectral bases on arbitrary spatiotemporal domains, whereby the basis is\ndefined on a hyperrectangle containing the true domain. We find the\ncoefficients of the basis expansion by solving an optimization problem whereby\nboth the equations, the boundary conditions and any optimization targets are\nenforced by a loss function, building on a key idea from Physics-Informed\nNeural Networks (PINNs). Since the representation of the function natively has\nexponential convergence, so does the solution of the optimization problem, as\nlong as it can be solved efficiently. We find empirically that the optimization\nprotocols developed for machine learning find solutions with exponential\nconvergence on a wide range of equations. The method naturally allows for the\nincorporation of data assimilation by including additional terms in the loss\nfunction, and for the efficient solution of optimization problems over the PDE\nsolutions.\n","authors":["James V. Roggeveen","Michael P. Brenner"],"pdf_url":"https://arxiv.org/pdf/2510.25752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20762v3","updated":"2025-10-29T17:47:39Z","published":"2025-03-26T17:50:13Z","title":"ASGO: Adaptive Structured Gradient Optimization","summary":"  Training deep neural networks is a structured optimization problem, because\nthe parameters are naturally represented by matrices and tensors rather than by\nvectors. Under this structural representation, it has been widely observed that\ngradients are low-rank and Hessians are approximately block diagonal. These\nstructured properties are crucial for designing efficient optimization\nalgorithms, but are not utilized by many current popular optimizers like Adam.\nIn this paper, we present a novel optimization algorithm ASGO that capitalizes\non these properties by employing a preconditioner that is adaptively updated\nusing structured gradients. By a fine-grained theoretical analysis, ASGO is\nproven to achieve superior convergence rates compared to existing structured\ngradient methods. Based on this convergence theory, we further demonstrate that\nASGO can benefit from low-rank gradients and block diagonal Hessians. We also\ndiscuss practical modifications of ASGO and empirically verify ASGO's\neffectiveness on language model tasks. Code is available at\nhttps://github.com/infinity-stars/ASGO.\n","authors":["Kang An","Yuxing Liu","Rui Pan","Yi Ren","Shiqian Ma","Donald Goldfarb","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20762v3.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2502.21269v3","updated":"2025-10-29T17:46:23Z","published":"2025-02-28T17:45:26Z","title":"Dynamical Decoupling of Generalization and Overfitting in Large\n  Two-Layer Networks","summary":"  Understanding the inductive bias and generalization properties of large\noverparametrized machine learning models requires to characterize the dynamics\nof the training algorithm. We study the learning dynamics of large two-layer\nneural networks via dynamical mean field theory, a well established technique\nof non-equilibrium statistical physics. We show that, for large network width\n$m$, and large number of samples per input dimension $n/d$, the training\ndynamics exhibits a separation of timescales which implies: $(i)$~The emergence\nof a slow time scale associated with the growth in Gaussian/Rademacher\ncomplexity of the network; $(ii)$~Inductive bias towards small complexity if\nthe initialization has small enough complexity; $(iii)$~A dynamical decoupling\nbetween feature learning and overfitting regimes; $(iv)$~A non-monotone\nbehavior of the test error, associated `feature unlearning' regime at large\ntimes.\n","authors":["Andrea Montanari","Pierfrancesco Urbani"],"pdf_url":"https://arxiv.org/pdf/2502.21269v3.pdf","comment":"88 pages; 63 pdf figures"},{"id":"http://arxiv.org/abs/2510.25739v1","updated":"2025-10-29T17:43:31Z","published":"2025-10-29T17:43:31Z","title":"Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image\n  Generation","summary":"  Autoregressive (AR) image generation models are capable of producing\nhigh-fidelity images but often suffer from slow inference due to their\ninherently sequential, token-by-token decoding process. Speculative decoding,\nwhich employs a lightweight draft model to approximate the output of a larger\nAR model, has shown promise in accelerating text generation without\ncompromising quality. However, its application to image generation remains\nlargely underexplored. The challenges stem from a significantly larger sampling\nspace, which complicates the alignment between the draft and target model\noutputs, coupled with the inadequate use of the two-dimensional spatial\nstructure inherent in images, thereby limiting the modeling of local\ndependencies. To overcome these challenges, we introduce Hawk, a new approach\nthat harnesses the spatial structure of images to guide the speculative model\ntoward more accurate and efficient predictions. Experimental results on\nmultiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR\nmodels, while preserving both image fidelity and diversity.\n","authors":["Zhi-Kai Chen","Jun-Peng Jiang","Han-Jia Ye","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2510.25739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25731v1","updated":"2025-10-29T17:37:27Z","published":"2025-10-29T17:37:27Z","title":"LieSolver: A PDE-constrained solver for IBVPs using Lie symmetries","summary":"  We introduce a method for efficiently solving initial-boundary value problems\n(IBVPs) that uses Lie symmetries to enforce the associated partial differential\nequation (PDE) exactly by construction. By leveraging symmetry transformations,\nthe model inherently incorporates the physical laws and learns solutions from\ninitial and boundary data. As a result, the loss directly measures the model's\naccuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our\nmethod enables rigorous error estimation. The approach yields compact models,\nfacilitating an efficient optimization. We implement LieSolver and demonstrate\nits application to linear homogeneous PDEs with a range of initial conditions,\nshowing that it is faster and more accurate than physics-informed neural\nnetworks (PINNs). Overall, our method improves both computational efficiency\nand the reliability of predictions for PDE-constrained problems.\n","authors":["René P. Klausen","Ivan Timofeev","Johannes Frank","Jonas Naujoks","Thomas Wiegand","Sebastian Lapuschkin","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2510.25731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25729v1","updated":"2025-10-29T17:34:10Z","published":"2025-10-29T17:34:10Z","title":"Physics-Guided Conditional Diffusion Networks for Microwave Image\n  Reconstruction","summary":"  A conditional latent-diffusion based framework for solving the\nelectromagnetic inverse scattering problem associated with microwave imaging is\nintroduced. This generative machine-learning model explicitly mirrors the\nnon-uniqueness of the ill-posed inverse problem. Unlike existing inverse\nsolvers utilizing deterministic machine learning techniques that produce a\nsingle reconstruction, the proposed latent-diffusion model generates multiple\nplausible permittivity maps conditioned on measured scattered-field data,\nthereby generating several potential instances in the range-space of the\nnon-unique inverse mapping. A forward electromagnetic solver is integrated into\nthe reconstruction pipeline as a physics-based evaluation mechanism. The space\nof candidate reconstructions form a distribution of possibilities consistent\nwith the conditioning data and the member of this space yielding the lowest\nscattered-field data discrepancy between the predicted and measured scattered\nfields is reported as the final solution. Synthetic and experimental labeled\ndatasets are used for training and evaluation of the model. An innovative\nlabeled synthetic dataset is created that exemplifies a varied set of\nscattering features. Training of the model using this new dataset produces high\nquality permittivity reconstructions achieving improved generalization with\nexcellent fidelity to shape recognition. The results highlight the potential of\nhybrid generative physics frameworks as a promising direction for robust,\ndata-driven microwave imaging.\n","authors":["Shirin Chehelgami","Joe LoVetri","Vahab Khoshdel"],"pdf_url":"https://arxiv.org/pdf/2510.25729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.21355v2","updated":"2025-10-29T17:23:18Z","published":"2025-06-26T15:08:18Z","title":"SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context\n  Learning","summary":"  Multimodal in-context learning (ICL) remains underexplored despite\nsignificant potential for domains such as medicine. Clinicians routinely\nencounter diverse, specialized tasks requiring adaptation from limited\nexamples, such as drawing insights from a few relevant prior cases or\nconsidering a constrained set of differential diagnoses. While multimodal large\nlanguage models (MLLMs) have shown advances in medical visual question\nanswering (VQA), their ability to learn multimodal tasks from context is\nlargely unknown. We introduce SMMILE, the first expert-driven multimodal ICL\nbenchmark for medical tasks. Eleven medical experts curated problems, each\nincluding a multimodal query and multimodal in-context examples as task\ndemonstrations. SMMILE encompasses 111 problems (517 question-image-answer\ntriplets) covering 6 medical specialties and 13 imaging modalities. We further\nintroduce SMMILE++, an augmented variant with 1038 permuted problems. A\ncomprehensive evaluation of 15 MLLMs demonstrates that most models exhibit\nmoderate to poor multimodal ICL ability in medical tasks. In open-ended\nevaluations, ICL contributes only an 8% average improvement over zero-shot on\nSMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant\nin-context examples: even a single noisy or irrelevant example can degrade\nperformance by up to 9.5%. Moreover, we observe that MLLMs are affected by a\nrecency bias, where placing the most relevant example last can lead to\nsubstantial performance improvements of up to 71%. Our findings highlight\ncritical limitations and biases in current MLLMs when learning multimodal\nmedical tasks from context. SMMILE is available at\nhttps://smmile-benchmark.github.io.\n","authors":["Melanie Rieff","Maya Varma","Ossian Rabow","Subathra Adithan","Julie Kim","Ken Chang","Hannah Lee","Nidhi Rohatgi","Christian Bluethgen","Mohamed S. Muneer","Jean-Benoit Delbrouck","Michael Moor"],"pdf_url":"https://arxiv.org/pdf/2506.21355v2.pdf","comment":"NeurIPS 2025 (Datasets & Benchmarks Track)"},{"id":"http://arxiv.org/abs/2502.02270v2","updated":"2025-10-29T17:15:10Z","published":"2025-02-04T12:31:00Z","title":"Exact Sequence Interpolation with Transformers","summary":"  We prove that transformers can exactly interpolate datasets of finite input\nsequences in $\\mathbb{R}^d$, $d\\geq 2$, with corresponding output sequences of\nsmaller or equal length. Specifically, given $N$ sequences of arbitrary but\nfinite lengths in $\\mathbb{R}^d$ and output sequences of lengths $m^1, \\dots,\nm^N \\in \\mathbb{N}$, we construct a transformer with $\\mathcal{O}(\\sum_{j=1}^N\nm^j)$ blocks and $\\mathcal{O}(d \\sum_{j=1}^N m^j)$ parameters that exactly\ninterpolates the dataset. Our construction provides complexity estimates that\nare independent of the input sequence length, by alternating feed-forward and\nself-attention layers and by capitalizing on the clustering effect inherent to\nthe latter. Our novel constructive method also uses low-rank parameter matrices\nin the self-attention mechanism, a common feature of practical transformer\nimplementations. These results are first established in the hardmax\nself-attention setting, where the geometric structure permits an explicit and\nquantitative analysis, and are then extended to the softmax setting. Finally,\nwe demonstrate the applicability of our exact interpolation construction to\nlearning problems, in particular by providing convergence guarantees to a\nglobal minimizer under regularized training strategies. Our analysis\ncontributes to the theoretical understanding of transformer models, offering an\nexplanation for their excellent performance in exact sequence-to-sequence\ninterpolation tasks.\n","authors":["Albert Alcalde","Giovanni Fantuzzi","Enrique Zuazua"],"pdf_url":"https://arxiv.org/pdf/2502.02270v2.pdf","comment":"27 pages, 9 figures. Funded by the European Union (Horizon Europe\n  MSCA project ModConFlex, grant number 101073558)"},{"id":"http://arxiv.org/abs/2510.25704v1","updated":"2025-10-29T17:12:21Z","published":"2025-10-29T17:12:21Z","title":"Scaling flow-based approaches for topology sampling in $\\mathrm{SU}(3)$\n  gauge theory","summary":"  We develop a methodology based on out-of-equilibrium simulations to mitigate\ntopological freezing when approaching the continuum limit of lattice gauge\ntheories. We reduce the autocorrelation of the topological charge employing\nopen boundary conditions, while removing exactly their unphysical effects using\na non-equilibrium Monte Carlo approach in which periodic boundary conditions\nare gradually switched on. We perform a detailed analysis of the computational\ncosts of this strategy in the case of the four-dimensional $\\mathrm{SU}(3)$\nYang-Mills theory. After achieving full control of the scaling, we outline a\nclear strategy to sample topology efficiently in the continuum limit, which we\ncheck at lattice spacings as small as $0.045$ fm. We also generalize this\napproach by designing a customized Stochastic Normalizing Flow for evolutions\nin the boundary conditions, obtaining superior performances with respect to the\npurely stochastic non-equilibrium approach, and paving the way for more\nefficient future flow-based solutions.\n","authors":["Claudio Bonanno","Andrea Bulgarelli","Elia Cellini","Alessandro Nada","Dario Panfalone","Davide Vadacchino","Lorenzo Verzichelli"],"pdf_url":"https://arxiv.org/pdf/2510.25704v1.pdf","comment":"1+39 pages, 14 figures"},{"id":"http://arxiv.org/abs/2310.02806v3","updated":"2025-10-29T17:05:43Z","published":"2023-10-04T13:33:37Z","title":"MP-FVM: Enhancing Finite Volume Method for Water Infiltration Modeling\n  in Unsaturated Soils via Message-passing Encoder-decoder Network","summary":"  The spatiotemporal water flow dynamics in unsaturated soils can generally be\nmodeled by the Richards equation. To overcome the computational challenges\nassociated with solving this highly nonlinear partial differential equation\n(PDE), we present a novel solution algorithm, which we name as the MP-FVM\n(Message Passing-Finite Volume Method), to holistically integrate adaptive\nfixed-point iteration scheme, encoder-decoder neural network architecture,\nSobolev training, and message passing mechanism in a finite volume\ndiscretization framework. We thoroughly discuss the need and benefits of\nintroducing these components to achieve synergistic improvements in accuracy\nand stability of the solution. We also show that our MP-FVM algorithm can\naccurately solve the mixed-form $n$-dimensional Richards equation with\nguaranteed convergence under reasonable assumptions. Through several\nillustrative examples, we demonstrate that our MP-FVM algorithm not only\nachieves superior accuracy, but also better preserves the underlying physical\nlaws and mass conservation of the Richards equation compared to\nstate-of-the-art solution algorithms and the commercial HYDRUS solver.\n","authors":["Zeyuan Song","Zheyu Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.02806v3.pdf","comment":"36 pages, 14 figures, Accepted by Computers and Geotechnics"},{"id":"http://arxiv.org/abs/2510.25696v1","updated":"2025-10-29T17:00:36Z","published":"2025-10-29T17:00:36Z","title":"Convolutional Spiking-based GRU Cell for Spatio-temporal Data","summary":"  Spike-based temporal messaging enables SNNs to efficiently process both\npurely temporal and spatio-temporal time-series or event-driven data. Combining\nSNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,\ngives rise to a robust framework for sequential data processing; however,\ntraditional RNNs often lose local details when handling long sequences.\nPrevious approaches, such as SpikGRU, fail to capture fine-grained local\ndependencies in event-based spatio-temporal data. In this paper, we introduce\nthe Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional\noperations to preserve local structure and dependencies while integrating the\ntemporal precision of spiking neurons with the efficient gating mechanisms of\nGRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,\nSHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our\nexperiments show that CS-GRU outperforms state-of-the-art GRU variants by an\naverage of 4.35%, achieving over 90% accuracy on sequential tasks and up to\n99.31% on MNIST. It is worth noting that our solution achieves 69% higher\nefficiency compared to SpikGRU. The code is available at:\nhttps://github.com/YesmineAbdennadher/CS-GRU.\n","authors":["Yesmine Abdennadher","Eleonora Cicciarella","Michele Rossi"],"pdf_url":"https://arxiv.org/pdf/2510.25696v1.pdf","comment":"6 pages, 1 figure. Published in 2025 IEEE International Workshop On\n  Machine Learning for Signal Processing, Aug. 31-Sep. 3, 2025, Istanbul,\n  Turkey"},{"id":"http://arxiv.org/abs/2510.23323v2","updated":"2025-10-29T16:59:39Z","published":"2025-10-24T14:47:49Z","title":"Towards Scaling Deep Neural Networks with Predictive Coding: Theory and\n  Practice","summary":"  Backpropagation (BP) is the standard algorithm for training the deep neural\nnetworks that power modern artificial intelligence including large language\nmodels. However, BP is energy inefficient and unlikely to be implemented by the\nbrain. This thesis studies an alternative, potentially more efficient\nbrain-inspired algorithm called predictive coding (PC). Unlike BP, PC networks\n(PCNs) perform inference by iterative equilibration of neuron activities before\nlearning or weight updates. Recent work has suggested that this iterative\ninference procedure provides a range of benefits over BP, such as faster\ntraining. However, these advantages have not been consistently observed, the\ninference and learning dynamics of PCNs are still poorly understood, and deep\nPCNs remain practically untrainable. Here, we make significant progress towards\nscaling PCNs by taking a theoretical approach grounded in optimisation theory.\nFirst, we show that the learning dynamics of PC can be understood as an\napproximate trust-region method using second-order information, despite\nexplicitly using only first-order local updates. Second, going beyond this\napproximation, we show that PC can in principle make use of arbitrarily\nhigher-order information, such that for feedforward networks the effective\nlandscape on which PC learns is far more benign and robust to vanishing\ngradients than the (mean squared error) loss landscape. Third, motivated by a\nstudy of the inference dynamics of PCNs, we propose a new parameterisation\ncalled \"$\\mu$PC\", which for the first time allows stable training of 100+ layer\nnetworks with little tuning and competitive performance on simple tasks.\nOverall, this thesis significantly advances our fundamental understanding of\nthe inference and learning dynamics of PCNs, while highlighting the need for\nfuture research to focus on hardware co-design if PC is to compete with BP at\nscale.\n","authors":["Francesco Innocenti"],"pdf_url":"https://arxiv.org/pdf/2510.23323v2.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2510.25693v1","updated":"2025-10-29T16:57:54Z","published":"2025-10-29T16:57:54Z","title":"PyDPF: A Python Package for Differentiable Particle Filtering","summary":"  State-space models (SSMs) are a widely used tool in time series analysis. In\nthe complex systems that arise from real-world data, it is common to employ\nparticle filtering (PF), an efficient Monte Carlo method for estimating the\nhidden state corresponding to a sequence of observations. Applying particle\nfiltering requires specifying both the parametric form and the parameters of\nthe system, which are often unknown and must be estimated. Gradient-based\noptimisation techniques cannot be applied directly to standard particle\nfilters, as the filters themselves are not differentiable. However, several\nrecently proposed methods modify the resampling step to make particle filtering\ndifferentiable. In this paper, we present an implementation of several such\ndifferentiable particle filters (DPFs) with a unified API built on the popular\nPyTorch framework. Our implementation makes these algorithms easily accessible\nto a broader research community and facilitates straightforward comparison\nbetween them. We validate our framework by reproducing experiments from several\nexisting studies and demonstrate how DPFs can be applied to address several\ncommon challenges with state space modelling.\n","authors":["John-Joseph Brady","Benjamin Cox","Víctor Elvira","Yunpeng Li"],"pdf_url":"https://arxiv.org/pdf/2510.25693v1.pdf","comment":"42 pages, 0 figures, under review at the Journal of Statistical\n  Software, the python package can be found at https://pypi.org/project/pydpf/\n  , the full documentation at\n  https://python-dpf.readthedocs.io/en/latest/#documentation-index , and the\n  source code including experiments and replication material at\n  https://github.com/John-JoB/pydpf"},{"id":"http://arxiv.org/abs/2510.25692v1","updated":"2025-10-29T16:57:33Z","published":"2025-10-29T16:57:33Z","title":"A Configuration-First Framework for Reproducible, Low-Code Localization","summary":"  Machine learning is increasingly permeating radio-based localization\nservices. To keep results credible and comparable, everyday workflows should\nmake rigorous experiment specification and exact repeatability the default,\nwithout blocking advanced experimentation. However, in practice, researchers\nface a three-way gap that could be filled by a framework that offers (i) low\ncoding effort for end-to-end studies, (ii) reproducibility by default including\nversioned code, data, and configurations, controlled randomness, isolated runs,\nand recorded artifacts, and (iii) built-in extensibility so new models,\nmetrics, and stages can be added with minimal integration effort. Existing\ntools rarely deliver all three for machine learning in general and localization\nworkflows in particular. In this paper we introduce LOCALIZE, a low-code,\nconfiguration-first framework for radio localization in which experiments are\ndeclared in human-readable configuration, a workflow orchestrator runs\nstandardized pipelines from data preparation to reporting, and all artifacts,\nsuch as datasets, models, metrics, and reports, are versioned. The\npreconfigured, versioned datasets reduce initial setup and boilerplate,\nspeeding up model development and evaluation. The design, with clear extension\npoints, allows experts to add components without reworking the infrastructure.\nIn a qualitative comparison and a head-to-head study against a plain Jupyter\nnotebook baseline, we show that the framework reduces authoring effort while\nmaintaining comparable runtime and memory behavior. Furthermore, using a\nBluetooth Low Energy dataset, we show that scaling across training data (1x to\n10x) keeps orchestration overheads bounded as data grows. Overall, the\nframework makes reproducible machine-learning-based localization\nexperimentation practical, accessible, and extensible.\n","authors":["Tim Strnad","Blaž Bertalanič","Carolina Fortuna"],"pdf_url":"https://arxiv.org/pdf/2510.25692v1.pdf","comment":"20 pages, 7 figures. Preprint submitted to ACM Transactions on\n  Software Engineering and Methodology (TOSEM), 2025"},{"id":"http://arxiv.org/abs/2510.25687v1","updated":"2025-10-29T16:50:54Z","published":"2025-10-29T16:50:54Z","title":"Model Inversion Attacks Meet Cryptographic Fuzzy Extractors","summary":"  Model inversion attacks pose an open challenge to privacy-sensitive\napplications that use machine learning (ML) models. For example, face\nauthentication systems use modern ML models to compute embedding vectors from\nface images of the enrolled users and store them. If leaked, inversion attacks\ncan accurately reconstruct user faces from the leaked vectors. There is no\nsystematic characterization of properties needed in an ideal defense against\nmodel inversion, even for the canonical example application of a face\nauthentication system susceptible to data breaches, despite a decade of\nbest-effort solutions.\n  In this paper, we formalize the desired properties of a provably strong\ndefense against model inversion and connect it, for the first time, to the\ncryptographic concept of fuzzy extractors. We further show that existing fuzzy\nextractors are insecure for use in ML-based face authentication. We do so\nthrough a new model inversion attack called PIPE, which achieves a success rate\nof over 89% in most cases against prior schemes. We then propose L2FE-Hash, the\nfirst candidate fuzzy extractor which supports standard Euclidean distance\ncomparators as needed in many ML-based applications, including face\nauthentication. We formally characterize its computational security guarantees,\neven in the extreme threat model of full breach of stored secrets, and\nempirically show its usable accuracy in face authentication for practical face\ndistributions. It offers attack-agnostic security without requiring any\nre-training of the ML model it protects. Empirically, it nullifies both prior\nstate-of-the-art inversion attacks as well as our new PIPE attack.\n","authors":["Mallika Prabhakar","Louise Xu","Prateek Saxena"],"pdf_url":"https://arxiv.org/pdf/2510.25687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02804v3","updated":"2025-10-29T16:49:31Z","published":"2023-12-05T14:44:58Z","title":"Score-Aware Policy-Gradient and Performance Guarantees using Local\n  Lyapunov Stability","summary":"  In this paper, we introduce a policy-gradient method for model-based\nreinforcement learning (RL) that exploits a type of stationary distributions\ncommonly obtained from Markov decision processes (MDPs) in stochastic networks,\nqueueing systems, and statistical mechanics. Specifically, when the stationary\ndistribution of the MDP belongs to an exponential family that is parametrized\nby policy parameters, we can improve existing policy gradient methods for\naverage-reward RL. Our key identification is a family of gradient estimators,\ncalled score-aware gradient estimators (SAGEs), that enable policy gradient\nestimation without relying on value-function estimation in the aforementioned\nsetting. We show that SAGE-based policy-gradient locally converges, and we\nobtain its regret. This includes cases when the state space of the MDP is\ncountable and unstable policies can exist. Under appropriate assumptions such\nas starting sufficiently close to a maximizer and the existence of a local\nLyapunov function, the policy under SAGE-based stochastic gradient ascent has\nan overwhelming probability of converging to the associated optimal policy.\nFurthermore, we conduct a numerical comparison between a SAGE-based\npolicy-gradient method and an actor-critic method on several examples inspired\nfrom stochastic networks, queueing systems, and models derived from statistical\nphysics. Our results demonstrate that a SAGE-based method finds\nclose-to-optimal policies faster than an actor-critic method.\n","authors":["Céline Comte","Matthieu Jonckheere","Jaron Sanders","Albert Senen-Cerda"],"pdf_url":"https://arxiv.org/pdf/2312.02804v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25683v1","updated":"2025-10-29T16:47:24Z","published":"2025-10-29T16:47:24Z","title":"Graph Network-based Structural Simulator: Graph Neural Networks for\n  Structural Dynamics","summary":"  Graph Neural Networks (GNNs) have recently been explored as surrogate models\nfor numerical simulations. While their applications in computational fluid\ndynamics have been investigated, little attention has been given to structural\nproblems, especially for dynamic cases. To address this gap, we introduce the\nGraph Network-based Structural Simulator (GNSS), a GNN framework for surrogate\nmodeling of dynamic structural problems.\n  GNSS follows the encode-process-decode paradigm typical of GNN-based machine\nlearning models, and its design makes it particularly suited for dynamic\nsimulations thanks to three key features: (i) expressing node kinematics in\nnode-fixed local frames, which avoids catastrophic cancellation in\nfinite-difference velocities; (ii) employing a sign-aware regression loss,\nwhich reduces phase errors in long rollouts; and (iii) using a\nwavelength-informed connectivity radius, which optimizes graph construction.\n  We evaluate GNSS on a case study involving a beam excited by a 50kHz\nHanning-modulated pulse. The results show that GNSS accurately reproduces the\nphysics of the problem over hundreds of timesteps and generalizes to unseen\nloading conditions, where existing GNNs fail to converge or deliver meaningful\npredictions.\n  Compared with explicit finite element baselines, GNSS achieves substantial\ninference speedups while preserving spatial and temporal fidelity. These\nfindings demonstrate that locality-preserving GNNs with physics-consistent\nupdate rules are a competitive alternative for dynamic, wave-dominated\nstructural simulations.\n","authors":["Alessandro Lucchetti","Francesco Cadini","Marco Giglio","Luca Lomazzi"],"pdf_url":"https://arxiv.org/pdf/2510.25683v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2510.25674v1","updated":"2025-10-29T16:42:07Z","published":"2025-10-29T16:42:07Z","title":"Mechanistic Interpretability of RNNs emulating Hidden Markov Models","summary":"  Recurrent neural networks (RNNs) provide a powerful approach in neuroscience\nto infer latent dynamics in neural populations and to generate hypotheses about\nthe neural computations underlying behavior. However, past work has focused on\nrelatively simple, input-driven, and largely deterministic behaviors - little\nis known about the mechanisms that would allow RNNs to generate the richer,\nspontaneous, and potentially stochastic behaviors observed in natural settings.\nModeling with Hidden Markov Models (HMMs) has revealed a segmentation of\nnatural behaviors into discrete latent states with stochastic transitions\nbetween them, a type of dynamics that may appear at odds with the continuous\nstate spaces implemented by RNNs. Here we first show that RNNs can replicate\nHMM emission statistics and then reverse-engineer the trained networks to\nuncover the mechanisms they implement. In the absence of inputs, the activity\nof trained RNNs collapses towards a single fixed point. When driven by\nstochastic input, trajectories instead exhibit noise-sustained dynamics along\nclosed orbits. Rotation along these orbits modulates the emission probabilities\nand is governed by transitions between regions of slow, noise-driven dynamics\nconnected by fast, deterministic transitions. The trained RNNs develop highly\nstructured connectivity, with a small set of \"kick neurons\" initiating\ntransitions between these regions. This mechanism emerges during training as\nthe network shifts into a regime of stochastic resonance, enabling it to\nperform probabilistic computations. Analyses across multiple HMM architectures\n- fully connected, cyclic, and linear-chain - reveal that this solution\ngeneralizes through the modular reuse of the same dynamical motif, suggesting a\ncompositional principle by which RNNs can emulate complex discrete latent\ndynamics.\n","authors":["Elia Torre","Michele Viscione","Lucas Pompe","Benjamin F Grewe","Valerio Mante"],"pdf_url":"https://arxiv.org/pdf/2510.25674v1.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25670v1","updated":"2025-10-29T16:36:00Z","published":"2025-10-29T16:36:00Z","title":"Spectral Perturbation Bounds for Low-Rank Approximation with\n  Applications to Privacy","summary":"  A central challenge in machine learning is to understand how noise or\nmeasurement errors affect low-rank approximations, particularly in the spectral\nnorm. This question is especially important in differentially private low-rank\napproximation, where one aims to preserve the top-$p$ structure of a\ndata-derived matrix while ensuring privacy. Prior work often analyzes Frobenius\nnorm error or changes in reconstruction quality, but these metrics can over- or\nunder-estimate true subspace distortion. The spectral norm, by contrast,\ncaptures worst-case directional error and provides the strongest utility\nguarantees. We establish new high-probability spectral-norm perturbation bounds\nfor symmetric matrices that refine the classical Eckart--Young--Mirsky theorem\nand explicitly capture interactions between a matrix $A \\in \\mathbb{R}^{n\n\\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and\nnorm conditions, our bounds yield sharp estimates for $\\|(A + E)_p - A_p\\|$,\nwhere $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up\nto a factor of $\\sqrt{n}$. As an application, we derive improved utility\nguarantees for differentially private PCA, resolving an open problem in the\nliterature. Our analysis relies on a novel contour bootstrapping method from\ncomplex analysis and extends it to a broad class of spectral functionals,\nincluding polynomials and matrix exponentials. Empirical results on real-world\ndatasets confirm that our bounds closely track the actual spectral error under\ndiverse perturbation regimes.\n","authors":["Phuc Tran","Nisheeth K. Vishnoi","Van H. Vu"],"pdf_url":"https://arxiv.org/pdf/2510.25670v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.21717v5","updated":"2025-10-29T16:25:55Z","published":"2025-05-27T20:02:59Z","title":"Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient\n  Sequence Modeling","summary":"  We present LrcSSM, a $\\textit{non-linear}$ recurrent model that processes\nlong sequences as fast as today's linear state-space layers. By forcing its\nJacobian matrix to be diagonal, the full sequence can be solved in parallel,\ngiving $\\mathcal{O}(TD)$ time and memory and only $\\mathcal{O}(\\log T)$\nsequential depth, for input-sequence length $T$ and a state dimension $D$.\nMoreover, LrcSSM offers a formal gradient-stability guarantee that other\ninput-varying systems such as Liquid-S4 and Mamba do not provide. Importantly,\nthe diagonal Jacobian structure of our model results in no performance loss\ncompared to the original model with dense Jacobian, and the approach can be\ngeneralized to other non-linear recurrent models, demonstrating broader\napplicability. On a suite of long-range forecasting tasks, we demonstrate that\nLrcSSM outperforms Transformers, LRU, S5, and Mamba.\n","authors":["Mónika Farsang","Ramin Hasani","Daniela Rus","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2505.21717v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25657v1","updated":"2025-10-29T16:22:32Z","published":"2025-10-29T16:22:32Z","title":"Subgraph Federated Learning via Spectral Methods","summary":"  We consider the problem of federated learning (FL) with graph-structured data\ndistributed across multiple clients. In particular, we address the prevalent\nscenario of interconnected subgraphs, where interconnections between clients\nsignificantly influence the learning process. Existing approaches suffer from\ncritical limitations, either requiring the exchange of sensitive node\nembeddings, thereby posing privacy risks, or relying on\ncomputationally-intensive steps, which hinders scalability. To tackle these\nchallenges, we propose FedLap, a novel framework that leverages global\nstructure information via Laplacian smoothing in the spectral domain to\neffectively capture inter-node dependencies while ensuring privacy and\nscalability. We provide a formal analysis of the privacy of FedLap,\ndemonstrating that it preserves privacy. Notably, FedLap is the first subgraph\nFL scheme with strong privacy guarantees. Extensive experiments on benchmark\ndatasets demonstrate that FedLap achieves competitive or superior utility\ncompared to existing techniques.\n","authors":["Javad Aliakbari","Johan Östman","Ashkan Panahi","Alexandre Graell i Amat"],"pdf_url":"https://arxiv.org/pdf/2510.25657v1.pdf","comment":"To be presented at The Annual Conference on Neural Information\n  Processing Systems (NeurIPS) 2025"},{"id":"http://arxiv.org/abs/2510.25648v1","updated":"2025-10-29T16:09:24Z","published":"2025-10-29T16:09:24Z","title":"Continuous subsurface property retrieval from sparse radar observations\n  using physics informed neural networks","summary":"  Estimating subsurface dielectric properties is essential for applications\nranging from environmental surveys of soils to nondestructive evaluation of\nconcrete in infrastructure. Conventional wave inversion methods typically\nassume few discrete homogeneous layers and require dense measurements or strong\nprior knowledge of material boundaries, limiting scalability and accuracy in\nrealistic settings where properties vary continuously. We present a physics\ninformed machine learning framework that reconstructs subsurface permittivity\nas a fully neural, continuous function of depth, trained to satisfy both\nmeasurement data and Maxwells equations. We validate the framework with both\nsimulations and custom built radar experiments on multilayered natural\nmaterials. Results show close agreement with in-situ permittivity measurements\n(R^2=0.93), with sensitivity to even subtle variations (Delta eps_r=2).\nParametric analysis reveals that accurate profiles can be recovered with as few\nas three strategically placed sensors in two layer systems. This approach\nreframes subsurface inversion from boundary-driven to continuous property\nestimation, enabling accurate characterization of smooth permittivity\nvariations and advancing electromagnetic imaging using low cost radar systems.\n","authors":["Ishfaq Aziz","Mohamad Alipour"],"pdf_url":"https://arxiv.org/pdf/2510.25648v1.pdf","comment":"22 pages, 9 main text figures + 2 supplementary figures"},{"id":"http://arxiv.org/abs/2507.14785v2","updated":"2025-10-29T15:56:28Z","published":"2025-07-20T02:00:21Z","title":"Exploring the In-Context Learning Capabilities of LLMs for Money\n  Laundering Detection in Financial Graphs","summary":"  The complexity and interconnectivity of entities involved in money laundering\ndemand investigative reasoning over graph-structured data. This paper explores\nthe use of large language models (LLMs) as reasoning engines over localized\nsubgraphs extracted from a financial knowledge graph. We propose a lightweight\npipeline that retrieves k-hop neighborhoods around entities of interest,\nserializes them into structured text, and prompts an LLM via few-shot\nin-context learning to assess suspiciousness and generate justifications. Using\nsynthetic anti-money laundering (AML) scenarios that reflect common laundering\nbehaviors, we show that LLMs can emulate analyst-style logic, highlight red\nflags, and provide coherent explanations. While this study is exploratory, it\nillustrates the potential of LLM-based graph reasoning in AML and lays\ngroundwork for explainable, language-driven financial crime analytics.\n","authors":["Erfan Pirmorad"],"pdf_url":"https://arxiv.org/pdf/2507.14785v2.pdf","comment":"Accepted at AI4FCF-ICDM 2025"},{"id":"http://arxiv.org/abs/2510.23965v2","updated":"2025-10-29T15:51:35Z","published":"2025-10-28T00:42:38Z","title":"The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity","summary":"  Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.\n","authors":["Ali Aouad","Aymane El Gadarri","Vivek F. Farias"],"pdf_url":"https://arxiv.org/pdf/2510.23965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00812v4","updated":"2025-10-29T15:40:34Z","published":"2025-05-01T19:12:58Z","title":"Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic\n  Optimization","summary":"  Recent studies indicate that deep neural networks degrade in generalization\nperformance under noisy supervision. Existing methods focus on isolating clean\nsubsets or correcting noisy labels, facing limitations such as high\ncomputational costs, heavy hyperparameter tuning process, and coarse-grained\noptimization. To address these challenges, we propose a novel two-stage noisy\nlearning framework that enables instance-level optimization through a\ndynamically weighted loss function, avoiding hyperparameter tuning. To obtain\nstable and accurate information about noise modeling, we introduce a simple yet\neffective metric, termed wrong event, which dynamically models the cleanliness\nand difficulty of individual samples while maintaining computational costs. Our\nframework first collects wrong event information and builds a strong base\nmodel. Then we perform noise-robust training on the base model, using a\nprobabilistic model to handle the wrong event information of samples.\nExperiments on five synthetic and real-world LNL benchmarks demonstrate our\nmethod surpasses state-of-the-art methods in performance, achieves a nearly 75%\nreduction in computational time and improves model scalability.\n","authors":["Kuan Zhang","Chengliang Chai","Jingzhe Xu","Chi Zhang","Han Han","Ye Yuan","Guoren Wang","Lei Cao"],"pdf_url":"https://arxiv.org/pdf/2505.00812v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23117v2","updated":"2025-10-29T15:34:30Z","published":"2025-05-29T05:37:53Z","title":"Decom-Renorm-Merge: Model Merging on the Right Space Improves\n  Multitasking","summary":"  In the era of large-scale training, model merging has evolved into a tool for\ncreating multitasking models efficiently. It enables the knowledge of models to\nbe fused, without the need for heavy computation as required in traditional\nmultitask learning. Existing merging methods often assume that entries at\nidentical positions in weight matrices serve the same function, enabling\nstraightforward entry-wise comparison and merging. However, this assumption\noverlooks the complexity of finetuned neural networks, where neurons may\ndevelop distinct feature compositions, making direct entry-wise merging\nproblematic. We present Decom-Renorm-Merge (DRM), a simple yet effective\napproach that leverages Singular Value Decomposition to decompose and\ncoordinate weight matrices into an aligned joint space, where entry-wise\nmerging becomes possible. We showcase the effectiveness of DRM across various\nsettings ranging from smaller encoder-based such as ViT and DeBERTa,\nencoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B.\nOur experimental results show that DRM outperforms several state-of-the-art\nmerging techniques across full finetuning and low-rank adaptation settings.\nMoreover, our analysis reveals renormalization as the crucial component for\ncreating a robust and even joint space for merging, significantly contributing\nto the method's performance.\n","authors":["Yuatyong Chaichana","Thanapat Trachu","Peerat Limkonchotiwat","Konpat Preechakul","Tirasan Khandhawit","Ekapol Chuangsuwanich"],"pdf_url":"https://arxiv.org/pdf/2505.23117v2.pdf","comment":"Code and models are available at\n  https://github.com/yophis/decom-renorm-merge"},{"id":"http://arxiv.org/abs/2510.01850v3","updated":"2025-10-29T15:33:51Z","published":"2025-10-02T09:47:56Z","title":"NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset\n  for Narrowband Powerline Communications","summary":"  To effectively process impulse noise for narrowband powerline communications\n(NB-PLCs) transceivers, capturing comprehensive statistics of nonperiodic\nasynchronous impulsive noise (APIN) is a critical task. However, existing\nmathematical noise generative models only capture part of the characteristics\nof noise. In this study, we propose a novel generative adversarial network\n(GAN) called noise generation GAN (NGGAN) that learns the complicated\ncharacteristics of practically measured noise samples for data synthesis. To\nclosely match the statistics of complicated noise over the NB-PLC systems, we\nmeasured the NB-PLC noise via the analog coupling and bandpass filtering\ncircuits of a commercial NB-PLC modem to build a realistic dataset. To train\nNGGAN, we adhere to the following principles: 1) we design the length of input\nsignals that the NGGAN model can fit to facilitate cyclostationary noise\ngeneration; 2) the Wasserstein distance is used as a loss function to enhance\nthe similarity between the generated noise and training data; and 3) to measure\nthe similarity performances of GAN-based models based on the mathematical and\npractically measured datasets, we conduct both quantitative and qualitative\nanalyses. The training datasets include: 1) a piecewise spectral\ncyclostationary Gaussian model (PSCGM); 2) a frequency-shift (FRESH) filter;\nand 3) practical measurements from NB-PLC systems. Simulation results\ndemonstrate that the generated noise samples from the proposed NGGAN are highly\nclose to the real noise samples. The principal component analysis (PCA) scatter\nplots and Fr\\'echet inception distance (FID) analysis have shown that NGGAN\noutperforms other GAN-based models by generating noise samples with superior\nfidelity and higher diversity.\n","authors":["Ying-Ren Chien","Po-Heng Chou","You-Jie Peng","Chun-Yuan Huang","Hen-Wai Tsao","Yu Tsao"],"pdf_url":"https://arxiv.org/pdf/2510.01850v3.pdf","comment":"16 pages, 15 figures, 11 tables, and published in IEEE Transactions\n  on Instrumentation and Measurement, 2025"},{"id":"http://arxiv.org/abs/2510.25626v1","updated":"2025-10-29T15:30:31Z","published":"2025-10-29T15:30:31Z","title":"Are Language Models Efficient Reasoners? A Perspective from Logic\n  Programming","summary":"  Modern language models (LMs) exhibit strong deductive reasoning capabilities,\nyet standard evaluations emphasize correctness while overlooking a key aspect\nof human-like reasoning: efficiency. In real-world reasoning scenarios, much of\nthe available information is irrelevant, and effective deductive inference\nrequires identifying and ignoring such distractions. We propose a framework for\nassessing LM reasoning efficiency through the lens of logic programming,\nintroducing a simple method to align proofs written in natural language -- as\ngenerated by an LM -- with shortest proofs found by executing the logic\nprogram. Efficiency is quantified by measuring how well a model avoids\nunnecessary inference. Empirically, we construct a dataset of math word\nproblems injected with various number of irrelevant axioms that vary in\nsemantic overlap with the goal theorem. We find that current LMs show marked\naccuracy declines under such conditions -- even with minimal, domain-consistent\ndistractions -- and the proofs they generate frequently exhibit detours through\nirrelevant inferences.\n","authors":["Andreas Opedal","Yanick Zengaffinen","Haruki Shirakami","Clemente Pasti","Mrinmaya Sachan","Abulhair Saparov","Ryan Cotterell","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2510.25626v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25616v1","updated":"2025-10-29T15:20:10Z","published":"2025-10-29T15:20:10Z","title":"Don't Blind Your VLA: Aligning Visual Representations for OOD\n  Generalization","summary":"  The growing success of Vision-Language-Action (VLA) models stems from the\npromise that pretrained Vision-Language Models (VLMs) can endow agents with\ntransferable world knowledge and vision-language (VL) grounding, laying a\nfoundation for action models with broader generalization. Yet when these VLMs\nare adapted to the action modality, it remains unclear to what extent their\noriginal VL representations and knowledge are preserved. In this work, we\nconduct a systematic study of representation retention during VLA fine-tuning,\nshowing that naive action fine-tuning leads to degradation of visual\nrepresentations. To characterize and measure these effects, we probe VLA's\nhidden representations and analyze attention maps, further, we design a set of\ntargeted tasks and methods that contrast VLA models with their counterpart\nVLMs, isolating changes in VL capabilities induced by action fine-tuning. We\nfurther evaluate a range of strategies for aligning visual representations and\nintroduce a simple yet effective method that mitigates degradation and yields\nimproved generalization to out-of-distribution (OOD) scenarios. Taken together,\nour analysis clarifies the trade-off between action fine-tuning and the\ndegradation of VL representations and highlights practical approaches to\nrecover inherited VL capabilities. Code is publicly available:\nhttps://blind-vla-paper.github.io\n","authors":["Nikita Kachaev","Mikhail Kolosov","Daniil Zelezetsky","Alexey K. Kovalev","Aleksandr I. Panov"],"pdf_url":"https://arxiv.org/pdf/2510.25616v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2510.25609v1","updated":"2025-10-29T15:16:50Z","published":"2025-10-29T15:16:50Z","title":"BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training","summary":"  We introduce BOLT-GAN, a simple yet effective modification of the WGAN\nframework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that\nwith a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a\ndifferent metric distance than the Earth Mover (Wasserstein) distance and\nachieves better training stability. Empirical evaluations on four standard\nimage generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN\nChurch-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60%\nlower Frechet Inception Distance (FID). Our results suggest that BOLT is a\nbroadly applicable principle for enhancing GAN training.\n","authors":["Mohammadreza Tavasoli Naeini","Ali Bereyhi","Morteza Noshad","Ben Liang","Alfred O. Hero III"],"pdf_url":"https://arxiv.org/pdf/2510.25609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25602v1","updated":"2025-10-29T15:11:53Z","published":"2025-10-29T15:11:53Z","title":"INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization\n  Formats","summary":"  Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly\nembracing low-precision floating-point (FP) formats to handle the pervasive\nactivation outliers in Large Language Models (LLMs). Despite this industry\ntrend, a unified comparison of FP and integer (INT) quantization across varying\ngranularities has been missing, leaving algorithm and hardware co-design\nwithout clear guidance. This paper fills that gap by systematically\ninvestigating the trade-offs between FP and INT formats. We reveal a critical\nperformance crossover: while FP excels in coarse-grained quantization, the\ncomparison at fine-grained (block-wise) levels is more nuanced. Our\ncomprehensive comparison demonstrates that for popular 8-bit fine-grained\nformats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart\nin both algorithmic accuracy and hardware efficiency. However, for 4-bit\nformats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we\nshow that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like\nHadamard rotation are applied. We also introduce a symmetric clipping method\nthat resolves gradient bias in fine-grained low-bit INT training, enabling\nnearly lossless performance for MXINT8 training. These findings challenge the\ncurrent hardware trajectory, demonstrating that a one-size-fits-all FP approach\nis suboptimal and advocating that fine-grained INT formats, particularly\nMXINT8, offer a better balance of accuracy, power, and efficiency for future AI\naccelerators.\n","authors":["Mengzhao Chen","Meng Wu","Hui Jin","Zhihang Yuan","Jing Liu","Chaoyi Zhang","Yunshui Li","Jie Huang","Jin Ma","Zeyue Xue","Zhiheng Liu","Xingyan Bin","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2510.25602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12437v2","updated":"2025-10-29T15:09:38Z","published":"2025-05-18T14:19:52Z","title":"A method for the systematic generation of graph XAI benchmarks via\n  Weisfeiler-Leman coloring","summary":"  Graph neural networks have become the de facto model for learning from\nstructured data. However, the decision-making process of GNNs remains opaque to\nthe end user, which undermines their use in safety-critical applications.\nSeveral explainable AI techniques for graphs have been developed to address\nthis major issue. Focusing on graph classification, these explainers identify\nsubgraph motifs that explain predictions. Therefore, a robust benchmarking of\ngraph explainers is required to ensure that the produced explanations are of\nhigh quality, i.e., aligned with the GNN's decision process. However, current\ngraph-XAI benchmarks are limited to simplistic synthetic datasets or a few\nreal-world tasks curated by domain experts, hindering rigorous and reproducible\nevaluation, and consequently stalling progress in the field. To overcome these\nlimitations, we propose a method to automate the construction of graph XAI\nbenchmarks from generic graph classification datasets. Our approach leverages\nthe Weisfeiler-Leman color refinement algorithm to efficiently perform\napproximate subgraph matching and mine class-discriminating motifs, which serve\nas proxy ground-truth class explanations. At the same time, we ensure that\nthese motifs can be learned by GNNs because their discriminating power aligns\nwith WL expressiveness. This work also introduces the OpenGraphXAI benchmark\nsuite, which consists of 15 ready-made graph-XAI datasets derived by applying\nour method to real-world molecular classification datasets. The suite is\navailable to the public along with a codebase to generate over 2,000 additional\ngraph-XAI benchmarks. Finally, we present a use case that illustrates how the\nsuite can be used to assess the effectiveness of a selection of popular graph\nexplainers, demonstrating the critical role of a sufficiently large benchmark\ncollection for improving the significance of experimental results.\n","authors":["Michele Fontanesi","Alessio Micheli","Marco Podda","Domenico Tortorella"],"pdf_url":"https://arxiv.org/pdf/2505.12437v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25599v1","updated":"2025-10-29T15:08:41Z","published":"2025-10-29T15:08:41Z","title":"Uncertainty Quantification for Regression: A Unified Framework based on\n  kernel scores","summary":"  Regression tasks, notably in safety-critical domains, require proper\nuncertainty quantification, yet the literature remains largely\nclassification-focused. In this light, we introduce a family of measures for\ntotal, aleatoric, and epistemic uncertainty based on proper scoring rules, with\na particular emphasis on kernel scores. The framework unifies several\nwell-known measures and provides a principled recipe for designing new ones\nwhose behavior, such as tail sensitivity, robustness, and out-of-distribution\nresponsiveness, is governed by the choice of kernel. We prove explicit\ncorrespondences between kernel-score characteristics and downstream behavior,\nyielding concrete design guidelines for task-specific measures. Extensive\nexperiments demonstrate that these measures are effective in downstream tasks\nand reveal clear trade-offs among instantiations, including robustness and\nout-of-distribution detection performance.\n","authors":["Christopher Bülte","Yusuf Sale","Gitta Kutyniok","Eyke Hüllermeier"],"pdf_url":"https://arxiv.org/pdf/2510.25599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25594v1","updated":"2025-10-29T15:03:46Z","published":"2025-10-29T15:03:46Z","title":"Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for\n  Local Learning","summary":"  Training deep neural networks (DNNs) with backpropagation (BP) achieves\nstate-of-the-art accuracy but requires global error propagation and full\nparameterization, leading to substantial memory and computational overhead.\nDirect Feedback Alignment (DFA) enables local, parallelizable updates with\nlower memory requirements but is limited by unstructured feedback and poor\nscalability in deeper architectures, specially convolutional neural networks.\nTo address these limitations, we propose a structured local learning framework\nthat operates directly on low-rank manifolds defined by the Singular Value\nDecomposition (SVD) of weight matrices. Each layer is trained in its decomposed\nform, with updates applied to the SVD components using a composite loss that\nintegrates cross-entropy, subspace alignment, and orthogonality regularization.\nFeedback matrices are constructed to match the SVD structure, ensuring\nconsistent alignment between forward and feedback pathways. Our method reduces\nthe number of trainable parameters relative to the original DFA model, without\nrelying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,\nand ImageNet show that our method achieves accuracy comparable to that of BP.\nAblation studies confirm the importance of each loss term in the low-rank\nsetting. These results establish local learning on low-rank manifolds as a\nprincipled and scalable alternative to full-rank gradient-based training.\n","authors":["Arani Roy","Marco P. Apolinario","Shristi Das Biswas","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2510.25594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25591v1","updated":"2025-10-29T14:59:26Z","published":"2025-10-29T14:59:26Z","title":"Generalized Sobolev IPM for Graph-Based Measures","summary":"  We study the Sobolev IPM problem for measures supported on a graph metric\nspace, where critic function is constrained to lie within the unit ball defined\nby Sobolev norm. While Le et al. (2025) achieved scalable computation by\nrelating Sobolev norm to weighted $L^p$-norm, the resulting framework remains\nintrinsically bound to $L^p$ geometric structure, limiting its ability to\nincorporate alternative structural priors beyond the $L^p$ geometry paradigm.\nTo overcome this limitation, we propose to generalize Sobolev IPM through the\nlens of \\emph{Orlicz geometric structure}, which employs convex functions to\ncapture nuanced geometric relationships, building upon recent advances in\noptimal transport theory -- particularly Orlicz-Wasserstein (OW) and\ngeneralized Sobolev transport -- that have proven instrumental in advancing\nmachine learning methodologies. This generalization encompasses classical\nSobolev IPM as a special case while accommodating diverse geometric priors\nbeyond traditional $L^p$ structure. It however brings up significant\ncomputational hurdles that compound those already inherent in Sobolev IPM. To\naddress these challenges, we establish a novel theoretical connection between\nOrlicz-Sobolev norm and Musielak norm which facilitates a novel regularization\nfor the generalized Sobolev IPM (GSI). By further exploiting the underlying\ngraph structure, we show that GSI with Musielak regularization (GSI-M) reduces\nto a simple \\emph{univariate optimization} problem, achieving remarkably\ncomputational efficiency. Empirically, GSI-M is several-order faster than the\npopular OW in computation, and demonstrates its practical advantages in\ncomparing probability measures on a given graph for document classification and\nseveral tasks in topological data analysis.\n","authors":["Tam Le","Truyen Nguyen","Hideitsu Hino","Kenji Fukumizu"],"pdf_url":"https://arxiv.org/pdf/2510.25591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12484v4","updated":"2025-10-29T14:52:49Z","published":"2025-06-14T12:49:51Z","title":"Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption\n  Masking And Normalization","summary":"  Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new\nstate-of-the-art for robust unlearning.\n","authors":["Filip Sondej","Yushi Yang","Mikołaj Kniejski","Marcel Windys"],"pdf_url":"https://arxiv.org/pdf/2506.12484v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25582v1","updated":"2025-10-29T14:47:18Z","published":"2025-10-29T14:47:18Z","title":"Learning-Augmented Online Bidding in Stochastic Settings","summary":"  Online bidding is a classic optimization problem, with several applications\nin online decision-making, the design of interruptible systems, and the\nanalysis of approximation algorithms. In this work, we study online bidding\nunder learning-augmented settings that incorporate stochasticity, in either the\nprediction oracle or the algorithm itself. In the first part, we study bidding\nunder distributional predictions, and find Pareto-optimal algorithms that offer\nthe best-possible tradeoff between the consistency and the robustness of the\nalgorithm. In the second part, we study the power and limitations of randomized\nbidding algorithms, by presenting upper and lower bounds on the\nconsistency/robustness tradeoffs. Previous works focused predominantly on\noracles that do not leverage stochastic information on the quality of the\nprediction, and deterministic algorithms.\n","authors":["Spyros Angelopoulos","Bertrand Simon"],"pdf_url":"https://arxiv.org/pdf/2510.25582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25573v1","updated":"2025-10-29T14:42:10Z","published":"2025-10-29T14:42:10Z","title":"Monitoring the calibration of probability forecasts with an application\n  to concept drift detection involving image classification","summary":"  Machine learning approaches for image classification have led to impressive\nadvances in that field. For example, convolutional neural networks are able to\nachieve remarkable image classification accuracy across a wide range of\napplications in industry, defense, and other areas. While these machine\nlearning models boast impressive accuracy, a related concern is how to assess\nand maintain calibration in the predictions these models make. A classification\nmodel is said to be well calibrated if its predicted probabilities correspond\nwith the rates events actually occur. While there are many available methods to\nassess machine learning calibration and recalibrate faulty predictions, less\neffort has been spent on developing approaches that continually monitor\npredictive models for potential loss of calibration as time passes. We propose\na cumulative sum-based approach with dynamic limits that enable detection of\nmiscalibration in both traditional process monitoring and concept drift\napplications. This enables early detection of operational context changes that\nimpact image classification performance in the field. The proposed chart can be\nused broadly in any situation where the user needs to monitor probability\npredictions over time for potential lapses in calibration. Importantly, our\nmethod operates on probability predictions and event outcomes and does not\nrequire under-the-hood access to the machine learning model.\n","authors":["Christopher T. Franck","Anne R. Driscoll","Zoe Szajnfarber","William H. Woodall"],"pdf_url":"https://arxiv.org/pdf/2510.25573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24670v2","updated":"2025-10-29T14:41:45Z","published":"2025-10-28T17:36:51Z","title":"Pearl: A Foundation Model for Placing Every Atom in the Right Location","summary":"  Accurately predicting the three-dimensional structures of protein-ligand\ncomplexes remains a fundamental challenge in computational drug discovery that\nlimits the pace and success of therapeutic design. Deep learning methods have\nrecently shown strong potential as structural prediction tools, achieving\npromising accuracy across diverse biomolecular systems. However, their\nperformance and utility are constrained by scarce experimental data,\ninefficient architectures, physically invalid poses, and the limited ability to\nexploit auxiliary information available at inference. To address these issues,\nwe introduce Pearl (Placing Every Atom in the Right Location), a foundation\nmodel for protein-ligand cofolding at scale. Pearl addresses these challenges\nwith three key innovations: (1) training recipes that include large-scale\nsynthetic data to overcome data scarcity; (2) architectures that incorporate an\nSO(3)-equivariant diffusion module to inherently respect 3D rotational\nsymmetries, improving generalization and sample efficiency, and (3)\ncontrollable inference, including a generalized multi-chain templating system\nsupporting both protein and non-polymeric components as well as dual\nunconditional/conditional modes. Pearl establishes a new state-of-the-art\nperformance in protein-ligand cofolding. On the key metric of generating\naccurate (RMSD < 2 \\r{A}) and physically valid poses, Pearl surpasses AlphaFold\n3 and other open source baselines on the public Runs N' Poses and PoseBusters\nbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over the\nnext best model. In the pocket-conditional cofolding regime, Pearl delivers\n$3.6\\times$ improvement on a proprietary set of challenging, real-world drug\ntargets at the more rigorous RMSD < 1 \\r{A} threshold. Finally, we demonstrate\nthat model performance correlates directly with synthetic dataset size used in\ntraining.\n","authors":[" Genesis Research Team","Alejandro Dobles","Nina Jovic","Kenneth Leidal","Pranav Murugan","David C. Williams","Drausin Wulsin","Nate Gruver","Christina X. Ji","Korrawat Pruegsanusak","Gianluca Scarpellini","Ansh Sharma","Wojciech Swiderski","Andrea Bootsma","Richard Strong Bowen","Charlotte Chen","Jamin Chen","Marc André Dämgen","Benjamin DiFrancesco","J. D. Fishman","Alla Ivanova","Zach Kagin","David Li-Bland","Zuli Liu","Igor Morozov","Jeffrey Ouyang-Zhang","Frank C. Pickard IV","Kushal S. Shah","Ben Shor","Gabriel Monteiro da Silva","Roy Tal","Maxx Tessmer","Carl Tilbury","Cyr Vetcher","Daniel Zeng","Maruan Al-Shedivat","Aleksandra Faust","Evan N. Feinberg","Michael V. LeVine","Matteus Pan"],"pdf_url":"https://arxiv.org/pdf/2510.24670v2.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2510.25571v1","updated":"2025-10-29T14:40:12Z","published":"2025-10-29T14:40:12Z","title":"Perturbation Bounds for Low-Rank Inverse Approximations under Noise","summary":"  Low-rank pseudoinverses are widely used to approximate matrix inverses in\nscalable machine learning, optimization, and scientific computing. However,\nreal-world matrices are often observed with noise, arising from sampling,\nsketching, and quantization. The spectral-norm robustness of low-rank inverse\napproximations remains poorly understood. We systematically study the\nspectral-norm error $\\| (\\tilde{A}^{-1})_p - A_p^{-1} \\|$ for an $n\\times n$\nsymmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\\(p\\)\napproximation of $A^{-1}$, and $\\tilde{A} = A + E$ is a noisy observation.\nUnder mild assumptions on the noise, we derive sharp non-asymptotic\nperturbation bounds that reveal how the error scales with the eigengap,\nspectral decay, and noise alignment with low-curvature directions of $A$. Our\nanalysis introduces a novel application of contour integral techniques to the\n\\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over\nnaive adaptations of classical full-inverse bounds by up to a factor of\n$\\sqrt{n}$. Empirically, our bounds closely track the true perturbation error\nacross a variety of real-world and synthetic matrices, while estimates based on\nclassical results tend to significantly overpredict. These findings offer\npractical, spectrum-aware guarantees for low-rank inverse approximations in\nnoisy computational environments.\n","authors":["Phuc Tran","Nisheeth K. Vishnoi"],"pdf_url":"https://arxiv.org/pdf/2510.25571v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25569v1","updated":"2025-10-29T14:38:35Z","published":"2025-10-29T14:38:35Z","title":"A Framework for Bounding Deterministic Risk with PAC-Bayes: Applications\n  to Majority Votes","summary":"  PAC-Bayes is a popular and efficient framework for obtaining generalization\nguarantees in situations involving uncountable hypothesis spaces.\nUnfortunately, in its classical formulation, it only provides guarantees on the\nexpected risk of a randomly sampled hypothesis. This requires stochastic\npredictions at test time, making PAC-Bayes unusable in many practical\nsituations where a single deterministic hypothesis must be deployed. We propose\na unified framework to extract guarantees holding for a single hypothesis from\nstochastic PAC-Bayesian guarantees. We present a general oracle bound and\nderive from it a numerical bound and a specialization to majority vote. We\nempirically show that our approach consistently outperforms popular baselines\n(by up to a factor of 2) when it comes to generalization bounds on\ndeterministic classifiers.\n","authors":["Benjamin Leblanc","Pascal Germain"],"pdf_url":"https://arxiv.org/pdf/2510.25569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23999v2","updated":"2025-10-29T14:38:26Z","published":"2025-10-28T02:03:39Z","title":"Auto-Adaptive PINNs with Applications to Phase Transitions","summary":"  We propose an adaptive sampling method for the training of Physics Informed\nNeural Networks (PINNs) which allows for sampling based on an arbitrary\nproblem-specific heuristic which may depend on the network and its gradients.\nIn particular we focus our analysis on the Allen-Cahn equations, attempting to\naccurately resolve the characteristic interfacial regions using a PINN without\nany post-hoc resampling. In experiments, we show the effectiveness of these\nmethods over residual-adaptive frameworks.\n","authors":["Kevin Buck","Woojeong Kim"],"pdf_url":"https://arxiv.org/pdf/2510.23999v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25566v1","updated":"2025-10-29T14:33:35Z","published":"2025-10-29T14:33:35Z","title":"PitchFlower: A flow-based neural audio codec with pitch controllability","summary":"  We present PitchFlower, a flow-based neural audio codec with explicit pitch\ncontrollability. Our approach enforces disentanglement through a simple\nperturbation: during training, F0 contours are flattened and randomly shifted,\nwhile the true F0 is provided as conditioning. A vector-quantization bottleneck\nprevents pitch recovery, and a flow-based decoder generates high quality audio.\nExperiments show that PitchFlower achieves more accurate pitch control than\nWORLD at much higher audio quality, and outperforms SiFiGAN in controllability\nwhile maintaining comparable quality. Beyond pitch, this framework provides a\nsimple and extensible path toward disentangling other speech attributes.\n","authors":["Diego Torres","Axel Roebel","Nicolas Obin"],"pdf_url":"https://arxiv.org/pdf/2510.25566v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.13519v2","updated":"2025-10-29T14:31:32Z","published":"2025-05-17T12:39:45Z","title":"Continuous Domain Generalization","summary":"  Real-world data distributions often shift continuously across multiple latent\nfactors such as time, geography, and socioeconomic contexts. However, existing\ndomain generalization approaches typically treat domains as discrete or as\nevolving along a single axis (e.g., time). This oversimplification fails to\ncapture the complex, multidimensional nature of real-world variation. This\npaper introduces the task of Continuous Domain Generalization (CDG), which aims\nto generalize predictive models to unseen domains defined by arbitrary\ncombinations of continuous variations. We present a principled framework\ngrounded in geometric and algebraic theories, showing that optimal model\nparameters across domains lie on a low-dimensional manifold. To model this\nstructure, we propose a Neural Lie Transport Operator (NeuralLio), which\nenables structure-preserving parameter transitions by enforcing geometric\ncontinuity and algebraic consistency. To handle noisy or incomplete domain\nvariation descriptors, we introduce a gating mechanism to suppress irrelevant\ndimensions and a local chart-based strategy for robust generalization.\nExtensive experiments on synthetic and real-world datasets, including remote\nsensing, scientific documents, and traffic forecasting, demonstrate that our\nmethod significantly outperforms existing baselines in both generalization\naccuracy and robustness.\n","authors":["Zekun Cai","Yiheng Yao","Guangji Bai","Renhe Jiang","Xuan Song","Ryosuke Shibasaki","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.13519v2.pdf","comment":"23 pages, 9 figures. Accepted by NeurIPS25"},{"id":"http://arxiv.org/abs/2510.25563v1","updated":"2025-10-29T14:30:12Z","published":"2025-10-29T14:30:12Z","title":"Leveraging an Atmospheric Foundational Model for Subregional Sea Surface\n  Temperature Forecasting","summary":"  The accurate prediction of oceanographic variables is crucial for\nunderstanding climate change, managing marine resources, and optimizing\nmaritime activities. Traditional ocean forecasting relies on numerical models;\nhowever, these approaches face limitations in terms of computational cost and\nscalability. In this study, we adapt Aurora, a foundational deep learning model\noriginally designed for atmospheric forecasting, to predict sea surface\ntemperature (SST) in the Canary Upwelling System. By fine-tuning this model\nwith high-resolution oceanographic reanalysis data, we demonstrate its ability\nto capture complex spatiotemporal patterns while reducing computational\ndemands. Our methodology involves a staged fine-tuning process, incorporating\nlatitude-weighted error metrics and optimizing hyperparameters for efficient\nlearning. The experimental results show that the model achieves a low RMSE of\n0.119K, maintaining high anomaly correlation coefficients (ACC $\\approx\n0.997$). The model successfully reproduces large-scale SST structures but faces\nchallenges in capturing finer details in coastal regions. This work contributes\nto the field of data-driven ocean forecasting by demonstrating the feasibility\nof using deep learning models pre-trained in different domains for oceanic\napplications. Future improvements include integrating additional oceanographic\nvariables, increasing spatial resolution, and exploring physics-informed neural\nnetworks to enhance interpretability and understanding. These advancements can\nimprove climate modeling and ocean prediction accuracy, supporting\ndecision-making in environmental and economic sectors.\n","authors":["Víctor Medina","Giovanny A. Cuervo-Londoño","Javier Sánchez"],"pdf_url":"https://arxiv.org/pdf/2510.25563v1.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.23455v3","updated":"2025-10-29T14:29:54Z","published":"2025-10-27T15:56:19Z","title":"SGFusion: Stochastic Geographic Gradient Fusion in Federated Learning","summary":"  This paper proposes Stochastic Geographic Gradient Fusion (SGFusion), a novel\ntraining algorithm to leverage the geographic information of mobile users in\nFederated Learning (FL). SGFusion maps the data collected by mobile devices\nonto geographical zones and trains one FL model per zone, which adapts well to\nthe data and behaviors of users in that zone. SGFusion models the local\ndata-based correlation among geographical zones as a hierarchical random graph\n(HRG) optimized by Markov Chain Monte Carlo sampling. At each training step,\nevery zone fuses its local gradient with gradients derived from a small set of\nother zones sampled from the HRG. This approach enables knowledge fusion and\nsharing among geographical zones in a probabilistic and stochastic gradient\nfusion process with self-attention weights, such that \"more similar\" zones have\n\"higher probabilities\" of sharing gradients with \"larger attention weights.\"\nSGFusion remarkably improves model utility without introducing undue\ncomputational cost. Extensive theoretical and empirical results using a\nheart-rate prediction dataset collected across 6 countries show that models\ntrained with SGFusion converge with upper-bounded expected errors and\nsignificantly improve utility in all countries compared to existing approaches\nwithout notable cost in system scalability.\n","authors":["Khoa Nguyen","Khang Tran","NhatHai Phan","Cristian Borcea","Ruoming Jin","Issa Khalil"],"pdf_url":"https://arxiv.org/pdf/2510.23455v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25557v1","updated":"2025-10-29T14:21:49Z","published":"2025-10-29T14:21:49Z","title":"Hybrid Quantum-Classical Recurrent Neural Networks","summary":"  We present a hybrid quantum-classical recurrent neural network (QRNN)\narchitecture in which the entire recurrent core is realized as a parametrized\nquantum circuit (PQC) controlled by a classical feedforward network. The hidden\nstate is the quantum state of an $n$-qubit PQC, residing in an exponentially\nlarge Hilbert space $\\mathbb{C}^{2^n}$. The PQC is unitary by construction,\nmaking the hidden-state evolution norm-preserving without external constraints.\nAt each timestep, mid-circuit readouts are combined with the input embedding\nand processed by the feedforward network, which provides explicit classical\nnonlinearity. The outputs parametrize the PQC, which updates the hidden state\nvia unitary dynamics. The QRNN is compact and physically consistent, and it\nunifies (i) unitary recurrence as a high-capacity memory, (ii) partial\nobservation via mid-circuit measurements, and (iii) nonlinear classical control\nfor input-conditioned parametrization. We evaluate the model in simulation with\nup to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,\nand language modeling, adopting projective measurements as a limiting case to\nobtain mid-circuit readouts while maintaining a coherent recurrent quantum\nmemory. We further devise a soft attention mechanism over the mid-circuit\nreadouts in a sequence-to-sequence model and show its effectiveness for machine\ntranslation. To our knowledge, this is the first model (RNN or otherwise)\ngrounded in quantum operations to achieve competitive performance against\nstrong classical baselines across a broad class of sequence-learning tasks.\n","authors":["Wenduan Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25550v1","updated":"2025-10-29T14:18:08Z","published":"2025-10-29T14:18:08Z","title":"Robust variable selection for spatial point processes observed with\n  noise","summary":"  We propose a method for variable selection in the intensity function of\nspatial point processes that combines sparsity-promoting estimation with\nnoise-robust model selection. As high-resolution spatial data becomes\nincreasingly available through remote sensing and automated image analysis,\nidentifying spatial covariates that influence the localization of events is\ncrucial to understand the underlying mechanism. However, results from automated\nacquisition techniques are often noisy, for example due to measurement\nuncertainties or detection errors, which leads to spurious displacements and\nmissed events. We study the impact of such noise on sparse point-process\nestimation across different models, including Poisson and Thomas processes. To\nimprove noise robustness, we propose to use stability selection based on\npoint-process subsampling and to incorporate a non-convex best-subset penalty\nto enhance model-selection performance. In extensive simulations, we\ndemonstrate that such an approach reliably recovers true covariates under\ndiverse noise scenarios and improves both selection accuracy and stability. We\nthen apply the proposed method to a forestry data set, analyzing the\ndistribution of trees in relation to elevation and soil nutrients in a tropical\nrain forest. This shows the practical utility of the method, which provides a\nsystematic framework for robust variable selection in spatial point-process\nmodels under noise, without requiring additional knowledge of the process.\n","authors":["Dominik Sturm","Ivo F. Sbalzarini"],"pdf_url":"https://arxiv.org/pdf/2510.25550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12828v2","updated":"2025-10-29T14:17:09Z","published":"2024-02-20T08:54:07Z","title":"Tracking the Median of Gradients with a Stochastic Proximal Point Method","summary":"  There are several applications of stochastic optimization where one can\nbenefit from a robust estimate of the gradient. For example, domains such as\ndistributed learning with corrupted nodes, the presence of large outliers in\nthe training data, learning under privacy constraints, or even heavy-tailed\nnoise due to the dynamics of the algorithm itself. Here we study SGD with\nrobust gradient estimators based on estimating the median.\n  We first derive iterative methods based on the stochastic proximal point\nmethod for computing the median gradient and generalizations thereof. Then we\npropose an algorithm estimating the median gradient across iterations, and find\nthat several well known methods are particular cases of this framework. For\ninstance, we observe that different forms of clipping allow to compute online\nestimators of the median of gradients, in contrast to (heavy-ball) momentum,\nwhich corresponds to an online estimator of the mean. Finally, we provide a\ntheoretical framework for an algorithm computing the median gradient across\nsamples, and show that the resulting method can converge even under\nheavy-tailed, state-dependent noise.\n","authors":["Fabian Schaipp","Guillaume Garrigos","Umut Simsekli","Robert Gower"],"pdf_url":"https://arxiv.org/pdf/2402.12828v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12662v3","updated":"2025-10-29T14:16:37Z","published":"2025-03-16T21:34:11Z","title":"TuneNSearch: a hybrid transfer learning and local search approach for\n  solving vehicle routing problems","summary":"  This paper introduces TuneNSearch, a hybrid transfer learning and local\nsearch approach for addressing diverse variants of the vehicle routing problem\n(VRP). Our method uses reinforcement learning to generate high-quality\nsolutions, which are subsequently refined by an efficient local search\nprocedure. To ensure broad adaptability across VRP variants, TuneNSearch begins\nwith a pre-training phase on the multi-depot VRP (MDVRP), followed by a\nfine-tuning phase to adapt it to other problem formulations. The learning phase\nutilizes a Transformer-based architecture enhanced with edge-aware attention,\nwhich integrates edge distances directly into the attention mechanism to better\ncapture spatial relationships inherent to routing problems. We show that the\npre-trained model generalizes effectively to single-depot variants, achieving\nperformance comparable to models trained specifically on single-depot\ninstances. Simultaneously, it maintains strong performance on multi-depot\nvariants, an ability that models pre-trained solely on single-depot problems\nlack. For example, on 100-node instances of multi-depot variants, TuneNSearch\noutperforms a model pre-trained on the CVRP by 44%. In contrast, on 100-node\ninstances of single-depot variants, TuneNSearch performs similar to the CVRP\nmodel. To validate the effectiveness of our method, we conduct extensive\ncomputational experiments on public benchmark and randomly generated instances.\nAcross multiple CVRPLIB datasets, TuneNSearch consistently achieves performance\ndeviations of less than 3% from the best-known solutions in the literature,\ncompared to 6-25% for other neural-based models, depending on problem\ncomplexity. Overall, our approach demonstrates strong generalization to\ndifferent problem sizes, instance distributions, and VRP formulations, while\nmaintaining polynomial runtime complexity despite the integration of the local\nsearch algorithm.\n","authors":["Arthur Corrêa","Cristóvão Silva","Liming Xu","Alexandra Brintrup","Samuel Moniz"],"pdf_url":"https://arxiv.org/pdf/2503.12662v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.18962v2","updated":"2025-10-29T14:11:14Z","published":"2025-09-23T13:14:37Z","title":"Lift What You Can: Green Online Learning with Heterogeneous Ensembles","summary":"  Ensemble methods for stream mining necessitate managing multiple models and\nupdating them as data distributions evolve. Considering the calls for more\nsustainability, established methods are however not sufficiently considerate of\nensemble members' computational expenses and instead overly focus on predictive\ncapabilities. To address these challenges and enable green online learning, we\npropose heterogeneous online ensembles (HEROS). For every training step, HEROS\nchooses a subset of models from a pool of models initialized with diverse\nhyperparameter choices under resource constraints to train. We introduce a\nMarkov decision process to theoretically capture the trade-offs between\npredictive performance and sustainability constraints. Based on this framework,\nwe present different policies for choosing which models to train on incoming\ndata. Most notably, we propose the novel $\\zeta$-policy, which focuses on\ntraining near-optimal models at reduced costs. Using a stochastic model, we\ntheoretically prove that our $\\zeta$-policy achieves near optimal performance\nwhile using fewer resources compared to the best performing policy. In our\nexperiments across 11 benchmark datasets, we find empiric evidence that our\n$\\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating\nhighly accurate performance, in some cases even outperforming competitors, and\nsimultaneously being much more resource-friendly.\n","authors":["Kirsten Köbschall","Sebastian Buschjäger","Raphael Fischer","Lisa Hartung","Stefan Kramer"],"pdf_url":"https://arxiv.org/pdf/2509.18962v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25544v1","updated":"2025-10-29T14:11:03Z","published":"2025-10-29T14:11:03Z","title":"Error Bounds and Optimal Schedules for Masked Diffusions with Factorized\n  Approximations","summary":"  Recently proposed generative models for discrete data, such as Masked\nDiffusion Models (MDMs), exploit conditional independence approximations to\nreduce the computational cost of popular Auto-Regressive Models (ARMs), at the\nprice of some bias in the sampling distribution. We study the resulting\ncomputation-vs-accuracy trade-off, providing general error bounds (in relative\nentropy) that depend only on the average number of tokens generated per\niteration and are independent of the data dimensionality (i.e. sequence\nlength), thus supporting the empirical success of MDMs. We then investigate the\ngain obtained by using non-constant schedule sizes (i.e. varying the number of\nunmasked tokens during the generation process) and identify the optimal\nschedule as a function of a so-called information profile of the data\ndistribution, thus allowing for a principled optimization of schedule sizes. We\ndefine methods directly as sampling algorithms and do not use classical\nderivations as time-reversed diffusion processes, leading us to simple and\ntransparent proofs.\n","authors":["Hugo Lavenant","Giacomo Zanella"],"pdf_url":"https://arxiv.org/pdf/2510.25544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25542v1","updated":"2025-10-29T14:07:12Z","published":"2025-10-29T14:07:12Z","title":"Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided\n  Mutual Information","summary":"  Uncovering hidden graph structures underlying real-world data is a critical\nchallenge with broad applications across scientific domains. Recently,\ntransformer-based models leveraging the attention mechanism have demonstrated\nstrong empirical success in capturing complex dependencies within graphs.\nHowever, the theoretical understanding of their training dynamics has been\nlimited to tree-like graphs, where each node depends on a single parent.\nExtending provable guarantees to more general directed acyclic graphs (DAGs) --\nwhich involve multiple parents per node -- remains challenging, primarily due\nto the difficulty in designing training objectives that enable different\nattention heads to separately learn multiple different parent relationships.\n  In this work, we address this problem by introducing a novel\ninformation-theoretic metric: the kernel-guided mutual information (KG-MI),\nbased on the $f$-divergence. Our objective combines KG-MI with a multi-head\nattention framework, where each head is associated with a distinct marginal\ntransition kernel to model diverse parent-child dependencies effectively. We\nprove that, given sequences generated by a $K$-parent DAG, training a\nsingle-layer, multi-head transformer via gradient ascent converges to the\nglobal optimum in polynomial time. Furthermore, we characterize the attention\nscore patterns at convergence. In addition, when particularizing the\n$f$-divergence to the KL divergence, the learned attention scores accurately\nreflect the ground-truth adjacency matrix, thereby provably recovering the\nunderlying graph structure. Experimental results validate our theoretical\nfindings.\n","authors":["Yuan Cheng","Yu Huang","Zhe Xiong","Yingbin Liang","Vincent Y. F. Tan"],"pdf_url":"https://arxiv.org/pdf/2510.25542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08388v3","updated":"2025-10-29T14:02:55Z","published":"2025-06-10T02:53:24Z","title":"Reinforcement Learning Teachers of Test Time Scaling","summary":"  Training reasoning language models (LMs) with reinforcement learning (RL) for\none-hot correctness inherently relies on the LM being able to explore and solve\nits task with some chance at initialization. Furthermore, a key use case of\nreasoning LMs is to act as teachers for distilling new students and\ncold-starting future RL iterations rather than being deployed themselves. From\nthese considerations, we introduce a new framework that avoids RL's exploration\nchallenge by training a new class of Reinforcement-Learned Teachers (RLTs)\nfocused on yielding the most effective downstream distillation. RLTs are\nprompted with both the question and solution to each problem, and tasked to\nsimply \"connect-the-dots\" with detailed explanations tailored for their\nstudents. We train RLTs with dense rewards obtained by feeding each explanation\nto the student and testing its understanding of the problem's solution. In\npractice, the raw outputs of a 7B RLT provide higher final performance on\ncompetition and graduate-level tasks than existing distillation and\ncold-starting pipelines that collect and postprocess the reasoning traces of\norders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness\nwhen training larger students and when applied zero-shot to out-of-distribution\ntasks, unlocking new levels of efficiency and re-usability for the RL reasoning\nframework. Code available at: https://github.com/SakanaAI/RLT\n","authors":["Edoardo Cetin","Tianyu Zhao","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2506.08388v3.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25531v1","updated":"2025-10-29T13:56:44Z","published":"2025-10-29T13:56:44Z","title":"Using latent representations to link disjoint longitudinal data for\n  mixed-effects regression","summary":"  Many rare diseases offer limited established treatment options, leading\npatients to switch therapies when new medications emerge. To analyze the impact\nof such treatment switches within the low sample size limitations of rare\ndisease trials, it is important to use all available data sources. This,\nhowever, is complicated when usage of measurement instruments change during the\nobservation period, for example when instruments are adapted to specific age\nranges. The resulting disjoint longitudinal data trajectories, complicate the\napplication of traditional modeling approaches like mixed-effects regression.\nWe tackle this by mapping observations of each instrument to a aligned\nlow-dimensional temporal trajectory, enabling longitudinal modeling across\ninstruments. Specifically, we employ a set of variational autoencoder\narchitectures to embed item values into a shared latent space for each time\npoint. Temporal disease dynamics and treatment switch effects are then captured\nthrough a mixed-effects regression model applied to latent representations. To\nenable statistical inference, we present a novel statistical testing approach\nthat accounts for the joint parameter estimation of mixed-effects regression\nand variational autoencoders. The methodology is applied to quantify the impact\nof treatment switches for patients with spinal muscular atrophy. Here, our\napproach aligns motor performance items from different measurement instruments\nfor mixed-effects regression and maps estimated effects back to the observed\nitem level to quantify the treatment switch effect. Our approach allows for\nmodel selection as well as for assessing effects of treatment switching. The\nresults highlight the potential of modeling in joint latent representations for\naddressing small data challenges.\n","authors":["Clemens Schächter","Maren Hackenberg","Michelle Pfaffenlehner","Félix B. Tambe-Ndonfack","Thorsten Schmidt","Astrid Pechmann","Janbernd Kirschner","Jan Hasenauser","Harald Binder"],"pdf_url":"https://arxiv.org/pdf/2510.25531v1.pdf","comment":"31 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2510.23906v2","updated":"2025-10-29T13:42:56Z","published":"2025-10-27T22:26:20Z","title":"Group Interventions on Deep Networks for Causal Discovery in Subsystems","summary":"  Causal discovery uncovers complex relationships between variables, enhancing\npredictions, decision-making, and insights into real-world systems, especially\nin nonlinear multivariate time series. However, most existing methods primarily\nfocus on pairwise cause-effect relationships, overlooking interactions among\ngroups of variables, i.e., subsystems and their collective causal influence. In\nthis study, we introduce gCDMI, a novel multi-group causal discovery method\nthat leverages group-level interventions on trained deep neural networks and\nemploys model invariance testing to infer causal relationships. Our approach\ninvolves three key steps. First, we use deep learning to jointly model the\nstructural relationships among groups of all time series. Second, we apply\ngroup-wise interventions to the trained model. Finally, we conduct model\ninvariance testing to determine the presence of causal links among variable\ngroups. We evaluate our method on simulated datasets, demonstrating its\nsuperior performance in identifying group-level causal relationships compared\nto existing methods. Additionally, we validate our approach on real-world\ndatasets, including brain networks and climate ecosystems. Our results\nhighlight that applying group-level interventions to deep learning models,\ncombined with invariance testing, can effectively reveal complex causal\nstructures, offering valuable insights for domains such as neuroscience and\nclimate science.\n","authors":["Wasim Ahmad","Joachim Denzler","Maha Shadaydeh"],"pdf_url":"https://arxiv.org/pdf/2510.23906v2.pdf","comment":"Submitted to IEEE Access. We are working on the revised version"},{"id":"http://arxiv.org/abs/2510.25514v1","updated":"2025-10-29T13:38:24Z","published":"2025-10-29T13:38:24Z","title":"Convergence of off-policy TD(0) with linear function approximation for\n  reversible Markov chains","summary":"  We study the convergence of off-policy TD(0) with linear function\napproximation when used to approximate the expected discounted reward in a\nMarkov chain. It is well known that the combination of off-policy learning and\nfunction approximation can lead to divergence of the algorithm. Existing\nresults for this setting modify the algorithm, for instance by reweighing the\nupdates using importance sampling. This establishes convergence at the expense\nof additional complexity. In contrast, our approach is to analyse the standard\nalgorithm, but to restrict our attention to the class of reversible Markov\nchains. We demonstrate convergence under this mild reversibility condition on\nthe structure of the chain, which in many applications can be assumed using\ndomain knowledge. In particular, we establish a convergence guarantee under an\nupper bound on the discount factor in terms of the difference between the\non-policy and off-policy process. This improves upon known results in the\nliterature that state that convergence holds for a sufficiently small discount\nfactor by establishing an explicit bound. Convergence is with probability one\nand achieves projected Bellman error equal to zero. To obtain these results, we\nadapt the stochastic approximation framework that was used by Tsitsiklis and\nVan Roy [1997 for the on-policy case, to the off-policy case. We illustrate our\nresults using different types of reversible Markov chains, such as\none-dimensional random walks and random walks on a weighted graph.\n","authors":["Maik Overmars","Jasper Goseling","Richard Boucherie"],"pdf_url":"https://arxiv.org/pdf/2510.25514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25512v1","updated":"2025-10-29T13:35:46Z","published":"2025-10-29T13:35:46Z","title":"FaCT: Faithful Concept Traces for Explaining Neural Network Decisions","summary":"  Deep networks have shown remarkable performance across a wide range of tasks,\nyet getting a global concept-level understanding of how they function remains a\nkey challenge. Many post-hoc concept-based approaches have been introduced to\nunderstand their workings, yet they are not always faithful to the model.\nFurther, they make restrictive assumptions on the concepts a model learns, such\nas class-specificity, small spatial extent, or alignment to human expectations.\nIn this work, we put emphasis on the faithfulness of such concept-based\nexplanations and propose a new model with model-inherent mechanistic\nconcept-explanations. Our concepts are shared across classes and, from any\nlayer, their contribution to the logit and their input-visualization can be\nfaithfully traced. We also leverage foundation models to propose a new\nconcept-consistency metric, C$^2$-Score, that can be used to evaluate\nconcept-based methods. We show that, compared to prior work, our concepts are\nquantitatively more consistent and users find our concepts to be more\ninterpretable, all while retaining competitive ImageNet performance.\n","authors":["Amin Parchami-Araghi","Sukrut Rao","Jonas Fischer","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2510.25512v1.pdf","comment":"Accepted to NeurIPS 2025; Code is available at\n  https://github.com/m-parchami/FaCT"},{"id":"http://arxiv.org/abs/2510.25509v1","updated":"2025-10-29T13:32:59Z","published":"2025-10-29T13:32:59Z","title":"Support Vector Machine-Based Burnout Risk Prediction with an Interactive\n  Interface for Organizational Use","summary":"  Burnout is a psychological syndrome marked by emotional exhaustion,\ndepersonalization, and reduced personal accomplishment, with a significant\nimpact on individual well-being and organizational performance. This study\nproposes a machine learning approach to predict burnout risk using the\nHackerEarth Employee Burnout Challenge dataset. Three supervised algorithms\nwere evaluated: nearest neighbors (KNN), random forest, and support vector\nmachine (SVM), with model performance evaluated through 30-fold\ncross-validation using the determination coefficient (R2). Among the models\ntested, SVM achieved the highest predictive performance (R2 = 0.84) and was\nstatistically superior to KNN and Random Forest based on paired $t$-tests. To\nensure practical applicability, an interactive interface was developed using\nStreamlit, allowing non-technical users to input data and receive burnout risk\npredictions. The results highlight the potential of machine learning to support\nearly detection of burnout and promote data-driven mental health strategies in\norganizational settings.\n","authors":["Bruno W. G. Teodosio","Mário J. O. T. Lira","Pedro H. M. Araújo","Lucas R. C. Farias"],"pdf_url":"https://arxiv.org/pdf/2510.25509v1.pdf","comment":"12 pages, including figures and references. Streamlit app available\n  at: https://employee-burnout-svm.streamlit.app/"},{"id":"http://arxiv.org/abs/2510.25502v1","updated":"2025-10-29T13:27:18Z","published":"2025-10-29T13:27:18Z","title":"TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time\n  Series Forecasting","summary":"  Foundation models for zero-shot time series forecasting face challenges in\nefficient long-horizon prediction and reproducibility, with existing\nsynthetic-only approaches underperforming on challenging benchmarks. This paper\npresents TempoPFN, a univariate time series foundation model based on linear\nRecurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The\nmodel uses a GatedDeltaProduct architecture with state-weaving for fully\nparallelizable training across sequence lengths, eliminating the need for\nwindowing or summarization techniques while maintaining robust temporal\nstate-tracking. Our comprehensive synthetic data pipeline unifies diverse\ngenerators, including stochastic differential equations, Gaussian processes,\nand audio synthesis, with novel augmentations. In zero-shot evaluations on the\nGift-Eval benchmark, TempoPFN achieves top-tier competitive performance,\noutperforming all existing synthetic-only approaches and surpassing the vast\nmajority of models trained on real-world data, while being more efficient than\nexisting baselines by leveraging fully parallelizable training and inference.\nWe open-source our complete data generation pipeline and training code,\nproviding a reproducible foundation for future research.\n","authors":["Vladyslav Moroshan","Julien Siems","Arber Zela","Timur Carstensen","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2510.25502v1.pdf","comment":"30 pages, 18 figures, 13 tables"},{"id":"http://arxiv.org/abs/2510.25497v1","updated":"2025-10-29T13:21:28Z","published":"2025-10-29T13:21:28Z","title":"Right for the Right Reasons: Avoiding Reasoning Shortcuts via\n  Prototypical Neurosymbolic AI","summary":"  Neurosymbolic AI is growing in popularity thanks to its ability to combine\nneural perception and symbolic reasoning in end-to-end trainable models.\nHowever, recent findings reveal these are prone to shortcut reasoning, i.e., to\nlearning unindented concepts--or neural predicates--which exploit spurious\ncorrelations to satisfy the symbolic constraints. In this paper, we address\nreasoning shortcuts at their root cause and we introduce prototypical\nneurosymbolic architectures. These models are able to satisfy the symbolic\nconstraints (be right) because they have learnt the correct basic concepts (for\nthe right reasons) and not because of spurious correlations, even in extremely\nlow data regimes. Leveraging the theory of prototypical learning, we\ndemonstrate that we can effectively avoid reasoning shortcuts by training the\nmodels to satisfy the background knowledge while taking into account the\nsimilarity of the input with respect to the handful of labelled datapoints. We\nextensively validate our approach on the recently proposed rsbench benchmark\nsuite in a variety of settings and tasks with very scarce supervision: we show\nsignificant improvements in learning the right concepts both in synthetic tasks\n(MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our\nfindings pave the way to prototype grounding as an effective,\nannotation-efficient strategy for safe and reliable neurosymbolic learning.\n","authors":["Luca Andolfi","Eleonora Giunchiglia"],"pdf_url":"https://arxiv.org/pdf/2510.25497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25480v1","updated":"2025-10-29T13:04:17Z","published":"2025-10-29T13:04:17Z","title":"Gradient-Weight Alignment as a Train-Time Proxy for Generalization in\n  Classification Tasks","summary":"  Robust validation metrics remain essential in contemporary deep learning, not\nonly to detect overfitting and poor generalization, but also to monitor\ntraining dynamics. In the supervised classification setting, we investigate\nwhether interactions between training data and model weights can yield such a\nmetric that both tracks generalization during training and attributes\nperformance to individual training samples. We introduce Gradient-Weight\nAlignment (GWA), quantifying the coherence between per-sample gradients and\nmodel weights. We show that effective learning corresponds to coherent\nalignment, while misalignment indicates deteriorating generalization. GWA is\nefficiently computable during training and reflects both sample-specific\ncontributions and dataset-wide learning dynamics. Extensive experiments show\nthat GWA accurately predicts optimal early stopping, enables principled model\ncomparisons, and identifies influential training samples, providing a\nvalidation-set-free approach for model analysis directly from the training\ndata.\n","authors":["Florian A. Hölzl","Daniel Rueckert","Georgios Kaissis"],"pdf_url":"https://arxiv.org/pdf/2510.25480v1.pdf","comment":"39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)"},{"id":"http://arxiv.org/abs/2510.25470v1","updated":"2025-10-29T12:43:18Z","published":"2025-10-29T12:43:18Z","title":"An In-Depth Analysis of Cyber Attacks in Secured Platforms","summary":"  There is an increase in global malware threats. To address this, an\nencryption-type ransomware has been introduced on the Android operating system.\nThe challenges associated with malicious threats in phone use have become a\npressing issue in mobile communication, disrupting user experiences and posing\nsignificant privacy threats. This study surveys commonly used machine learning\ntechniques for detecting malicious threats in phones and examines their\nperformance. The majority of past research focuses on customer feedback and\nreviews, with concerns that people might create false reviews to promote or\ndevalue products and services for personal gain. Hence, the development of\ntechniques for detecting malicious threats using machine learning has been a\nkey focus. This paper presents a comprehensive comparative study of current\nresearch on the issue of malicious threats and methods for tackling these\nchallenges. Nevertheless, a huge amount of information is required by these\nmethods, presenting a challenge for developing robust, specialized automated\nanti-malware systems. This research describes the Android Applications dataset,\nand the accuracy of the techniques is measured using the accuracy levels of the\nmetrics employed in this study.\n","authors":["Parick Ozoh","John K Omoniyi","Bukola Ibitoye"],"pdf_url":"https://arxiv.org/pdf/2510.25470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25458v1","updated":"2025-10-29T12:32:14Z","published":"2025-10-29T12:32:14Z","title":"Scalable Utility-Aware Multiclass Calibration","summary":"  Ensuring that classifiers are well-calibrated, i.e., their predictions align\nwith observed frequencies, is a minimal and fundamental requirement for\nclassifiers to be viewed as trustworthy. Existing methods for assessing\nmulticlass calibration often focus on specific aspects associated with\nprediction (e.g., top-class confidence, class-wise calibration) or utilize\ncomputationally challenging variational formulations. In this work, we study\nscalable \\emph{evaluation} of multiclass calibration. To this end, we propose\nutility calibration, a general framework that measures the calibration error\nrelative to a specific utility function that encapsulates the goals or decision\ncriteria relevant to the end user. We demonstrate how this framework can unify\nand re-interpret several existing calibration metrics, particularly allowing\nfor more robust versions of the top-class and class-wise calibration metrics,\nand, going beyond such binarized approaches, toward assessing calibration for\nricher classes of downstream utilities.\n","authors":["Mahmoud Hegazy","Michael I. Jordan","Aymeric Dieuleveut"],"pdf_url":"https://arxiv.org/pdf/2510.25458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17897v4","updated":"2025-10-29T12:15:39Z","published":"2025-07-23T19:48:27Z","title":"Multimodal Recurrent Ensembles for Predicting Brain Responses to\n  Naturalistic Movies (Algonauts 2025)","summary":"  Accurately predicting distributed cortical responses to naturalistic stimuli\nrequires models that integrate visual, auditory and semantic information over\ntime. We present a hierarchical multimodal recurrent ensemble that maps\npretrained video, audio, and language embeddings to fMRI time series recorded\nwhile four subjects watched almost 80 hours of movies provided by the Algonauts\n2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;\ntheir hidden states are fused and passed to a second recurrent layer, and\nlightweight subject-specific heads output responses for 1000 cortical parcels.\nTraining relies on a composite MSE-correlation loss and a curriculum that\ngradually shifts emphasis from early sensory to late association regions.\nAveraging 100 model variants further boosts robustness. The resulting system\nranked third on the competition leaderboard, achieving an overall Pearson r =\n0.2094 and the highest single-parcel peak score (mean r = 0.63) among all\nparticipants, with particularly strong gains for the most challenging subject\n(Subject 5). The approach establishes a simple, extensible baseline for future\nmultimodal brain-encoding benchmarks.\n","authors":["Semih Eren","Deniz Kucukahmetler","Nico Scherf"],"pdf_url":"https://arxiv.org/pdf/2507.17897v4.pdf","comment":"8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts\n  Project session (3rd-place team). Code:\n  https://github.com/erensemih/Algonauts2025_ModalityRNN v3: Added equal\n  contribution footnote to author list. Corrected reference list"},{"id":"http://arxiv.org/abs/2510.25445v1","updated":"2025-10-29T12:11:34Z","published":"2025-10-29T12:11:34Z","title":"Agentic AI: A Comprehensive Survey of Architectures, Applications, and\n  Future Directions","summary":"  Agentic AI represents a transformative shift in artificial intelligence, but\nits rapid advancement has led to a fragmented understanding, often conflating\nmodern neural systems with outdated symbolic models -- a practice known as\nconceptual retrofitting. This survey cuts through this confusion by introducing\na novel dual-paradigm framework that categorizes agentic systems into two\ndistinct lineages: the Symbolic/Classical (relying on algorithmic planning and\npersistent state) and the Neural/Generative (leveraging stochastic generation\nand prompt-driven orchestration). Through a systematic PRISMA-based review of\n90 studies (2018--2025), we provide a comprehensive analysis structured around\nthis framework across three dimensions: (1) the theoretical foundations and\narchitectural principles defining each paradigm; (2) domain-specific\nimplementations in healthcare, finance, and robotics, demonstrating how\napplication constraints dictate paradigm selection; and (3) paradigm-specific\nethical and governance challenges, revealing divergent risks and mitigation\nstrategies. Our analysis reveals that the choice of paradigm is strategic:\nsymbolic systems dominate safety-critical domains (e.g., healthcare), while\nneural systems prevail in adaptive, data-rich environments (e.g., finance).\nFurthermore, we identify critical research gaps, including a significant\ndeficit in governance models for symbolic systems and a pressing need for\nhybrid neuro-symbolic architectures. The findings culminate in a strategic\nroadmap arguing that the future of Agentic AI lies not in the dominance of one\nparadigm, but in their intentional integration to create systems that are both\nadaptable and reliable. This work provides the essential conceptual toolkit to\nguide future research, development, and policy toward robust and trustworthy\nhybrid intelligent systems.\n","authors":["Mohamad Abou Ali","Fadi Dornaika"],"pdf_url":"https://arxiv.org/pdf/2510.25445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16368v2","updated":"2025-10-29T12:06:15Z","published":"2025-05-22T08:23:10Z","title":"SATURN: SAT-based Reinforcement Learning to Unleash Language Model\n  Reasoning","summary":"  How to design reinforcement learning (RL) tasks that effectively unleash the\nreasoning capability of large language models (LLMs) remains an open question.\nExisting RL tasks (e.g., math, programming, and constructing reasoning tasks)\nsuffer from three key limitations: (1) Scalability. They rely heavily on human\nannotation or expensive LLM synthesis to generate sufficient training data. (2)\nVerifiability. LLMs' outputs are hard to verify automatically and reliably. (3)\nControllable Difficulty. Most tasks lack fine-grained difficulty control,\nmaking it hard to train LLMs to develop reasoning ability from easy to hard.\n  To address these limitations, we propose Saturn, a SAT-based RL framework\nthat uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs\nreasoning. Saturn enables scalable task construction, rule-based verification,\nand precise difficulty control. Saturn designs a curriculum learning pipeline\nthat continuously improves LLMs' reasoning capability by constructing SAT tasks\nof increasing difficulty and training LLMs from easy to hard. To ensure stable\ntraining, we design a principled mechanism to control difficulty transitions.\n  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying\ndifficulty. It supports the evaluation of how LLM reasoning changes with\nproblem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain\nSaturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT\nproblems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of\n+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B\nand Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,\nAIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in\nconstructing RL tasks, Saturn achieves further improvements of +8.8%. We\nrelease the source code, data, and models to support future research.\n","authors":["Huanyu Liu","Jia Li","Hao Zhu","Kechi Zhang","Yihong Dong","Ge Li"],"pdf_url":"https://arxiv.org/pdf/2505.16368v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07870v3","updated":"2025-10-29T12:00:29Z","published":"2023-08-15T16:37:16Z","title":"Brain-inspired Computational Intelligence via Predictive Coding","summary":"  Artificial intelligence (AI) is rapidly becoming one of the key technologies\nof this century. The majority of results in AI thus far have been achieved\nusing deep neural networks trained with a learning algorithm called error\nbackpropagation, always considered biologically implausible. To this end,\nrecent works have studied learning algorithms for deep neural networks inspired\nby the neurosciences. One such theory, called predictive coding (PC), has shown\npromising properties that make it potentially valuable for the machine learning\ncommunity: it can model information processing in different areas of the brain,\ncan be used in control and robotics, has a solid mathematical foundation in\nvariational inference, and performs its computations asynchronously. Inspired\nby such properties, works that propose novel PC-like algorithms are starting to\nbe present in multiple sub-fields of machine learning and AI at large. Here, we\nsurvey such efforts by first providing a broad overview of the history of PC to\nprovide common ground for the understanding of the recent developments, then by\ndescribing current efforts and results, and concluding with a large discussion\nof possible implications and ways forward.\n","authors":["Tommaso Salvatori","Ankur Mali","Christopher L. Buckley","Thomas Lukasiewicz","Rajesh P. N. Rao","Karl Friston","Alexander Ororbia"],"pdf_url":"https://arxiv.org/pdf/2308.07870v3.pdf","comment":"26 Pages, 9 Figures"},{"id":"http://arxiv.org/abs/2412.04233v4","updated":"2025-10-29T11:37:54Z","published":"2024-12-05T15:09:51Z","title":"HyperMARL: Adaptive Hypernetworks for Multi-Agent RL","summary":"  Adaptive cooperation in multi-agent reinforcement learning (MARL) requires\npolicies to express homogeneous, specialised, or mixed behaviours, yet\nachieving this adaptivity remains a critical challenge. While parameter sharing\n(PS) is standard for efficient learning, it notoriously suppresses the\nbehavioural diversity required for specialisation. This failure is largely due\nto cross-agent gradient interference, a problem we find is surprisingly\nexacerbated by the common practice of coupling agent IDs with observations.\nExisting remedies typically add complexity through altered objectives, manual\npreset diversity levels, or sequential updates -- raising a fundamental\nquestion: can shared policies adapt without these intricacies? We propose a\nsolution built on a key insight: an agent-conditioned hypernetwork can generate\nagent-specific parameters and decouple observation- and agent-conditioned\ngradients, directly countering the interference from coupling agent IDs with\nobservations. Our resulting method, HyperMARL, avoids the complexities of prior\nwork and empirically reduces policy gradient variance. Across diverse MARL\nbenchmarks (22 scenarios, up to 30 agents), HyperMARL achieves performance\ncompetitive with six key baselines while preserving behavioural diversity\ncomparable to non-parameter sharing methods, establishing it as a versatile and\nprincipled approach for adaptive MARL. The code is publicly available at\nhttps://github.com/KaleabTessera/HyperMARL.\n","authors":["Kale-ab Abebe Tessera","Arrasy Rahman","Amos Storkey","Stefano V. Albrecht"],"pdf_url":"https://arxiv.org/pdf/2412.04233v4.pdf","comment":"To appear at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025). A preliminary version of this work was presented at\n  the CoCoMARL workshop, RLC 2025"},{"id":"http://arxiv.org/abs/2406.07222v3","updated":"2025-10-29T11:36:28Z","published":"2024-06-11T13:01:50Z","title":"Reliable Evaluation and Benchmarks for Statement Autoformalization","summary":"  Evaluating statement autoformalization, translating natural language\nmathematics into formal languages like Lean 4, remains a significant challenge,\nwith few metrics, datasets, and standards to robustly measure progress. In this\nwork, we present a comprehensive approach combining improved metrics, robust\nbenchmarks, and systematic evaluation, to fill this gap. First, we introduce\nBEq+, an automated metric that correlates strongly with human judgment, along\nwith ProofNetVerif, a new dataset for assessing the quality of evaluation\nmetrics, containing 3,752 annotated examples. Second, we develop two new\nautoformalization benchmarks: ProofNet#, a corrected version of ProofNet, and\nRLM25, with 619 new pairs of research-level mathematics from six formalization\nprojects. Through systematic experimentation across these benchmarks, we find\nthat current techniques can achieve up to 45.1% accuracy on undergraduate\nmathematics but struggle with research-level content without proper context.\nOur work establishes a reliable foundation for evaluating and advancing\nautoformalization systems.\n","authors":["Auguste Poiroux","Gail Weiss","Viktor Kunčak","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2406.07222v3.pdf","comment":"Accepted to EMNLP 2025. New benchmarks released, see\n  https://github.com/augustepoiroux/RLMEval ,\n  https://huggingface.co/datasets/PAug/ProofNetSharp , and\n  https://huggingface.co/datasets/PAug/ProofNetVerif . For code, see\n  https://github.com/augustepoiroux/LeanInteract"},{"id":"http://arxiv.org/abs/2504.17247v2","updated":"2025-10-29T11:30:12Z","published":"2025-04-24T04:53:04Z","title":"OmegAMP: Targeted AMP Discovery through Biologically Informed Generation","summary":"  Deep learning-based antimicrobial peptide (AMP) discovery faces critical\nchallenges such as limited controllability, lack of representations that\nefficiently model antimicrobial properties, and low experimental hit rates. To\naddress these challenges, we introduce OmegAMP, a framework designed for\nreliable AMP generation with increased controllability. Its diffusion-based\ngenerative model leverages a novel conditioning mechanism to achieve\nfine-grained control over desired physicochemical properties and to direct\ngeneration towards specific activity profiles, including species-specific\neffectiveness. This is further enhanced by a biologically informed encoding\nspace that significantly improves overall generative performance. Complementing\nthese generative capabilities, OmegAMP leverages a novel synthetic data\naugmentation strategy to train classifiers for AMP filtering, drastically\nreducing false positive rates and thereby increasing the likelihood of\nexperimental success. Our in silico experiments demonstrate that OmegAMP\ndelivers state-of-the-art performance across key stages of the AMP discovery\npipeline, enabling us to achieve an unprecedented success rate in wet lab\nexperiments. We tested 25 candidate peptides, 24 of them (96%) demonstrated\nantimicrobial activity, proving effective even against multi-drug resistant\nstrains. Our findings underscore OmegAMP's potential to significantly advance\ncomputational frameworks in the fight against antimicrobial resistance.\n","authors":["Diogo Soares","Leon Hetzel","Paulina Szymczak","Marcelo Der Torossian Torres","Johanna Sommer","Cesar de la Fuente-Nunez","Fabian Theis","Stephan Günnemann","Ewa Szczurek"],"pdf_url":"https://arxiv.org/pdf/2504.17247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25404v1","updated":"2025-10-29T11:21:55Z","published":"2025-10-29T11:21:55Z","title":"GPTOpt: Towards Efficient LLM-Based Black-Box Optimization","summary":"  Global optimization of expensive, derivative-free black-box functions demands\nextreme sample efficiency. Classical methods such as Bayesian Optimization (BO)\ncan be effective, but they often require careful parameter tuning to each\napplication domain. At the same time, Large Language Models (LLMs) have shown\nbroad capabilities, yet state-of-the-art models remain limited in solving\ncontinuous black-box optimization tasks. We introduce GPTOpt, an LLM-based\noptimization method that equips LLMs with continuous black-box optimization\ncapabilities. By fine-tuning large language models on extensive synthetic\ndatasets derived from diverse BO parameterizations, GPTOpt leverages LLM\npre-training to generalize across optimization tasks. On a variety of black-box\noptimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting\nthe capacity of LLMs for advanced numerical reasoning and introducing a\nflexible framework for global optimization without parameter tuning.\n","authors":["Jamison Meindl","Yunsheng Tian","Tony Cui","Veronika Thost","Zhang-Wei Hong","Jie Chen","Wojciech Matusik","Mina Konaković Luković"],"pdf_url":"https://arxiv.org/pdf/2510.25404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23665v2","updated":"2025-10-29T11:16:57Z","published":"2025-10-26T13:48:03Z","title":"Transformers from Compressed Representations","summary":"  Compressed file formats are the corner stone of efficient data storage and\ntransmission, yet their potential for representation learning remains largely\nunderexplored. We introduce TEMPEST (TransformErs froM comPressed\nrEpreSenTations), a method that exploits the inherent byte-stream structure of\ncompressed files to design an effective tokenization and encoding strategy. By\nleveraging this compact encoding, a standard transformer can directly learn\nsemantic representations from compressed data streams, bypassing the need for\nraw byte-level processing or full media decoding. Our proposal substantially\nreduces the number of tokens required for semantic classification, thereby\nlowering both computational complexity and memory usage. Through extensive\nexperiments across diverse datasets, coding schemes, and modalities, we show\nthat TEMPEST achieves accuracy competitive wit the state-of-the-art while\ndelivering efficiency gains in memory and compute.\n","authors":["Juan C. Leon Alcazar","Mattia Soldan","Mohammad Saatialsoruji","Alejandro Pardo","Hani Itani","Juan Camilo Perez","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2510.23665v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23463v2","updated":"2025-10-29T11:16:37Z","published":"2025-10-27T16:01:15Z","title":"Differential Privacy as a Perk: Federated Learning over Multiple-Access\n  Fading Channels with a Multi-Antenna Base Station","summary":"  Federated Learning (FL) is a distributed learning paradigm that preserves\nprivacy by eliminating the need to exchange raw data during training. In its\nprototypical edge instantiation with underlying wireless transmissions enabled\nby analog over-the-air computing (AirComp), referred to as \\emph{over-the-air\nFL (AirFL)}, the inherent channel noise plays a unique role of \\emph{frenemy}\nin the sense that it degrades training due to noisy global aggregation while\nproviding a natural source of randomness for privacy-preserving mechanisms,\nformally quantified by \\emph{differential privacy (DP)}. It remains,\nnevertheless, challenging to effectively harness such channel impairments, as\nprior arts, under assumptions of either simple channel models or restricted\ntypes of loss functions, mostly considering (local) DP enhancement with a\nsingle-round or non-convergent bound on privacy loss. In this paper, we study\nAirFL over multiple-access fading channels with a multi-antenna base station\n(BS) subject to user-level DP requirements. Despite a recent study, which\nclaimed in similar settings that artificial noise (AN) must be injected to\nensure DP in general, we demonstrate, on the contrary, that DP can be gained as\na \\emph{perk} even \\emph{without} employing any AN. Specifically, we derive a\nnovel bound on DP that converges under general bounded-domain assumptions on\nmodel parameters, along with a convergence bound with general smooth and\nnon-convex loss functions. Next, we optimize over receive beamforming and power\nallocations to characterize the optimal convergence-privacy trade-offs, which\nalso reveal explicit conditions in which DP is achievable without compromising\ntraining. Finally, our theoretical findings are validated by extensive\nnumerical results.\n","authors":["Hao Liang","Haifeng Wen","Kaishun Wu","Hong Xing"],"pdf_url":"https://arxiv.org/pdf/2510.23463v2.pdf","comment":"15 pages, 5 figures, submitted for possible publication"},{"id":"http://arxiv.org/abs/2509.04317v2","updated":"2025-10-29T11:06:35Z","published":"2025-09-04T15:38:37Z","title":"Improving Robustness of AlphaZero Algorithms to Test-Time Environment\n  Changes","summary":"  The AlphaZero framework provides a standard way of combining Monte Carlo\nplanning with prior knowledge provided by a previously trained policy-value\nneural network. AlphaZero usually assumes that the environment on which the\nneural network was trained will not change at test time, which constrains its\napplicability. In this paper, we analyze the problem of deploying AlphaZero\nagents in potentially changed test environments and demonstrate how the\ncombination of simple modifications to the standard framework can significantly\nboost performance, even in settings with a low planning budget available. The\ncode is publicly available on GitHub.\n","authors":["Isidoro Tamassia","Wendelin Böhmer"],"pdf_url":"https://arxiv.org/pdf/2509.04317v2.pdf","comment":"Presented at the 37th Benelux Conference on Artificial Intelligence\n  and the 34th Belgian Dutch Conference on Machine Learning (BNAIC/BeNeLearn\n  2025)"},{"id":"http://arxiv.org/abs/2407.13420v2","updated":"2025-10-29T11:02:41Z","published":"2024-07-18T11:42:58Z","title":"Exploring End-to-end Differentiable Neural Charged Particle Tracking --\n  A Loss Landscape Perspective","summary":"  Measurement and analysis of high energetic particles for scientific, medical\nor industrial applications is a complex procedure, requiring the design of\nsophisticated detector and data processing systems. The development of adaptive\nand differentiable software pipelines using a combination of conventional and\nmachine learning algorithms is therefore getting ever more important to\noptimize and operate the system efficiently while maintaining end-to-end (E2E)\ndifferentiability. We propose for the application of charged particle tracking\nan E2E differentiable decision-focused learning scheme using graph neural\nnetworks with combinatorial components solving a linear assignment problem for\neach detector layer. We demonstrate empirically that including differentiable\nvariations of discrete assignment operations allows for efficient network\noptimization, working better or on par with approaches that lack E2E\ndifferentiability. In additional studies, we dive deeper into the optimization\nprocess and provide further insights from a loss landscape perspective. We\ndemonstrate that while both methods converge into similar performing, globally\nwell-connected regions, they suffer under substantial predictive instability\nacross initialization and optimization methods, which can have unpredictable\nconsequences on the performance of downstream tasks such as image\nreconstruction. We also point out a dependency between the interpolation factor\nof the gradient estimator and the prediction stability of the model, suggesting\nthe choice of sufficiently small values. Given the strong global connectivity\nof learned solutions and the excellent training performance, we argue that E2E\ndifferentiability provides, besides the general availability of gradient\ninformation, an important tool for robust particle tracking to mitigate\nprediction instabilities by favoring solutions that perform well on downstream\ntasks.\n","authors":["Tobias Kortus","Ralf Keidel","Nicolas R. Gauger"],"pdf_url":"https://arxiv.org/pdf/2407.13420v2.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR), 2025"},{"id":"http://arxiv.org/abs/2510.21758v3","updated":"2025-10-29T11:02:07Z","published":"2025-10-11T20:16:32Z","title":"Taxonomy and Trends in Reinforcement Learning for Robotics and Control\n  Systems: A Structured Review","summary":"  Reinforcement learning (RL) has become a foundational approach for enabling\nintelligent robotic behavior in dynamic and uncertain environments. This work\npresents an in-depth review of RL principles, advanced deep reinforcement\nlearning (DRL) algorithms, and their integration into robotic and control\nsystems. Beginning with the formalism of Markov Decision Processes (MDPs), the\nstudy outlines essential elements of the agent-environment interaction and\nexplores core algorithmic strategies including actor-critic methods,\nvalue-based learning, and policy gradients. Emphasis is placed on modern DRL\ntechniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving\nhigh-dimensional, continuous control tasks. A structured taxonomy is introduced\nto categorize RL applications across domains such as locomotion, manipulation,\nmulti-agent coordination, and human-robot interaction, along with training\nmethodologies and deployment readiness levels. The review synthesizes recent\nresearch efforts, highlighting technical trends, design patterns, and the\ngrowing maturity of RL in real-world robotics. Overall, this work aims to\nbridge theoretical advances with practical implementations, providing a\nconsolidated perspective on the evolving role of RL in autonomous robotic\nsystems.\n","authors":["Kumater Ter","Ore-Ofe Ajayi","Daniel Udekwe"],"pdf_url":"https://arxiv.org/pdf/2510.21758v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.16791v3","updated":"2025-10-29T10:58:12Z","published":"2025-06-20T07:14:48Z","title":"TabArena: A Living Benchmark for Machine Learning on Tabular Data","summary":"  With the growing popularity of deep learning and foundation models for\ntabular data, the need for standardized and reliable benchmarks is higher than\never. However, current benchmarks are static. Their design is not updated even\nif flaws are discovered, model versions are updated, or new models are\nreleased. To address this, we introduce TabArena, the first continuously\nmaintained living tabular benchmarking system. To launch TabArena, we manually\ncurate a representative collection of datasets and well-implemented models,\nconduct a large-scale benchmarking study to initialize a public leaderboard,\nand assemble a team of experienced maintainers. Our results highlight the\ninfluence of validation method and ensembling of hyperparameter configurations\nto benchmark models at their full potential. While gradient-boosted trees are\nstill strong contenders on practical tabular datasets, we observe that deep\nlearning methods have caught up under larger time budgets with ensembling. At\nthe same time, foundation models excel on smaller datasets. Finally, we show\nthat ensembles across models advance the state-of-the-art in tabular machine\nlearning. We observe that some deep learning models are overrepresented in\ncross-model ensembles due to validation set overfitting, and we encourage model\ndevelopers to address this issue. We launch TabArena with a public leaderboard,\nreproducible code, and maintenance protocols to create a living benchmark\navailable at https://tabarena.ai.\n","authors":["Nick Erickson","Lennart Purucker","Andrej Tschalzev","David Holzmüller","Prateek Mutalik Desai","David Salinas","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2506.16791v3.pdf","comment":"Accepted (spotlight) at NeurIPS 2025 Datasets and Benchmarks Track.\n  v3: NeurIPS camera-ready version. v2: fixed author list. 51 pages. Code\n  available at https://tabarena.ai/code; examples at\n  https://tabarena.ai/code-examples; dataset curation at\n  https://tabarena.ai/data-tabular-ml-iid-study and\n  https://tabarena.ai/dataset-curation"},{"id":"http://arxiv.org/abs/2509.18376v2","updated":"2025-10-29T10:57:22Z","published":"2025-09-22T19:58:17Z","title":"GnnXemplar: Exemplars to Explanations -- Natural Language Rules for\n  Global GNN Interpretability","summary":"  Graph Neural Networks (GNNs) are widely used for node classification, yet\ntheir opaque decision-making limits trust and adoption. While local\nexplanations offer insights into individual predictions, global explanation\nmethods, those that characterize an entire class, remain underdeveloped.\nExisting global explainers rely on motif discovery in small graphs, an approach\nthat breaks down in large, real-world settings where subgraph repetition is\nrare, node attributes are high-dimensional, and predictions arise from complex\nstructure-attribute interactions. We propose GnnXemplar, a novel global\nexplainer inspired from Exemplar Theory from cognitive science. GnnXemplar\nidentifies representative nodes in the GNN embedding space, exemplars, and\nexplains predictions using natural language rules derived from their\nneighborhoods. Exemplar selection is framed as a coverage maximization problem\nover reverse k-nearest neighbors, for which we provide an efficient greedy\napproximation. To derive interpretable rules, we employ a self-refining prompt\nstrategy using large language models (LLMs). Experiments across diverse\nbenchmarks show that GnnXemplar significantly outperforms existing methods in\nfidelity, scalability, and human interpretability, as validated by a user study\nwith 60 participants.\n","authors":["Burouj Armgaan","Eshan Jain","Harsh Pandey","Mahesh Chandran","Sayan Ranu"],"pdf_url":"https://arxiv.org/pdf/2509.18376v2.pdf","comment":"38 pages, 20 figures, NeurIPS 2025 (Oral)"},{"id":"http://arxiv.org/abs/2510.25379v1","updated":"2025-10-29T10:52:02Z","published":"2025-10-29T10:52:02Z","title":"A Deep Learning Framework for Multi-Operator Learning: Architectures and\n  Approximation Theory","summary":"  While many problems in machine learning focus on learning mappings between\nfinite-dimensional spaces, scientific applications require approximating\nmappings between function spaces, i.e., operators. We study the problem of\nlearning collections of operators and provide both theoretical and empirical\nadvances. We distinguish between two regimes: (i) multiple operator learning,\nwhere a single network represents a continuum of operators parameterized by a\nparametric function, and (ii) learning several distinct single operators, where\neach operator is learned independently. For the multiple operator case, we\nintroduce two new architectures, $\\mathrm{MNO}$ and $\\mathrm{MONet}$, and\nestablish universal approximation results in three settings: continuous,\nintegrable, or Lipschitz operators. For the latter, we further derive explicit\nscaling laws that quantify how the network size must grow to achieve a target\napproximation accuracy. For learning several single operators, we develop a\nframework for balancing architectural complexity across subnetworks and show\nhow approximation order determines computational efficiency. Empirical\nexperiments on parametric PDE benchmarks confirm the strong expressive power\nand efficiency of the proposed architectures. Overall, this work establishes a\nunified theoretical and practical foundation for scalable neural operator\nlearning across multiple operators.\n","authors":["Adrien Weihs","Jingmin Sun","Zecheng Zhang","Hayden Schaeffer"],"pdf_url":"https://arxiv.org/pdf/2510.25379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20274v2","updated":"2025-10-29T10:47:16Z","published":"2025-05-26T17:53:28Z","title":"Probabilistic Kernel Function for Fast Angle Testing","summary":"  In this paper, we study the angle testing problem in the context of\nsimilarity search in high-dimensional Euclidean spaces and propose two\nprojection-based probabilistic kernel functions, one designed for angle\ncomparison and the other for angle thresholding. Unlike existing approaches\nthat rely on random projection vectors drawn from Gaussian distributions, our\napproach leverages reference angles and employs a deterministic structure for\nthe projection vectors. Notably, our kernel functions do not require asymptotic\nassumptions, such as the number of projection vectors tending to infinity, and\ncan be both theoretically and experimentally shown to outperform\nGaussian-distribution-based kernel functions. We apply the proposed kernel\nfunction to Approximate Nearest Neighbor Search (ANNS) and demonstrate that our\napproach achieves a 2.5X ~ 3X higher query-per-second (QPS) throughput compared\nto the widely-used graph-based search algorithm HNSW.\n","authors":["Kejing Lu","Chuan Xiao","Yoshiharu Ishikawa"],"pdf_url":"https://arxiv.org/pdf/2505.20274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25372v1","updated":"2025-10-29T10:42:56Z","published":"2025-10-29T10:42:56Z","title":"Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision\n  Transformers","summary":"  Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has\nproven highly effective as a parameter-efficient fine-tuning technique for\nadapting large models to downstream tasks with limited data. Its parameter\nefficiency makes it particularly suitable for Federated Learning (FL), where\nboth communication and computation budgets are often constrained. However,\nglobal prompt tuning struggles to generalize across heterogeneous clients,\nwhile personalized tuning overfits to local data and lacks generalization. We\npropose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt\nTuning), a unified framework designed to achieve both generalization and\npersonalization in federated prompt tuning of ViTs. Within this framework, we\nintroduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on\nclass-specific prompts maintained alongside a globally shared prompt. For each\ninput, CCMP adaptively combines class-specific prompts using weights derived\nfrom global class prototypes and client class priors. This approach enables\nper-sample prompt personalization without storing client-dependent trainable\nparameters. The prompts are collaboratively optimized via traditional federated\naveraging technique on the same. Comprehensive evaluations on CIFAR-100,\nTinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT\nconsistently surpasses the state-of-the-art baselines under diverse data\nheterogeneity scenarios, establishing a strong foundation for efficient and\ngeneralizable federated prompt tuning of Vision Transformers.\n","authors":["M Yashwanth","Sharannya Ghosh","Aditay Tripathi","Anirban Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2510.25372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25368v1","updated":"2025-10-29T10:39:29Z","published":"2025-10-29T10:39:29Z","title":"Position: Biology is the Challenge Physics-Informed ML Needs to Evolve","summary":"  Physics-Informed Machine Learning (PIML) has successfully integrated\nmechanistic understanding into machine learning, particularly in domains\ngoverned by well-known physical laws. This success has motivated efforts to\napply PIML to biology, a field rich in dynamical systems but shaped by\ndifferent constraints. Biological modeling, however, presents unique\nchallenges: multi-faceted and uncertain prior knowledge, heterogeneous and\nnoisy data, partial observability, and complex, high-dimensional networks. In\nthis position paper, we argue that these challenges should not be seen as\nobstacles to PIML, but as catalysts for its evolution. We propose\nBiology-Informed Machine Learning (BIML): a principled extension of PIML that\nretains its structural grounding while adapting to the practical realities of\nbiology. Rather than replacing PIML, BIML retools its methods to operate under\nsofter, probabilistic forms of prior knowledge. We outline four foundational\npillars as a roadmap for this transition: uncertainty quantification,\ncontextualization, constrained latent structure inference, and scalability.\nFoundation Models and Large Language Models will be key enablers, bridging\nhuman expertise with computational modeling. We conclude with concrete\nrecommendations to build the BIML ecosystem and channel PIML-inspired\ninnovation toward challenges of high scientific and societal relevance.\n","authors":["Julien Martinelli"],"pdf_url":"https://arxiv.org/pdf/2510.25368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25366v1","updated":"2025-10-29T10:37:24Z","published":"2025-10-29T10:37:24Z","title":"A Convexity-dependent Two-Phase Training Algorithm for Deep Neural\n  Networks","summary":"  The key task of machine learning is to minimize the loss function that\nmeasures the model fit to the training data. The numerical methods to do this\nefficiently depend on the properties of the loss function. The most decisive\namong these properties is the convexity or non-convexity of the loss function.\nThe fact that the loss function can have, and frequently has, non-convex\nregions has led to a widespread commitment to non-convex methods such as Adam.\nHowever, a local minimum implies that, in some environment around it, the\nfunction is convex. In this environment, second-order minimizing methods such\nas the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We\npropose a novel framework grounded in the hypothesis that loss functions in\nreal-world tasks swap from initial non-convexity to convexity towards the\noptimum. This is a property we leverage to design an innovative two-phase\noptimization algorithm. The presented algorithm detects the swap point by\nobserving the gradient norm dependence on the loss. In these regions,\nnon-convex (Adam) and convex (CG) algorithms are used, respectively. Computing\nexperiments confirm the hypothesis that this simple convexity structure is\nfrequent enough to be practically exploited to substantially improve\nconvergence and accuracy.\n","authors":["Tomas Hrycej","Bernhard Bermeitinger","Massimo Pavone","Götz-Henrik Wiegand","Siegfried Handschuh"],"pdf_url":"https://arxiv.org/pdf/2510.25366v1.pdf","comment":"Appeared on KDIR IC3K Conference 2025 (Best Paper Award)"},{"id":"http://arxiv.org/abs/2506.14866v2","updated":"2025-10-29T10:34:04Z","published":"2025-06-17T17:59:31Z","title":"OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents","summary":"  Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.\n","authors":["Thomas Kuntz","Agatha Duzan","Hao Zhao","Francesco Croce","Zico Kolter","Nicolas Flammarion","Maksym Andriushchenko"],"pdf_url":"https://arxiv.org/pdf/2506.14866v2.pdf","comment":"NeurIPS 2025 Datasets & Benchmarks Track (Spotlight)"},{"id":"http://arxiv.org/abs/2510.25361v1","updated":"2025-10-29T10:32:39Z","published":"2025-10-29T10:32:39Z","title":"Parameter Averaging in Link Prediction","summary":"  Ensemble methods are widely employed to improve generalization in machine\nlearning. This has also prompted the adoption of ensemble learning for the\nknowledge graph embedding (KGE) models in performing link prediction. Typical\napproaches to this end train multiple models as part of the ensemble, and the\ndiverse predictions are then averaged. However, this approach has some\nsignificant drawbacks. For instance, the computational overhead of training\nmultiple models increases latency and memory overhead. In contrast, model\nmerging approaches offer a promising alternative that does not require training\nmultiple models. In this work, we introduce model merging, specifically\nweighted averaging, in KGE models. Herein, a running average of model\nparameters from a training epoch onward is maintained and used for predictions.\nTo address this, we additionally propose an approach that selectively updates\nthe running average of the ensemble model parameters only when the\ngeneralization performance improves on a validation dataset. We evaluate these\ntwo different weighted averaging approaches on link prediction tasks, comparing\nthe state-of-the-art benchmark ensemble approach. Additionally, we evaluate the\nweighted averaging approach considering literal-augmented KGE models and\nmulti-hop query answering tasks as well. The results demonstrate that the\nproposed weighted averaging approach consistently improves performance across\ndiverse evaluation settings.\n","authors":["Rupesh Sapkota","Caglar Demir","Arnab Sharma","Axel-Cyrille Ngonga Ngomo"],"pdf_url":"https://arxiv.org/pdf/2510.25361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12308v4","updated":"2025-10-29T10:28:05Z","published":"2024-11-19T07:49:22Z","title":"SNN-Based Online Learning of Concepts and Action Laws in an Open World","summary":"  We present the architecture of a fully autonomous, bio-inspired cognitive\nagent built around a spiking neural network (SNN) implementing the agent's\nsemantic memory. This agent explores its universe and learns concepts of\nobjects/situations and of its own actions in a one-shot manner. While\nobject/situation concepts are unary, action concepts are triples made up of an\ninitial situation, a motor activity, and an outcome. They embody the agent's\nknowledge of its universe's action laws. Both kinds of concepts have different\ndegrees of generality. To make decisions the agent queries its semantic memory\nfor the expected outcomes of envisaged actions and chooses the action to take\non the basis of these predictions. Our experiments show that the agent handles\nnew situations by appealing to previously learned general concepts and rapidly\nmodifies its concepts to adapt to environment changes.\n","authors":["Christel Grimaud","Dominique Longin","Andreas Herzig"],"pdf_url":"https://arxiv.org/pdf/2411.12308v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19252v2","updated":"2025-10-29T10:20:13Z","published":"2025-05-25T18:15:29Z","title":"Learning-Augmented Online Bipartite Fractional Matching","summary":"  Online bipartite matching is a fundamental problem in online optimization,\nextensively studied both in its integral and fractional forms due to its\ntheoretical significance and practical applications, such as online advertising\nand resource allocation. Motivated by recent progress in learning-augmented\nalgorithms, we study online bipartite fractional matching when the algorithm is\ngiven advice in the form of a suggested matching in each iteration. We develop\nalgorithms for both the vertex-weighted and unweighted variants that provably\ndominate the naive \"coin flip\" strategy of randomly choosing between the\nadvice-following and advice-free algorithms. Moreover, our algorithm for the\nvertex-weighted setting extends to the AdWords problem under the small bids\nassumption, yielding a significant improvement over the seminal work of\nMahdian, Nazerzadeh, and Saberi (EC 2007, TALG 2012). Complementing our\npositive results, we establish a hardness bound on the robustness-consistency\ntradeoff that is attainable by any algorithm. We empirically validate our\nalgorithms through experiments on synthetic and real-world data.\n","authors":["Davin Choo","Billy Jin","Yongho Shin"],"pdf_url":"https://arxiv.org/pdf/2505.19252v2.pdf","comment":"To appear in NeurIPS 2025. Full version"},{"id":"http://arxiv.org/abs/2510.25354v1","updated":"2025-10-29T10:19:32Z","published":"2025-10-29T10:19:32Z","title":"Analysis of Semi-Supervised Learning on Hypergraphs","summary":"  Hypergraphs provide a natural framework for modeling higher-order\ninteractions, yet their theoretical underpinnings in semi-supervised learning\nremain limited. We provide an asymptotic consistency analysis of variational\nlearning on random geometric hypergraphs, precisely characterizing the\nconditions ensuring the well-posedness of hypergraph learning as well as\nshowing convergence to a weighted $p$-Laplacian equation. Motivated by this, we\npropose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers\nof Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to\na higher-order Sobolev seminorm. Empirically, it performs strongly on standard\nbaselines.\n","authors":["Adrien Weihs","Andrea Bertozzi","Matthew Thorpe"],"pdf_url":"https://arxiv.org/pdf/2510.25354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06204v2","updated":"2025-10-29T10:17:57Z","published":"2025-07-08T17:30:14Z","title":"Differential Mamba","summary":"  Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available: https://github.com/NadavSc/Diff-Mamba\n","authors":["Nadav Schneider","Itamar Zimerman","Eliya Nachmani"],"pdf_url":"https://arxiv.org/pdf/2507.06204v2.pdf","comment":"AACL 2025. We provide the code at\n  https://github.com/NadavSc/Diff-Mamba"},{"id":"http://arxiv.org/abs/2502.04864v2","updated":"2025-10-29T10:11:05Z","published":"2025-02-07T12:07:57Z","title":"Redistributing Rewards Across Time and Agents for Multi-Agent\n  Reinforcement Learning","summary":"  Credit assignmen, disentangling each agent's contribution to a shared reward,\nis a critical challenge in cooperative multi-agent reinforcement learning\n(MARL). To be effective, credit assignment methods must preserve the\nenvironment's optimal policy. Some recent approaches attempt this by enforcing\nreturn equivalence, where the sum of distributed rewards must equal the team\nreward. However, their guarantees are conditional on a learned model's\nregression accuracy, making them unreliable in practice. We introduce\nTemporal-Agent Reward Redistribution (TAR$^2$), an approach that decouples\ncredit modeling from this constraint. A neural network learns unnormalized\ncontribution scores, while a separate, deterministic normalization step\nenforces return equivalence by construction. We demonstrate that this method is\nequivalent to a valid Potential-Based Reward Shaping (PBRS), which guarantees\nthe optimal policy is preserved regardless of model accuracy. Empirically, on\nchallenging SMACLite and Google Research Football (GRF) benchmarks, TAR$^2$\naccelerates learning and achieves higher final performance than strong\nbaselines. These results establish our method as an effective solution for the\nagent-temporal credit assignment problem.\n","authors":["Aditya Kapoor","Kale-ab Tessera","Mayank Baranwal","Harshad Khadilkar","Jan Peters","Stefano Albrecht","Mingfei Sun"],"pdf_url":"https://arxiv.org/pdf/2502.04864v2.pdf","comment":"16 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2510.25348v1","updated":"2025-10-29T10:06:08Z","published":"2025-10-29T10:06:08Z","title":"Beyond Leakage and Complexity: Towards Realistic and Efficient\n  Information Cascade Prediction","summary":"  Information cascade popularity prediction is a key problem in analyzing\ncontent diffusion in social networks. However, current related works suffer\nfrom three critical limitations: (1) temporal leakage in current\nevaluation--random cascade-based splits allow models to access future\ninformation, yielding unrealistic results; (2) feature-poor datasets that lack\ndownstream conversion signals (e.g., likes, comments, or purchases), which\nlimits more practical applications; (3) computational inefficiency of complex\ngraph-based methods that require days of training for marginal gains. We\nsystematically address these challenges from three perspectives: task setup,\ndataset construction, and model design. First, we propose a time-ordered\nsplitting strategy that chronologically partitions data into consecutive\nwindows, ensuring models are evaluated on genuine forecasting tasks without\nfuture information leakage. Second, we introduce Taoke, a large-scale\ne-commerce cascade dataset featuring rich promoter/product attributes and\nground-truth purchase conversions--capturing the complete diffusion lifecycle\nfrom promotion to monetization. Third, we develop CasTemp, a lightweight\nframework that efficiently models cascade dynamics through temporal walks,\nJaccard-based neighbor selection for inter-cascade dependencies, and GRU-based\nencoding with time-aware attention. Under leak-free evaluation, CasTemp\nachieves state-of-the-art performance across four datasets with\norders-of-magnitude speedup. Notably, it excels at predicting second-stage\npopularity conversions--a practical task critical for real-world applications.\n","authors":["Jie Peng","Rui Wang","Qiang Wang","Zhewei Wei","Bin Tong","Guan Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25347v1","updated":"2025-10-29T10:04:47Z","published":"2025-10-29T10:04:47Z","title":"3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine\n  Learning Framework","summary":"  Coronary artery calcium (CAC) scoring plays a crucial role in the early\ndetection and risk stratification of coronary artery disease (CAD). In this\nstudy, we focus on non-contrast coronary computed tomography angiography (CCTA)\nscans, which are commonly used for early calcification detection in clinical\nsettings. To address the challenge of limited annotated data, we propose a\nradiomics-based pipeline that leverages pseudo-labeling to generate training\nlabels, thereby eliminating the need for expert-defined segmentations.\nAdditionally, we explore the use of pretrained foundation models, specifically\nCT-FM and RadImageNet, to extract image features, which are then used with\ntraditional classifiers. We compare the performance of these deep learning\nfeatures with that of radiomics features. Evaluation is conducted on a clinical\nCCTA dataset comprising 182 patients, where individuals are classified into two\ngroups: zero versus non-zero calcium scores. We further investigate the impact\nof training on non-contrast datasets versus combined contrast and non-contrast\ndatasets, with testing performed only on non contrast scans. Results show that\nradiomics-based models significantly outperform CNN-derived embeddings from\nfoundation models (achieving 84% accuracy and p<0.05), despite the\nunavailability of expert annotations.\n","authors":["Ayman Abaid","Gianpiero Guidone","Sara Alsubai","Foziyah Alquahtani","Talha Iqbal","Ruth Sharif","Hesham Elzomor","Emiliano Bianchini","Naeif Almagal","Michael G. Madden","Faisal Sharif","Ihsan Ullah"],"pdf_url":"https://arxiv.org/pdf/2510.25347v1.pdf","comment":"11 pages, 2 Figures, MICCAI AMAI 2025 workshop, to be published in\n  Volume 16206 of the Lecture Notes in Computer Science series"},{"id":"http://arxiv.org/abs/2510.25327v1","updated":"2025-10-29T09:41:03Z","published":"2025-10-29T09:41:03Z","title":"MMEdge: Accelerating On-device Multimodal Inference via Pipelined\n  Sensing and Encoding","summary":"  Real-time multimodal inference on resource-constrained edge devices is\nessential for applications such as autonomous driving, human-computer\ninteraction, and mobile health. However, prior work often overlooks the tight\ncoupling between sensing dynamics and model execution, as well as the complex\ninter-modality dependencies. In this paper, we propose MMEdge, an new on-device\nmulti-modal inference framework based on pipelined sensing and encoding.\nInstead of waiting for complete sensor inputs, MMEdge decomposes the entire\ninference process into a sequence of fine-grained sensing and encoding units,\nallowing computation to proceed incrementally as data arrive. MMEdge also\nintroduces a lightweight but effective temporal aggregation module that\ncaptures rich temporal dynamics across different pipelined units to maintain\naccuracy performance. Such pipelined design also opens up opportunities for\nfine-grained cross-modal optimization and early decision-making during\ninference. To further enhance system performance under resource variability and\ninput data complexity, MMEdge incorporates an adaptive multimodal configuration\noptimizer that dynamically selects optimal sensing and model configurations for\neach modality under latency constraints, and a cross-modal speculative skipping\nmechanism that bypasses future units of slower modalities when early\npredictions reach sufficient confidence. We evaluate MMEdge using two public\nmultimodal datasets and deploy it on a real-world unmanned aerial vehicle\n(UAV)-based multimodal testbed. The results show that MMEdge significantly\nreduces end-to-end latency while maintaining high task accuracy across various\nsystem and data dynamics.\n","authors":["Runxi Huang","Mingxuan Yu","Mingyu Tsoi","Xiaomin Ouyang"],"pdf_url":"https://arxiv.org/pdf/2510.25327v1.pdf","comment":"Accepted by SenSys 2026"},{"id":"http://arxiv.org/abs/2510.25323v1","updated":"2025-10-29T09:38:50Z","published":"2025-10-29T09:38:50Z","title":"CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices","summary":"  Normalizing flows are deep generative models that enable efficient likelihood\nestimation and sampling through invertible transformations. A key challenge is\nto design linear layers that enhance expressiveness while maintaining efficient\ncomputation of the Jacobian determinant and inverse. We introduce a novel\ninvertible linear layer based on the product of circulant and diagonal\nmatrices. This decomposition reduces parameter complexity from\n$\\mathcal{O}(n^2)$ to $\\mathcal{O}(mn)$ using $m$ diagonal matrices and $m-1$\ncirculant matrices while still approximating general linear transformations. By\nleveraging the Fast Fourier Transform, our approach reduces the time complexity\nof matrix inversion from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(mn\\log n)$ and that\nof computing the log-determinant from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(mn)$,\nwhere $n$ is the input dimension. We build upon this layer to develop\nCirculant-Diagonal Flow (CDFlow), which achieves strong density estimation on\nnatural image datasets and effectively models data with inherent periodic\nstructure. Furthermore, CDFlow significantly accelerates key operations in\nnormalizing flows, providing practical benefits for scalable generative\nmodeling.\n","authors":["Xuchen Feng","Siyu Liao"],"pdf_url":"https://arxiv.org/pdf/2510.25323v1.pdf","comment":"Accepted at NeurIPS 2025. Camera-ready version. 10 pages, 12 figures,\n  2 tables"},{"id":"http://arxiv.org/abs/2506.03595v2","updated":"2025-10-29T09:34:18Z","published":"2025-06-04T05:55:41Z","title":"Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its\n  Preconditioner","summary":"  The recent success of Shampoo in the AlgoPerf contest has sparked renewed\ninterest in Kronecker-factorization-based optimization algorithms for training\nneural networks. Despite its success, Shampoo relies heavily on several\nheuristics such as learning rate grafting and stale preconditioning to achieve\nperformance at-scale. These heuristics increase algorithmic complexity,\nnecessitate further hyperparameter tuning, and lack theoretical justification.\nThis paper investigates these heuristics from the angle of Frobenius norm\napproximation to full-matrix Adam and decouples the preconditioner's\neigenvalues and eigenbasis updates. We show that grafting from Adam mitigates\nthe staleness and mis-scaling of the preconditioner's eigenvalues and how\ncorrecting the eigenvalues directly eliminates the need for learning rate\ngrafting. To manage the error induced by infrequent eigenbasis computations, we\npropose an adaptive criterion for determining the eigenbasis computation\nfrequency motivated by terminating a warm-started QR algorithm. This criterion\ndecouples the update frequency of different preconditioner matrices and enables\nus to investigate the impact of approximation error on convergence. These\npractical techniques offer a principled angle towards removing Shampoo's\nheuristics and developing improved Kronecker-factorization-based training\nalgorithms.\n","authors":["Runa Eschenhagen","Aaron Defazio","Tsung-Hsien Lee","Richard E. Turner","Hao-Jun Michael Shi"],"pdf_url":"https://arxiv.org/pdf/2506.03595v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25311v1","updated":"2025-10-29T09:23:21Z","published":"2025-10-29T09:23:21Z","title":"Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning","summary":"  Reinforcement Learning algorithms are primarily focused on learning a policy\nthat maximizes expected return. As a result, the learned policy can exploit one\nor few reward sources. However, in many natural situations, it is desirable to\nlearn a policy that induces a dispersed marginal state distribution over\nrewarding states, while maximizing the expected return which is typically tied\nto reaching a goal state. This aspect remains relatively unexplored. Existing\ntechniques based on entropy regularization and intrinsic rewards use\nstochasticity for encouraging exploration to find an optimal policy which may\nnot necessarily lead to dispersed marginal state distribution over rewarding\nstates. Other RL algorithms which match a target distribution assume the latter\nto be available apriori. This may be infeasible in large scale systems where\nenumeration of all states is not possible and a state is determined to be a\ngoal state only upon reaching it. We formalize the problem of maximizing the\nexpected return while uniformly visiting the goal states as Multi Goal RL in\nwhich an oracle classifier over the state space determines the goal states. We\npropose a novel algorithm that learns a high-return policy mixture with\nmarginal state distribution dispersed over the set of goal states. Our\nalgorithm is based on optimizing a custom RL reward which is computed - based\non the current policy mixture - at each iteration for a set of sampled\ntrajectories. The latter are used via an offline RL algorithm to update the\npolicy mixture. We prove performance guarantees for our algorithm, showing\nefficient convergence bounds for optimizing a natural objective which captures\nthe expected return as well as the dispersion of the marginal state\ndistribution over the goal states. We design and perform experiments on\nsynthetic MDPs and standard RL environments to evaluate the effectiveness of\nour algorithm.\n","authors":["Sagalpreet Singh","Rishi Saket","Aravindan Raghuveer"],"pdf_url":"https://arxiv.org/pdf/2510.25311v1.pdf","comment":"21 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.25306v1","updated":"2025-10-29T09:18:41Z","published":"2025-10-29T09:18:41Z","title":"Hierarchical Physics-Embedded Learning for Spatiotemporal Dynamical\n  Systems","summary":"  Modeling complex spatiotemporal dynamics, particularly in\nfar-from-equilibrium systems, remains a grand challenge in science. The\ngoverning partial differential equations (PDEs) for these systems are often\nintractable to derive from first principles, due to their inherent complexity,\ncharacterized by high-order derivatives and strong nonlinearities, coupled with\nincomplete physical knowledge. This has spurred the development of data-driven\nmethods, yet these approaches face limitations: Purely data-driven models are\noften physically inconsistent and data-intensive, while existing\nphysics-informed methods lack the structural capacity to represent complex\noperators or systematically integrate partial physical knowledge. Here, we\npropose a hierarchical physics-embedded learning framework that fundamentally\nadvances both the forward spatiotemporal prediction and inverse discovery of\nphysical laws from sparse and noisy data. The key innovation is a two-level\narchitecture that mirrors the process of scientific discovery: the first level\nlearns fundamental symbolic components of a PDE, while the second learns their\ngoverning combinations. This hierarchical decomposition not only reduces\nlearning complexity but, more importantly, enables a structural integration of\nprior knowledge. Known physical laws are directly embedded into the models\ncomputational graph, guaranteeing physical consistency and improving data\nefficiency. By building the framework upon adaptive Fourier Neural Operators,\nwe can effectively capture the non-local dependencies and high-order operators\ncharacteristic of dynamical systems. Additionally, by structurally decoupling\nknown and unknown terms, the framework further enables interpretable discovery\nof underlying governing equations through symbolic regression, without\npresupposing functional forms.\n","authors":["Xizhe Wang","Xiaobin Song","Qingshan Jia","Hongbo Zhao","Benben Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.25306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.23051v2","updated":"2025-10-29T09:09:07Z","published":"2025-09-27T02:12:09Z","title":"Activation Matching for Explanation Generation","summary":"  In this paper we introduce an activation-matching--based approach to generate\nminimal, faithful explanations for the decision-making of a pretrained\nclassifier on any given image. Given an input image $x$ and a frozen model $f$,\nwe train a lightweight autoencoder to output a binary mask $m$ such that the\nexplanation $e = m \\odot x$ preserves both the model's prediction and the\nintermediate activations of \\(x\\). Our objective combines: (i) multi-layer\nactivation matching with KL divergence to align distributions and cross-entropy\nto retain the top-1 label for both the image and the explanation; (ii) mask\npriors -- L1 area for minimality, a binarization penalty for crisp 0/1 masks,\nand total variation for compactness; and (iii) abductive constraints for\nfaithfulness and necessity. Together, these objectives yield small,\nhuman-interpretable masks that retain classifier behavior while discarding\nirrelevant input regions, providing practical and faithful minimalist\nexplanations for the decision making of the underlying model.\n","authors":["Pirzada Suhail","Aditya Anand","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2509.23051v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12593v3","updated":"2025-10-29T08:45:38Z","published":"2024-10-16T14:12:11Z","title":"Expand and Compress: Exploring Tuning Principles for Continual\n  Spatio-Temporal Graph Forecasting","summary":"  The widespread deployment of sensing devices leads to a surge in data for\nspatio-temporal forecasting applications such as traffic flow, air quality, and\nwind energy. Although spatio-temporal graph neural networks have achieved\nsuccess in modeling various static spatio-temporal forecasting scenarios,\nreal-world spatio-temporal data are typically received in a streaming manner,\nand the network continuously expands with the installation of new sensors.\nThus, spatio-temporal forecasting in streaming scenarios faces dual challenges:\nthe inefficiency of retraining models over newly arrived data and the\ndetrimental effects of catastrophic forgetting over long-term history. To\naddress these challenges, we propose a novel prompt tuning-based continuous\nforecasting method, following two fundamental tuning principles guided by\nempirical and theoretical analysis: expand and compress, which effectively\nresolve the aforementioned problems with lightweight tuning parameters.\nSpecifically, we integrate the base spatio-temporal graph neural network with a\ncontinuous prompt pool, utilizing stored prompts (i.e., few learnable\nparameters) in memory, and jointly optimize them with the base spatio-temporal\ngraph neural network. This method ensures that the model sequentially learns\nfrom the spatio-temporal data stream to accomplish tasks for corresponding\nperiods. Extensive experimental results on multiple real-world datasets\ndemonstrate the multi-faceted superiority of our method over the\nstate-of-the-art baselines, including effectiveness, efficiency, universality,\netc.\n","authors":["Wei Chen","Yuxuan Liang"],"pdf_url":"https://arxiv.org/pdf/2410.12593v3.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2510.25282v1","updated":"2025-10-29T08:38:43Z","published":"2025-10-29T08:38:43Z","title":"On the Stability of Neural Networks in Deep Learning","summary":"  Deep learning has achieved remarkable success across a wide range of tasks,\nbut its models often suffer from instability and vulnerability: small changes\nto the input may drastically affect predictions, while optimization can be\nhindered by sharp loss landscapes. This thesis addresses these issues through\nthe unifying perspective of sensitivity analysis, which examines how neural\nnetworks respond to perturbations at both the input and parameter levels.\n  We study Lipschitz networks as a principled way to constrain sensitivity to\ninput perturbations, thereby improving generalization, adversarial robustness,\nand training stability. To complement this architectural approach, we introduce\nregularization techniques based on the curvature of the loss function,\npromoting smoother optimization landscapes and reducing sensitivity to\nparameter variations. Randomized smoothing is also explored as a probabilistic\nmethod for enhancing robustness at decision boundaries.\n  By combining these perspectives, we develop a unified framework where\nLipschitz continuity, randomized smoothing, and curvature regularization\ninteract to address fundamental challenges in stability. The thesis contributes\nboth theoretical analysis and practical methodologies, including efficient\nspectral norm computation, novel Lipschitz-constrained layers, and improved\ncertification procedures.\n","authors":["Blaise Delattre"],"pdf_url":"https://arxiv.org/pdf/2510.25282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00635v2","updated":"2025-10-29T08:25:53Z","published":"2025-05-31T16:48:27Z","title":"Learning with Calibration: Exploring Test-Time Computing of\n  Spatio-Temporal Forecasting","summary":"  Spatio-temporal forecasting is crucial in many domains, such as\ntransportation, meteorology, and energy. However, real-world scenarios\nfrequently present challenges such as signal anomalies, noise, and\ndistributional shifts. Existing solutions primarily enhance robustness by\nmodifying network architectures or training procedures. Nevertheless, these\napproaches are computationally intensive and resource-demanding, especially for\nlarge-scale applications. In this paper, we explore a novel test-time computing\nparadigm, namely learning with calibration, ST-TTC, for spatio-temporal\nforecasting. Through learning with calibration, we aim to capture periodic\nstructural biases arising from non-stationarity during the testing phase and\nperform real-time bias correction on predictions to improve accuracy.\nSpecifically, we first introduce a spectral-domain calibrator with\nphase-amplitude modulation to mitigate periodic shift and then propose a flash\nupdating mechanism with a streaming memory queue for efficient test-time\ncomputation. ST-TTC effectively bypasses complex training-stage techniques,\noffering an efficient and generalizable paradigm. Extensive experiments on\nreal-world datasets demonstrate the effectiveness, universality, flexibility\nand efficiency of our proposed method.\n","authors":["Wei Chen","Yuxuan Liang"],"pdf_url":"https://arxiv.org/pdf/2506.00635v2.pdf","comment":"Accepted by NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.25262v1","updated":"2025-10-29T08:21:32Z","published":"2025-10-29T08:21:32Z","title":"IBNorm: Information-Bottleneck Inspired Normalization for Representation\n  Learning","summary":"  Normalization is fundamental to deep learning, but existing approaches such\nas BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero\nmean and unit variance, stabilizing training without controlling how\nrepresentations capture task-relevant information. We propose IB-Inspired\nNormalization (IBNorm), a simple yet powerful family of methods grounded in the\nInformation Bottleneck principle. IBNorm introduces bounded compression\noperations that encourage embeddings to preserve predictive information while\nsuppressing nuisance variability, yielding more informative representations\nwhile retaining the stability and compatibility of standard normalization.\nTheoretically, we prove that IBNorm achieves a higher IB value and tighter\ngeneralization bounds than variance-centric methods. Empirically, IBNorm\nconsistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale\nlanguage models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual\ninformation analysis confirming superior information bottleneck behavior. Code\nwill be released publicly.\n","authors":["Xiandong Zou","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.25262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25259v1","updated":"2025-10-29T08:14:03Z","published":"2025-10-29T08:14:03Z","title":"TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation","summary":"  Recently, convolutional filters have been increasingly adopted in sequential\nrecommendation for their ability to capture local sequential patterns. However,\nmost of these models complement convolutional filters with self-attention. This\nis because convolutional filters alone, generally fixed filters, struggle to\ncapture global interactions necessary for accurate recommendation. We propose\nTime-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a\nmodel inspired by graph signal processing, where time-variant graph filters\ncapture position-dependent temporal variations in user sequences. By replacing\nboth fixed kernels and self-attention with time-variant filters, TV-Rec\nachieves higher expressive power and better captures complex interaction\npatterns in user behavior. This design not only eliminates the need for\nself-attention but also reduces computation while accelerating inference.\nExtensive experiments on six public benchmarks show that TV-Rec outperforms\nstate-of-the-art baselines by an average of 7.49%.\n","authors":["Yehjin Shin","Jeongwhan Choi","Seojin Kim","Noseong Park"],"pdf_url":"https://arxiv.org/pdf/2510.25259v1.pdf","comment":"The 39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)"},{"id":"http://arxiv.org/abs/2510.25254v1","updated":"2025-10-29T08:06:20Z","published":"2025-10-29T08:06:20Z","title":"Scaling Up Bayesian DAG Sampling","summary":"  Bayesian inference of Bayesian network structures is often performed by\nsampling directed acyclic graphs along an appropriately constructed Markov\nchain. We present two techniques to improve sampling. First, we give an\nefficient implementation of basic moves, which add, delete, or reverse a single\narc. Second, we expedite summing over parent sets, an expensive task required\nfor more sophisticated moves: we devise a preprocessing method to prune\npossible parent sets so as to approximately preserve the sums. Our empirical\nstudy shows that our techniques can yield substantial efficiency gains compared\nto previous methods.\n","authors":["Daniele Nikzad","Alexander Zhilkin","Juha Harviainen","Jack Kuipers","Giusi Moffa","Mikko Koivisto"],"pdf_url":"https://arxiv.org/pdf/2510.25254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14257v2","updated":"2025-10-29T07:56:51Z","published":"2024-10-18T08:05:37Z","title":"Revisiting Service Level Objectives and System Level Metrics in Large\n  Language Model Serving","summary":"  User experience is a critical factor Large Language Model (LLM) serving\nsystems must consider, where service level objectives (SLOs) considering the\nexperience of individual requests and system level metrics (SLMs) considering\nthe overall system performance are two key performance measures. However, we\nobserve two notable issues in existing metrics: 1) manually delaying the\ndelivery of some tokens can improve SLOs, and 2) actively abandoning requests\nthat do not meet SLOs can improve SLMs, both of which are counterintuitive.\n  In this paper, we revisit SLOs and SLMs in LLM serving, and propose a new SLO\nthat aligns with user experience. Based on the SLO, we propose a comprehensive\nmetric framework called smooth goodput, which integrates SLOs and SLMs to\nreflect the nature of user experience in LLM serving. Through this unified\nframework, we reassess the performance of different LLM serving systems under\nmultiple workloads. Evaluation results show that our metric framework provides\na more comprehensive view of token delivery and request processing, and\neffectively captures the optimal point of user experience and system\nperformance with different serving strategies.\n","authors":["Zhibin Wang","Shipeng Li","Yuhang Zhou","Xue Li","Zhonghui Zhang","Nguyen Cam-Tu","Rong Gu","Chen Tian","Guihai Chen","Sheng Zhong"],"pdf_url":"https://arxiv.org/pdf/2410.14257v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25244v1","updated":"2025-10-29T07:51:35Z","published":"2025-10-29T07:51:35Z","title":"BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network\n  Training","summary":"  Recent studies \\citep{gur2018gradient,song2024does, wen2024understanding}\nhighlight a fundamental dichotomy in deep learning optimization: Although\nparameter updates along the top eigendirections of the loss Hessian (Dom-space)\ncapture most of the update magnitude, they often contribute minimally to loss\nreduction. In contrast, updates in the orthogonal component (Bulk-space) have\nsmaller magnitudes but drive most learning progress. In this work, we further\nadvance the understanding of this phenomenon and introduce the\n\\textbf{Bulk-Space-Filtration-Accelerator (BSFA)}, a novel plug-and-play\nframework. BSFA accelerates training by differentially scaling update\ncomponents projected onto these distinct subspaces, simultaneously enhancing\nstability by moderating updates in the dominant subspace and boosting\nconvergence speed by amplifying those in the bulk-space. To ensure BSFA is both\npractical and scalable for contemporary large models, we introduce two key\ninnovations: an efficient estimator using Principal Component Analysis (PCA) on\nhistorical updates for fast subspace estimation, and a block-wise strategy that\napplies this estimation on a per-parameter-block basis. These designs make BSFA\ncomputationally tractable and highly effective. We demonstrate BSFA's\nacceleration across various tasks, notably achieving approximately 2$\\times$\nspeedup when pre-training LLaMA-72M on WikiText-103 and LLaMA-134M on\nOpenWebText compared to vanilla AdamW.\n","authors":["Wenjie Zhou","Bohan Wang","Wei Chen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2510.25244v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2510.25240v1","updated":"2025-10-29T07:42:25Z","published":"2025-10-29T07:42:25Z","title":"Generative Bayesian Optimization: Generative Models as Acquisition\n  Functions","summary":"  We present a general strategy for turning generative models into candidate\nsolution samplers for batch Bayesian optimization (BO). The use of generative\nmodels for BO enables large batch scaling as generative sampling, optimization\nof non-continuous design spaces, and high-dimensional and combinatorial design.\nInspired by the success of direct preference optimization (DPO), we show that\none can train a generative model with noisy, simple utility values directly\ncomputed from observations to then form proposal distributions whose densities\nare proportional to the expected utility, i.e., BO's acquisition function\nvalues. Furthermore, this approach is generalizable beyond preference-based\nfeedback to general types of reward signals and loss functions. This\nperspective avoids the construction of surrogate (regression or classification)\nmodels, common in previous methods that have used generative models for\nblack-box optimization. Theoretically, we show that the generative models\nwithin the BO process approximately follow a sequence of distributions which\nasymptotically concentrate at the global optima under certain conditions. We\nalso demonstrate this effect through experiments on challenging optimization\nproblems involving large batches in high dimensions.\n","authors":["Rafael Oliveira","Daniel M. Steinberg","Edwin V. Bonilla"],"pdf_url":"https://arxiv.org/pdf/2510.25240v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2510.24616v2","updated":"2025-10-29T07:31:22Z","published":"2025-10-28T16:44:34Z","title":"Statistical physics of deep learning: Optimal learning of a multi-layer\n  perceptron near interpolation","summary":"  For three decades statistical physics has been providing a framework to\nanalyse neural networks. A long-standing question remained on its capacity to\ntackle deep learning models capturing rich feature learning effects, thus going\nbeyond the narrow networks or kernel methods analysed until now. We positively\nanswer through the study of the supervised learning of a multi-layer\nperceptron. Importantly, (i) its width scales as the input dimension, making it\nmore prone to feature learning than ultra wide networks, and more expressive\nthan narrow ones or with fixed embedding layers; and (ii) we focus on the\nchallenging interpolation regime where the number of trainable parameters and\ndata are comparable, which forces the model to adapt to the task. We consider\nthe matched teacher-student setting. It provides the fundamental limits of\nlearning random deep neural network targets and helps in identifying the\nsufficient statistics describing what is learnt by an optimally trained network\nas the data budget increases. A rich phenomenology emerges with various\nlearning transitions. With enough data optimal performance is attained through\nmodel's \"specialisation\" towards the target, but it can be hard to reach for\ntraining algorithms which get attracted by sub-optimal solutions predicted by\nthe theory. Specialisation occurs inhomogeneously across layers, propagating\nfrom shallow towards deep ones, but also across neurons in each layer.\nFurthermore, deeper targets are harder to learn. Despite its simplicity, the\nBayesian-optimal setting provides insights on how the depth, non-linearity and\nfinite (proportional) width influence neural networks in the feature learning\nregime that are potentially relevant way beyond it.\n","authors":["Jean Barbier","Francesco Camilli","Minh-Toan Nguyen","Mauro Pastore","Rudy Skerk"],"pdf_url":"https://arxiv.org/pdf/2510.24616v2.pdf","comment":"30 pages, 19 figures + appendix. This submission supersedes both\n  arXiv:2505.24849 and arXiv:2501.18530. v2 fixes figures"},{"id":"http://arxiv.org/abs/2509.06272v2","updated":"2025-10-29T07:12:09Z","published":"2025-09-08T01:39:32Z","title":"Explainable Framework for Swarm Intelligence Based on Fitness Landscape\n  Features and Machine Learning","summary":"  Swarm based optimization algorithms have demonstrated remarkable success in\nsolving complex optimization problems. However, their widespread adoption\nremains sceptical due to limited transparency in how different algorithmic\ncomponents influence the overall performance of the algorithm. This work\npresents a multi-faceted interpretability related investigations of one of the\npopular swarm algorithms, Particle Swarm Optimization. Through this work, we\nprovide a framework that makes the role of different topologies and parameters\nin PSO interpretable and explainable using novel machine learning approach. We\nfirst developed a comprehensive landscape characterization framework using\nExploratory Landscape Analysis to quantify problem difficulty and identify\ncritical features in the problem that affects the optimization performance of\nPSO. Secondly, we rigorously compare three topologies -- Ring, Star, and Von\nNeumann -- analyzing their distinct impacts on exploration-exploitation\nbalance, convergence behavior, and solution quality and eventually develop an\nexplainable benchmarking framework for PSO. The work successfully decodes how\nswarm topologies affect information flow, diversity, and convergence. Through\nsystematic experimentation across 24 benchmark functions in multiple\ndimensions, we establish practical guidelines for topology selection and\nparameter configuration. These findings uncover the black-box nature of PSO,\nproviding more transparency and interpretability to swarm intelligence systems.\nThe source code is available at https://github.com/GitNitin02/ioh_pso.\n","authors":["Nitin Gupta","Bapi Dutta","Anupam Yadav"],"pdf_url":"https://arxiv.org/pdf/2509.06272v2.pdf","comment":"Upated: 29-10-25"},{"id":"http://arxiv.org/abs/2502.12128v4","updated":"2025-10-29T07:10:42Z","published":"2025-02-17T18:49:13Z","title":"LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked\n  Entities","summary":"  Generative models are spearheading recent progress in deep learning,\nshowcasing strong promise for trajectory sampling in dynamical systems as well.\nHowever, whereas latent space modeling paradigms have transformed image and\nvideo generation, similar approaches are more difficult for most dynamical\nsystems. Such systems -- from chemical molecule structures to collective human\nbehavior -- are described by interactions of entities, making them inherently\nlinked to connectivity patterns, entity conservation, and the traceability of\nentities over time. Our approach, LaM-SLidE (Latent Space Modeling of Spatial\nDynamical Systems via Linked Entities), bridges the gap between: (1) keeping\nthe traceability of individual entities in a latent system representation, and\n(2) leveraging the efficiency and scalability of recent advances in image and\nvideo generation, where pre-trained encoder and decoder enable generative\nmodeling directly in latent space. The core idea of LaM-SLidE is the\nintroduction of identifier representations (IDs) that enable the retrieval of\nentity properties and entity composition from latent system representations,\nthus fostering traceability. Experimentally, across different domains, we show\nthat LaM-SLidE performs favorably in terms of speed, accuracy, and\ngeneralizability. Code is available at https://github.com/ml-jku/LaM-SLidE .\n","authors":["Florian Sestak","Artur Toshev","Andreas Fürst","Günter Klambauer","Andreas Mayr","Johannes Brandstetter"],"pdf_url":"https://arxiv.org/pdf/2502.12128v4.pdf","comment":"Project page: https://ml-jku.github.io/LaM-SLidE/"},{"id":"http://arxiv.org/abs/2510.25226v1","updated":"2025-10-29T07:01:32Z","published":"2025-10-29T07:01:32Z","title":"Cost-Sensitive Unbiased Risk Estimation for Multi-Class\n  Positive-Unlabeled Learning","summary":"  Positive--Unlabeled (PU) learning considers settings in which only positive\nand unlabeled data are available, while negatives are missing or left\nunlabeled. This situation is common in real applications where annotating\nreliable negatives is difficult or costly. Despite substantial progress in PU\nlearning, the multi-class case (MPU) remains challenging: many existing\napproaches do not ensure \\emph{unbiased risk estimation}, which limits\nperformance and stability. We propose a cost-sensitive multi-class PU method\nbased on \\emph{adaptive loss weighting}. Within the empirical risk minimization\nframework, we assign distinct, data-dependent weights to the positive and\n\\emph{inferred-negative} (from the unlabeled mixture) loss components so that\nthe resulting empirical objective is an unbiased estimator of the target risk.\nWe formalize the MPU data-generating process and establish a generalization\nerror bound for the proposed estimator. Extensive experiments on \\textbf{eight}\npublic datasets, spanning varying class priors and numbers of classes, show\nconsistent gains over strong baselines in both accuracy and stability.\n","authors":["Miao Zhang","Junpeng Li","Changchun Hua","Yana Yang"],"pdf_url":"https://arxiv.org/pdf/2510.25226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25220v1","updated":"2025-10-29T06:54:42Z","published":"2025-10-29T06:54:42Z","title":"GReF: A Unified Generative Framework for Efficient Reranking via Ordered\n  Multi-token Prediction","summary":"  In a multi-stage recommendation system, reranking plays a crucial role in\nmodeling intra-list correlations among items. A key challenge lies in exploring\noptimal sequences within the combinatorial space of permutations. Recent\nresearch follows a two-stage (generator-evaluator) paradigm, where a generator\nproduces multiple feasible sequences, and an evaluator selects the best one. In\npractice, the generator is typically implemented as an autoregressive model.\nHowever, these two-stage methods face two main challenges. First, the\nseparation of the generator and evaluator hinders end-to-end training. Second,\nautoregressive generators suffer from inference efficiency. In this work, we\npropose a Unified Generative Efficient Reranking Framework (GReF) to address\nthe two primary challenges. Specifically, we introduce Gen-Reranker, an\nautoregressive generator featuring a bidirectional encoder and a dynamic\nautoregressive decoder to generate causal reranking sequences. Subsequently, we\npre-train Gen-Reranker on the item exposure order for high-quality parameter\ninitialization. To eliminate the need for the evaluator while integrating\nsequence-level evaluation during training for end-to-end optimization, we\npropose post-training the model through Rerank-DPO. Moreover, for efficient\nautoregressive inference, we introduce ordered multi-token prediction (OMTP),\nwhich trains Gen-Reranker to simultaneously generate multiple future items\nwhile preserving their order, ensuring practical deployment in real-time\nrecommender systems. Extensive offline experiments demonstrate that GReF\noutperforms state-of-the-art reranking methods while achieving latency that is\nnearly comparable to non-autoregressive models. Additionally, GReF has also\nbeen deployed in a real-world video app Kuaishou with over 300 million daily\nactive users, significantly improving online recommendation quality.\n","authors":["Zhijie Lin","Zhuofeng Li","Chenglei Dai","Wentian Bao","Shuai Lin","Enyun Yu","Haoxiang Zhang","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.25220v1.pdf","comment":"Accepted by CIKM 2025"},{"id":"http://arxiv.org/abs/2505.03280v2","updated":"2025-10-29T06:48:01Z","published":"2025-05-06T08:06:45Z","title":"MDPs with a State Sensing Cost","summary":"  In many practical sequential decision-making problems, tracking the state of\nthe environment incurs a sensing/communication/computation cost. In these\nsettings, the agent's interaction with its environment includes the additional\ncomponent of deciding when to sense the state, in a manner that balances the\nvalue associated with optimal (state-specific) actions and the cost of sensing.\nWe formulate this as an expected discounted cost Markov Decision Process (MDP),\nwherein the agent incurs an additional cost for sensing its next state, but has\nthe option to take actions while remaining `blind' to the system state. We pose\nthis problem as a classical discounted cost MDP with an expanded (countably\ninfinite) state space. While computing the optimal policy for this MDP is\nintractable in general, we derive lower bounds on the optimal value function,\nwhich allow us to bound the suboptimality gap of any policy. We also propose a\ncomputationally efficient algorithm SPI, based on policy improvement, which in\npractice performs close to the optimal policy. Finally, we benchmark against\nthe state-of-the-art via a numerical case study.\n","authors":["Vansh Kapoor","Jayakrishnan Nair"],"pdf_url":"https://arxiv.org/pdf/2505.03280v2.pdf","comment":"This revision adds a real-world healthcare benchmark and provides a\n  more detailed explanation of SPI"},{"id":"http://arxiv.org/abs/2412.15695v2","updated":"2025-10-29T06:46:52Z","published":"2024-12-20T09:15:06Z","title":"Hypergraph clustering using Ricci curvature: an edge transport\n  perspective","summary":"  In this paper, we introduce a novel method for extending Ricci flow to\nhypergraphs by defining probability measures on the edges and transporting them\non the line expansion. This approach yields a new weighting on the edges, which\nproves particularly effective for community detection. We extensively compare\nthis method with a similar notion of Ricci flow defined on the clique\nexpansion, demonstrating its enhanced sensitivity to the hypergraph structure,\nespecially in the presence of large hyperedges. The two methods are\ncomplementary and together form a powerful and highly interpretable framework\nfor community detection in hypergraphs.\n","authors":["Olympio Hacquard"],"pdf_url":"https://arxiv.org/pdf/2412.15695v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15201v2","updated":"2025-10-29T06:45:46Z","published":"2025-05-21T07:26:36Z","title":"Pass@K Policy Optimization: Solving Harder Reinforcement Learning\n  Problems","summary":"  Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts\nfor each problem and reward them independently. This optimizes for pass@1\nperformance and prioritizes the strength of isolated samples at the expense of\nthe diversity and collective utility of sets of samples. This under-utilizes\nthe sampling capacity, limiting exploration and eventual improvement on harder\nexamples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a\ntransformation on the final rewards which leads to direct optimization of\npass@k performance, thus optimizing for sets of samples that maximize reward\nwhen considered jointly. Our contribution is to derive novel low variance\nunbiased estimators for pass@k and its gradient, in both the binary and\ncontinuous reward settings. We show optimization with our estimators reduces to\nstandard RL with rewards that have been jointly transformed by a stable and\nefficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable\nrobust optimization of pass@k for any arbitrary k <= n. Moreover, instead of\ntrading off pass@1 performance for pass@k gains, our method allows annealing k\nduring training, optimizing both metrics and often achieving strong pass@1\nnumbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the\nvariance reducing properties of our formulations. We also include real-world\nexamples using the open-source LLM, GEMMA-2. We find that our transformation\neffectively optimizes for the target k. Furthermore, higher k values enable\nsolving more and harder problems, while annealing k boosts both the pass@1 and\npass@k . Crucially, for challenging task sets where conventional pass@1\noptimization stalls, our pass@k approach unblocks learning, likely due to\nbetter exploration by prioritizing joint utility over the utility of individual\nsamples.\n","authors":["Christian Walder","Deep Karkhanis"],"pdf_url":"https://arxiv.org/pdf/2505.15201v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21797v2","updated":"2025-10-29T06:31:09Z","published":"2025-10-20T15:42:43Z","title":"Quantifying Multimodal Imbalance: A GMM-Guided Adaptive Loss for\n  Audio-Visual Learning","summary":"  The heterogeneity of multimodal data leads to inconsistencies and imbalance,\nallowing a dominant modality to steer gradient updates. Existing solutions\nmainly focus on optimization- or data-based strategies but rarely exploit the\ninformation inherent in multimodal imbalance or conduct its quantitative\nanalysis. To address this gap, we propose a novel quantitative analysis\nframework for Multimodal Imbalance and design a sample-level adaptive loss\nfunction. We define the Modality Gap as the Softmax score difference between\nmodalities for the correct class and model its distribution using a bimodal\nGaussian Mixture Model(GMM), representing balanced and imbalanced samples.\nUsing Bayes' theorem, we estimate each sample's posterior probability of\nbelonging to these two groups. Based on this, our adaptive loss (1) minimizes\nthe overall Modality Gap, (2) aligns imbalanced samples with balanced ones, and\n(3) adaptively penalizes each according to its imbalance degree. A two-stage\ntraining strategy-warm-up and adaptive phases,yields state-of-the-art\nperformance on CREMA-D (80.65%), AVE (70.40%), and KineticSound (72.42%).\nFine-tuning with high-quality samples identified by the GMM further improves\nresults, highlighting their value for effective multimodal fusion.\n","authors":["Zhaocheng Liu","Zhiwen Yu","Xiaoqing Liu"],"pdf_url":"https://arxiv.org/pdf/2510.21797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.01737v2","updated":"2025-10-29T06:27:21Z","published":"2025-04-02T13:49:31Z","title":"Enlightenment Period Improving DNN Performance","summary":"  The start of deep neural network training is characterized by a brief yet\ncritical phase that lasts from the beginning of the training until the accuracy\nreaches approximately 50\\%. During this phase, disordered representations\nrapidly transition toward ordered structure, and we term this phase the\nEnlightenment Period. Through theoretical modeling based on phase transition\ntheory and experimental validation, we reveal that applying Mixup data\naugmentation during this phase has a dual effect: it introduces a Gradient\nInterference Effect that hinders performance, while also providing a beneficial\nActivation Revival Effect to restore gradient updates for saturated neurons. We\nfurther demonstrate that this negative interference diminishes as the sample\nset size or the model parameter size increases, thereby shifting the balance\nbetween these two effects. Based on these findings, we propose three strategies\nthat improve performance by solely adjusting the training data distribution\nwithin this brief period: the Mixup Pause Strategy for small-scale scenarios,\nthe Alpha Boost Strategy for large-scale scenarios with underfitting, and the\nHigh-Loss Removal Strategy for tasks where Mixup is inapplicable (e.g., time\nseries and large language models). Extensive experiments show that these\nstrategies achieve superior performance across diverse architectures such as\nViT and ResNet on datasets including CIFAR and ImageNet-1K. Ultimately, this\nwork offers a novel perspective on enhancing model performance by strategically\ncapitalizing on the dynamics of the brief and crucial early stages of training.\nCode is available at https://anonymous.4open.science/r/code-A5F1/.\n","authors":["Tiantian Liu","Meng Wan","Jue Wang","Ningming Nie"],"pdf_url":"https://arxiv.org/pdf/2504.01737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24025v2","updated":"2025-10-29T06:20:10Z","published":"2025-10-28T03:07:06Z","title":"NeuroPathNet: Dynamic Path Trajectory Learning for Brain Functional\n  Connectivity Analysis","summary":"  Understanding the evolution of brain functional networks over time is of\ngreat significance for the analysis of cognitive mechanisms and the diagnosis\nof neurological diseases. Existing methods often have difficulty in capturing\nthe temporal evolution characteristics of connections between specific\nfunctional communities. To this end, this paper proposes a new path-level\ntrajectory modeling framework (NeuroPathNet) to characterize the dynamic\nbehavior of connection pathways between brain functional partitions. Based on\nmedically supported static partitioning schemes (such as Yeo and Smith ICA), we\nextract the time series of connection strengths between each pair of functional\npartitions and model them using a temporal neural network. We validate the\nmodel performance on three public functional Magnetic Resonance Imaging (fMRI)\ndatasets, and the results show that it outperforms existing mainstream methods\nin multiple indicators. This study can promote the development of dynamic graph\nlearning methods for brain network analysis, and provide possible clinical\napplications for the diagnosis of neurological diseases.\n","authors":["Tianqi Guo","Liping Chen","Ciyuan Peng","Jingjing Zhou","Jing Ren"],"pdf_url":"https://arxiv.org/pdf/2510.24025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25207v1","updated":"2025-10-29T06:18:52Z","published":"2025-10-29T06:18:52Z","title":"Selective Learning for Deep Time Series Forecasting","summary":"  Benefiting from high capacity for capturing complex temporal patterns, deep\nlearning (DL) has significantly advanced time series forecasting (TSF).\nHowever, deep models tend to suffer from severe overfitting due to the inherent\nvulnerability of time series to noise and anomalies. The prevailing DL paradigm\nuniformly optimizes all timesteps through the MSE loss and learns those\nuncertain and anomalous timesteps without difference, ultimately resulting in\noverfitting. To address this, we propose a novel selective learning strategy\nfor deep TSF. Specifically, selective learning screens a subset of the whole\ntimesteps to calculate the MSE loss in optimization, guiding the model to focus\non generalizable timesteps while disregarding non-generalizable ones. Our\nframework introduces a dual-mask mechanism to target timesteps: (1) an\nuncertainty mask leveraging residual entropy to filter uncertain timesteps, and\n(2) an anomaly mask employing residual lower bound estimation to exclude\nanomalous timesteps. Extensive experiments across eight real-world datasets\ndemonstrate that selective learning can significantly improve the predictive\nperformance for typical state-of-the-art deep models, including 37.4% MSE\nreduction for Informer, 8.4% for TimesNet, and 6.5% for iTransformer.\n","authors":["Yisong Fu","Zezhi Shao","Chengqing Yu","Yujie Li","Zhulin An","Qi Wang","Yongjun Xu","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25207v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25206v1","updated":"2025-10-29T06:18:37Z","published":"2025-10-29T06:18:37Z","title":"RAVR: Reference-Answer-guided Variational Reasoning for Large Language\n  Models","summary":"  Reinforcement learning (RL) can refine the reasoning abilities of large\nlanguage models (LLMs), but critically depends on a key prerequisite: the LLM\ncan already generate high-utility reasoning paths with non-negligible\nprobability. For tasks beyond the LLM's current competence, such reasoning path\ncan be hard to sample, and learning risks reinforcing familiar but suboptimal\nreasoning. We are motivated by the insight from cognitive science that Why is\nthis the answer is often an easier question than What is the answer, as it\navoids the heavy cognitive load of open-ended exploration, opting instead for\nexplanatory reconstruction-systematically retracing the reasoning that links a\nquestion to its answer. We show that LLMs can similarly leverage answers to\nderive high-quality reasoning paths. We formalize this phenomenon and prove\nthat conditioning on answer provably increases the expected utility of sampled\nreasoning paths, thereby transforming intractable problems into learnable ones.\nBuilding on this insight, we introduce RAVR (Reference-Answer-guided\nVariational Reasoning), an end-to-end framework that uses answer-conditioned\nreasoning as a variational surrogate for question-only reasoning. Experiments\nin both general and math domains demonstrate consistent improvements over\nstrong baselines. We further analyze the reasoning behavior and find that RAVR\nreduces hesitation, strengthens conclusion consolidation, and promotes\nproblem-specific strategies in reasoning.\n","authors":["Tianqianjin Lin","Xi Zhao","Xingyao Zhang","Rujiao Long","Yi Xu","Zhuoren Jiang","Wenbo Su","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2510.25206v1.pdf","comment":"17 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.16301v2","updated":"2025-10-29T06:16:03Z","published":"2025-05-22T06:56:19Z","title":"Artificial Intelligence for Direct Prediction of Molecular Dynamics\n  Across Chemical Space","summary":"  Molecular dynamics (MD) is a powerful tool for exploring the behavior of\natomistic systems, but its reliance on sequential numerical integration limits\nsimulation efficiency. We present a novel neural network architecture,\nMDtrajNet, and a pre-trained foundational model, MDtrajNet-1, that directly\ngenerates MD trajectories across chemical space, bypassing force calculations\nand integration. This approach accelerates simulations by up to two orders of\nmagnitude compared to traditional MD, even those enhanced by machine-learning\ninteratomic potentials. MDtrajNet combines equivariant neural networks with a\ntransformer-based architecture to achieve strong accuracy and transferability\nin predicting long-time trajectories. Remarkably, the errors of the\ntrajectories generated by MDtrajNet-1 for various known and unseen molecular\nsystems are close to those of the conventional ab initio MD. The architecture's\nflexible design supports diverse application scenarios, including different\nstatistical ensembles, boundary conditions, and interaction types. By\novercoming the intrinsic speed barrier of conventional MD, MDtrajNet opens new\nfrontiers in efficient and scalable atomistic simulations.\n","authors":["Fuchun Ge","Pavlo O. Dral"],"pdf_url":"https://arxiv.org/pdf/2505.16301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23190v4","updated":"2025-10-29T05:54:21Z","published":"2025-05-29T07:28:36Z","title":"DeepRTE: Pre-trained Attention-based Neural Network for Radiative\n  Transfer","summary":"  In this paper, we propose a novel neural network approach, termed DeepRTE, to\naddress the steady-state Radiative Transfer Equation (RTE). The RTE is a\ndifferential-integral equation that governs the propagation of radiation\nthrough a participating medium, with applications spanning diverse domains such\nas neutron transport, atmospheric radiative transfer, heat transfer, and\noptical imaging. Our DeepRTE framework demonstrates superior computational\nefficiency for solving the steady-state RTE, surpassing traditional methods and\nexisting neural network approaches. This efficiency is achieved by embedding\nphysical information through derivation of the RTE and mathematically-informed\nnetwork architecture. Concurrently, DeepRTE achieves high accuracy with\nsignificantly fewer parameters, largely due to its incorporation of mechanisms\nsuch as multi-head attention. Furthermore, DeepRTE is a mesh-free neural\noperator framework with inherent zero-shot capability. This is achieved by\nincorporating Green's function theory and pre-training with delta-function\ninflow boundary conditions into both its architecture design and training data\nconstruction. The efficacy of the proposed approach is substantiated through\ncomprehensive numerical experiments.\n","authors":["Yekun Zhu","Min Tang","Zheng Ma"],"pdf_url":"https://arxiv.org/pdf/2505.23190v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.07382v2","updated":"2025-10-29T05:49:56Z","published":"2025-08-10T15:14:05Z","title":"Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized\n  via Two-Stage Reinforcement Learning","summary":"  Automating penetration testing is crucial for enhancing cybersecurity, yet\ncurrent Large Language Models (LLMs) face significant limitations in this\ndomain, including poor error handling, inefficient reasoning, and an inability\nto perform complex end-to-end tasks autonomously. To address these challenges,\nwe introduce Pentest-R1, a novel framework designed to optimize LLM reasoning\ncapabilities for this task through a two-stage reinforcement learning pipeline.\nWe first construct a dataset of over 500 real-world, multi-step walkthroughs,\nwhich Pentest-R1 leverages for offline reinforcement learning (RL) to instill\nfoundational attack logic. Subsequently, the LLM is fine-tuned via online RL in\nan interactive Capture The Flag (CTF) environment, where it learns directly\nfrom environmental feedback to develop robust error self-correction and\nadaptive strategies. Our extensive experiments on the Cybench and AutoPenBench\nbenchmarks demonstrate the framework's effectiveness. On AutoPenBench,\nPentest-R1 achieves a 24.2\\% success rate, surpassing most state-of-the-art\nmodels and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a\n15.0\\% success rate in unguided tasks, establishing a new state-of-the-art for\nopen-source LLMs and matching the performance of top proprietary models.\nAblation studies confirm that the synergy of both training stages is critical\nto its success.\n","authors":["He Kong","Die Hu","Jingguo Ge","Liangxiong Li","Hui Li","Tong Li"],"pdf_url":"https://arxiv.org/pdf/2508.07382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06426v2","updated":"2025-10-29T05:47:30Z","published":"2025-04-08T20:54:00Z","title":"S'MoRE: Structural Mixture of Residual Experts for Parameter-Efficient\n  LLM Fine-tuning","summary":"  Fine-tuning pre-trained large language models (LLMs) presents a dual\nchallenge of balancing parameter efficiency and model capacity. Existing\nmethods like low-rank adaptations (LoRA) are efficient but lack flexibility,\nwhile Mixture-of-Experts (MoE) enhance model capacity at the cost of more &\nunder-utilized parameters. To address these limitations, we propose Structural\nMixture of Residual Experts (S'MoRE), a novel framework that seamlessly\nintegrates the efficiency of LoRA with the flexibility of MoE. Conceptually,\nS'MoRE employs hierarchical low-rank decomposition of expert weights, yielding\nresiduals of varying orders interconnected in a multi-layer structure. By\nrouting input tokens through sub-trees of residuals, S'MoRE emulates the\ncapacity of numerous experts by instantiating and assembling just a few\nlow-rank matrices. We craft the inter-layer propagation of S'MoRE's residuals\nas a special type of Graph Neural Network (GNN), and prove that under similar\nparameter budget, S'MoRE improves structural flexibility of traditional MoE (or\nMixture-of-LoRA) by exponential order. Comprehensive theoretical analysis and\nempirical results demonstrate that S'MoRE achieves superior fine-tuning\nperformance, offering a transformative approach for efficient LLM adaptation.\nOur implementation is available at: https://github.com/ZimpleX/SMoRE-LLM.\n","authors":["Hanqing Zeng","Yinglong Xia","Zhuokai Zhao","Chuan Jiang","Qiang Zhang","Jiayi Liu","Qunshu Zhang","Lizhu Zhang","Xiangjun Fan","Benyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.06426v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.09887v2","updated":"2025-10-29T05:25:47Z","published":"2025-06-11T15:59:53Z","title":"Learning single-index models via harmonic decomposition","summary":"  We study the problem of learning single-index models, where the label $y \\in\n\\mathbb{R}$ depends on the input $\\boldsymbol{x} \\in \\mathbb{R}^d$ only through\nan unknown one-dimensional projection $\\langle\n\\boldsymbol{w}_*,\\boldsymbol{x}\\rangle$. Prior work has shown that under\nGaussian inputs, the statistical and computational complexity of recovering\n$\\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.\nIn this paper, we propose a new perspective: we argue that $spherical$\n$harmonics$ -- rather than $Hermite$ $polynomials$ -- provide the natural basis\nfor this problem, as they capture its intrinsic $rotational$ $symmetry$.\nBuilding on this insight, we characterize the complexity of learning\nsingle-index models under arbitrary spherically symmetric input distributions.\nWe introduce two families of estimators -- based on tensor unfolding and online\nSGD -- that respectively achieve either optimal sample complexity or optimal\nruntime, and argue that estimators achieving both may not exist in general.\nWhen specialized to Gaussian inputs, our theory not only recovers and clarifies\nexisting results but also reveals new phenomena that had previously been\noverlooked.\n","authors":["Nirmit Joshi","Hugo Koubbi","Theodor Misiakiewicz","Nathan Srebro"],"pdf_url":"https://arxiv.org/pdf/2506.09887v2.pdf","comment":"84 pages. Comments are welcome. To appear at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25176v1","updated":"2025-10-29T05:21:32Z","published":"2025-10-29T05:21:32Z","title":"Machine Learning and CPU (Central Processing Unit) Scheduling\n  Co-Optimization over a Network of Computing Centers","summary":"  In the rapidly evolving research on artificial intelligence (AI) the demand\nfor fast, computationally efficient, and scalable solutions has increased in\nrecent years. The problem of optimizing the computing resources for distributed\nmachine learning (ML) and optimization is considered in this paper. Given a set\nof data distributed over a network of computing-nodes/servers, the idea is to\noptimally assign the CPU (central processing unit) usage while simultaneously\ntraining each computing node locally via its own share of data. This formulates\nthe problem as a co-optimization setup to (i) optimize the data processing and\n(ii) optimally allocate the computing resources. The information-sharing\nnetwork among the nodes might be time-varying, but with balanced weights to\nensure consensus-type convergence of the algorithm. The algorithm is all-time\nfeasible, which implies that the computing resource-demand balance constraint\nholds at all iterations of the proposed solution. Moreover, the solution allows\naddressing possible log-scale quantization over the information-sharing\nchannels to exchange log-quantized data. For some example applications,\ndistributed support-vector-machine (SVM) and regression are considered as the\nML training models. Results from perturbation theory, along with Lyapunov\nstability and eigen-spectrum analysis, are used to prove the convergence\ntowards the optimal case. As compared to existing CPU scheduling solutions, the\nproposed algorithm improves the cost optimality gap by more than $50\\%$.\n","authors":["Mohammadreza Doostmohammadian","Zulfiya R. Gabidullina","Hamid R. Rabiee"],"pdf_url":"https://arxiv.org/pdf/2510.25176v1.pdf","comment":"EAAI Journal"},{"id":"http://arxiv.org/abs/2510.25166v1","updated":"2025-10-29T04:57:49Z","published":"2025-10-29T04:57:49Z","title":"A Study on Inference Latency for Vision Transformers on Mobile Devices","summary":"  Given the significant advances in machine learning techniques on mobile\ndevices, particularly in the domain of computer vision, in this work we\nquantitatively study the performance characteristics of 190 real-world vision\ntransformers (ViTs) on mobile devices. Through a comparison with 102 real-world\nconvolutional neural networks (CNNs), we provide insights into the factors that\ninfluence the latency of ViT architectures on mobile devices. Based on these\ninsights, we develop a dataset including measured latencies of 1000 synthetic\nViTs with representative building blocks and state-of-the-art architectures\nfrom two machine learning frameworks and six mobile platforms. Using this\ndataset, we show that inference latency of new ViTs can be predicted with\nsufficient accuracy for real-world applications.\n","authors":["Zhuojin Li","Marco Paolieri","Leana Golubchik"],"pdf_url":"https://arxiv.org/pdf/2510.25166v1.pdf","comment":"To appear in Springer LNICST, volume 663, Proceedings of VALUETOOLS\n  2024"},{"id":"http://arxiv.org/abs/2505.04083v2","updated":"2025-10-29T04:33:18Z","published":"2025-05-07T02:49:52Z","title":"Plexus: Taming Billion-edge Graphs with 3D Parallel Full-graph GNN\n  Training","summary":"  Graph neural networks (GNNs) leverage the connectivity and structure of\nreal-world graphs to learn intricate properties and relationships between\nnodes. Many real-world graphs exceed the memory capacity of a GPU due to their\nsheer size, and training GNNs on such graphs requires techniques such as\nmini-batch sampling to scale. The alternative approach of distributed\nfull-graph training suffers from high communication overheads and load\nimbalance due to the irregular structure of graphs. We propose a\nthree-dimensional (3D) parallel approach for full-graph training that tackles\nthese issues and scales to billion-edge graphs. In addition, we introduce\noptimizations such as a double permutation scheme for load balancing, and a\nperformance model to predict the optimal 3D configuration of our parallel\nimplementation -- Plexus. We evaluate Plexus on six different graph datasets\nand show scaling results on up to 2048 GPUs of Perlmutter, and 1024 GPUs of\nFrontier. Plexus achieves unprecedented speedups of 2.3-12.5x over prior state\nof the art, and a reduction in time-to-solution by 5.2-8.7x on Perlmutter and\n7.0-54.2x on Frontier.\n","authors":["Aditya K. Ranjan","Siddharth Singh","Cunyang Wei","Abhinav Bhatele"],"pdf_url":"https://arxiv.org/pdf/2505.04083v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.02476v5","updated":"2025-10-29T04:20:28Z","published":"2025-09-02T16:26:03Z","title":"Perturbing the Derivative: Wild Refitting for Model-Free Evaluation of\n  Machine Learning Models under Bregman Losses","summary":"  We study the excess risk evaluation of classical penalized empirical risk\nminimization (ERM) with Bregman losses. We show that by leveraging the idea of\nwild refitting, one can efficiently upper bound the excess risk through the\nso-called \"wild optimism,\" without relying on the global structure of the\nunderlying function class. This property makes our approach inherently\nmodel-free. Unlike conventional analysis, our framework operates with just one\ndataset and black-box access to the training procedure. The method involves\nrandomized Rademacher symmetrization and constructing artificially modified\noutputs by perturbation in the derivative space with appropriate scaling, upon\nwhich we retrain a second predictor for excess risk estimation. We establish\nhigh-probability performance guarantees both under the fixed design setting and\nthe random design setting, demonstrating that wild refitting under Bregman\nlosses, with an appropriately chosen wild noise scale, yields a valid upper\nbound on the excess risk. Thus, our work is promising for theoretically\nevaluating modern opaque ML models, such as deep neural networks and generative\nmodels, where the function class is too complex for classical learning theory\nand empirical process techniques.\n","authors":["Haichen Hu","David Simchi-Levi"],"pdf_url":"https://arxiv.org/pdf/2509.02476v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25147v1","updated":"2025-10-29T03:56:46Z","published":"2025-10-29T03:56:46Z","title":"Machine Learning Guided Optimal Transmission Switching to Mitigate\n  Wildfire Ignition Risk","summary":"  To mitigate acute wildfire ignition risks, utilities de-energize power lines\nin high-risk areas. The Optimal Power Shutoff (OPS) problem optimizes line\nenergization statuses to manage wildfire ignition risks through\nde-energizations while reducing load shedding. OPS problems are computationally\nchallenging Mixed-Integer Linear Programs (MILPs) that must be solved rapidly\nand frequently in operational settings. For a particular power system, OPS\ninstances share a common structure with varying parameters related to wildfire\nrisks, loads, and renewable generation. This motivates the use of Machine\nLearning (ML) for solving OPS problems by exploiting shared patterns across\ninstances. In this paper, we develop an ML-guided framework that quickly\nproduces high-quality de-energization decisions by extending existing ML-guided\nMILP solution methods while integrating domain knowledge on the number of\nenergized and de-energized lines. Results on a large-scale realistic\nCalifornia-based synthetic test system show that the proposed ML-guided method\nproduces high-quality solutions faster than traditional optimization methods.\n","authors":["Weimin Huang","Ryan Piansky","Bistra Dilkina","Daniel K. Molzahn"],"pdf_url":"https://arxiv.org/pdf/2510.25147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02504v2","updated":"2025-10-29T03:38:10Z","published":"2025-06-03T06:31:59Z","title":"Stochastic Momentum Methods for Non-smooth Non-Convex Finite-Sum Coupled\n  Compositional Optimization","summary":"  Finite-sum Coupled Compositional Optimization (FCCO), characterized by its\ncoupled compositional objective structure, emerges as an important optimization\nparadigm for addressing a wide range of machine learning problems. In this\npaper, we focus on a challenging class of non-convex non-smooth FCCO, where the\nouter functions are non-smooth weakly convex or convex and the inner functions\nare smooth or weakly convex. Existing state-of-the-art result face two key\nlimitations: (1) a high iteration complexity of $O(1/\\epsilon^6)$ under the\nassumption that the stochastic inner functions are Lipschitz continuous in\nexpectation; (2) reliance on vanilla SGD-type updates, which are not suitable\nfor deep learning applications. Our main contributions are two fold: (i) We\npropose stochastic momentum methods tailored for non-smooth FCCO that come with\nprovable convergence guarantees; (ii) We establish a new state-of-the-art\niteration complexity of $O(1/\\epsilon^5)$. Moreover, we apply our algorithms to\nmultiple inequality constrained non-convex optimization problems involving\nsmooth or weakly convex functional inequality constraints. By optimizing a\nsmoothed hinge penalty based formulation, we achieve a new state-of-the-art\ncomplexity of $O(1/\\epsilon^5)$ for finding an (nearly) $\\epsilon$-level KKT\nsolution. Experiments on three tasks demonstrate the effectiveness of the\nproposed algorithms.\n","authors":["Xingyu Chen","Bokun Wang","Ming Yang","Qihang Lin","Tianbao Yang"],"pdf_url":"https://arxiv.org/pdf/2506.02504v2.pdf","comment":"Accepted to Neurips 2025"},{"id":"http://arxiv.org/abs/2507.01131v2","updated":"2025-10-29T03:31:51Z","published":"2025-07-01T18:46:27Z","title":"Tensor Decomposition Networks for Fast Machine Learning Interatomic\n  Potential Computations","summary":"  $\\rm{SO}(3)$-equivariant networks are the dominant models for machine\nlearning interatomic potentials (MLIPs). The key operation of such networks is\nthe Clebsch-Gordan (CG) tensor product, which is computationally expensive. To\naccelerate the computation, we develop tensor decomposition networks (TDNs) as\na class of approximately equivariant networks in which CG tensor products are\nreplaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)\ndecomposition. With the CP decomposition, we prove (i) a uniform bound on the\ninduced error of $\\rm{SO}(3)$-equivariance, and (ii) the universality of\napproximating any equivariant bilinear map. To further reduce the number of\nparameters, we propose path-weight sharing that ties all multiplicity-space\nweights across the $\\mathcal{O}(L^3)$ CG paths into a single path without\ncompromising equivariance, where $L$ is the maximum angular degree. The\nresulting layer acts as a plug-and-play replacement for tensor products in\nexisting networks, and the computational complexity of tensor products is\nreduced from $\\mathcal{O}(L^6)$ to $\\mathcal{O}(L^4)$. We evaluate TDNs on\nPubChemQCR, a newly curated molecular relaxation dataset containing 105 million\nDFT-calculated snapshots. We also use existing datasets, including OC20, and\nOC22. Results show that TDNs achieve competitive performance with dramatic\nspeedup in computations. Our code is publicly available as part of the AIRS\nlibrary\n(\\href{https://github.com/divelab/AIRS/tree/main/OpenMol/TDN}{https://github.com/divelab/AIRS/}).\n","authors":["Yuchao Lin","Cong Fu","Zachary Krueger","Haiyang Yu","Maho Nakata","Jianwen Xie","Emine Kucukbenli","Xiaofeng Qian","Shuiwang Ji"],"pdf_url":"https://arxiv.org/pdf/2507.01131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12371v3","updated":"2025-10-29T03:30:11Z","published":"2025-06-14T06:45:53Z","title":"Path-specific effects for pulse-oximetry guided decisions in critical\n  care","summary":"  Identifying and measuring biases associated with sensitive attributes is a\ncrucial consideration in healthcare to prevent treatment disparities. One\nprominent issue is inaccurate pulse oximeter readings, which tend to\noverestimate oxygen saturation for dark-skinned patients and misrepresent\nsupplemental oxygen needs. Most existing research has revealed statistical\ndisparities linking device measurement errors to patient outcomes in intensive\ncare units (ICUs) without causal formalization. This study causally\ninvestigates how racial discrepancies in oximetry measurements affect invasive\nventilation in ICU settings. We employ a causal inference-based approach using\npath-specific effects to isolate the impact of bias by race on clinical\ndecision-making. To estimate these effects, we leverage a doubly robust\nestimator, propose its self-normalized variant for improved sample efficiency,\nand provide novel finite-sample guarantees. Our methodology is validated on\nsemi-synthetic data and applied to two large real-world health datasets:\nMIMIC-IV and eICU. Contrary to prior work, our analysis reveals minimal impact\nof racial discrepancies on invasive ventilation rates. However, path-specific\neffects mediated by oxygen saturation disparity are more pronounced on\nventilation duration, and the severity differs by dataset. Our work provides a\nnovel pipeline for investigating potential disparities in clinical\ndecision-making and, more importantly, highlights the necessity of causal\nmethods to robustly assess fairness in healthcare.\n","authors":["Kevin Zhang","Yonghan Jung","Divyat Mahajan","Karthikeyan Shanmugam","Shalmali Joshi"],"pdf_url":"https://arxiv.org/pdf/2506.12371v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25135v1","updated":"2025-10-29T03:29:10Z","published":"2025-10-29T03:29:10Z","title":"Conditional neural field for spatial dimension reduction of turbulence\n  data: a comparison study","summary":"  We investigate conditional neural fields (CNFs), mesh-agnostic,\ncoordinate-based decoders conditioned on a low-dimensional latent, for spatial\ndimensionality reduction of turbulent flows. CNFs are benchmarked against\nProper Orthogonal Decomposition and a convolutional autoencoder within a\nunified encoding-decoding framework and a common evaluation protocol that\nexplicitly separates in-range (interpolative) from out-of-range (strict\nextrapolative) testing beyond the training horizon, with identical\npreprocessing, metrics, and fixed splits across all baselines. We examine three\nconditioning mechanisms: (i) activation-only modulation (often termed FiLM),\n(ii) low-rank weight and bias modulation (termed FP), and (iii) last-layer\ninner-product coupling, and introduce a novel domain-decomposed CNF that\nlocalizes complexities. Across representative turbulence datasets (WMLES\nchannel inflow, DNS channel inflow, and wall pressure fluctuations over\nturbulent boundary layers), CNF-FP achieves the lowest training and in-range\ntesting errors, while CNF-FiLM generalizes best for out-of-range scenarios once\nmoderate latent capacity is available. Domain decomposition significantly\nimproves out-of-range accuracy, especially for the more demanding datasets. The\nstudy provides a rigorous, physics-aware basis for selecting conditioning,\ncapacity, and domain decomposition when using CNFs for turbulence compression\nand reconstruction.\n","authors":["Junyi Guo","Pan Du","Xiantao Fan","Yahui Li","Jian-Xun Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.25233v2","updated":"2025-10-29T03:27:25Z","published":"2025-09-25T04:51:38Z","title":"FedCLF -- Towards Efficient Participant Selection for Federated Learning\n  in Heterogeneous IoV Networks","summary":"  Federated Learning (FL) is a distributed machine learning technique that\npreserves data privacy by sharing only the trained parameters instead of the\nclient data. This makes FL ideal for highly dynamic, heterogeneous, and\ntime-critical applications, in particular, the Internet of Vehicles (IoV)\nnetworks. However, FL encounters considerable challenges in such networks owing\nto the high data and device heterogeneity. To address these challenges, we\npropose FedCLF, i.e., FL with Calibrated Loss and Feedback control, which\nintroduces calibrated loss as a utility in the participant selection process\nand a feedback control mechanism to dynamically adjust the sampling frequency\nof the clients. The envisaged approach (a) enhances the overall model accuracy\nin case of highly heterogeneous data and (b) optimizes the resource utilization\nfor resource constrained IoV networks, thereby leading to increased efficiency\nin the FL process. We evaluated FedCLF vis-\\`a-vis baseline models, i.e.,\nFedAvg, Newt, and Oort, using CIFAR-10 dataset with varying data heterogeneity.\nOur results depict that FedCLF significantly outperforms the baseline models by\nup to a 16% improvement in high data heterogeneity-related scenarios with\nimproved efficiency via reduced sampling frequency.\n","authors":["Kasun Eranda Wijethilake","Adnan Mahmood","Quan Z. Sheng"],"pdf_url":"https://arxiv.org/pdf/2509.25233v2.pdf","comment":"Already published in ADMA 2024 on 13th December 2024 Wijethilake,\n  K.E., Mahmood, A., Sheng, Q.Z. (2025). FedCLF - Towards Efficient Participant\n  Selection for Federated Learning in Heterogeneous IoV Networks. In: Sheng,\n  Q.Z., et al. Advanced Data Mining and Applications. ADMA 2024. Lecture Notes\n  in Computer Science(), vol 15388. Springer, Singapore.\n  https://doi.org/10.1007/978-981-96-0814-0_15"},{"id":"http://arxiv.org/abs/2510.25132v1","updated":"2025-10-29T03:22:32Z","published":"2025-10-29T03:22:32Z","title":"EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme\n  Backbone Generation","summary":"  Designing enzyme backbones with substrate-specific functionality is a\ncritical challenge in computational protein engineering. Current generative\nmodels excel in protein design but face limitations in binding data,\nsubstrate-specific control, and flexibility for de novo enzyme backbone\ngeneration. To address this, we introduce EnzyBind, a dataset with 11,100\nexperimentally validated enzyme-substrate pairs specifically curated from\nPDBbind. Building on this, we propose EnzyControl, a method that enables\nfunctional and substrate-specific control in enzyme backbone generation. Our\napproach generates enzyme backbones conditioned on MSA-annotated catalytic\nsites and their corresponding substrates, which are automatically extracted\nfrom curated enzyme-substrate data. At the core of EnzyControl is EnzyAdapter,\na lightweight, modular component integrated into a pretrained motif-scaffolding\nmodel, allowing it to become substrate-aware. A two-stage training paradigm\nfurther refines the model's ability to generate accurate and functional enzyme\nstructures. Experiments show that our EnzyControl achieves the best performance\nacross structural and functional metrics on EnzyBind and EnzyBench benchmarks,\nwith particularly notable improvements of 13\\% in designability and 13\\% in\ncatalytic efficiency compared to the baseline models. The code is released at\nhttps://github.com/Vecteur-libre/EnzyControl.\n","authors":["Chao Song","Zhiyuan Liu","Han Huang","Liang Wang","Qiong Wang","Jianyu Shi","Hui Yu","Yihang Zhou","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.25132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25130v1","updated":"2025-10-29T03:19:55Z","published":"2025-10-29T03:19:55Z","title":"Lipschitz-aware Linearity Grafting for Certified Robustness","summary":"  Lipschitz constant is a fundamental property in certified robustness, as\nsmaller values imply robustness to adversarial examples when a model is\nconfident in its prediction. However, identifying the worst-case adversarial\nexamples is known to be an NP-complete problem. Although over-approximation\nmethods have shown success in neural network verification to address this\nchallenge, reducing approximation errors remains a significant obstacle.\nFurthermore, these approximation errors hinder the ability to obtain tight\nlocal Lipschitz constants, which are crucial for certified robustness.\nOriginally, grafting linearity into non-linear activation functions was\nproposed to reduce the number of unstable neurons, enabling scalable and\ncomplete verification. However, no prior theoretical analysis has explained how\nlinearity grafting improves certified robustness. We instead consider linearity\ngrafting primarily as a means of eliminating approximation errors rather than\nreducing the number of unstable neurons, since linear functions do not require\nrelaxation. In this paper, we provide two theoretical contributions: 1) why\nlinearity grafting improves certified robustness through the lens of the\n$l_\\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear\nactivation functions, the dominant source of approximation errors, yields a\ntighter local Lipschitz constant. Based on these theoretical contributions, we\npropose a Lipschitz-aware linearity grafting method that removes dominant\napproximation errors, which are crucial for tightening the local Lipschitz\nconstant, thereby improving certified robustness, even without certified\ntraining. Our extensive experiments demonstrate that grafting linearity into\nthese influential activations tightens the $l_\\infty$ local Lipschitz constant\nand enhances certified robustness.\n","authors":["Yongjin Han","Suhyun Kim"],"pdf_url":"https://arxiv.org/pdf/2510.25130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25128v1","updated":"2025-10-29T03:17:19Z","published":"2025-10-29T03:17:19Z","title":"An Analysis of Causal Effect Estimation using Outcome Invariant Data\n  Augmentation","summary":"  The technique of data augmentation (DA) is often used in machine learning for\nregularization purposes to better generalize under i.i.d. settings. In this\nwork, we present a unifying framework with topics in causal inference to make a\ncase for the use of DA beyond just the i.i.d. setting, but for generalization\nacross interventions as well. Specifically, we argue that when the outcome\ngenerating mechanism is invariant to our choice of DA, then such augmentations\ncan effectively be thought of as interventions on the treatment generating\nmechanism itself. This can potentially help to reduce bias in causal effect\nestimation arising from hidden confounders. In the presence of such unobserved\nconfounding we typically make use of instrumental variables (IVs) -- sources of\ntreatment randomization that are conditionally independent of the outcome.\nHowever, IVs may not be as readily available as DA for many applications, which\nis the main motivation behind this work. By appropriately regularizing IV based\nestimators, we introduce the concept of IV-like (IVL) regression for mitigating\nconfounding bias and improving predictive performance across interventions even\nwhen certain IV properties are relaxed. Finally, we cast parameterized DA as an\nIVL regression problem and show that when used in composition can simulate a\nworst-case application of such DA, further improving performance on causal\nestimation and generalization tasks beyond what simple DA may offer. This is\nshown both theoretically for the population case and via simulation experiments\nfor the finite sample case using a simple linear example. We also present real\ndata experiments to support our case.\n","authors":["Uzair Akbar","Niki Kilbertus","Hao Shen","Krikamol Muandet","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2510.25128v1.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25126v1","updated":"2025-10-29T03:06:54Z","published":"2025-10-29T03:06:54Z","title":"Bridging the Divide: End-to-End Sequence-Graph Learning","summary":"  Many real-world datasets are both sequential and relational: each node\ncarries an event sequence while edges encode interactions. Existing methods in\nsequence modeling and graph modeling often neglect one modality or the other.\nWe argue that sequences and graphs are not separate problems but complementary\nfacets of the same dataset, and should be learned jointly. We introduce BRIDGE,\na unified end-to-end architecture that couples a sequence encoder with a GNN\nunder a single objective, allowing gradients to flow across both modules and\nlearning task-aligned representations. To enable fine-grained token-level\nmessage passing among neighbors, we add TOKENXATTN, a token-level\ncross-attention layer that passes messages between events in neighboring\nsequences. Across two settings, friendship prediction (Brightkite) and fraud\ndetection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graph\nmethods, and sequence-only baselines on ranking and classification metrics.\n","authors":["Yuen Chen","Yulun Wu","Samuel Sharpe","Igor Melnyk","Nam H. Nguyen","Furong Huang","C. Bayan Bruss","Rizal Fathony"],"pdf_url":"https://arxiv.org/pdf/2510.25126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15721v4","updated":"2025-10-29T03:04:39Z","published":"2025-06-04T17:01:38Z","title":"Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration","summary":"  Heterogeneous Large Language Model (LLM) fusion integrates the strengths of\nmultiple source LLMs with different architectures into a target LLM with low\ncomputational overhead. While promising, existing methods suffer from two major\nlimitations: 1) reliance on real data from limited domain for knowledge fusion,\npreventing the target LLM from fully acquiring knowledge across diverse\ndomains, and 2) fixed data allocation proportions across domains, failing to\ndynamically adjust according to the target LLM's varying capabilities across\ndomains, leading to a capability imbalance. To overcome these limitations, we\npropose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework.\nThrough the organization of knowledge domains into a hierarchical tree\nstructure, Bohdi enables automatic domain exploration and multi-domain data\ngeneration through multi-model collaboration, thereby comprehensively\nextracting knowledge from source LLMs. By formalizing domain expansion and data\nsampling proportion allocation on the knowledge tree as a Hierarchical\nMulti-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism\nto adaptively adjust sampling proportions based on the target LLM's performance\nfeedback across domains. Integrated with our proposed Introspection-Rebirth\n(IR) mechanism, DynaBranches dynamically tracks capability shifts during target\nLLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT),\nfurther enhancing its online adaptation capability. Comparative experimental\nresults on a comprehensive suite of benchmarks demonstrate that Bohdi\nsignificantly outperforms existing baselines on multiple target LLMs, exhibits\nhigher data efficiency, and virtually eliminates the imbalance in the target\nLLM's capabilities. Our code is available at\nhttps://github.com/gjq100/Bohdi.git.\n","authors":["Junqi Gao","Zhichang Guo","Dazhi Zhang","Dong Li","Runze Liu","Pengfei Li","Kai Tian","Biqing Qi"],"pdf_url":"https://arxiv.org/pdf/2506.15721v4.pdf","comment":"Accepted by NeurIPS2025"},{"id":"http://arxiv.org/abs/2510.25123v1","updated":"2025-10-29T03:01:09Z","published":"2025-10-29T03:01:09Z","title":"Learning Low Rank Neural Representations of Hyperbolic Wave Dynamics\n  from Data","summary":"  We present a data-driven dimensionality reduction method that is well-suited\nfor physics-based data representing hyperbolic wave propagation. The method\nutilizes a specialized neural network architecture called low rank neural\nrepresentation (LRNR) inside a hypernetwork framework. The architecture is\nmotivated by theoretical results that rigorously prove the existence of\nefficient representations for this wave class. We illustrate through archetypal\nexamples that such an efficient low-dimensional representation of propagating\nwaves can be learned directly from data through a combination of deep learning\ntechniques. We observe that a low rank tensor representation arises naturally\nin the trained LRNRs, and that this reveals a new decomposition of wave\npropagation where each decomposed mode corresponds to interpretable physical\nfeatures. Furthermore, we demonstrate that the LRNR architecture enables\nefficient inference via a compression scheme, which is a potentially important\nfeature when deploying LRNRs in demanding performance regimes.\n","authors":["Woojin Cho","Kookjin Lee","Noseong Park","Donsub Rim","Gerrit Welper"],"pdf_url":"https://arxiv.org/pdf/2510.25123v1.pdf","comment":"41 pages, 18 figures"},{"id":"http://arxiv.org/abs/2510.25121v1","updated":"2025-10-29T02:58:21Z","published":"2025-10-29T02:58:21Z","title":"A Unified Bilevel Model for Adversarial Learning and A Case Study","summary":"  Adversarial learning has been attracting more and more attention thanks to\nthe fast development of machine learning and artificial intelligence. However,\ndue to the complicated structure of most machine learning models, the mechanism\nof adversarial attacks is not well interpreted. How to measure the effect of\nattack is still not quite clear. In this paper, we propose a unified bilevel\nmodel for adversarial learning. We further investigate the adversarial attack\nin clustering models and interpret it from data perturbation point of view. We\nreveal that when the data perturbation is relatively small, the clustering\nmodel is robust, whereas if it is relatively large, the clustering result\nchanges, which leads to an attack. To measure the effect of attacks for\nclustering models, we analyse the well-definedness of the so-called\n$\\delta$-measure, which can be used in the proposed bilevel model for\nadversarial learning of clustering models.\n","authors":["Yutong Zheng","Qingna Li"],"pdf_url":"https://arxiv.org/pdf/2510.25121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22527v2","updated":"2025-10-29T02:45:00Z","published":"2025-05-28T16:13:36Z","title":"Symplectic Generative Networks (SGNs): A Hamiltonian Framework for\n  Invertible Deep Generative Modeling","summary":"  We introduce the \\emph{Symplectic Generative Network (SGN)}, a deep\ngenerative model that leverages Hamiltonian mechanics to construct an\ninvertible, volume-preserving mapping between a latent space and the data\nspace. By endowing the latent space with a symplectic structure and modeling\ndata generation as the time evolution of a Hamiltonian system, SGN achieves\nexact likelihood evaluation without incurring the computational overhead of\nJacobian determinant calculations. In this work, we provide a rigorous\nmathematical foundation for SGNs through a comprehensive theoretical framework\nthat includes: (i) complete proofs of invertibility and volume preservation,\n(ii) a formal complexity analysis with theoretical comparisons to Variational\nAutoencoders and Normalizing Flows, (iii) strengthened universal approximation\nresults with quantitative error bounds, (iv) an information-theoretic analysis\nbased on the geometry of statistical manifolds, and (v) an extensive stability\nanalysis with adaptive integration guarantees. These contributions highlight\nthe fundamental advantages of SGNs and establish a solid foundation for future\nempirical investigations and applications to complex, high-dimensional data.\n","authors":["Agnideep Aich","Ashit Aich"],"pdf_url":"https://arxiv.org/pdf/2505.22527v2.pdf","comment":"Submitted"},{"id":"http://arxiv.org/abs/2405.04605v4","updated":"2025-10-29T02:33:21Z","published":"2024-05-07T18:36:40Z","title":"AI in Lung Health: Benchmarking Detection and Diagnostic Models Across\n  Multiple CT Scan Datasets","summary":"  Background: Development of artificial intelligence (AI) models for lung\ncancer screening requires large, well-annotated low-dose computed tomography\n(CT) datasets and rigorous performance benchmarks. Purpose: To create a\nreproducible benchmarking resource leveraging the Duke Lung Cancer Screening\n(DLCS) and multiple public datasets to develop and evaluate models for nodule\ndetection and classification. Materials & Methods: This retrospective study\nuses the DLCS dataset (1,613 patients; 2,487 nodules) and external datasets\nincluding LUNA16, LUNA25, and NLST-3D. For detection, MONAI RetinaNet models\nwere trained on DLCS (DLCS-De) and LUNA16 (LUNA16-De) and evaluated using the\nCompetition Performance Metric (CPM). For nodule-level classification, we\ncompare five strategies: pretrained models (Models Genesis, Med3D), a\nself-supervised foundation model (FMCB), and ResNet50 with random\ninitialization versus Strategic Warm-Start (ResNet50-SWS) pretrained with\ndetection-derived candidate patches stratified by confidence. Results: For\ndetection on the DLCS test set, DLCS-De achieved sensitivity 0.82 at 2 false\npositives/scan (CPM 0.63) versus LUNA16-De (0.62, CPM 0.45). For external\nvalidation on NLST-3D, DLCS-De (sensitivity 0.72, CPM 0.58) also outperformed\nLUNA16-De (sensitivity 0.64, CPM 0.49). For classification across multiple\ndatasets, ResNet50-SWS attained AUCs of 0.71 (DLCS; 95% CI, 0.61-0.81), 0.90\n(LUNA16; 0.87-0.93), 0.81 (NLST-3D; 0.79-0.82), and 0.80 (LUNA25; 0.78-0.82),\nmatching or exceeding pretrained/self-supervised baselines. Performance\ndifferences reflected dataset label standards. Conclusion: This work\nestablishes a standardized benchmarking resource for lung cancer AI research,\nsupporting model development, validation, and translation. All code, models,\nand data are publicly released to promote reproducibility.\n","authors":["Fakrul Islam Tushar","Avivah Wang","Lavsen Dahal","Ehsan Samei","Michael R. Harowicz","Jayashree Kalpathy-Cramer","Kyle J. Lafata","Tina D. Tailor","Cynthia Rudin","Joseph Y. Lo"],"pdf_url":"https://arxiv.org/pdf/2405.04605v4.pdf","comment":"2 tables, 5 figures"},{"id":"http://arxiv.org/abs/2510.25114v1","updated":"2025-10-29T02:26:41Z","published":"2025-10-29T02:26:41Z","title":"Energy Approach from $\\varepsilon$-Graph to Continuum Diffusion Model\n  with Connectivity Functional","summary":"  We derive an energy-based continuum limit for $\\varepsilon$-graphs endowed\nwith a general connectivity functional. We prove that the discrete energy and\nits continuum counterpart differ by at most $O(\\varepsilon)$; the prefactor\ninvolves only the $W^{1,1}$-norm of the connectivity density as\n$\\varepsilon\\to0$, so the error bound remains valid even when that density has\nstrong local fluctuations. As an application, we introduce a neural-network\nprocedure that reconstructs the connectivity density from edge-weight data and\nthen embeds the resulting continuum model into a brain-dynamics framework. In\nthis setting, the usual constant diffusion coefficient is replaced by the\nspatially varying coefficient produced by the learned density, yielding\ndynamics that differ significantly from those obtained with conventional\nconstant-diffusion models.\n","authors":["Yahong Yang","Sun Lee","Jeff Calder","Wenrui Hao"],"pdf_url":"https://arxiv.org/pdf/2510.25114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25113v1","updated":"2025-10-29T02:24:27Z","published":"2025-10-29T02:24:27Z","title":"The Neural Differential Manifold: An Architecture with Explicit\n  Geometric Structure","summary":"  This paper introduces the Neural Differential Manifold (NDM), a novel neural\nnetwork architecture that explicitly incorporates geometric structure into its\nfundamental design. Departing from conventional Euclidean parameter spaces, the\nNDM re-conceptualizes a neural network as a differentiable manifold where each\nlayer functions as a local coordinate chart, and the network parameters\ndirectly parameterize a Riemannian metric tensor at every point. The\narchitecture is organized into three synergistic layers: a Coordinate Layer\nimplementing smooth chart transitions via invertible transformations inspired\nby normalizing flows, a Geometric Layer that dynamically generates the\nmanifold's metric through auxiliary sub-networks, and an Evolution Layer that\noptimizes both task performance and geometric simplicity through a\ndual-objective loss function. This geometric regularization penalizes excessive\ncurvature and volume distortion, providing intrinsic regularization that\nenhances generalization and robustness. The framework enables natural gradient\ndescent optimization aligned with the learned manifold geometry and offers\nunprecedented interpretability by endowing internal representations with clear\ngeometric meaning. We analyze the theoretical advantages of this approach,\nincluding its potential for more efficient optimization, enhanced continual\nlearning, and applications in scientific discovery and controllable generative\nmodeling. While significant computational challenges remain, the Neural\nDifferential Manifold represents a fundamental shift towards geometrically\nstructured, interpretable, and efficient deep learning systems.\n","authors":["Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.25113v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2510.25108v1","updated":"2025-10-29T02:18:15Z","published":"2025-10-29T02:18:15Z","title":"Shift is Good: Mismatched Data Mixing Improves Test Performance","summary":"  We consider training and testing on mixture distributions with different\ntraining and test proportions. We show that in many settings, and in some sense\ngenerically, distribution shift can be beneficial, and test performance can\nimprove due to mismatched training proportions, even if the components are\nunrelated and with no transfer between components. In a variety of scenarios,\nwe identify the optimal training proportions and the extent to which such\ndistribution shift can be beneficial. We show how the same analysis applies\nalso to a compositional setting with differing distribution of component\n\"skills'' at training and test.\n","authors":["Marko Medvedev","Kaifeng Lyu","Zhiyuan Li","Nathan Srebro"],"pdf_url":"https://arxiv.org/pdf/2510.25108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.22149v4","updated":"2025-10-29T02:12:31Z","published":"2025-07-29T18:27:13Z","title":"When Truthful Representations Flip Under Deceptive Instructions?","summary":"  Large language models (LLMs) tend to follow maliciously crafted instructions\nto generate deceptive responses, posing safety challenges. How deceptive\ninstructions alter the internal representations of LLM compared to truthful\nones remains poorly understood beyond output analysis. To bridge this gap, we\ninvestigate when and how these representations ``flip'', such as from truthful\nto deceptive, under deceptive versus truthful/neutral instructions. Analyzing\nthe internal representations of Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct\non a factual verification task, we find the model's instructed True/False\noutput is predictable via linear probes across all conditions based on the\ninternal representation. Further, we use Sparse Autoencoders (SAEs) to show\nthat the Deceptive instructions induce significant representational shifts\ncompared to Truthful/Neutral representations (which are similar), concentrated\nin early-to-mid layers and detectable even on complex datasets. We also\nidentify specific SAE features highly sensitive to deceptive instruction and\nuse targeted visualizations to confirm distinct truthful/deceptive\nrepresentational subspaces. % Our analysis pinpoints layer-wise and\nfeature-level correlates of instructed dishonesty, offering insights for LLM\ndetection and control. Our findings expose feature- and layer-level signatures\nof deception, offering new insights for detecting and mitigating instructed\ndishonesty in LLMs.\n","authors":["Xianxuan Long","Yao Fu","Runchao Li","Mu Sheng","Haotian Yu","Xiaotian Han","Pan Li"],"pdf_url":"https://arxiv.org/pdf/2507.22149v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25096v1","updated":"2025-10-29T02:02:12Z","published":"2025-10-29T02:02:12Z","title":"Learning Fair Graph Representations with Multi-view Information\n  Bottleneck","summary":"  Graph neural networks (GNNs) excel on relational data by passing messages\nover node features and structure, but they can amplify training data biases,\npropagating discriminatory attributes and structural imbalances into unfair\noutcomes. Many fairness methods treat bias as a single source, ignoring\ndistinct attribute and structure effects and leading to suboptimal fairness and\nutility trade-offs. To overcome this challenge, we propose FairMIB, a\nmulti-view information bottleneck framework designed to decompose graphs into\nfeature, structural, and diffusion views for mitigating complexity biases in\nGNNs. Especially, the proposed FairMIB employs contrastive learning to maximize\ncross-view mutual information for bias-free representation learning. It further\nintegrates multi-perspective conditional information bottleneck objectives to\nbalance task utility and fairness by minimizing mutual information with\nsensitive attributes. Additionally, FairMIB introduces an inverse\nprobability-weighted (IPW) adjacency correction in the diffusion view, which\nreduces the spread of bias propagation during message passing. Experiments on\nfive real-world benchmark datasets demonstrate that FairMIB achieves\nstate-of-the-art performance across both utility and fairness metrics.\n","authors":["Chuxun Liu","Debo Cheng","Qingfeng Chen","Jiangzhang Gan","Jiuyong Li","Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25093v1","updated":"2025-10-29T01:57:38Z","published":"2025-10-29T01:57:38Z","title":"Continual Low-Rank Adapters for LLM-based Generative Recommender Systems","summary":"  While large language models (LLMs) achieve strong performance in\nrecommendation, they face challenges in continual learning as users, items, and\nuser preferences evolve over time. Existing LoRA-based continual methods\nprimarily focus on preserving performance on previous tasks, but this overlooks\nthe unique nature of recommendation: the goal is not to predict past\npreferences, and outdated preferences can even harm performance when current\ninterests shift significantly. To address this, we propose PESO (Proximally\nrEgularized Single evolving lOra, a continual adaptation method for LoRA in\nrecommendation. PESO introduces a proximal regularizer that anchors the current\nadapter to its most recent frozen state, enabling the model to flexibly balance\nadaptation and preservation, and to better capture recent user behaviors.\nTheoretically, we show that this proximal design provides data-aware,\ndirection-wise guidance in the LoRA subspace. Empirically, PESO consistently\noutperforms existing LoRA-based continual learning methods.\n","authors":["Hyunsik Yoo","Ting-Wei Li","SeongKu Kang","Zhining Liu","Charlie Xu","Qilin Qi","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2510.25093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25087v1","updated":"2025-10-29T01:51:00Z","published":"2025-10-29T01:51:00Z","title":"BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs","summary":"  Coreference resolution in biomedical texts presents unique challenges due to\ncomplex domain-specific terminology, high ambiguity in mention forms, and\nlong-distance dependencies between coreferring expressions. In this work, we\npresent a comprehensive evaluation of generative large language models (LLMs)\nfor coreference resolution in the biomedical domain. Using the CRAFT corpus as\nour benchmark, we assess the LLMs' performance with four prompting experiments\nthat vary in their use of local, contextual enrichment, and domain-specific\ncues such as abbreviations and entity dictionaries. We benchmark these\napproaches against a discriminative span-based encoder, SpanBERT, to compare\nthe efficacy of generative versus discriminative methods. Our results\ndemonstrate that while LLMs exhibit strong surface-level coreference\ncapabilities, especially when supplemented with domain-grounding prompts, their\nperformance remains sensitive to long-range context and mentions ambiguity.\nNotably, the LLaMA 8B and 17B models show superior precision and F1 scores\nunder entity-augmented prompting, highlighting the potential of lightweight\nprompt engineering for enhancing LLM utility in biomedical NLP tasks.\n","authors":["Nourah M Salem","Elizabeth White","Michael Bada","Lawrence Hunter"],"pdf_url":"https://arxiv.org/pdf/2510.25087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25080v1","updated":"2025-10-29T01:38:19Z","published":"2025-10-29T01:38:19Z","title":"Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response\n  Games","summary":"  Card games are widely used to study sequential decision-making under\nuncertainty, with real-world analogues in negotiation, finance, and\ncybersecurity. Typically, these games fall into three categories based on the\nflow of control: strictly-sequential (where players alternate single actions),\ndeterministic-response (where some actions trigger a fixed outcome), and\nunbounded reciprocal-response (where alternating counterplays are permitted). A\nless-explored but strategically rich structure exists: the bounded one-sided\nresponse. This dynamic occurs when a player's action briefly transfers control\nto the opponent, who must satisfy a fixed condition through one or more\nsequential moves before the turn resolves. We term games featuring this\nmechanism Bounded One-Sided Response Games (BORGs).\n  We introduce a modified version of Monopoly Deal as a benchmark environment\nthat specifically isolates the BORG dynamic, where a Rent action forces the\nopponent to sequentially choose payment assets. We demonstrate that the\ngold-standard algorithm, Counterfactual Regret Minimization (CFR), successfully\nconverges on effective strategies for this domain without requiring novel\nalgorithmic extensions. To support efficient, reproducible experimentation, we\npresent a lightweight, full-stack research platform that unifies the\nenvironment, a parallelized CFR runtime, and a human-playable web interface,\nall runnable on a single workstation. This system provides a practical\nfoundation for exploring state representation and policy learning in bounded\none-sided response settings.\n  The trained CFR agent and source code are available at\nhttps://monopolydeal.ai.\n","authors":["Will Wolf"],"pdf_url":"https://arxiv.org/pdf/2510.25080v1.pdf","comment":"24 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.13966v3","updated":"2025-10-29T01:23:05Z","published":"2023-10-21T10:55:31Z","title":"Transfer Learning for Kernel-based Regression","summary":"  In recent years, transfer learning has garnered significant attention. Its\nability to leverage knowledge from related studies to improve generalization\nperformance in a target study has made it highly appealing. This paper focuses\non investigating the transfer learning problem within the context of\nnonparametric regression over a reproducing kernel Hilbert space. The aim is to\nbridge the gap between practical effectiveness and theoretical guarantees. We\nspecifically consider two scenarios: one where the transferable sources are\nknown and another where they are unknown. For the known transferable source\ncase, we propose a two-step kernel-based estimator by solely using kernel ridge\nregression. For the unknown case, we develop a novel method based on an\nefficient aggregation algorithm, which can automatically detect and alleviate\nthe effects of negative sources. This paper provides the statistical properties\nof the desired estimators and establishes the minimax rate. Through extensive\nnumerical experiments on synthetic data and real examples, we validate our\ntheoretical findings and demonstrate the effectiveness of our proposed method.\n","authors":["Chao Wang","Caixing Wang","Xin He","Xingdong Feng"],"pdf_url":"https://arxiv.org/pdf/2310.13966v3.pdf","comment":"revised version of \"Minimax optimal transfer learning for\n  kernel-based nonparametric regression\""},{"id":"http://arxiv.org/abs/2510.25074v1","updated":"2025-10-29T01:19:50Z","published":"2025-10-29T01:19:50Z","title":"Training Across Reservoirs: Using Numerical Differentiation To Couple\n  Trainable Networks With Black-Box Reservoirs","summary":"  We introduce Bounded Numerical Differentiation (BOND), a perturbative method\nfor estimating partial derivatives across network structures with inaccessible\ncomputational graphs. BOND demonstrates improved accuracy and scalability from\nexisting perturbative methods, enabling new explorations of trainable\narchitectures that integrate black-box functions. We observe that these\nblack-box functions, realized in our experiments as fixed, untrained networks,\ncan enhance model performance without increasing the number of trainable\nparameters. This improvement is achieved without extensive optimization of the\narchitecture or properties of the black-box function itself. Our findings\nhighlight the potential of leveraging fixed, non-trainable modules to expand\nmodel capacity, suggesting a path toward combining analogue and digital devices\nas a mechanism for scaling networks.\n","authors":["Andrew Clark","Jack Moursounidis","Osmaan Rasouli","William Gan","Cooper Doyle","Anna Leontjeva"],"pdf_url":"https://arxiv.org/pdf/2510.25074v1.pdf","comment":"12 pages main, Appendix 10 pages, 6 figures in main body, 10 overall"},{"id":"http://arxiv.org/abs/2506.14167v6","updated":"2025-10-29T01:00:08Z","published":"2025-06-17T04:07:32Z","title":"Kolmogorov-Arnold Energy Models: Fast and Interpretable Generative\n  Modeling","summary":"  Learning an energy-based model (EBM) in the latent space of a top-down\ngenerative model offers a powerful framework for generation across many data\nmodalities. However, it remains unclear how its interpretability can be used to\nguide model design, improve generative quality, and reduce training time.\nMoreover, the reliance on Langevin Monte Carlo (LMC) sampling presents\nchallenges in efficiency and sampling multimodal latent distributions. We\npropose a novel adaptation of the Kolmogorov-Arnold representation theorem for\ngenerative modeling and introduce the Kolmogorov-Arnold Energy Model (KAEM) to\ntake advantage of structural and inductive biases. By constraining the prior to\nunivariate relationships, KAEM enables fast and exact inference via the inverse\ntransform method. With the low dimensionality of the latent space and suitable\ninductive biases encoded, we demonstrate that importance sampling (IS) becomes\na viable, unbiased, and highly efficient posterior sampler. For domains where\nIS fails, we introduce a strategy based on population-based LMC, decomposing\nthe posterior into a sequence of annealed distributions to improve LMC mixing.\nKAEM balances common generative modeling trade-offs, offering fast inference,\ninterpretability, and stable training, while being naturally suited to\nZettascale Computing hardware.\n","authors":["Prithvi Raj"],"pdf_url":"https://arxiv.org/pdf/2506.14167v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25060v1","updated":"2025-10-29T01:00:07Z","published":"2025-10-29T01:00:07Z","title":"Nonlinear Dynamics In Optimization Landscape of Shallow Neural Networks\n  with Tunable Leaky ReLU","summary":"  In this work, we study the nonlinear dynamics of a shallow neural network\ntrained with mean-squared loss and leaky ReLU activation. Under Gaussian inputs\nand equal layer width k, (1) we establish, based on the equivariant gradient\ndegree, a theoretical framework, applicable to any number of neurons k>= 4, to\ndetect bifurcation of critical points with associated symmetries from global\nminimum as leaky parameter $\\alpha$ varies. Typically, our analysis reveals\nthat a multi-mode degeneracy consistently occurs at the critical number 0,\nindependent of k. (2) As a by-product, we further show that such bifurcations\nare width-independent, arise only for nonnegative $\\alpha$ and that the global\nminimum undergoes no further symmetry-breaking instability throughout the\nengineering regime $\\alpha$ in range (0,1). An explicit example with k=5 is\npresented to illustrate the framework and exhibit the resulting bifurcation\ntogether with their symmetries.\n","authors":["Jingzhou Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25055v1","updated":"2025-10-29T00:46:45Z","published":"2025-10-29T00:46:45Z","title":"GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using\n  Large Language Models","summary":"  Scientific progress is driven by the deliberate articulation of what remains\nunknown. This study investigates the ability of large language models (LLMs) to\nidentify research knowledge gaps in the biomedical literature. We define two\ncategories of knowledge gaps: explicit gaps, clear declarations of missing\nknowledge; and implicit gaps, context-inferred missing knowledge. While prior\nwork has focused mainly on explicit gap detection, we extend this line of\nresearch by addressing the novel task of inferring implicit gaps. We conducted\ntwo experiments on almost 1500 documents across four datasets, including a\nmanually annotated corpus of biomedical articles. We benchmarked both\nclosed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2)\nunder paragraph-level and full-paper settings. To address the reasoning of\nimplicit gaps inference, we introduce \\textbf{\\small TABI}, a Toulmin-Abductive\nBucketed Inference scheme that structures reasoning and buckets inferred\nconclusion candidates for validation. Our results highlight the robust\ncapability of LLMs in identifying both explicit and implicit knowledge gaps.\nThis is true for both open- and closed-weight models, with larger variants\noften performing better. This suggests a strong ability of LLMs for\nsystematically identifying candidate knowledge gaps, which can support\nearly-stage research formulation, policymakers, and funding decisions. We also\nreport observed failure modes and outline directions for robust deployment,\nincluding domain adaptation, human-in-the-loop verification, and benchmarking\nacross open- and closed-weight models.\n","authors":["Nourah M Salem","Elizabeth White","Michael Bada","Lawrence Hunter"],"pdf_url":"https://arxiv.org/pdf/2510.25055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25053v1","updated":"2025-10-29T00:39:09Z","published":"2025-10-29T00:39:09Z","title":"Scalable predictive processing framework for multitask caregiving robots","summary":"  The rapid aging of societies is intensifying demand for autonomous care\nrobots; however, most existing systems are task-specific and rely on\nhandcrafted preprocessing, limiting their ability to generalize across diverse\nscenarios. A prevailing theory in cognitive neuroscience proposes that the\nhuman brain operates through hierarchical predictive processing, which\nunderlies flexible cognition and behavior by integrating multimodal sensory\nsignals. Inspired by this principle, we introduce a hierarchical multimodal\nrecurrent neural network grounded in predictive processing under the\nfree-energy principle, capable of directly integrating over 30,000-dimensional\nvisuo-proprioceptive inputs without dimensionality reduction. The model was\nable to learn two representative caregiving tasks, rigid-body repositioning and\nflexible-towel wiping, without task-specific feature engineering. We\ndemonstrate three key properties: (i) self-organization of hierarchical latent\ndynamics that regulate task transitions, capture variability in uncertainty,\nand infer occluded states; (ii) robustness to degraded vision through\nvisuo-proprioceptive integration; and (iii) asymmetric interference in\nmultitask learning, where the more variable wiping task had little influence on\nrepositioning, whereas learning the repositioning task led to a modest\nreduction in wiping performance, while the model maintained overall robustness.\nAlthough the evaluation was limited to simulation, these results establish\npredictive processing as a universal and scalable computational principle,\npointing toward robust, flexible, and autonomous caregiving robots while\noffering theoretical insight into the human brain's ability to achieve flexible\nadaptation in uncertain real-world environments.\n","authors":["Hayato Idei","Tamon Miyake","Tetsuya Ogata","Yuichi Yamashita"],"pdf_url":"https://arxiv.org/pdf/2510.25053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25051v1","updated":"2025-10-29T00:37:18Z","published":"2025-10-29T00:37:18Z","title":"Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference\n  Models","summary":"  Breast cancer remains the most commonly diagnosed malignancy among women in\nthe developed world. Early detection through mammography screening plays a\npivotal role in reducing mortality rates. While computer-aided diagnosis (CAD)\nsystems have shown promise in assisting radiologists, existing approaches face\ncritical limitations in clinical deployment - particularly in handling the\nnuanced interpretation of multi-modal data and feasibility due to the\nrequirement of prior clinical history. This study introduces a novel framework\nthat synergistically combines visual features from 2D mammograms with\nstructured textual descriptors derived from easily accessible clinical metadata\nand synthesized radiological reports through innovative tokenization modules.\nOur proposed methods in this study demonstrate that strategic integration of\nconvolutional neural networks (ConvNets) with language representations achieves\nsuperior performance to vision transformer-based models while handling\nhigh-resolution images and enabling practical deployment across diverse\npopulations. By evaluating it on multi-national cohort screening mammograms,\nour multi-modal approach achieves superior performance in cancer detection and\ncalcification identification compared to unimodal baselines, with particular\nimprovements. The proposed method establishes a new paradigm for developing\nclinically viable VLM-based CAD systems that effectively leverage imaging data\nand contextual patient information through effective fusion mechanisms.\n","authors":["Shunjie-Fabian Zheng","Hyeonjun Lee","Thijs Kooi","Ali Diba"],"pdf_url":"https://arxiv.org/pdf/2510.25051v1.pdf","comment":"Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)\n  Workshop at ICCV 2025"},{"id":"http://arxiv.org/abs/2502.14819v4","updated":"2025-10-29T00:35:42Z","published":"2025-02-20T18:39:41Z","title":"Learning from Reward-Free Offline Data: A Case for Planning with Latent\n  Dynamics Models","summary":"  A long-standing goal in AI is to develop agents capable of solving diverse\ntasks across a range of environments, including those never seen during\ntraining. Two dominant paradigms address this challenge: (i) reinforcement\nlearning (RL), which learns policies via trial and error, and (ii) optimal\ncontrol, which plans actions using a known or learned dynamics model. However,\ntheir comparative strengths in the offline setting - where agents must learn\nfrom reward-free trajectories - remain underexplored. In this work, we\nsystematically evaluate RL and control-based methods on a suite of navigation\ntasks, using offline datasets of varying quality. On the RL side, we consider\ngoal-conditioned and zero-shot methods. On the control side, we train a latent\ndynamics model using the Joint Embedding Predictive Architecture (JEPA) and\nemploy it for planning. We investigate how factors such as data diversity,\ntrajectory quality, and environment variability influence the performance of\nthese approaches. Our results show that model-free RL benefits most from large\namounts of high-quality data, whereas model-based planning generalizes better\nto unseen layouts and is more data-efficient, while achieving trajectory\nstitching performance comparable to leading model-free methods. Notably,\nplanning with a latent dynamics model proves to be a strong approach for\nhandling suboptimal offline data and adapting to diverse environments.\n","authors":["Vlad Sobal","Wancong Zhang","Kyunghyun Cho","Randall Balestriero","Tim G. J. Rudner","Yann LeCun"],"pdf_url":"https://arxiv.org/pdf/2502.14819v4.pdf","comment":"Project web page: https://latent-planning.github.io/"},{"id":"http://arxiv.org/abs/2506.21894v2","updated":"2025-10-29T00:10:05Z","published":"2025-06-27T04:21:57Z","title":"Thompson Sampling in Function Spaces via Neural Operators","summary":"  We propose an extension of Thompson sampling to optimization problems over\nfunction spaces where the objective is a known functional of an unknown\noperator's output. We assume that queries to the operator (such as running a\nhigh-fidelity simulator or physical experiment) are costly, while functional\nevaluations on the operator's output are inexpensive. Our algorithm employs a\nsample-then-optimize approach using neural operator surrogates. This strategy\navoids explicit uncertainty quantification by treating trained neural operators\nas approximate samples from a Gaussian process (GP) posterior. We derive regret\nbounds and theoretical results connecting neural operators with GPs in\ninfinite-dimensional settings. Experiments benchmark our method against other\nBayesian optimization baselines on functional optimization tasks involving\npartial differential equations of physical systems, demonstrating better sample\nefficiency and significant performance gains.\n","authors":["Rafael Oliveira","Xuesong Wang","Kian Ming A. Chai","Edwin V. Bonilla"],"pdf_url":"https://arxiv.org/pdf/2506.21894v2.pdf","comment":"Revised, camera-ready version, accepted at the 39th Annual Conference\n  on Neural Information Processing Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.25042v1","updated":"2025-10-29T00:03:03Z","published":"2025-10-29T00:03:03Z","title":"Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient\n  Deep Network Training","summary":"  Within the current sphere of deep learning research, despite the extensive\napplication of optimization algorithms such as Stochastic Gradient Descent\n(SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced\ninadequacy in their capability to address fluctuations in learning efficiency,\nmeet the demands of complex models, and tackle non-convex optimization issues.\nThese challenges primarily arise from the algorithms' limitations in handling\ncomplex data structures and models, for instance, difficulties in selecting an\nappropriate learning rate, avoiding local optima, and navigating through\nhigh-dimensional spaces. To address these issues, this paper introduces a novel\noptimization algorithm named DWMGrad. This algorithm, building on the\nfoundations of traditional methods, incorporates a dynamic guidance mechanism\nreliant on historical data to dynamically update momentum and learning rates.\nThis allows the optimizer to flexibly adjust its reliance on historical\ninformation, adapting to various training scenarios. This strategy not only\nenables the optimizer to better adapt to changing environments and task\ncomplexities but also, as validated through extensive experimentation,\ndemonstrates DWMGrad's ability to achieve faster convergence rates and higher\naccuracies under a multitude of scenarios.\n","authors":["Zhifeng Wang","Longlong Li","Chunyan Zeng"],"pdf_url":"https://arxiv.org/pdf/2510.25042v1.pdf","comment":"45 pages, 12 figures"},{"id":"http://arxiv.org/abs/2510.22033v2","updated":"2025-10-29T23:56:23Z","published":"2025-10-24T21:33:12Z","title":"Linearized Optimal Transport for Analysis of High-Dimensional\n  Point-Cloud and Single-Cell Data","summary":"  Single-cell technologies generate high-dimensional point clouds of cells,\nenabling detailed characterization of complex patient states and treatment\nresponses. Yet each patient is represented by an irregular point cloud rather\nthan a simple vector, making it difficult to directly quantify and compare\nbiological differences between individuals. Nonlinear methods such as kernels\nand neural networks achieve predictive accuracy but act as black boxes,\noffering little biological interpretability.\n  To address these limitations, we adapt the Linear Optimal Transport (LOT)\nframework to this setting, embedding irregular point clouds into a\nfixed-dimensional Euclidean space while preserving distributional structure.\nThis embedding provides a principled linear representation that preserves\noptimal transport geometry while enabling downstream analysis. It also forms a\nregistration between any two patients, enabling direct comparison of their\ncellular distributions. Within this space, LOT enables: (i) \\textbf{accurate\nand interpretable classification} of COVID-19 patient states, where classifier\nweights map back to specific markers and spatial regions driving predictions;\nand (ii) \\textbf{synthetic data generation} for patient-derived organoids,\nexploiting the linearity of the LOT embedding. LOT barycenters yield averaged\ncellular profiles representing combined conditions or samples, supporting drug\ninteraction testing.\n  Together, these results establish LOT as a unified framework that bridges\npredictive performance, interpretability, and generative modeling. By\ntransforming heterogeneous point clouds into structured embeddings directly\ntraceable to the original data, LOT opens new opportunities for understanding\nimmune variation and treatment effects in high-dimensional biological systems.\n","authors":["Tianxiang Wang","Yingtong Ke","Dhananjay Bhaskar","Smita Krishnaswamy","Alexander Cloninger"],"pdf_url":"https://arxiv.org/pdf/2510.22033v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.26026v1","updated":"2025-10-29T23:45:44Z","published":"2025-10-29T23:45:44Z","title":"Conformal Prediction Beyond the Horizon: Distribution-Free Inference for\n  Policy Evaluation","summary":"  Reliable uncertainty quantification is crucial for reinforcement learning\n(RL) in high-stakes settings. We propose a unified conformal prediction\nframework for infinite-horizon policy evaluation that constructs\ndistribution-free prediction intervals {for returns} in both on-policy and\noff-policy settings. Our method integrates distributional RL with conformal\ncalibration, addressing challenges such as unobserved returns, temporal\ndependencies, and distributional shifts. We propose a modular pseudo-return\nconstruction based on truncated rollouts and a time-aware calibration strategy\nusing experience replay and weighted subsampling. These innovations mitigate\nmodel bias and restore approximate exchangeability, enabling uncertainty\nquantification even under policy shifts. Our theoretical analysis provides\ncoverage guarantees that account for model misspecification and importance\nweight estimation. Empirical results, including experiments in synthetic and\nbenchmark environments like Mountain Car, show that our method significantly\nimproves coverage and reliability over standard distributional RL baselines.\n","authors":["Feichen Gan","Youcun Lu","Yingying Zhang","Yukun Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.05014v3","updated":"2025-10-29T23:44:26Z","published":"2025-10-06T16:53:56Z","title":"Think Then Embed: Generative Context Improves Multimodal Embedding","summary":"  There is a growing interest in Universal Multimodal Embeddings (UME), where\nmodels are required to generate task-specific representations. While recent\nstudies show that Multimodal Large Language Models (MLLMs) perform well on such\ntasks, they treat MLLMs solely as encoders, overlooking their generative\ncapacity. However, such an encoding paradigm becomes less effective as\ninstructions become more complex and require compositional reasoning. Inspired\nby the proven effectiveness of chain-of-thought reasoning, we propose a general\nThink-Then-Embed (TTE) framework for UME, composed of a reasoner and an\nembedder. The reasoner MLLM first generates reasoning traces that explain\ncomplex queries, followed by an embedder that produces representations\nconditioned on both the original query and the intermediate reasoning. This\nexplicit reasoning step enables more nuanced understanding of complex\nmultimodal instructions. Our contributions are threefold. First, by leveraging\na powerful MLLM reasoner, we achieve state-of-the-art performance on the\nMMEB-V2 benchmark, surpassing proprietary models trained on massive in-house\ndatasets. Second, to reduce the dependency on large MLLM reasoners, we finetune\na smaller MLLM reasoner using high-quality embedding-centric reasoning traces,\nachieving the best performance among open-source models with a 7% absolute gain\nover recently proposed models. Third, we investigate strategies for integrating\nthe reasoner and embedder into a unified model for improved efficiency without\nsacrificing performance.\n","authors":["Xuanming Cui","Jianpeng Cheng","Hong-you Chen","Satya Narayan Shukla","Abhijeet Awasthi","Xichen Pan","Chaitanya Ahuja","Shlok Kumar Mishra","Yonghuan Yang","Jun Xiao","Qi Guo","Ser-Nam Lim","Aashu Singh","Xiangjun Fan"],"pdf_url":"https://arxiv.org/pdf/2510.05014v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26025v1","updated":"2025-10-29T23:40:40Z","published":"2025-10-29T23:40:40Z","title":"Exploring Human-AI Conceptual Alignment through the Prism of Chess","summary":"  Do AI systems truly understand human concepts or merely mimic surface\npatterns? We investigate this through chess, where human creativity meets\nprecise strategic concepts. Analyzing a 270M-parameter transformer that\nachieves grandmaster-level play, we uncover a striking paradox: while early\nlayers encode human concepts like center control and knight outposts with up to\n85\\% accuracy, deeper layers, despite driving superior performance, drift\ntoward alien representations, dropping to 50-65\\% accuracy. To test conceptual\nrobustness beyond memorization, we introduce the first Chess960 dataset: 240\nexpert-annotated positions across 6 strategic concepts. When opening theory is\neliminated through randomized starting positions, concept recognition drops\n10-20\\% across all methods, revealing the model's reliance on memorized\npatterns rather than abstract understanding. Our layer-wise analysis exposes a\nfundamental tension in current architectures: the representations that win\ngames diverge from those that align with human thinking. These findings suggest\nthat as AI systems optimize for performance, they develop increasingly alien\nintelligence, a critical challenge for creative AI applications requiring\ngenuine human-AI collaboration. Dataset and code are available at:\nhttps://github.com/slomasov/ChessConceptsLLM.\n","authors":["Semyon Lomaso","Judah Goldfeder","Mehmet Hamza Erol","Matthew So","Yao Yan","Addison Howard","Nathan Kutz","Ravid Shwartz Ziv"],"pdf_url":"https://arxiv.org/pdf/2510.26025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02820v3","updated":"2025-10-29T23:29:35Z","published":"2025-05-05T17:47:49Z","title":"AutoLibra: Agent Metric Induction from Open-Ended Human Feedback","summary":"  Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose **AutoLibra**, a framework for\nagent evaluation, that transforms open-ended human feedback *e.g.* \"If you find\nthat the button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\" into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra serve human prompt engineers for\ndiagonalize agent failures and improve prompts iterative. Moreover, we find\nthat AutoLibra can induce metrics for automatic optimization for agents, which\nmakes agents improve through self-regulation. Our results suggest that\nAutoLibra is a powerful task-agnostic tool for evaluating and improving\nlanguage agents.\n","authors":["Hao Zhu","Phil Cuvin","Xinkai Yu","Charlotte Ka Yee Yan","Jason Zhang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2505.02820v3.pdf","comment":"https://github.com/Open-Social-World/autolibra"},{"id":"http://arxiv.org/abs/2510.26020v1","updated":"2025-10-29T23:28:53Z","published":"2025-10-29T23:28:53Z","title":"PORTool: Tool-Use LLM Training with Rewarded Tree","summary":"  Current tool-use large language models (LLMs) are trained on static datasets,\nenabling them to interact with external tools and perform multi-step,\ntool-integrated reasoning, which produces tool-call trajectories. However,\nthese models imitate how a query is resolved in a generic tool-call routine,\nthereby failing to explore possible solutions and demonstrating limited\nperformance in an evolved, dynamic tool-call environment. In this work, we\npropose PORTool, a reinforcement learning (RL) method that encourages a\ntool-use LLM to explore various trajectories yielding the correct answer.\nSpecifically, this method starts with generating multiple rollouts for a given\nquery, and some of them share the first few tool-call steps, thereby forming a\ntree-like structure. Next, we assign rewards to each step, based on its ability\nto produce a correct answer and make successful tool calls. A shared step\nacross different trajectories receives the same reward, while different steps\nunder the same fork receive different rewards. Finally, these step-wise rewards\nare used to calculate fork-relative advantages, blended with\ntrajectory-relative advantages, to train the LLM for tool use. The experiments\nutilize 17 tools to address user queries, covering both time-sensitive and\ntime-invariant topics. We conduct ablation studies to systematically justify\nthe necessity and the design robustness of step-wise rewards. Furthermore, we\ncompare the proposed PORTool with other training approaches and demonstrate\nsignificant improvements in final accuracy and the number of tool-call steps.\n","authors":["Feijie Wu","Weiwu Zhu","Yuxiang Zhang","Soumya Chatterjee","Jiarong Zhu","Fan Mo","Rodin Luo","Jing Gao"],"pdf_url":"https://arxiv.org/pdf/2510.26020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02682v2","updated":"2025-10-29T23:22:45Z","published":"2024-03-05T06:10:22Z","title":"Time Weaver: A Conditional Time Series Generation Model","summary":"  Imagine generating a city's electricity demand pattern based on weather, the\npresence of an electric vehicle, and location, which could be used for capacity\nplanning during a winter freeze. Such real-world time series are often enriched\nwith paired heterogeneous contextual metadata (e.g., weather and location).\nCurrent approaches to time series generation often ignore this paired metadata.\nAdditionally, the heterogeneity in metadata poses several practical challenges\nin adapting existing conditional generation approaches from the image, audio,\nand video domains to the time series domain. To address this gap, we introduce\nTIME WEAVER, a novel diffusion-based model that leverages the heterogeneous\nmetadata in the form of categorical, continuous, and even time-variant\nvariables to significantly improve time series generation. Additionally, we\nshow that naive extensions of standard evaluation metrics from the image to the\ntime series domain are insufficient. These metrics do not penalize conditional\ngeneration approaches for their poor specificity in reproducing the\nmetadata-specific features in the generated time series. Thus, we innovate a\nnovel evaluation metric that accurately captures the specificity of conditional\ngeneration and the realism of the generated time series. We show that TIME\nWEAVER outperforms state-of-the-art benchmarks, such as Generative Adversarial\nNetworks (GANs), by up to 30% in downstream classification tasks on real-world\nenergy, medical, air quality, and traffic datasets.\n","authors":["Sai Shankar Narasimhan","Shubhankar Agarwal","Oguzhan Akcin","Sujay Sanghavi","Sandeep Chinchali"],"pdf_url":"https://arxiv.org/pdf/2403.02682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26014v1","updated":"2025-10-29T23:11:01Z","published":"2025-10-29T23:11:01Z","title":"Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis","summary":"  Survival analysis is a task to model the time until an event of interest\noccurs, widely used in clinical and biomedical research. A key challenge is to\nmodel patient heterogeneity while also adapting risk predictions to both\nindividual characteristics and temporal dynamics. We propose a dual\nmixture-of-experts (MoE) framework for discrete-time survival analysis. Our\napproach combines a feature-encoder MoE for subgroup-aware representation\nlearning with a hazard MoE that leverages patient features and time embeddings\nto capture temporal dynamics. This dual-MoE design flexibly integrates with\nexisting deep learning based survival pipelines. On METABRIC and GBSG breast\ncancer datasets, our method consistently improves performance, boosting the\ntime-dependent C-index up to 0.04 on the test sets, and yields further gains\nwhen incorporated into the Consurv framework.\n","authors":["Hyeonjun Lee","Hyungseob Shin","Gunhee Nam","Hyeonsoo Lee"],"pdf_url":"https://arxiv.org/pdf/2510.26014v1.pdf","comment":"Accepted to NeurIPS 2025 workshop Learning from Time Series for\n  Health (TS4H)"},{"id":"http://arxiv.org/abs/2406.13989v2","updated":"2025-10-29T23:06:02Z","published":"2024-06-20T04:32:34Z","title":"Random pairing MLE for estimation of item parameters in Rasch model","summary":"  The Rasch model, a classical model in the item response theory, is widely\nused in psychometrics to model the relationship between individuals' latent\ntraits and their binary responses to assessments or questionnaires. In this\npaper, we introduce a new likelihood-based estimator -- random pairing maximum\nlikelihood estimator ($\\mathrm{RP\\text{-}MLE}$) and its bootstrapped variant\nmultiple random pairing MLE ($\\mathrm{MRP\\text{-}MLE}$) which faithfully\nestimate the item parameters in the Rasch model. The new estimators have\nseveral appealing features compared to existing ones. First, both work for\nsparse observations, an increasingly important scenario in the big data era.\nSecond, both estimators are provably minimax optimal in terms of finite sample\n$\\ell_{\\infty}$ estimation error. Lastly, both admit precise distributional\ncharacterization that allows uncertainty quantification on the item parameters,\ne.g., construction of confidence intervals for the item parameters. The main\nidea underlying $\\mathrm{RP\\text{-}MLE}$ and $\\mathrm{MRP\\text{-}MLE}$ is to\nrandomly pair user-item responses to form item-item comparisons. This is\ncarefully designed to reduce the problem size while retaining statistical\nindependence. We also provide empirical evidence of the efficacy of the two new\nestimators using both simulated and real data.\n","authors":["Yuepeng Yang","Cong Ma"],"pdf_url":"https://arxiv.org/pdf/2406.13989v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12191v2","updated":"2025-10-29T23:02:31Z","published":"2025-05-18T01:37:58Z","title":"Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised\n  Learning from Data Curriculum","summary":"  Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2.\n","authors":["Wenquan Lu","Jiaqi Zhang","Hugues Van Assel","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2505.12191v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23639v2","updated":"2025-10-29T22:50:10Z","published":"2025-10-24T15:56:40Z","title":"Integrating Genomics into Multimodal EHR Foundation Models","summary":"  This paper introduces an innovative Electronic Health Record (EHR) foundation\nmodel that integrates Polygenic Risk Scores (PRS) as a foundational data\nmodality, moving beyond traditional EHR-only approaches to build more holistic\nhealth profiles. Leveraging the extensive and diverse data from the All of Us\n(AoU) Research Program, this multimodal framework aims to learn complex\nrelationships between clinical data and genetic predispositions. The\nmethodology extends advancements in generative AI to the EHR foundation model\nspace, enhancing predictive capabilities and interpretability. Evaluation on\nAoU data demonstrates the model's predictive value for the onset of various\nconditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay\nbetween PRS and EHR data. The work also explores transfer learning for custom\nclassification tasks, showcasing the architecture's versatility and efficiency.\nThis approach is pivotal for unlocking new insights into disease prediction,\nproactive health management, risk stratification, and personalized treatment\nstrategies, laying the groundwork for more personalized, equitable, and\nactionable real-world evidence generation in healthcare.\n","authors":["Jonathan Amar","Edward Liu","Alessandra Breschi","Liangliang Zhang","Pouya Kheradpour","Sylvia Li","Lisa Soleymani Lehmann","Alessandro Giulianelli","Matt Edwards","Yugang Jia","David Nola","Raghav Mani","Pankaj Vats","Jesse Tetreault","T. J. Chen","Cory Y. McLean"],"pdf_url":"https://arxiv.org/pdf/2510.23639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12652v2","updated":"2025-10-29T22:50:00Z","published":"2024-10-16T15:16:04Z","title":"Constrained Posterior Sampling: Time Series Generation with Hard\n  Constraints","summary":"  Generating realistic time series samples is crucial for stress-testing models\nand protecting user privacy by using synthetic data. In engineering and\nsafety-critical applications, these samples must meet certain hard constraints\nthat are domain-specific or naturally imposed by physics or nature. Consider,\nfor example, generating electricity demand patterns with constraints on peak\ndemand times. This can be used to stress-test the functioning of power grids\nduring adverse weather conditions. Existing approaches for generating\nconstrained time series are either not scalable or degrade sample quality. To\naddress these challenges, we introduce Constrained Posterior Sampling (CPS), a\ndiffusion-based sampling algorithm that aims to project the posterior mean\nestimate into the constraint set after each denoising update. Notably, CPS\nscales to a large number of constraints ($\\sim100$) without requiring\nadditional training. We provide theoretical justifications highlighting the\nimpact of our projection step on sampling. Empirically, CPS outperforms\nstate-of-the-art methods in sample quality and similarity to real time series\nby around 70\\% and 22\\%, respectively, on real-world stocks, traffic, and air\nquality datasets.\n","authors":["Sai Shankar Narasimhan","Shubhankar Agarwal","Litu Rout","Sanjay Shakkottai","Sandeep P. Chinchali"],"pdf_url":"https://arxiv.org/pdf/2410.12652v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17608v2","updated":"2025-10-29T22:44:11Z","published":"2023-05-28T02:12:00Z","title":"Reward Collapse in Aligning Large Language Models","summary":"  The extraordinary capabilities of large language models (LLMs) such as\nChatGPT and GPT-4 are in part unleashed by aligning them with reward models\nthat are trained on human preferences, which are often represented as rankings\nof responses to prompts. In this paper, we document the phenomenon of\n\\textit{reward collapse}, an empirical observation where the prevailing\nranking-based approach results in an \\textit{identical} reward distribution\n\\textit{regardless} of the prompts during the terminal phase of training. This\noutcome is undesirable as open-ended prompts like ``write a short story about\nyour best friend'' should yield a continuous range of rewards for their\ncompletions, while specific prompts like ``what is the capital of New Zealand''\nshould generate either high or low rewards. Our theoretical investigation\nreveals that reward collapse is primarily due to the insufficiency of the\nranking-based objective function to incorporate prompt-related information\nduring optimization. This insight allows us to derive closed-form expressions\nfor the reward distribution associated with a set of utility functions in an\nasymptotic regime. To overcome reward collapse, we introduce a prompt-aware\noptimization scheme that provably admits a prompt-dependent reward distribution\nwithin the interpolating regime. Our experimental results suggest that our\nproposed prompt-aware utility functions significantly alleviate reward collapse\nduring the training of reward models.\n","authors":["Ziang Song","Tianle Cai","Jason D. Lee","Weijie J. Su"],"pdf_url":"https://arxiv.org/pdf/2305.17608v2.pdf","comment":"Accepted for publication in the Journal of Data Science (JDS),\n  reference JDS1201"},{"id":"http://arxiv.org/abs/2510.26008v1","updated":"2025-10-29T22:39:09Z","published":"2025-10-29T22:39:09Z","title":"Detecting Anomalies in Machine Learning Infrastructure via Hardware\n  Telemetry","summary":"  Modern machine learning (ML) has grown into a tightly coupled, full-stack\necosystem that combines hardware, software, network, and applications. Many\nusers rely on cloud providers for elastic, isolated, and cost-efficient\nresources. Unfortunately, these platforms as a service use virtualization,\nwhich means operators have little insight into the users' workloads. This\nhinders resource optimizations by the operator, which is essential to ensure\ncost efficiency and minimize execution time. In this paper, we argue that\nworkload knowledge is unnecessary for system-level optimization. We propose\nSystem-X, which takes a \\emph{hardware-centric} approach, relying only on\nhardware signals -- fully accessible by operators. Using low-level signals\ncollected from the system, System-X detects anomalies through an unsupervised\nlearning pipeline. The pipeline is developed by analyzing over 30 popular ML\nmodels on various hardware platforms, ensuring adaptability to emerging\nworkloads and unknown deployment patterns. Using System-X, we successfully\nidentified both network and system configuration issues, accelerating the\nDeepSeek model by 5.97%.\n","authors":["Ziji Chen","Steven Chien","Peng Qian","Noa Zilberman"],"pdf_url":"https://arxiv.org/pdf/2510.26008v1.pdf","comment":"12 pages, 9 figures, submitted to nsdi 26"},{"id":"http://arxiv.org/abs/2510.26007v1","updated":"2025-10-29T22:35:34Z","published":"2025-10-29T22:35:34Z","title":"The Quest for Reliable Metrics of Responsible AI","summary":"  The development of Artificial Intelligence (AI), including AI in Science\n(AIS), should be done following the principles of responsible AI. Progress in\nresponsible AI is often quantified through evaluation metrics, yet there has\nbeen less work on assessing the robustness and reliability of the metrics\nthemselves. We reflect on prior work that examines the robustness of fairness\nmetrics for recommender systems as a type of AI application and summarise their\nkey takeaways into a set of non-exhaustive guidelines for developing reliable\nmetrics of responsible AI. Our guidelines apply to a broad spectrum of AI\napplications, including AIS.\n","authors":["Theresia Veronika Rampisela","Maria Maistro","Tuukka Ruotsalo","Christina Lioma"],"pdf_url":"https://arxiv.org/pdf/2510.26007v1.pdf","comment":"Accepted for presentation at the AI in Science Summit 2025"},{"id":"http://arxiv.org/abs/2411.10501v2","updated":"2025-10-29T22:32:43Z","published":"2024-11-15T11:19:25Z","title":"OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion\n  Models","summary":"  We consider the problem of text-to-video generation tasks with precise\ncontrol for various applications such as camera movement control and\nvideo-to-video editing. Most methods tacking this problem rely on providing\nuser-defined controls, such as binary masks or camera movement embeddings. In\nour approach we propose OnlyFlow, an approach leveraging the optical flow\nfirstly extracted from an input video to condition the motion of generated\nvideos. Using a text prompt and an input video, OnlyFlow allows the user to\ngenerate videos that respect the motion of the input video as well as the text\nprompt. This is implemented through an optical flow estimation model applied on\nthe input video, which is then fed to a trainable optical flow encoder. The\noutput feature maps are then injected into the text-to-video backbone model. We\nperform quantitative, qualitative and user preference studies to show that\nOnlyFlow positively compares to state-of-the-art methods on a wide range of\ntasks, even though OnlyFlow was not specifically trained for such tasks.\nOnlyFlow thus constitutes a versatile, lightweight yet efficient method for\ncontrolling motion in text-to-video generation. Models and code will be made\navailable on GitHub and HuggingFace.\n","authors":["Mathis Koroglu","Hugo Caselles-Dupré","Guillaume Jeanneret Sanmiguel","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2411.10501v2.pdf","comment":"8 pages, 1 supplementary page, 9 figures"},{"id":"http://arxiv.org/abs/2503.02878v3","updated":"2025-10-29T22:28:23Z","published":"2025-03-04T18:58:11Z","title":"Language Models can Self-Improve at State-Value Estimation for Better\n  Search","summary":"  Collecting ground-truth rewards or human demonstrations for multi-step\nreasoning tasks is often prohibitively expensive, particularly in interactive\ndomains such as web tasks. We introduce Self-Taught Lookahead (STL), a\nreward-free framework that improves language model-based value functions by\nreasoning explicitly about state transitions. STL can be viewed as a\nchain-of-thought analogue of the value iteration algorithm: instead of\nregressing directly on numeric values, a value LLM is trained to simulate a\nstep of lookahead in natural language - predicting the next action, resulting\nstate, and rationale for its value, thereby refining value estimates without\nany labeled data. This self-supervised procedure yields more accurate\nstate-value predictions, which in turn enable lightweight search algorithms to\nexpand fewer states while maintaining strong performance. Empirically,\nSTL-trained value models built on moderately sized (8B parameter) open-weight\nLLMs boost web agent success rates by 39%, achieving comparable performance\nwith proprietary models. STL also generalizes to multi-hop QA and math puzzles.\nWe find that STL enables small open-source models to guide efficient search,\nreducing inference costs by integrating explicit reasoning with value learning.\n","authors":["Ethan Mendes","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2503.02878v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26000v1","updated":"2025-10-29T22:25:43Z","published":"2025-10-29T22:25:43Z","title":"Infrequent Exploration in Linear Bandits","summary":"  We study the problem of infrequent exploration in linear bandits, addressing\na significant yet overlooked gap between fully adaptive exploratory methods\n(e.g., UCB and Thompson Sampling), which explore potentially at every time\nstep, and purely greedy approaches, which require stringent diversity\nassumptions to succeed. Continuous exploration can be impractical or unethical\nin safety-critical or costly domains, while purely greedy strategies typically\nfail without adequate contextual diversity. To bridge these extremes, we\nintroduce a simple and practical framework, INFEX, explicitly designed for\ninfrequent exploration. INFEX executes a base exploratory policy according to a\ngiven schedule while predominantly choosing greedy actions in between. Despite\nits simplicity, our theoretical analysis demonstrates that INFEX achieves\ninstance-dependent regret matching standard provably efficient algorithms,\nprovided the exploration frequency exceeds a logarithmic threshold.\nAdditionally, INFEX is a general, modular framework that allows seamless\nintegration of any fully adaptive exploration method, enabling wide\napplicability and ease of adoption. By restricting intensive exploratory\ncomputations to infrequent intervals, our approach can also enhance\ncomputational efficiency. Empirical evaluations confirm our theoretical\nfindings, showing state-of-the-art regret performance and runtime improvements\nover existing methods.\n","authors":["Harin Lee","Min-hwan Oh"],"pdf_url":"https://arxiv.org/pdf/2510.26000v1.pdf","comment":"NeurIPS 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2510.22094v2","updated":"2025-10-29T22:11:33Z","published":"2025-10-25T00:21:16Z","title":"Hierarchical Graph Networks for Accurate Weather Forecasting via\n  Lightweight Training","summary":"  Climate events arise from intricate, multivariate dynamics governed by\nglobal-scale drivers, profoundly impacting food, energy, and infrastructure.\nYet, accurate weather prediction remains elusive due to physical processes\nunfolding across diverse spatio-temporal scales, which fixed-resolution methods\ncannot capture. Hierarchical Graph Neural Networks (HGNNs) offer a multiscale\nrepresentation, but nonlinear downward mappings often erase global trends,\nweakening the integration of physics into forecasts. We introduce HiFlowCast\nand its ensemble variant HiAntFlow, HGNNs that embed physics within a\nmultiscale prediction framework. Two innovations underpin their design: a\nLatent-Memory-Retention mechanism that preserves global trends during downward\ntraversal, and a Latent-to-Physics branch that integrates PDE solution fields\nacross diverse scales. Our Flow models cut errors by over 5% at 13-day lead\ntimes and by 5-8% under 1st and 99th quantile extremes, improving reliability\nfor rare events. Leveraging pretrained model weights, they converge within a\nsingle epoch, reducing training cost and their carbon footprint. Such\nefficiency is vital as the growing scale of machine learning challenges\nsustainability and limits research accessibility. Code and model weights are in\nthe supplementary materials.\n","authors":["Thomas Bailie","S. Karthik Mukkavilli","Varvara Vetrova","Yun Sing Koh"],"pdf_url":"https://arxiv.org/pdf/2510.22094v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25993v1","updated":"2025-10-29T22:09:53Z","published":"2025-10-29T22:09:53Z","title":"Efficient Online Learning with Predictive Coding Networks: Exploiting\n  Temporal Correlations","summary":"  Robotic systems operating at the edge require efficient online learning\nalgorithms that can continuously adapt to changing environments while\nprocessing streaming sensory data. Traditional backpropagation, while\neffective, conflicts with biological plausibility principles and may be\nsuboptimal for continuous adaptation scenarios. The Predictive Coding (PC)\nframework offers a biologically plausible alternative with local, Hebbian-like\nupdate rules, making it suitable for neuromorphic hardware implementation.\nHowever, PC's main limitation is its computational overhead due to multiple\ninference iterations during training. We present Predictive Coding Network with\nTemporal Amortization (PCN-TA), which preserves latent states across temporal\nframes. By leveraging temporal correlations, PCN-TA significantly reduces\ncomputational demands while maintaining learning performance. Our experiments\non the COIL-20 robotic perception dataset demonstrate that PCN-TA achieves 10%\nfewer weight updates compared to backpropagation and requires 50% fewer\ninference steps than baseline PC networks. These efficiency gains directly\ntranslate to reduced computational overhead for moving another step toward edge\ndeployment and real-time adaptation support in resource-constrained robotic\nsystems. The biologically-inspired nature of our approach also makes it a\npromising candidate for future neuromorphic hardware implementations, enabling\nefficient online learning at the edge.\n","authors":["Darius Masoum Zadeh-Jousdani","Elvin Hajizada","Eyke Hüllermeier"],"pdf_url":"https://arxiv.org/pdf/2510.25993v1.pdf","comment":"Accepted at EdgeAI4R Workshop, IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2505.18125v2","updated":"2025-10-29T22:07:41Z","published":"2025-05-23T17:34:28Z","title":"TabSTAR: A Tabular Foundation Model for Tabular Data with Text Fields","summary":"  While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees. However, recent advancements are\npaving the way for Tabular Foundation Models, which can leverage real-world\nknowledge and generalize across diverse datasets, particularly when the data\ncontains free-text. Although incorporating language model capabilities into\ntabular tasks has been explored, most existing methods utilize static,\ntarget-agnostic textual representations, limiting their effectiveness. We\nintroduce TabSTAR: a Tabular Foundation Model with Semantically Target-Aware\nRepresentations. TabSTAR is designed to enable transfer learning on tabular\ndata with textual features, with an architecture free of dataset-specific\nparameters. It unfreezes a pretrained text encoder and takes as input target\ntokens, which provide the model with the context needed to learn task-specific\nembeddings. TabSTAR achieves state-of-the-art performance for both medium- and\nlarge-sized datasets across known benchmarks of classification tasks with text\nfeatures, and its pretraining phase exhibits scaling laws in the number of\ndatasets, offering a pathway for further performance improvements.\n","authors":["Alan Arazi","Eilam Shapira","Roi Reichart"],"pdf_url":"https://arxiv.org/pdf/2505.18125v2.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25992v1","updated":"2025-10-29T22:05:08Z","published":"2025-10-29T22:05:08Z","title":"Supervised Reinforcement Learning: From Expert Trajectories to Step-wise\n  Reasoning","summary":"  Large Language Models (LLMs) often struggle with problems that require\nmulti-step reasoning. For small-scale open-source models, Reinforcement\nLearning with Verifiable Rewards (RLVR) fails when correct solutions are rarely\nsampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to\noverfit long demonstrations through rigid token-by-token imitation. To address\nthis gap, we propose Supervised Reinforcement Learning (SRL), a framework that\nreformulates problem solving as generating a sequence of logical \"actions\". SRL\ntrains the model to generate an internal reasoning monologue before committing\nto each action. It provides smoother rewards based on the similarity between\nthe model's actions and expert actions extracted from the SFT dataset in a\nstep-wise manner. This supervision offers richer learning signals even when all\nrollouts are incorrect, while encouraging flexible reasoning guided by expert\ndemonstrations. As a result, SRL enables small models to learn challenging\nproblems previously unlearnable by SFT or RLVR. Moreover, initializing training\nwith SRL before refining with RLVR yields the strongest overall performance.\nBeyond reasoning benchmarks, SRL generalizes effectively to agentic software\nengineering tasks, establishing it as a robust and versatile training framework\nfor reasoning-oriented LLMs.\n","authors":["Yihe Deng","I-Hung Hsu","Jun Yan","Zifeng Wang","Rujun Han","Gufeng Zhang","Yanfei Chen","Wei Wang","Tomas Pfister","Chen-Yu Lee"],"pdf_url":"https://arxiv.org/pdf/2510.25992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12052v2","updated":"2025-10-29T22:00:08Z","published":"2024-11-18T20:46:02Z","title":"HoGA: Higher-Order Graph Attention via Diversity-Aware k-Hop Sampling","summary":"  Graphs model latent variable relationships in many real-world systems, and\nMessage Passing Neural Networks (MPNNs) are widely used to learn such\nstructures for downstream tasks. While edge-based MPNNs effectively capture\nlocal interactions, their expressive power is theoretically bounded, limiting\nthe discovery of higher-order relationships. We introduce the Higher-Order\nGraph Attention (HoGA) module, which constructs a k-order attention matrix by\nsampling subgraphs to maximize diversity among feature vectors. Unlike existing\nhigher-order attention methods that greedily resample similar k-order\nrelationships, HoGA targets diverse modalities in higher-order topology,\nreducing redundancy and expanding the range of captured substructures. Applied\nto two single-hop attention models, HoGA achieves at least a 5% accuracy gain\non all benchmark node classification datasets and outperforms recent baselines\non six of eight datasets. Code is available at\nhttps://github.com/TB862/Higher_Order.\n","authors":["Thomas Bailie","Yun Sing Koh","Karthik Mukkavilli"],"pdf_url":"https://arxiv.org/pdf/2411.12052v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.12344v2","updated":"2025-10-29T21:58:04Z","published":"2025-09-15T18:13:28Z","title":"FEDONet : Fourier-Embedded DeepONet for Spectrally Accurate Operator\n  Learning","summary":"  Deep Operator Networks (DeepONets) have recently emerged as powerful\ndata-driven frameworks for learning nonlinear operators, particularly suited\nfor approximating solutions to partial differential equations. Despite their\npromising capabilities, the standard implementation of DeepONets, which\ntypically employs fully connected linear layers in the trunk network, can\nencounter limitations in capturing complex spatial structures inherent to\nvarious PDEs. To address this limitation, we introduce Fourier-embedded trunk\nnetworks within the DeepONet architecture, leveraging random Fourier feature\nmappings to enrich spatial representation capabilities. Our proposed\nFourier-embedded DeepONet (FEDONet) demonstrates superior performance compared\nto the traditional DeepONet across a comprehensive suite of PDE-driven\ndatasets, including the two-dimensional Poisson, Burgers', Lorenz-63, Eikonal,\nAllen-Cahn, and the Kuramoto-Sivashinsky equation. Empirical evaluations of\nFEDONet consistently show significant improvements in solution reconstruction\naccuracy, with average relative $L^2$ performance gains ranging between\n2-3$\\times$ compared to the DeepONet baseline. This study highlights the\neffectiveness of Fourier embeddings in enhancing neural operator learning,\noffering a robust and broadly applicable methodology for PDE surrogate\nmodeling.\n","authors":["Arth Sojitra","Mrigank Dhingra","Omer San"],"pdf_url":"https://arxiv.org/pdf/2509.12344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25986v1","updated":"2025-10-29T21:42:36Z","published":"2025-10-29T21:42:36Z","title":"A General and Streamlined Differentiable Optimization Framework","summary":"  Differentiating through constrained optimization problems is increasingly\ncentral to learning, control, and large-scale decision-making systems, yet\npractical integration remains challenging due to solver specialization and\ninterface mismatches. This paper presents a general and streamlined\nframework-an updated DiffOpt.jl-that unifies modeling and differentiation\nwithin the Julia optimization stack. The framework computes forward - and\nreverse-mode solution and objective sensitivities for smooth, potentially\nnonconvex programs by differentiating the KKT system under standard regularity\nassumptions. A first-class, JuMP-native parameter-centric API allows users to\ndeclare named parameters and obtain derivatives directly with respect to them -\neven when a parameter appears in multiple constraints and objectives -\neliminating brittle bookkeeping from coefficient-level interfaces. We\nillustrate these capabilities on convex and nonconvex models, including\neconomic dispatch, mean-variance portfolio selection with conic risk\nconstraints, and nonlinear robot inverse kinematics. Two companion studies\nfurther demonstrate impact at scale: gradient-based iterative methods for\nstrategic bidding in energy markets and Sobolev-style training of end-to-end\noptimization proxies using solver-accurate sensitivities. Together, these\nresults demonstrate that differentiable optimization can be deployed as a\nroutine tool for experimentation, learning, calibration, and design-without\ndeviating from standard JuMP modeling practices and while retaining access to a\nbroad ecosystem of solvers.\n","authors":["Andrew W. Rosemberg","Joaquim Dias Garcia","François Pacaud","Robert B. Parker","Benoît Legat","Kaarthik Sundar","Russell Bent","Pascal Van Hentenryck"],"pdf_url":"https://arxiv.org/pdf/2510.25986v1.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.25983v1","updated":"2025-10-29T21:33:59Z","published":"2025-10-29T21:33:59Z","title":"Contrastive Predictive Coding Done Right for Mutual Information\n  Estimation","summary":"  The InfoNCE objective, originally introduced for contrastive representation\nlearning, has become a popular choice for mutual information (MI) estimation,\ndespite its indirect connection to MI. In this paper, we demonstrate why\nInfoNCE should not be regarded as a valid MI estimator, and we introduce a\nsimple modification, which we refer to as InfoNCE-anchor, for accurate MI\nestimation. Our modification introduces an auxiliary anchor class, enabling\nconsistent density ratio estimation and yielding a plug-in MI estimator with\nsignificantly reduced bias. Beyond this, we generalize our framework using\nproper scoring rules, which recover InfoNCE-anchor as a special case when the\nlog score is employed. This formulation unifies a broad spectrum of contrastive\nobjectives, including NCE, InfoNCE, and $f$-divergence variants, under a single\nprincipled framework. Empirically, we find that InfoNCE-anchor with the log\nscore achieves the most accurate MI estimates; however, in self-supervised\nrepresentation learning experiments, we find that the anchor does not improve\nthe downstream task performance. These findings corroborate that contrastive\nrepresentation learning benefits not from accurate MI estimation per se, but\nfrom the learning of structured density ratios.\n","authors":["J. Jon Ryu","Pavan Yeddanapudi","Xiangxiang Xu","Gregory W. Wornell"],"pdf_url":"https://arxiv.org/pdf/2510.25983v1.pdf","comment":"26 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.25982v1","updated":"2025-10-29T21:30:30Z","published":"2025-10-29T21:30:30Z","title":"Enabling Fast and Accurate Neutral Atom Readout through Image Denoising","summary":"  Neutral atom quantum computers hold promise for scaling up to hundreds of\nthousands of qubits, but their progress is constrained by slow qubit readout.\nMeasuring qubits currently takes milliseconds-much longer than the underlying\nquantum gate operations-making readout the primary bottleneck in deploying\nquantum error correction. Because each round of QEC depends on measurement,\nlong readout times increase cycle duration and slow down program execution.\nReducing the readout duration speeds up cycles and reduces decoherence errors\nthat accumulate while qubits idle, but it also lowers the number of collected\nphotons, making measurements noisier and more error-prone. This tradeoff leaves\nneutral atom systems stuck between slow but accurate readout and fast but\nunreliable readout.\n  We show that image denoising can resolve this tension. Our framework,\nGANDALF, uses explicit denoising using image translation to reconstruct clear\nsignals from short, low-photon measurements, enabling reliable classification\nat up to 1.6x shorter readout times. Combined with lightweight classifiers and\na pipelined readout design, our approach both reduces logical error rate by up\nto 35x and overall QEC cycle time up to 1.77x compared to state-of-the-art\nCNN-based readout for Cesium (Cs) Neutral Atom arrays.\n","authors":["Chaithanya Naik Mude","Linipun Phuttitarn","Satvik Maurya","Kunal Sinha","Mark Saffman","Swamit Tannu"],"pdf_url":"https://arxiv.org/pdf/2510.25982v1.pdf","comment":"12 pages, 15 figures"},{"id":"http://arxiv.org/abs/2510.25979v1","updated":"2025-10-29T21:26:17Z","published":"2025-10-29T21:26:17Z","title":"AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache","summary":"  Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation.\n","authors":["Dinghong Song","Yuan Feng","Yiwei Wang","Shangye Chen","Cyril Guyot","Filip Blagojevic","Hyeran Jeon","Pengfei Su","Dong Li"],"pdf_url":"https://arxiv.org/pdf/2510.25979v1.pdf","comment":"10 pages, 6 figures, submitted to Ninth Annual Conference on Machine\n  Learning and Systems (MLSys'26)"},{"id":"http://arxiv.org/abs/2510.16629v2","updated":"2025-10-29T21:20:51Z","published":"2025-10-18T19:58:31Z","title":"On the Impossibility of Retrain Equivalence in Machine Unlearning","summary":"  Machine unlearning seeks to selectively remove the \"influence\" of specific\ntraining data on a model's outputs. The ideal goal is Retrain\nEquivalence--behavior identical to a model trained from scratch on only the\nretained data. This goal was formulated for models trained on i.i.d. data\nbatches, but modern pipelines often involve multi-stage training, with each\nstage having a distinct data distribution and objective. Examples include LLM\nfine-tuning for alignment, reasoning ability, etc. Our study shows via theory\nand experiments that this shift to multi-stage training introduces a\nfundamental barrier for machine unlearning. The theory indicates that the\noutcome of local unlearning--methods that only use gradients computed on the\nforget set--is path-dependent. That is, a model's behavior during unlearning is\ninfluenced by the order of its training stages during learning, making it\nimpossible for path-oblivious algorithms to universally achieve Retrain\nEquivalence. We empirically demonstrate the same phenomenon in LLM\npost-training across Llama and Qwen models (1B to 14B) with gradient ascent,\nNPO, and SimNPO local unlearning algorithms. Models fine-tuned via different\norderings of identical training stages diverge in behavior during unlearning,\nwith the degradation in GSM8K accuracy after unlearning varying by over 20%\nacross paths. We also observe that some learning paths consistently produce\nmodels that unlearn slowly. During unlearning, whether the probability mass\ngets squeezed into paraphrasing or alternative concepts is also path-dependent.\nThese results consistently show that Retrain Equivalence is an ill-posed target\nfor local unlearning algorithms, so long as the target models are trained in\nstages. In situations where access to models' training histories is hard, the\ncurrent work calls for rethinking the definition and desiderata of machine\nunlearning.\n","authors":["Jiatong Yu","Yinghui He","Anirudh Goyal","Sanjeev Arora"],"pdf_url":"https://arxiv.org/pdf/2510.16629v2.pdf","comment":"Code available at\n  https://princeton-pli.github.io/impossibility-unlearning/"},{"id":"http://arxiv.org/abs/2510.25974v1","updated":"2025-10-29T21:17:50Z","published":"2025-10-29T21:17:50Z","title":"Risks and Opportunities in Human-Machine Teaming in Operationalizing\n  Machine Learning Target Variables","summary":"  Predictive modeling has the potential to enhance human decision-making.\nHowever, many predictive models fail in practice due to problematic problem\nformulation in cases where the prediction target is an abstract concept or\nconstruct and practitioners need to define an appropriate target variable as a\nproxy to operationalize the construct of interest. The choice of an appropriate\nproxy target variable is rarely self-evident in practice, requiring both domain\nknowledge and iterative data modeling. This process is inherently\ncollaborative, involving both domain experts and data scientists. In this work,\nwe explore how human-machine teaming can support this process by accelerating\niterations while preserving human judgment. We study the impact of two\nhuman-machine teaming strategies on proxy construction: 1) relevance-first:\nhumans leading the process by selecting relevant proxies, and 2)\nperformance-first: machines leading the process by recommending proxies based\non predictive performance. Based on a controlled user study of a proxy\nconstruction task (N = 20), we show that the performance-first strategy\nfacilitated faster iterations and decision-making, but also biased users\ntowards well-performing proxies that are misaligned with the application goal.\nOur study highlights the opportunities and risks of human-machine teaming in\noperationalizing machine learning target variables, yielding insights for\nfuture research to explore the opportunities and mitigate the risks.\n","authors":["Mengtian Guo","David Gotz","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25974v1.pdf","comment":"23 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.05901v3","updated":"2025-10-29T21:11:25Z","published":"2024-09-05T20:45:44Z","title":"Diffusion Map Autoencoder","summary":"  Diffusion-Map-AutoEncoder (DMAE) pairs a diffusion-map encoder (using the\nNystr\\\"om method) with linear or RBF Gaussian-Process latent mean decoders,\nyielding closed-form inductive mappings and strong reconstructions.\n","authors":["Julio Candanedo"],"pdf_url":"https://arxiv.org/pdf/2409.05901v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25962v1","updated":"2025-10-29T21:01:31Z","published":"2025-10-29T21:01:31Z","title":"On the Dataless Training of Neural Networks","summary":"  This paper surveys studies on the use of neural networks for optimization in\nthe training-data-free setting. Specifically, we examine the dataless\napplication of neural network architectures in optimization by\nre-parameterizing problems using fully connected (or MLP), convolutional,\ngraph, and quadratic neural networks. Although MLPs have been used to solve\nlinear programs a few decades ago, this approach has recently gained increasing\nattention due to its promising results across diverse applications, including\nthose based on combinatorial optimization, inverse problems, and partial\ndifferential equations. The motivation for this setting stems from two key\n(possibly over-lapping) factors: (i) data-driven learning approaches are still\nunderdeveloped and have yet to demonstrate strong results, as seen in\ncombinatorial optimization, and (ii) the availability of training data is\ninherently limited, such as in medical image reconstruction and other\nscientific applications. In this paper, we define the dataless setting and\ncategorize it into two variants based on how a problem instance -- defined by a\nsingle datum -- is encoded onto the neural network: (i) architecture-agnostic\nmethods and (ii) architecture-specific methods. Additionally, we discuss\nsimilarities and clarify distinctions between the dataless neural network (dNN)\nsettings and related concepts such as zero-shot learning, one-shot learning,\nlifting in optimization, and over-parameterization.\n","authors":["Alvaro Velasquez","Susmit Jha","Ismail R. Alkhouri"],"pdf_url":"https://arxiv.org/pdf/2510.25962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25954v1","updated":"2025-10-29T20:53:07Z","published":"2025-10-29T20:53:07Z","title":"Application and Validation of Geospatial Foundation Model Data for the\n  Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi","summary":"  The reliability of routine health data in low and middle-income countries\n(LMICs) is often constrained by reporting delays and incomplete coverage,\nnecessitating the exploration of novel data sources and analytics. Geospatial\nFoundation Models (GeoFMs) offer a promising avenue by synthesizing diverse\nspatial, temporal, and behavioral data into mathematical embeddings that can be\nefficiently used for downstream prediction tasks. This study evaluated the\npredictive performance of three GeoFM embedding sources - Google Population\nDynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite\nimagery), and mobile phone call detail records (CDR) - for modeling 15 routine\nhealth programmatic outputs in Malawi, and compared their utility to\ntraditional geospatial interpolation methods. We used XGBoost models on data\nfrom 552 health catchment areas (January 2021-May 2023), assessing performance\nwith R2, and using an 80/20 training and test data split with 5-fold\ncross-validation used in training. While predictive performance was mixed, the\nembedding-based approaches improved upon baseline geostatistical methods in 13\nof 15 (87%) indicators tested. A Multi-GeoFM model integrating all three\nembedding sources produced the most robust predictions, achieving average\n5-fold cross validated R2 values for indicators like population density (0.63),\nnew HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64,\n0.68, and 0.55, respectively. Prediction was poor for prediction targets with\nlow primary data availability, such as TB and malnutrition cases. These results\ndemonstrate that GeoFM embeddings imbue a modest predictive improvement for\nselect health and demographic outcomes in an LMIC context. We conclude that the\nintegration of multiple GeoFM sources is an efficient and valuable tool for\nsupplementing and strengthening constrained routine health information systems.\n","authors":["Lynn Metz","Rachel Haggard","Michael Moszczynski","Samer Asbah","Chris Mwase","Patricia Khomani","Tyler Smith","Hannah Cooper","Annie Mwale","Arbaaz Muslim","Gautam Prasad","Mimi Sun","Tomer Shekel","Joydeep Paul","Anna Carter","Shravya Shetty","Dylan Green"],"pdf_url":"https://arxiv.org/pdf/2510.25954v1.pdf","comment":"13 pages, 3010 words, 2 tables, 2 figures"},{"id":"http://arxiv.org/abs/2510.25952v1","updated":"2025-10-29T20:52:01Z","published":"2025-10-29T20:52:01Z","title":"Modular Linear Tokenization (MLT)","summary":"  This paper introduces Modular Linear Tokenization (MLT), a reversible and\ndeterministic technique for encoding high-cardinality categorical identifiers\ninto compact numerical vectors. Unlike traditional hashing or one-hot\nencodings, MLT preserves bijective mappings by leveraging modular arithmetic\nover finite fields and invertible linear transformations. The method offers\nexplicit control of dimensionality and computational scalability while\nmaintaining full reversibility, even for millions of identifiers. Experimental\nresults on the MovieLens 20M dataset show that MLT achieves comparable\npredictive performance to supervised embeddings while requiring significantly\nfewer parameters and lower training cost. An open-source implementation of MLT\nis available on PyPI (https://pypi.org/project/light-mlt/) and GitHub\n(https://github.com/tcharliesschmitz/light-mlt).\n","authors":["Tcharlies Schmitz"],"pdf_url":"https://arxiv.org/pdf/2510.25952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25947v1","updated":"2025-10-29T20:46:03Z","published":"2025-10-29T20:46:03Z","title":"Revisiting Multilingual Data Mixtures in Language Model Pretraining","summary":"  The impact of different multilingual data mixtures in pretraining large\nlanguage models (LLMs) has been a topic of ongoing debate, often raising\nconcerns about potential trade-offs between language coverage and model\nperformance (i.e., the curse of multilinguality). In this work, we investigate\nthese assumptions by training 1.1B and 3B parameter LLMs on diverse\nmultilingual corpora, varying the number of languages from 25 to 400. Our study\nchallenges common beliefs surrounding multilingual training. First, we find\nthat combining English and multilingual data does not necessarily degrade the\nin-language performance of either group, provided that languages have a\nsufficient number of tokens included in the pretraining corpus. Second, we\nobserve that using English as a pivot language (i.e., a high-resource language\nthat serves as a catalyst for multilingual generalization) yields benefits\nacross language families, and contrary to expectations, selecting a pivot\nlanguage from within a specific family does not consistently improve\nperformance for languages within that family. Lastly, we do not observe a\nsignificant \"curse of multilinguality\" as the number of training languages\nincreases in models at this scale. Our findings suggest that multilingual data,\nwhen balanced appropriately, can enhance language model capabilities without\ncompromising performance, even in low-resource settings\n","authors":["Negar Foroutan","Paul Teiletche","Ayush Kumar Tarun","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2510.25947v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2503.04852v3","updated":"2025-10-29T20:44:13Z","published":"2025-03-06T03:40:01Z","title":"CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data","summary":"  True intelligence hinges on the ability to uncover and leverage hidden causal\nrelations. Despite significant progress in AI and computer vision (CV), there\nremains a lack of benchmarks for assessing models' abilities to infer latent\ncausality from complex visual data. In this paper, we introduce\n\\textsc{\\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates\nstructured data (tables) with corresponding visual representations (images) to\nevaluate causal reasoning. Designed within a systematic framework, Causal3D\ncomprises 19 3D-scene datasets capturing diverse causal relations, views, and\nbackgrounds, enabling evaluations across scenes of varying complexity. We\nassess multiple state-of-the-art methods, including classical causal discovery,\ncausal representation learning, and large/vision-language models (LLMs/VLMs).\nOur experiments show that as causal structures grow more complex without prior\nknowledge, performance declines significantly, highlighting the challenges even\nadvanced methods face in complex causal scenarios. Causal3D serves as a vital\nresource for advancing causal reasoning in CV and fostering trustworthy AI in\ncritical domains.\n","authors":["Disheng Liu","Yiran Qiao","Wuche Liu","Yiren Lu","Yunlai Zhou","Tuo Liang","Yu Yin","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2503.04852v3.pdf","comment":"Datasets link:\n  https://huggingface.co/datasets/LLDDSS/Causal3D_Dataset"},{"id":"http://arxiv.org/abs/2510.25943v1","updated":"2025-10-29T20:38:03Z","published":"2025-10-29T20:38:03Z","title":"InputDSA: Demixing then Comparing Recurrent and Externally Driven\n  Dynamics","summary":"  In control problems and basic scientific modeling, it is important to compare\nobservations with dynamical simulations. For example, comparing two neural\nsystems can shed light on the nature of emergent computations in the brain and\ndeep neural networks. Recently, Ostrow et al. (2023) introduced Dynamical\nSimilarity Analysis (DSA), a method to measure the similarity of two systems\nbased on their recurrent dynamics rather than geometry or topology. However,\nDSA does not consider how inputs affect the dynamics, meaning that two similar\nsystems, if driven differently, may be classified as different. Because\nreal-world dynamical systems are rarely autonomous, it is important to account\nfor the effects of input drive. To this end, we introduce a novel metric for\ncomparing both intrinsic (recurrent) and input-driven dynamics, called InputDSA\n(iDSA). InputDSA extends the DSA framework by estimating and comparing both\ninput and intrinsic dynamic operators using a variant of Dynamic Mode\nDecomposition with control (DMDc) based on subspace identification. We\ndemonstrate that InputDSA can successfully compare partially observed,\ninput-driven systems from noisy data. We show that when the true inputs are\nunknown, surrogate inputs can be substituted without a major deterioration in\nsimilarity estimates. We apply InputDSA on Recurrent Neural Networks (RNNs)\ntrained with Deep Reinforcement Learning, identifying that high-performing\nnetworks are dynamically similar to one another, while low-performing networks\nare more diverse. Lastly, we apply InputDSA to neural data recorded from rats\nperforming a cognitive task, demonstrating that it identifies a transition from\ninput-driven evidence accumulation to intrinsically-driven decision-making. Our\nwork demonstrates that InputDSA is a robust and efficient method for comparing\nintrinsic dynamics and the effect of external input on dynamical systems.\n","authors":["Ann Huang","Mitchell Ostrow","Satpreet H. Singh","Leo Kozachkov","Ila Fiete","Kanaka Rajan"],"pdf_url":"https://arxiv.org/pdf/2510.25943v1.pdf","comment":"36 pages, 14 figures"},{"id":"http://arxiv.org/abs/2505.15811v2","updated":"2025-10-29T20:37:11Z","published":"2025-05-21T17:59:21Z","title":"On the creation of narrow AI: hierarchy and nonlocality of neural\n  network skills","summary":"  We study the problem of creating strong, yet narrow, AI systems. While recent\nAI progress has been driven by the training of large general-purpose foundation\nmodels, the creation of smaller models specialized for narrow domains could be\nvaluable for both efficiency and safety. In this work, we explore two\nchallenges involved in creating such systems, having to do with basic\nproperties of how neural networks learn and structure their representations.\nThe first challenge regards when it is possible to train narrow models from\nscratch. Through experiments on a synthetic task, we find that it is sometimes\nnecessary to train networks on a wide distribution of data to learn certain\nnarrow skills within that distribution. This effect arises when skills depend\non each other hierarchically, and training on a broad distribution introduces a\ncurriculum which substantially accelerates learning. The second challenge\nregards how to transfer particular skills from large general models into small\nspecialized models. We find that model skills are often not perfectly localized\nto a particular set of prunable components. However, we find that methods based\non pruning can still outperform distillation. We investigate the use of a\nregularization objective to align desired skills with prunable components while\nunlearning unnecessary skills.\n","authors":["Eric J. Michaud","Asher Parker-Sartori","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2505.15811v2.pdf","comment":"NeurIPS 2025; 20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.03348v5","updated":"2025-10-29T20:32:11Z","published":"2024-10-04T12:12:36Z","title":"Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning","summary":"  Neurosymbolic learning enables the integration of symbolic reasoning with\ndeep learning but faces significant challenges in scaling to complex symbolic\nprograms, large datasets, or both. We introduce DOLPHIN, a framework that\ntackles these challenges by supporting neurosymbolic programs in Python,\nexecuting complex symbolic reasoning on the CPU while vectorizing probabilistic\ncomputations and gradient propagation on the GPU. Across 13 benchmarks spanning\ntasks over text, image, and video data, with symbolic reasoning features like\nrecursion and black-box functions, DOLPHIN converges to state-of-the-art\naccuracies on the more complex benchmarks while existing frameworks such as\nScallop, ISED, and IndeCateR+ fail to converge within the time limit. On\nsimpler benchmarks, DOLPHIN matches their performance, while achieving these\nresults 1.71x to 62x faster than the baselines. Overall, DOLPHIN advances the\nscalability of neurosymbolic frameworks, achieving state-of-the-art efficiency\nand convergence on difficult benchmarks where existing frameworks struggle. The\ncode is published at https://github.com/Dolphin-NeSy/Dolphin.\n","authors":["Aaditya Naik","Jason Liu","Claire Wang","Amish Sethi","Saikat Dutta","Mayur Naik","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2410.03348v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25934v1","updated":"2025-10-29T20:12:42Z","published":"2025-10-29T20:12:42Z","title":"Robust GNN Watermarking via Implicit Perception of Topological\n  Invariants","summary":"  Graph Neural Networks (GNNs) are valuable intellectual property, yet many\nwatermarks rely on backdoor triggers that break under common model edits and\ncreate ownership ambiguity. We present InvGNN-WM, which ties ownership to a\nmodel's implicit perception of a graph invariant, enabling trigger-free,\nblack-box verification with negligible task impact. A lightweight head predicts\nnormalized algebraic connectivity on an owner-private carrier set; a\nsign-sensitive decoder outputs bits, and a calibrated threshold controls the\nfalse-positive rate. Across diverse node and graph classification datasets and\nbackbones, InvGNN-WM matches clean accuracy while yielding higher watermark\naccuracy than trigger- and compression-based baselines. It remains strong under\nunstructured pruning, fine-tuning, and post-training quantization; plain\nknowledge distillation (KD) weakens the mark, while KD with a watermark loss\n(KD+WM) restores it. We provide guarantees for imperceptibility and robustness,\nand we prove that exact removal is NP-complete.\n","authors":["Jipeng Li","Yannning Shen"],"pdf_url":"https://arxiv.org/pdf/2510.25934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25933v1","updated":"2025-10-29T20:12:36Z","published":"2025-10-29T20:12:36Z","title":"Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual\n  Accuracy by Directed Exoskeleton Reasoning","summary":"  We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS\nGrounding public subset within a $\\pm 5$ pp equivalence margin.\n  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI\n69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference\nis 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's\n$d = 0.023$). TOST establishes equivalence at $\\pm 5$ pp (not at $\\pm 3$ pp).\nWhen purchased as managed APIs, Humans-Junior's base model\n(Phi-3.5-mini-instruct) is $\\approx 19\\times$ less expensive than GPT-4o on\nMicrosoft AI Foundry pricing; self-hosted or edge deployments can drive\nincremental inference cost toward zero. Measured vs estimated pricing sources\nare tabulated in Appendix E.\n  Method. Our approach combines minimal directed \"Exoskeleton Reasoning\"\nscaffolds with behavioral fine-tuning that teaches protocol compliance\n(epistemic discipline) rather than domain answers. Fine-tuning alone adds\nlittle; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance\n($\\approx 25\\%$). In prompt-only settings on frontier models (Q1--Q100;\nnon-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and\nGemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.\n  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within\n$\\pm 5$ pp on Q1--Q500). Cloud pricing shows $\\approx 19\\times$ lower cost\nversus GPT-4o, and self-hosted/edge deployments can approach zero marginal\ncost. Pricing sources are listed in Appendix E. Frontier prompt-only gains\n(Q1--Q100; non-comparable) and optimized-prompt exploratory results under\nearlier judges are summarized in Appendix F.\n  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,\nFine-Tuning, Model Alignment, Cost-Efficient AI\n","authors":["Nissan Yaron","Dan Bystritsky","Ben-Etzion Yaron"],"pdf_url":"https://arxiv.org/pdf/2510.25933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25926v1","updated":"2025-10-29T19:54:04Z","published":"2025-10-29T19:54:04Z","title":"Active Learning with Task-Driven Representations for Messy Pools","summary":"  Active learning has the potential to be especially useful for messy,\nuncurated pools where datapoints vary in relevance to the target task. However,\nstate-of-the-art approaches to this problem currently rely on using fixed,\nunsupervised representations of the pool, focusing on modifying the acquisition\nfunction instead. We show that this model setup can undermine their\neffectiveness at dealing with messy pools, as such representations can fail to\ncapture important information relevant to the task. To address this, we propose\nusing task-driven representations that are periodically updated during the\nactive learning process using the previously collected labels. We introduce two\nspecific strategies for learning these representations, one based on directly\nlearning semi-supervised representations and the other based on supervised\nfine-tuning of an initial unsupervised representation. We find that both\nsignificantly improve empirical performance over using unsupervised or\npretrained representations.\n","authors":["Kianoosh Ashouritaklimi","Tom Rainforth"],"pdf_url":"https://arxiv.org/pdf/2510.25926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25924v1","updated":"2025-10-29T19:53:51Z","published":"2025-10-29T19:53:51Z","title":"Transferring Causal Effects using Proxies","summary":"  We consider the problem of estimating a causal effect in a multi-domain\nsetting. The causal effect of interest is confounded by an unobserved\nconfounder and can change between the different domains. We assume that we have\naccess to a proxy of the hidden confounder and that all variables are discrete\nor categorical. We propose methodology to estimate the causal effect in the\ntarget domain, where we assume to observe only the proxy variable. Under these\nconditions, we prove identifiability (even when treatment and response\nvariables are continuous). We introduce two estimation techniques, prove\nconsistency, and derive confidence intervals. The theoretical results are\nsupported by simulation studies and a real-world example studying the causal\neffect of website rankings on consumer choices.\n","authors":["Manuel Iglesias-Alonso","Felix Schur","Julius von Kügelgen","Jonas Peters"],"pdf_url":"https://arxiv.org/pdf/2510.25924v1.pdf","comment":"Advances in Neural Information Processing Systems (NeurIPS 2025)\n  camera-ready version"},{"id":"http://arxiv.org/abs/2510.25897v1","updated":"2025-10-29T18:59:17Z","published":"2025-10-29T18:59:17Z","title":"MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and\n  efficiency","summary":"  Current text-to-image generative models are trained on large uncurated\ndatasets to enable diverse generation capabilities. However, this does not\nalign well with user preferences. Recently, reward models have been\nspecifically designed to perform post-hoc selection of generated images and\nalign them to a reward, typically user preference. This discarding of\ninformative data together with the optimizing for a single reward tend to harm\ndiversity, semantic fidelity and efficiency. Instead of this post-processing,\nwe propose to condition the model on multiple reward models during training to\nlet the model learn user preferences directly. We show that this not only\ndramatically improves the visual quality of the generated images but it also\nsignificantly speeds up the training. Our proposed method, called MIRO,\nachieves state-of-the-art performances on the GenEval compositional benchmark\nand user-preference scores (PickAScore, ImageReward, HPSv2).\n","authors":["Nicolas Dufour","Lucas Degeorge","Arijit Ghosh","Vicky Kalogeiton","David Picard"],"pdf_url":"https://arxiv.org/pdf/2510.25897v1.pdf","comment":"Project page: https://nicolas-dufour.github.io/miro"},{"id":"http://arxiv.org/abs/2407.11873v3","updated":"2025-10-29T18:52:40Z","published":"2024-07-16T15:59:49Z","title":"Infinite-dimensional Mahalanobis Distance with Applications to\n  Kernelized Novelty Detection","summary":"  The Mahalanobis distance is a classical tool used to measure the\ncovariance-adjusted distance between points in $\\bbR^d$. In this work, we\nextend the concept of Mahalanobis distance to separable Banach spaces by\nreinterpreting it as a Cameron-Martin norm associated with a probability\nmeasure. This approach leads to a basis-free, data-driven notion of anomaly\ndistance through the so-called variance norm, which can naturally be estimated\nusing empirical measures of a sample. Our framework generalizes the classical\n$\\bbR^d$, functional $(L^2[0,1])^d$, and kernelized settings; importantly, it\nincorporates non-injective covariance operators. We prove that the variance\nnorm is invariant under invertible bounded linear transformations of the data,\nextending previous results which are limited to unitary operators. In the\nHilbert space setting, we connect the variance norm to the RKHS of the\ncovariance operator, and establish consistency and convergence results for\nestimation using empirical measures with Tikhonov regularization. Using the\nvariance norm, we introduce the notion of a kernelized nearest-neighbour\nMahalanobis distance, and study some of its finite-sample concentration\nproperties. In an empirical study on 12 real-world data sets, we demonstrate\nthat the kernelized nearest-neighbour Mahalanobis distance outperforms the\ntraditional kernelized Mahalanobis distance for multivariate time series\nnovelty detection, using state-of-the-art time series kernels such as the\nsignature, global alignment, and Volterra reservoir kernels.\n","authors":["Nikita Zozoulenko","Thomas Cass","Lukas Gonon"],"pdf_url":"https://arxiv.org/pdf/2407.11873v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25892v1","updated":"2025-10-29T18:49:24Z","published":"2025-10-29T18:49:24Z","title":"Topology-Aware Active Learning on Graphs","summary":"  We propose a graph-topological approach to active learning that directly\ntargets the core challenge of exploration versus exploitation under scarce\nlabel budgets. To guide exploration, we introduce a coreset construction\nalgorithm based on Balanced Forman Curvature (BFC), which selects\nrepresentative initial labels that reflect the graph's cluster structure. This\nmethod includes a data-driven stopping criterion that signals when the graph\nhas been sufficiently explored. We further use BFC to dynamically trigger the\nshift from exploration to exploitation within active learning routines,\nreplacing hand-tuned heuristics. To improve exploitation, we introduce a\nlocalized graph rewiring strategy that efficiently incorporates multiscale\ninformation around labeled nodes, enhancing label propagation while preserving\nsparsity. Experiments on benchmark classification tasks show that our methods\nconsistently outperform existing graph-based semi-supervised baselines at low\nlabel rates.\n","authors":["Harris Hardiman-Mostow","Jack Mauro","Adrien Weihs","Andrea L. Bertozzi"],"pdf_url":"https://arxiv.org/pdf/2510.25892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.01223v3","updated":"2025-10-29T18:37:50Z","published":"2025-04-01T22:22:25Z","title":"Explainable post-training bias mitigation with distribution-based\n  fairness metrics","summary":"  We develop a novel bias mitigation framework with distribution-based fairness\nconstraints suitable for producing demographically blind and explainable\nmachine-learning models across a wide range of fairness levels. This is\naccomplished through post-processing, allowing fairer models to be generated\nefficiently without retraining the underlying model. Our framework, which is\nbased on stochastic gradient descent, can be applied to a wide range of model\ntypes, with a particular emphasis on the post-processing of gradient-boosted\ndecision trees. Additionally, we design a broad family of global fairness\nmetrics, along with differentiable and consistent estimators compatible with\nour framework, building on previous work. We empirically test our methodology\non a variety of datasets and compare it with alternative post-processing\napproaches, including Bayesian search, optimal transport projection, and direct\nneural network training.\n","authors":["Ryan Franks","Alexey Miroshnikov","Konstandinos Kotsiopoulos"],"pdf_url":"https://arxiv.org/pdf/2504.01223v3.pdf","comment":"45 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.25889v1","updated":"2025-10-29T18:37:39Z","published":"2025-10-29T18:37:39Z","title":"$π_\\texttt{RL}$: Online RL Fine-tuning for Flow-based\n  Vision-Language-Action Models","summary":"  Vision-Language-Action (VLA) models enable robots to understand and perform\ncomplex tasks from multimodal input. Although recent work explores using\nreinforcement learning (RL) to automate the laborious data collection process\nin scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based\nVLAs (e.g., $\\pi_0$, $\\pi_{0.5}$) remains challenging due to intractable action\nlog-likelihoods from iterative denoising.\n  We address this challenge with $\\pi_{\\text{RL}}$, an open-source framework\nfor training flow-based VLAs in parallel simulation. $\\pi_{\\text{RL}}$\nimplements two RL algorithms: (1) {Flow-Noise} models the denoising process as\na discrete-time MDP with a learnable noise network for exact log-likelihood\ncomputation. (2) {Flow-SDE} integrates denoising with agent-environment\ninteraction, formulating a two-layer MDP that employs ODE-to-SDE conversion for\nefficient RL exploration.\n  We evaluate $\\pi_{\\text{RL}}$ on LIBERO and ManiSkill benchmarks. On LIBERO,\n$\\pi_{\\text{RL}}$ boosts few-shot SFT models $\\pi_0$ and $\\pi_{0.5}$ from 57.6%\nto 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train\n$\\pi_{\\text{RL}}$ in 320 parallel environments, improving $\\pi_0$ from 41.6% to\n85.7% and $\\pi_{0.5}$ from 40.0% to 84.8% across 4352 pick-and-place tasks,\ndemonstrating scalable multitask RL under heterogeneous simulation.\n  Overall, $\\pi_{\\text{RL}}$ achieves significant performance gains and\nstronger generalization over SFT-models, validating the effectiveness of online\nRL for flow-based VLAs.\n","authors":["Kang Chen","Zhihao Liu","Tonghe Zhang","Zhen Guo","Si Xu","Hao Lin","Hongzhi Zang","Quanlu Zhang","Zhaofei Yu","Guoliang Fan","Tiejun Huang","Yu Wang","Chao Yu"],"pdf_url":"https://arxiv.org/pdf/2510.25889v1.pdf","comment":"Preprint, work in progress. 24 pages"},{"id":"http://arxiv.org/abs/2510.25884v1","updated":"2025-10-29T18:32:53Z","published":"2025-10-29T18:32:53Z","title":"Approximating Human Preferences Using a Multi-Judge Learned System","summary":"  Aligning LLM-based judges with human preferences is a significant challenge,\nas they are difficult to calibrate and often suffer from rubric sensitivity,\nbias, and instability. Overcoming this challenge advances key applications,\nsuch as creating reliable reward models for Reinforcement Learning from Human\nFeedback (RLHF) and building effective routing systems that select the\nbest-suited model for a given user query. In this work, we propose a framework\nfor modeling diverse, persona-based preferences by learning to aggregate\noutputs from multiple rubric-conditioned judges. We investigate the performance\nof this approach against naive baselines and assess its robustness through case\nstudies on both human and LLM-judges biases. Our primary contributions include\na persona-based method for synthesizing preference labels at scale and two\ndistinct implementations of our aggregator: Generalized Additive Model (GAM)\nand a Multi-Layer Perceptron (MLP).\n","authors":["Eitán Sprejer","Fernando Avalos","Augusto Bernardi","Jose Pedro Brito de Azevedo Faustino","Jacob Haimes","Narmeen Fatimah Oozeer"],"pdf_url":"https://arxiv.org/pdf/2510.25884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.16336v2","updated":"2025-10-29T18:17:28Z","published":"2025-09-19T18:24:41Z","title":"Neural Atlas Graphs for Dynamic Scene Decomposition and Editing","summary":"  Learning editable high-resolution scene representations for dynamic scenes is\nan open problem with applications across the domains from autonomous driving to\ncreative editing - the most successful approaches today make a trade-off\nbetween editability and supporting scene complexity: neural atlases represent\ndynamic scenes as two deforming image layers, foreground and background, which\nare editable in 2D, but break down when multiple objects occlude and interact.\nIn contrast, scene graph models make use of annotated data such as masks and\nbounding boxes from autonomous-driving datasets to capture complex 3D spatial\nrelationships, but their implicit volumetric node representations are\nchallenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a\nhybrid high-resolution scene representation, where every graph node is a\nview-dependent neural atlas, facilitating both 2D appearance editing and 3D\nordering and positioning of scene elements. Fit at test-time, NAGs achieve\nstate-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR\nincrease compared to existing methods - and make environmental editing possible\nin high resolution and visual quality - creating counterfactual driving\nscenarios with new backgrounds and edited vehicle appearance. We find that the\nmethod also generalizes beyond driving scenes and compares favorably - by more\nthan 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS\nvideo dataset with a diverse set of human and animal-centric scenes.\n  Project Page: https://princeton-computational-imaging.github.io/nag/\n","authors":["Jan Philipp Schneider","Pratik Singh Bisht","Ilya Chugunov","Andreas Kolb","Michael Moeller","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2509.16336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25867v1","updated":"2025-10-29T18:10:44Z","published":"2025-10-29T18:10:44Z","title":"MedVLSynther: Synthesizing High-Quality Visual Question Answering from\n  Medical Documents with Generator-Verifier LMMs","summary":"  Large Multimodal Models (LMMs) are increasingly capable of answering medical\nquestions that require joint reasoning over images and text, yet training\ngeneral medical VQA systems is impeded by the lack of large, openly usable,\nhigh-quality corpora. We present MedVLSynther, a rubric-guided\ngenerator-verifier framework that synthesizes high-quality multiple-choice VQA\nitems directly from open biomedical literature by conditioning on figures,\ncaptions, and in-text references. The generator produces self-contained stems\nand parallel, mutually exclusive options under a machine-checkable JSON schema;\na multi-stage verifier enforces essential gates (self-containment, single\ncorrect answer, clinical validity, image-text consistency), awards fine-grained\npositive points, and penalizes common failure modes before acceptance. Applying\nthis pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over\n14,803 images spanning 13 imaging modalities and 28 anatomical regions.\nTraining open-weight LMMs with reinforcement learning using verifiable rewards\nimproves accuracy across six medical VQA benchmarks, achieving averages of\n55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA,\noutperforming strong medical LMMs. A Ablations verify that both generation and\nverification are necessary and that more verified data consistently helps, and\na targeted contamination analysis detects no leakage from evaluation suites. By\noperating entirely on open literature and open-weight models, MedVLSynther\noffers an auditable, reproducible, and privacy-preserving path to scalable\nmedical VQA training data.\n","authors":["Xiaoke Huang","Ningsen Wang","Hui Liu","Xianfeng Tang","Yuyin Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.25867v1.pdf","comment":"Project page, code, data, and models:\n  https://ucsc-vlaa.github.io/MedVLSynther/"},{"id":"http://arxiv.org/abs/2505.10361v2","updated":"2025-10-29T18:07:27Z","published":"2025-05-15T14:52:16Z","title":"Plasticity as the Mirror of Empowerment","summary":"  Agents are minimally entities that are influenced by their past observations\nand act to influence future observations. This latter capacity is captured by\nempowerment, which has served as a vital framing concept across artificial\nintelligence and cognitive science. This former capacity, however, is equally\nfoundational: In what ways, and to what extent, can an agent be influenced by\nwhat it observes? In this paper, we ground this concept in a universal\nagent-centric measure that we refer to as plasticity, and reveal a fundamental\nconnection to empowerment. Following a set of desiderata on a suitable\ndefinition, we define plasticity using a new information-theoretic quantity we\ncall the generalized directed information. We show that this new quantity\nstrictly generalizes the directed information introduced by Massey (1990) while\npreserving all of its desirable properties. Under this definition, we find that\nplasticity is well thought of as the mirror of empowerment: The two concepts\nare defined using the same measure, with only the direction of influence\nreversed. Our main result establishes a tension between the plasticity and\nempowerment of an agent, suggesting that agent design needs to be mindful of\nboth characteristics. We explore the implications of these findings, and\nsuggest that plasticity, empowerment, and their relationship are essential to\nunderstanding agency\n","authors":["David Abel","Michael Bowling","André Barreto","Will Dabney","Shi Dong","Steven Hansen","Anna Harutyunyan","Khimya Khetarpal","Clare Lyle","Razvan Pascanu","Georgios Piliouras","Doina Precup","Jonathan Richens","Mark Rowland","Tom Schaul","Satinder Singh"],"pdf_url":"https://arxiv.org/pdf/2505.10361v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25850v1","updated":"2025-10-29T18:00:16Z","published":"2025-10-29T18:00:16Z","title":"Debate2Create: Robot Co-design via Large Language Model Debates","summary":"  Automating the co-design of a robot's morphology and control is a\nlong-standing challenge due to the vast design space and the tight coupling\nbetween body and behavior. We introduce Debate2Create (D2C), a framework in\nwhich large language model (LLM) agents engage in a structured dialectical\ndebate to jointly optimize a robot's design and its reward function. In each\nround, a design agent proposes targeted morphological modifications, and a\ncontrol agent devises a reward function tailored to exploit the new design. A\npanel of pluralistic judges then evaluates the design-control pair in\nsimulation and provides feedback that guides the next round of debate. Through\niterative debates, the agents progressively refine their proposals, producing\nincreasingly effective robot designs. Notably, D2C yields diverse and\nspecialized morphologies despite no explicit diversity objective. On a\nquadruped locomotion benchmark, D2C discovers designs that travel 73% farther\nthan the default, demonstrating that structured LLM-based debate can serve as a\npowerful mechanism for emergent robot co-design. Our results suggest that\nmulti-agent debate, when coupled with physics-grounded feedback, is a promising\nnew paradigm for automated robot design.\n","authors":["Kevin Qiu","Marek Cygan"],"pdf_url":"https://arxiv.org/pdf/2510.25850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25818v1","updated":"2025-10-29T17:17:32Z","published":"2025-10-29T17:17:32Z","title":"ScaleDiff: Higher-Resolution Image Synthesis via Efficient and\n  Model-Agnostic Diffusion","summary":"  Text-to-image diffusion models often exhibit degraded performance when\ngenerating images beyond their training resolution. Recent training-free\nmethods can mitigate this limitation, but they often require substantial\ncomputation or are incompatible with recent Diffusion Transformer models. In\nthis paper, we propose ScaleDiff, a model-agnostic and highly efficient\nframework for extending the resolution of pretrained diffusion models without\nany additional training. A core component of our framework is Neighborhood\nPatch Attention (NPA), an efficient mechanism that reduces computational\nredundancy in the self-attention layer with non-overlapping patches. We\nintegrate NPA into an SDEdit pipeline and introduce Latent Frequency Mixing\n(LFM) to better generate fine details. Furthermore, we apply Structure Guidance\nto enhance global structure during the denoising process. Experimental results\ndemonstrate that ScaleDiff achieves state-of-the-art performance among\ntraining-free methods in terms of both image quality and inference speed on\nboth U-Net and Diffusion Transformer architectures.\n","authors":["Sungho Koh","SeungJu Cha","Hyunwoo Oh","Kwanyoung Lee","Dong-Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2510.25818v1.pdf","comment":"NeurIPS 2025. Code: https://github.com/KSH00906/ScaleDiff"},{"id":"http://arxiv.org/abs/2510.25816v1","updated":"2025-10-29T16:41:44Z","published":"2025-10-29T16:41:44Z","title":"Beyond Long Context: When Semantics Matter More than Tokens","summary":"  Electronic Health Records (EHR) store clinical documentation as base64\nencoded attachments in FHIR DocumentReference resources, which makes semantic\nquestion answering difficult. Traditional vector database methods often miss\nnuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)\nmethod, introduced by Lopez et al. 2025, uses entity aware retrieval and\nachieved improved performance with an F1 score of 0.90 versus 0.86 for\nembedding based retrieval, while using over 70 percent fewer tokens. We\ndeveloped a Clinical Notes QA Evaluation Platform to validate CLEAR against\nzero shot large context inference and traditional chunk based retrieval\naugmented generation. The platform was tested on 12 clinical notes ranging from\n10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a\n58.3 percent win rate, an average semantic similarity of 0.878, and used 78\npercent fewer tokens than wide context processing. The largest performance\ngains occurred on long notes, with a 75 percent win rate for documents\nexceeding 65,000 tokens. These findings confirm that entity aware retrieval\nimproves both efficiency and accuracy in clinical natural language processing.\nThe evaluation framework provides a reusable and transparent benchmark for\nassessing clinical question answering systems where semantic precision and\ncomputational efficiency are critical.\n","authors":["Tarun Kumar Chawdhury","Jon D. Duke"],"pdf_url":"https://arxiv.org/pdf/2510.25816v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.25814v1","updated":"2025-10-29T14:40:13Z","published":"2025-10-29T14:40:13Z","title":"Optimizing Mirror-Image Peptide Sequence Design for Data Storage via\n  Peptide Bond Cleavage Prediction","summary":"  Traditional non-biological storage media, such as hard drives, face\nlimitations in both storage density and lifespan due to the rapid growth of\ndata in the big data era. Mirror-image peptides composed of D-amino acids have\nemerged as a promising biological storage medium due to their high storage\ndensity, structural stability, and long lifespan. The sequencing of\nmirror-image peptides relies on \\textit{de-novo} technology. However, its\naccuracy is limited by the scarcity of tandem mass spectrometry datasets and\nthe challenges that current algorithms encounter when processing these peptides\ndirectly. This study is the first to propose improving sequencing accuracy\nindirectly by optimizing the design of mirror-image peptide sequences. In this\nwork, we introduce DBond, a deep neural network based model that integrates\nsequence features, precursor ion properties, and mass spectrometry\nenvironmental factors for the prediction of mirror-image peptide bond cleavage.\nIn this process, sequences with a high peptide bond cleavage ratio, which are\neasy to sequence, are selected. The main contributions of this study are as\nfollows. First, we constructed MiPD513, a tandem mass spectrometry dataset\ncontaining 513 mirror-image peptides. Second, we developed the peptide bond\ncleavage labeling algorithm (PBCLA), which generated approximately 12.5 million\nlabeled data based on MiPD513. Third, we proposed a dual prediction strategy\nthat combines multi-label and single-label classification. On an independent\ntest set, the single-label classification strategy outperformed other methods\nin both single and multiple peptide bond cleavage prediction tasks, offering a\nstrong foundation for sequence optimization.\n","authors":["Yilong Lu","Si Chen","Songyan Gao","Han Liu","Xin Dong","Wenfeng Shen","Guangtai Ding"],"pdf_url":"https://arxiv.org/pdf/2510.25814v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.25811v1","updated":"2025-10-29T12:32:07Z","published":"2025-10-29T12:32:07Z","title":"Multimodal Bandits: Regret Lower Bounds and Optimal Algorithms","summary":"  We consider a stochastic multi-armed bandit problem with i.i.d. rewards where\nthe expected reward function is multimodal with at most m modes. We propose the\nfirst known computationally tractable algorithm for computing the solution to\nthe Graves-Lai optimization problem, which in turn enables the implementation\nof asymptotically optimal algorithms for this bandit problem. The code for the\nproposed algorithms is publicly available at\nhttps://github.com/wilrev/MultimodalBandits\n","authors":["William Réveillard","Richard Combes"],"pdf_url":"https://arxiv.org/pdf/2510.25811v1.pdf","comment":"31 pages; NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25809v1","updated":"2025-10-29T09:33:12Z","published":"2025-10-29T09:33:12Z","title":"Flex-GAD : Flexible Graph Anomaly Detection","summary":"  Detecting anomalous nodes in attributed networks, where each node is\nassociated with both structural connections and descriptive attributes, is\nessential for identifying fraud, misinformation, and suspicious behavior in\ndomains such as social networks, academic citation graphs, and e-commerce\nplatforms. We propose Flex-GAD, a novel unsupervised framework for graph\nanomaly detection at the node level. Flex-GAD integrates two encoders to\ncapture complementary aspects of graph data. The framework incorporates a novel\ncommunity-based GCN encoder to model intra-community and inter-community\ninformation into node embeddings, thereby ensuring structural consistency,\nalong with a standard attribute encoder. These diverse representations are\nfused using a self-attention-based representation fusion module, which enables\nadaptive weighting and effective integration of the encoded information. This\nfusion mechanism allows automatic emphasis of the most relevant node\nrepresentation across different encoders. We evaluate Flex-GAD on seven\nreal-world attributed graphs with varying sizes, node degrees, and attribute\nhomogeneity. Flex-GAD achieves an average AUC improvement of 7.98% over the\npreviously best-performing method, GAD-NR, demonstrating its effectiveness and\nflexibility across diverse graph structures. Moreover, it significantly reduces\ntraining time, running 102x faster per epoch than Anomaly DAE and 3x faster per\nepoch than GAD-NR on average across seven benchmark datasets.\n","authors":["Apu Chakraborty","Anshul Kumar","Gagan Raj Gupta"],"pdf_url":"https://arxiv.org/pdf/2510.25809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25808v1","updated":"2025-10-29T09:07:53Z","published":"2025-10-29T09:07:53Z","title":"PRESTO: Preimage-Informed Instruction Optimization for Prompting\n  Black-Box LLMs","summary":"  Large language models (LLMs) have achieved remarkable success across diverse\ndomains, due to their strong instruction-following capabilities. This has led\nto increasing interest in optimizing instructions for black-box LLMs, whose\ninternal parameters are inaccessible but widely used due to their strong\nperformance. To optimize instructions for black-box LLMs, recent methods employ\nwhite-box LLMs to generate candidate instructions from optimized soft prompts.\nHowever, white-box LLMs often map different soft prompts to the same\ninstruction, leading to redundant queries. While previous studies regarded this\nmany-to-one mapping as a structure that hinders optimization efficiency, we\nreinterpret it as a useful prior knowledge that can accelerate the\noptimization. To this end, we introduce PREimage-informed inSTruction\nOptimization (PRESTO), a novel framework that leverages the preimage structure\nof soft prompts for efficient optimization. PRESTO consists of three key\ncomponents: (1) score sharing, which shares the evaluation score with all soft\nprompts in a preimage; (2) preimage-based initialization, which selects initial\ndata points that maximize search space coverage using preimage information; and\n(3) score consistency regularization, which enforces prediction consistency\nwithin each preimage. By leveraging preimages, PRESTO achieves the effect of\neffectively obtaining 14 times more scored data under the same query budget,\nresulting in more efficient optimization. Experimental results on 33\ninstruction optimization tasks demonstrate the superior performance of PRESTO.\nCode is available at https://github.com/mlvlab/PRESTO\n","authors":["Jaewon Chu","Seunghun Lee","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2510.25808v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25807v1","updated":"2025-10-29T08:52:55Z","published":"2025-10-29T08:52:55Z","title":"Discovering Interpretable Biological Concepts in Single-cell RNA-seq\n  Foundation Models","summary":"  Single-cell RNA-seq foundation models achieve strong performance on\ndownstream tasks but remain black boxes, limiting their utility for biological\ndiscovery. Recent work has shown that sparse dictionary learning can extract\nconcepts from deep learning models, with promising applications in biomedical\nimaging and protein models. However, interpreting biological concepts remains\nchallenging, as biological sequences are not inherently human-interpretable. We\nintroduce a novel concept-based interpretability framework for single-cell\nRNA-seq models with a focus on concept interpretation and evaluation. We\npropose an attribution method with counterfactual perturbations that identifies\ngenes that influence concept activation, moving beyond correlational approaches\nlike differential expression analysis. We then provide two complementary\ninterpretation approaches: an expert-driven analysis facilitated by an\ninteractive interface and an ontology-driven method with attribution-based\nbiological pathway enrichment. Applying our framework to two well-known\nsingle-cell RNA-seq models from the literature, we interpret concepts extracted\nby Top-K Sparse Auto-Encoders trained on two immune cell datasets. With a\ndomain expert in immunology, we show that concepts improve interpretability\ncompared to individual neurons while preserving the richness and\ninformativeness of the latent representations. This work provides a principled\nframework for interpreting what biological knowledge foundation models have\nencoded, paving the way for their use for hypothesis generation and discovery.\n","authors":["Charlotte Claye","Pierre Marschall","Wassila Ouerdane","Céline Hudelot","Julien Duquesne"],"pdf_url":"https://arxiv.org/pdf/2510.25807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25803v1","updated":"2025-10-29T05:47:41Z","published":"2025-10-29T05:47:41Z","title":"Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training","summary":"  Pre-training has proven effective in addressing data scarcity and performance\nlimitations in solving PDE problems with neural operators. However, challenges\nremain due to the heterogeneity of PDE datasets in equation types, which leads\nto high errors in mixed training. Additionally, dense pre-training models that\nscale parameters by increasing network width or depth incur significant\ninference costs. To tackle these challenges, we propose a novel\nMixture-of-Experts Pre-training Operator Transformer (MoE-POT), a\nsparse-activated architecture that scales parameters efficiently while\ncontrolling inference costs. Specifically, our model adopts a layer-wise\nrouter-gating network to dynamically select 4 routed experts from 16 expert\nnetworks during inference, enabling the model to focus on equation-specific\nfeatures. Meanwhile, we also integrate 2 shared experts, aiming to capture\ncommon properties of PDE and reduce redundancy among routed experts. The final\noutput is computed as the weighted average of the results from all activated\nexperts. We pre-train models with parameters from 30M to 0.5B on 6 public PDE\ndatasets. Our model with 90M activated parameters achieves up to a 40%\nreduction in zero-shot error compared with existing models with 120M activated\nparameters. Additionally, we conduct interpretability analysis, showing that\ndataset types can be inferred from router-gating network decisions, which\nvalidates the rationality and effectiveness of the MoE architecture.\n","authors":["Hong Wang","Haiyang Xin","Jie Wang","Xuanze Yang","Fei Zha","Huanshuo Dong","Yan Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.25803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25802v1","updated":"2025-10-29T03:47:02Z","published":"2025-10-29T03:47:02Z","title":"Attention Augmented GNN RNN-Attention Models for Advanced Cybersecurity\n  Intrusion Detection","summary":"  In this paper, we propose a novel hybrid deep learning architecture that\nsynergistically combines Graph Neural Networks (GNNs), Recurrent Neural\nNetworks (RNNs), and multi-head attention mechanisms to significantly enhance\ncy- bersecurity intrusion detection capabilities. By leveraging the\ncomprehensive UNSW-NB15 dataset containing diverse network traffic patterns,\nour approach effectively captures both spatial dependencies through graph\nstructural relationships and tem- poral dynamics through sequential analysis of\nnetwork events. The integrated attention mechanism provides dual benefits of\nimproved model interpretability and enhanced feature selection, enabling\ncybersecurity analysts to focus computational resources on high-impact security\nevents - a critical requirement in modern real-time intrusion detection\nsystems. Our extensive experimental evaluation demonstrates that the proposed\nhybrid model achieves superior performance compared to traditional machine\nlearning approaches and standalone deep learning models across multiple\nevaluation metrics, including accuracy, precision, recall, and F1-score. The\nmodel achieves particularly strong performance in detecting sophisticated\nattack patterns such as Advanced Persistent Threats (APTs), Distributed Denial\nof Service (DDoS) attacks, and zero-day exploits, making it a promising\nsolution for next-generation cybersecurity applications in complex network\nenvironments.\n","authors":["Jayant Biradar","Smit Shah","Tanmay Naik"],"pdf_url":"https://arxiv.org/pdf/2510.25802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25801v1","updated":"2025-10-29T03:42:23Z","published":"2025-10-29T03:42:23Z","title":"Metis-SPECS: Decoupling Multimodal Learning via Self-distilled\n  Preference-based Cold Start","summary":"  Reinforcement learning (RL) with verifiable rewards has recently catalyzed a\nwave of \"MLLM-r1\" approaches that bring RL to vision language models. Most\nrepresentative paradigms begin with a cold start, typically employing\nsupervised fine-tuning (SFT), to initialize the policy before RL. However,\nSFT-based cold start adopts the reasoning paradigm intertwined with task\nsolution and output format, which may induce instruction-style overfitting,\nweakens out-of-distribution generalization, and ultimately affects downstream\nRL. We revisit the cold start along two views, its training method and data\nconstruction, and introduce the Generalization Factor (GF) coefficient to\nquantify the generalization capability under different methods. Our empirical\nstudy finds that preference-based training methods (e.g. DPO) generalizes\nbetter than SFT-based methods in cold start. Motivated by this, we propose\nSPECS-a Self-distilled, Preference-based Cold Start framework that decouples\nmultimodal learning: (1) generates introspective preference data pairs via\nself-distillation, avoiding reliance on larger teachers or manual annotation;\n(2) performs preference-based training to learn, focusing on shallow,\ntransferable surface-form criteria (format, structure, style) rather than\nmemorizing content; and (3) hands off to RL with verifiable rewards for deep\nreasoning results. Experimental results across multiple multimodal benchmarks\nshow that our decoupling learning framework yields consistent performance gains\nover strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%.\nAdditional experiments indicate that SPECS contributes to reducing\nin-distribution \"stuckness,\" improving exploration, stabilizing training, and\nraising the performance ceiling.\n","authors":["Kun Chen","Peng Shi","Haibo Qiu","Zhixiong Zeng","Siqi Yang","Wenji Mao","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2510.25801v1.pdf","comment":"Project Page: https://github.com/Kwen-Chen/SPECS-VL"},{"id":"http://arxiv.org/abs/2510.25800v1","updated":"2025-10-29T03:22:51Z","published":"2025-10-29T03:22:51Z","title":"FreIE: Low-Frequency Spectral Bias in Neural Networks for Time-Series\n  Tasks","summary":"  The inherent autocorrelation of time series data presents an ongoing\nchallenge to multivariate time series prediction. Recently, a widely adopted\napproach has been the incorporation of frequency domain information to assist\nin long-term prediction tasks. Many researchers have independently observed the\nspectral bias phenomenon in neural networks, where models tend to fit\nlow-frequency signals before high-frequency ones. However, these observations\nhave often been attributed to the specific architectures designed by the\nresearchers, rather than recognizing the phenomenon as a universal\ncharacteristic across models. To unify the understanding of the spectral bias\nphenomenon in long-term time series prediction, we conducted extensive\nempirical experiments to measure spectral bias in existing mainstream models.\nOur findings reveal that virtually all models exhibit this phenomenon. To\nmitigate the impact of spectral bias, we propose the FreLE (Frequency Loss\nEnhancement) algorithm, which enhances model generalization through both\nexplicit and implicit frequency regularization. This is a plug-and-play model\nloss function unit. A large number of experiments have proven the superior\nperformance of FreLE. Code is available at\nhttps://github.com/Chenxing-Xuan/FreLE.\n","authors":["Jialong Sun","Xinpeng Ling","Jiaxuan Zou","Jiawen Kang","Kejia Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.25800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25798v1","updated":"2025-10-29T03:11:59Z","published":"2025-10-29T03:11:59Z","title":"MemEIC: A Step Toward Continual and Compositional Knowledge Editing","summary":"  The dynamic nature of information necessitates continuously updating large\nvision-language models (LVLMs). While recent knowledge editing techniques hint\nat promising directions, they often focus on editing a single modality (vision\nor language) in isolation. This prevalent practice neglects the inherent\nmultimodality of LVLMs and the continuous nature of knowledge updates,\npotentially leading to suboptimal editing outcomes when considering the\ninterplay between modalities and the need for ongoing knowledge refinement. To\naddress these limitations, we propose MemEIC, a novel method for Continual and\nCompositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional\nediting of both visual and textual knowledge sequentially. Our approach employs\na hybrid external-internal editor featuring a dual external memory for\ncross-modal evidence retrieval and dual LoRA adapters that facilitate\ndisentangled parameter updates for each modality. A key component is a\nbrain-inspired knowledge connector, activated selectively for compositional\nreasoning, that integrates information across different modalities. Experiments\ndemonstrate that MemEIC significantly improves performance on complex\nmultimodal questions and effectively preserves prior edits, setting a new\nbenchmark for CCKE in LVLMs.\n","authors":["Jin Seong","Jiyun Park","Wencke Liermann","Hongseok Choi","Yoonji Nam","Hyun Kim","Soojong Lim","Namhoon Lee"],"pdf_url":"https://arxiv.org/pdf/2510.25798v1.pdf","comment":"NeurIPS 2025, 38 pages, 8 figures"}]},"2025-10-30T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2510.26802v1","updated":"2025-10-30T17:59:55Z","published":"2025-10-30T17:59:55Z","title":"Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark","summary":"  Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io\n","authors":["Ziyu Guo","Xinyan Chen","Renrui Zhang","Ruichuan An","Yu Qi","Dongzhi Jiang","Xiangtai Li","Manyuan Zhang","Hongsheng Li","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2510.26802v1.pdf","comment":"Project Page: https://video-cof.github.io"},{"id":"http://arxiv.org/abs/2510.26800v1","updated":"2025-10-30T17:59:51Z","published":"2025-10-30T17:59:51Z","title":"OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes","summary":"  There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.\n","authors":["Yukun Huang","Jiwen Yu","Yanning Zhou","Jianan Wang","Xintao Wang","Pengfei Wan","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26800v1.pdf","comment":"Project page: https://yukun-huang.github.io/OmniX/"},{"id":"http://arxiv.org/abs/2510.26799v1","updated":"2025-10-30T17:59:46Z","published":"2025-10-30T17:59:46Z","title":"Masked Diffusion Captioning for Visual Feature Learning","summary":"  We learn visual features by captioning images with an image-conditioned\nmasked diffusion language model, a formulation we call masked diffusion\ncaptioning (MDC). During training, text tokens in each image-caption pair are\nmasked at a randomly chosen ratio, and a decoder conditioned on visual features\nis trained to reconstruct the original text. After training, the learned visual\nfeatures can be applied to downstream vision tasks. Unlike autoregressive\ncaptioning, the strength of the visual learning signal in MDC does not depend\non each token's position in the sequence, reducing the need for auxiliary\nobjectives. Linear probing experiments across a variety of academic-scale\nmodels and datasets show that the learned visual features are competitive with\nthose produced by autoregressive and contrastive approaches.\n","authors":["Chao Feng","Zihao Wei","Andrew Owens"],"pdf_url":"https://arxiv.org/pdf/2510.26799v1.pdf","comment":"EMNLP 2025 (Findings). Project page:\n  https://cfeng16.github.io/mdlm4vfl/"},{"id":"http://arxiv.org/abs/2510.26796v1","updated":"2025-10-30T17:59:39Z","published":"2025-10-30T17:59:39Z","title":"SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting","summary":"  Immersive applications call for synthesizing spatiotemporal 4D content from\ncasual videos without costly 3D supervision. Existing video-to-4D methods\ntypically rely on manually annotated camera poses, which are labor-intensive\nand brittle for in-the-wild footage. Recent warp-then-inpaint approaches\nmitigate the need for pose labels by warping input frames along a novel camera\ntrajectory and using an inpainting model to fill missing regions, thereby\ndepicting the 4D scene from diverse viewpoints. However, this\ntrajectory-to-trajectory formulation often entangles camera motion with scene\ndynamics and complicates both modeling and inference. We introduce SEE4D, a\npose-free, trajectory-to-camera framework that replaces explicit trajectory\nprediction with rendering to a bank of fixed virtual cameras, thereby\nseparating camera control from scene modeling. A view-conditional video\ninpainting model is trained to learn a robust geometry prior by denoising\nrealistically synthesized warped images and to inpaint occluded or missing\nregions across virtual viewpoints, eliminating the need for explicit 3D\nannotations. Building on this inpainting core, we design a spatiotemporal\nautoregressive inference pipeline that traverses virtual-camera splines and\nextends videos with overlapping windows, enabling coherent generation at\nbounded per-step complexity. We validate See4D on cross-view video generation\nand sparse reconstruction benchmarks. Across quantitative metrics and\nqualitative assessments, our method achieves superior generalization and\nimproved performance relative to pose- or trajectory-conditioned baselines,\nadvancing practical 4D world modeling from casual videos.\n","authors":["Dongyue Lu","Ao Liang","Tianxin Huang","Xiao Fu","Yuyang Zhao","Baorui Ma","Liang Pan","Wei Yin","Lingdong Kong","Wei Tsang Ooi","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26796v1.pdf","comment":"26 pages; 21 figures; 3 tables; project page:\n  https://see-4d.github.io/"},{"id":"http://arxiv.org/abs/2510.26795v1","updated":"2025-10-30T17:59:35Z","published":"2025-10-30T17:59:35Z","title":"Scaling Image Geo-Localization to Continent Level","summary":"  Determining the precise geographic location of an image at a global scale\nremains an unsolved challenge. Standard image retrieval techniques are\ninefficient due to the sheer volume of images (>100M) and fail when coverage is\ninsufficient. Scalable solutions, however, involve a trade-off: global\nclassification typically yields coarse results (10+ kilometers), while\ncross-view retrieval between ground and aerial imagery suffers from a domain\ngap and has been primarily studied on smaller regions. This paper introduces a\nhybrid approach that achieves fine-grained geo-localization across a large\ngeographic expanse the size of a continent. We leverage a proxy classification\ntask during training to learn rich feature representations that implicitly\nencode precise location information. We combine these learned prototypes with\nembeddings of aerial imagery to increase robustness to the sparsity of\nground-level data. This enables direct, fine-grained retrieval over areas\nspanning multiple countries. Our extensive evaluation demonstrates that our\napproach can localize within 200m more than 68\\% of queries of a dataset\ncovering a large part of Europe. The code is publicly available at\nhttps://scaling-geoloc.github.io.\n","authors":["Philipp Lindenberger","Paul-Edouard Sarlin","Jan Hosang","Matteo Balice","Marc Pollefeys","Simon Lynen","Eduard Trulls"],"pdf_url":"https://arxiv.org/pdf/2510.26795v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26794v1","updated":"2025-10-30T17:59:27Z","published":"2025-10-30T17:59:27Z","title":"The Quest for Generalizable Motion Generation: Data, Model, and\n  Evaluation","summary":"  Despite recent advances in 3D human motion generation (MoGen) on standard\nbenchmarks, existing models still face a fundamental bottleneck in their\ngeneralization capability. In contrast, adjacent generative fields, most\nnotably video generation (ViGen), have demonstrated remarkable generalization\nin modeling human behaviors, highlighting transferable insights that MoGen can\nleverage. Motivated by this observation, we present a comprehensive framework\nthat systematically transfers knowledge from ViGen to MoGen across three key\npillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a\nlarge-scale dataset comprising 228,000 high-quality motion samples that\nintegrates high-fidelity optical MoCap data with semantically annotated motions\nfrom web videos and synthesized samples generated by state-of-the-art ViGen\nmodels. The dataset includes both text-motion pairs and text-video-motion\ntriplets, substantially expanding semantic diversity. Second, we propose\nViMoGen, a flow-matching-based diffusion transformer that unifies priors from\nMoCap data and ViGen models through gated multimodal conditioning. To enhance\nefficiency, we further develop ViMoGen-light, a distilled variant that\neliminates video generation dependencies while preserving strong\ngeneralization. Finally, we present MBench, a hierarchical benchmark designed\nfor fine-grained evaluation across motion quality, prompt fidelity, and\ngeneralization ability. Extensive experiments show that our framework\nsignificantly outperforms existing approaches in both automatic and human\nevaluations. The code, data, and benchmark will be made publicly available.\n","authors":["Jing Lin","Ruisi Wang","Junzhe Lu","Ziqi Huang","Guorui Song","Ailing Zeng","Xian Liu","Chen Wei","Wanqi Yin","Qingping Sun","Zhongang Cai","Lei Yang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26786v1","updated":"2025-10-30T17:57:40Z","published":"2025-10-30T17:57:40Z","title":"HEIR: Learning Graph-Based Motion Hierarchies","summary":"  Hierarchical structures of motion exist across research fields, including\ncomputer vision, graphics, and robotics, where complex dynamics typically arise\nfrom coordinated interactions among simpler motion components. Existing methods\nto model such dynamics typically rely on manually-defined or heuristic\nhierarchies with fixed motion primitives, limiting their generalizability\nacross different tasks. In this work, we propose a general hierarchical motion\nmodeling method that learns structured, interpretable motion relationships\ndirectly from data. Our method represents observed motions using graph-based\nhierarchies, explicitly decomposing global absolute motions into\nparent-inherited patterns and local motion residuals. We formulate hierarchy\ninference as a differentiable graph learning problem, where vertices represent\nelemental motions and directed edges capture learned parent-child dependencies\nthrough graph neural networks. We evaluate our hierarchical reconstruction\napproach on three examples: 1D translational motion, 2D rotational motion, and\ndynamic 3D scene deformation via Gaussian splatting. Experimental results show\nthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,\nand produces more realistic and interpretable deformations compared to the\nbaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,\ndata-driven hierarchical modeling paradigm, our method offers a formulation\napplicable to a broad range of motion-centric tasks. Project Page:\nhttps://light.princeton.edu/HEIR/\n","authors":["Cheng Zheng","William Koch","Baiang Li","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2510.26786v1.pdf","comment":"Code link: https://github.com/princeton-computational-imaging/HEIR"},{"id":"http://arxiv.org/abs/2510.26782v1","updated":"2025-10-30T17:56:43Z","published":"2025-10-30T17:56:43Z","title":"Clone Deterministic 3D Worlds with Geometrically-Regularized World\n  Models","summary":"  A world model is an internal model that simulates how the world evolves.\nGiven past observations and actions, it predicts the future of both the\nembodied agent and its environment. Accurate world models are essential for\nenabling agents to think, plan, and reason effectively in complex, dynamic\nsettings. Despite rapid progress, current world models remain brittle and\ndegrade over long horizons. We argue that a central cause is representation\nquality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or\nentangled latents make dynamics learning unnecessarily hard. We therefore ask\nwhether improving representation learning alone can substantially improve\nworld-model performance. In this work, we take a step toward building a truly\naccurate world model by addressing a fundamental yet open problem: constructing\na model that can fully clone and overfit to a deterministic 3D world. We\npropose Geometrically-Regularized World Models (GRWM), which enforces that\nconsecutive points along a natural sensory trajectory remain close in latent\nrepresentation space. This approach yields significantly improved latent\nrepresentations that align closely with the true topology of the environment.\nGRWM is plug-and-play, requires only minimal architectural modification, scales\nwith trajectory length, and is compatible with diverse latent generative\nbackbones. Across deterministic 3D settings and long-horizon prediction tasks,\nGRWM significantly increases rollout fidelity and stability. Analyses show that\nits benefits stem from learning a latent manifold with superior geometric\nstructure. These findings support a clear takeaway: improving representation\nlearning is a direct and useful path to robust world models, delivering\nreliable long-horizon predictions without enlarging the dynamics module.\n","authors":["Zaishuo Xia","Yukuan Lu","Xinyi Li","Yifan Xu","Yubei Chen"],"pdf_url":"https://arxiv.org/pdf/2510.26782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26781v1","updated":"2025-10-30T17:56:31Z","published":"2025-10-30T17:56:31Z","title":"ChartAB: A Benchmark for Chart Grounding & Dense Alignment","summary":"  Charts play an important role in visualization, reasoning, data analysis, and\nthe exchange of ideas among humans. However, existing vision-language models\n(VLMs) still lack accurate perception of details and struggle to extract\nfine-grained structures from charts. Such limitations in chart grounding also\nhinder their ability to compare multiple charts and reason over them. In this\npaper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a\ncomprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting\ntabular data, localizing visualization elements, and recognizing various\nattributes from charts of diverse types and complexities. We design a JSON\ntemplate to facilitate the calculation of evaluation metrics specifically\ntailored for each grounding task. By incorporating a novel two-stage inference\nworkflow, the benchmark can further evaluate VLMs' capability to align and\ncompare elements/attributes across two charts. Our analysis of evaluations on\nseveral recent VLMs reveals new insights into their perception biases,\nweaknesses, robustness, and hallucinations in chart understanding. These\nfindings highlight the fine-grained discrepancies among VLMs in chart\nunderstanding tasks and point to specific skills that need to be strengthened\nin current models.\n","authors":["Aniruddh Bansal","Davit Soselia","Dang Nguyen","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26778v1","updated":"2025-10-30T17:55:46Z","published":"2025-10-30T17:55:46Z","title":"Surpassing state of the art on AMD area estimation from RGB fundus\n  images through careful selection of U-Net architectures and loss functions\n  for class imbalance","summary":"  Age-related macular degeneration (AMD) is one of the leading causes of\nirreversible vision impairment in people over the age of 60. This research\nfocuses on semantic segmentation for AMD lesion detection in RGB fundus images,\na non-invasive and cost-effective imaging technique. The results of the ADAM\nchallenge - the most comprehensive AMD detection from RGB fundus images\nresearch competition and open dataset to date - serve as a benchmark for our\nevaluation. Taking the U-Net connectivity as a base of our framework, we\nevaluate and compare several approaches to improve the segmentation model's\narchitecture and training pipeline, including pre-processing techniques,\nencoder (backbone) deep network types of varying complexity, and specialized\nloss functions to mitigate class imbalances on image and pixel levels. The main\noutcome of this research is the final configuration of the AMD detection\nframework, which outperforms all the prior ADAM challenge submissions on the\nmulti-class segmentation of different AMD lesion types in non-invasive RGB\nfundus images. The source code used to conduct the experiments presented in\nthis paper is made freely available.\n","authors":["Valentyna Starodub","Mantas Lukoševičius"],"pdf_url":"https://arxiv.org/pdf/2510.26778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26769v1","updated":"2025-10-30T17:52:39Z","published":"2025-10-30T17:52:39Z","title":"SteerVLM: Robust Model Control through Lightweight Activation Steering\n  for Vision Language Models","summary":"  This work introduces SteerVLM, a lightweight steering module designed to\nguide Vision-Language Models (VLMs) towards outputs that better adhere to\ndesired instructions. Our approach learns from the latent embeddings of paired\nprompts encoding target and converse behaviors to dynamically adjust\nactivations connecting the language modality with image context. This allows\nfor fine-grained, inference-time control over complex output semantics without\nmodifying model weights while preserving performance on off-target tasks. Our\nsteering module requires learning parameters equal to 0.14% of the original\nVLM's size. Our steering module gains model control through dimension-wise\nactivation modulation and adaptive steering across layers without requiring\npre-extracted static vectors or manual tuning of intervention points.\nFurthermore, we introduce VNIA (Visual Narrative Intent Alignment), a\nmultimodal dataset specifically created to facilitate the development and\nevaluation of VLM steering techniques. Our method outperforms existing\nintervention techniques on steering and hallucination mitigation benchmarks for\nVLMs and proposes a robust solution for multimodal model control through\nactivation engineering.\n","authors":["Anushka Sivakumar","Andrew Zhang","Zaber Hakim","Chris Thomas"],"pdf_url":"https://arxiv.org/pdf/2510.26769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26759v1","updated":"2025-10-30T17:49:49Z","published":"2025-10-30T17:49:49Z","title":"MORE: Multi-Organ Medical Image REconstruction Dataset","summary":"  CT reconstruction provides radiologists with images for diagnosis and\ntreatment, yet current deep learning methods are typically limited to specific\nanatomies and datasets, hindering generalization ability to unseen anatomies\nand lesions. To address this, we introduce the Multi-Organ medical image\nREconstruction (MORE) dataset, comprising CT scans across 9 diverse anatomies\nwith 15 lesion types. This dataset serves two key purposes: (1) enabling robust\ntraining of deep learning models on extensive, heterogeneous data, and (2)\nfacilitating rigorous evaluation of model generalization for CT reconstruction.\nWe further establish a strong baseline solution that outperforms prior\napproaches under these challenging conditions. Our results demonstrate that:\n(1) a comprehensive dataset helps improve the generalization capability of\nmodels, and (2) optimization-based methods offer enhanced robustness for unseen\nanatomies. The MORE dataset is freely accessible under CC-BY-NC 4.0 at our\nproject page https://more-med.github.io/\n","authors":["Shaokai Wu","Yapan Guo","Yanbiao Ji","Jing Tong","Yuxiang Lu","Mei Li","Suizhi Huang","Yue Ding","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2510.26759v1.pdf","comment":"Accepted to ACMMM 2025"},{"id":"http://arxiv.org/abs/2508.05417v2","updated":"2025-10-30T17:46:35Z","published":"2025-08-07T14:09:33Z","title":"Smoothing Slot Attention Iterations and Recurrences","summary":"  Slot Attention (SA) and its variants lie at the heart of mainstream\nObject-Centric Learning (OCL). Objects in an image can be aggregated into\nrespective slot vectors, by \\textit{iteratively} refining cold-start query\nvectors, typically three times, via SA on image features. For video, such\naggregation is \\textit{recurrently} shared across frames, with queries\ncold-started on the first frame while transitioned from the previous frame's\nslots on non-first frames. However, the cold-start queries lack sample-specific\ncues thus hinder precise aggregation on the image or video's first frame; Also,\nnon-first frames' queries are already sample-specific thus require transforms\ndifferent from the first frame's aggregation. We address these issues for the\nfirst time with our \\textit{SmoothSA}: (1) To smooth SA iterations on the image\nor video's first frame, we \\textit{preheat} the cold-start queries with rich\ninformation of input features, via a tiny module self-distilled inside OCL; (2)\nTo smooth SA recurrences across all video frames, we \\textit{differentiate} the\nhomogeneous transforms on the first and non-first frames, by using full and\nsingle iterations respectively. Comprehensive experiments on object discovery,\nrecognition and downstream benchmarks validate our method's effectiveness.\nFurther analyses intuitively illuminate how our method smooths SA iterations\nand recurrences. Our source code, model checkpoints and training logs are\navailable on https://github.com/Genera1Z/SmoothSA.\n","authors":["Rongzhen Zhao","Wenyan Yang","Juho Kannala","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2508.05417v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01345v3","updated":"2025-10-30T17:43:18Z","published":"2025-08-02T12:48:04Z","title":"Predicting Video Slot Attention Queries from Random Slot-Feature Pairs","summary":"  Unsupervised video Object-Centric Learning (OCL) is promising as it enables\nobject-level scene representation and dynamics modeling as we humans do.\nMainstream video OCL methods adopt a recurrent architecture: An aggregator\naggregates current video frame into object features, termed slots, under some\nqueries; A transitioner transits current slots to queries for the next frame.\nThis is an effective architecture but all existing implementations both\n(\\textit{i1}) neglect to incorporate next frame features, the most informative\nsource for query prediction, and (\\textit{i2}) fail to learn transition\ndynamics, the knowledge essential for query prediction. To address these\nissues, we propose Random Slot-Feature pair for learning Query prediction\n(RandSF.Q): (\\textit{t1}) We design a new transitioner to incorporate both\nslots and features, which provides more information for query prediction;\n(\\textit{t2}) We train the transitioner to predict queries from slot-feature\npairs randomly sampled from available recurrences, which drives it to learn\ntransition dynamics. Experiments on scene representation demonstrate that our\nmethod surpass existing video OCL methods significantly, e.g., up to 10 points\non object discovery, setting new state-of-the-art. Such superiority also\nbenefits downstream tasks like dynamics modeling. Our core source code, model\ncheckpoints and training logs are available on\nhttps://github.com/Genera1Z/RandSF.Q.\n","authors":["Rongzhen Zhao","Jian Li","Juho Kannala","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2508.01345v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09672v2","updated":"2025-10-30T17:40:53Z","published":"2025-09-11T17:59:08Z","title":"Locality in Image Diffusion Models Emerges from Data Statistics","summary":"  Recent work has shown that the generalization ability of image diffusion\nmodels arises from the locality properties of the trained neural network. In\nparticular, when denoising a particular pixel, the model relies on a limited\nneighborhood of the input image around that pixel, which, according to the\nprevious work, is tightly related to the ability of these models to produce\nnovel images. Since locality is central to generalization, it is crucial to\nunderstand why diffusion models learn local behavior in the first place, as\nwell as the factors that govern the properties of locality patterns. In this\nwork, we present evidence that the locality in deep diffusion models emerges as\na statistical property of the image dataset and is not due to the inductive\nbias of convolutional neural networks, as suggested in previous work.\nSpecifically, we demonstrate that an optimal parametric linear denoiser\nexhibits similar locality properties to deep neural denoisers. We show, both\ntheoretically and experimentally, that this locality arises directly from pixel\ncorrelations present in the image datasets. Moreover, locality patterns are\ndrastically different on specialized datasets, approximating principal\ncomponents of the data's covariance. We use these insights to craft an\nanalytical denoiser that better matches scores predicted by a deep diffusion\nmodel than prior expert-crafted alternatives. Our key takeaway is that while\nneural network architectures influence generation quality, their primary role\nis to capture locality patterns inherent in the data.\n","authors":["Artem Lukoianov","Chenyang Yuan","Justin Solomon","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2509.09672v2.pdf","comment":"31 pages, 20 figures, 7 tables"},{"id":"http://arxiv.org/abs/2507.06078v2","updated":"2025-10-30T17:35:26Z","published":"2025-07-08T15:17:24Z","title":"ScoreAdv: Score-based Targeted Generation of Natural Adversarial\n  Examples via Diffusion Models","summary":"  Despite the success of deep learning across various domains, it remains\nvulnerable to adversarial attacks. Although many existing adversarial attack\nmethods achieve high success rates, they typically rely on $\\ell_{p}$-norm\nperturbation constraints, which do not align with human perceptual\ncapabilities. Consequently, researchers have shifted their focus toward\ngenerating natural, unrestricted adversarial examples (UAEs). GAN-based\napproaches suffer from inherent limitations, such as poor image quality due to\ninstability and mode collapse. Meanwhile, diffusion models have been employed\nfor UAE generation, but they still rely on iterative PGD perturbation\ninjection, without fully leveraging their central denoising capabilities. In\nthis paper, we introduce a novel approach for generating UAEs based on\ndiffusion models, named ScoreAdv. This method incorporates an interpretable\nadversarial guidance mechanism to gradually shift the sampling distribution\ntowards the adversarial distribution, while using an interpretable saliency map\nto inject the visual information of a reference image into the generated\nsamples. Notably, our method is capable of generating an unlimited number of\nnatural adversarial examples and can attack not only classification models but\nalso retrieval models. We conduct extensive experiments on ImageNet and CelebA\ndatasets, validating the performance of ScoreAdv across ten target models in\nboth black-box and white-box settings. Our results demonstrate that ScoreAdv\nachieves state-of-the-art attack success rates and image quality, while\nmaintaining inference efficiency. Furthermore, the dynamic balance between\ndenoising and adversarial perturbation enables ScoreAdv to remain robust even\nunder defensive measures.\n","authors":["Chihan Huang","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2507.06078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26703v1","updated":"2025-10-30T17:07:04Z","published":"2025-10-30T17:07:04Z","title":"ProstNFound+: A Prospective Study using Medical Foundation Models for\n  Prostate Cancer Detection","summary":"  Purpose: Medical foundation models (FMs) offer a path to build\nhigh-performance diagnostic systems. However, their application to prostate\ncancer (PCa) detection from micro-ultrasound ({\\mu}US) remains untested in\nclinical settings. We present ProstNFound+, an adaptation of FMs for PCa\ndetection from {\\mu}US, along with its first prospective validation. Methods:\nProstNFound+ incorporates a medical FM, adapter tuning, and a custom prompt\nencoder that embeds PCa-specific clinical biomarkers. The model generates a\ncancer heatmap and a risk score for clinically significant PCa. Following\ntraining on multi-center retrospective data, the model is prospectively\nevaluated on data acquired five years later from a new clinical site. Model\npredictions are benchmarked against standard clinical scoring protocols\n(PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to the\nprospective data, with no performance degradation compared to retrospective\nevaluation. It aligns closely with clinical scores and produces interpretable\nheatmaps consistent with biopsy-confirmed lesions. Conclusion: The results\nhighlight its potential for clinical deployment, offering a scalable and\ninterpretable alternative to expert-driven protocols.\n","authors":["Paul F. R. Wilson","Mohamed Harmanani","Minh Nguyen Nhat To","Amoon Jamzad","Tarek Elghareb","Zhuoxin Guo","Adam Kinnaird","Brian Wodlinger","Purang Abolmaesumi","Parvin Mousavi"],"pdf_url":"https://arxiv.org/pdf/2510.26703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26694v1","updated":"2025-10-30T17:01:18Z","published":"2025-10-30T17:01:18Z","title":"The Impact and Outlook of 3D Gaussian Splatting","summary":"  Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed\nthe landscape of 3D scene representations, inspiring an extensive body of\nassociated research. Follow-up work includes analyses and contributions that\nenhance the efficiency, scalability, and real-world applicability of 3DGS. In\nthis summary, we present an overview of several key directions that have\nemerged in the wake of 3DGS. We highlight advances enabling resource-efficient\ntraining and rendering, the evolution toward dynamic (or four-dimensional,\n4DGS) representations, and deeper exploration of the mathematical foundations\nunderlying its appearance modeling and rendering process. Furthermore, we\nexamine efforts to bring 3DGS to mobile and virtual reality platforms, its\nextension to massive-scale environments, and recent progress toward\nnear-instant radiance field reconstruction via feed-forward or distributed\ncomputation. Collectively, these developments illustrate how 3DGS has evolved\nfrom a breakthrough representation into a versatile and foundational tool for\n3D vision and graphics.\n","authors":["Bernhard Kerbl"],"pdf_url":"https://arxiv.org/pdf/2510.26694v1.pdf","comment":"Article written for Frontiers of Science Award, International\n  Congress on Basic Science, 2025"},{"id":"http://arxiv.org/abs/2510.26684v1","updated":"2025-10-30T16:54:16Z","published":"2025-10-30T16:54:16Z","title":"Process Integrated Computer Vision for Real-Time Failure Prediction in\n  Steel Rolling Mill","summary":"  We present a long-term deployment study of a machine vision-based anomaly\ndetection system for failure prediction in a steel rolling mill. The system\nintegrates industrial cameras to monitor equipment operation, alignment, and\nhot bar motion in real time along the process line. Live video streams are\nprocessed on a centralized video server using deep learning models, enabling\nearly prediction of equipment failures and process interruptions, thereby\nreducing unplanned breakdown costs. Server-based inference minimizes the\ncomputational load on industrial process control systems (PLCs), supporting\nscalable deployment across production lines with minimal additional resources.\nBy jointly analyzing sensor data from data acquisition systems and visual\ninputs, the system identifies the location and probable root causes of\nfailures, providing actionable insights for proactive maintenance. This\nintegrated approach enhances operational reliability, productivity, and\nprofitability in industrial manufacturing environments.\n","authors":["Vaibhav Kurrey","Sivakalyan Pujari","Gagan Raj Gupta"],"pdf_url":"https://arxiv.org/pdf/2510.26684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26681v1","updated":"2025-10-30T16:51:18Z","published":"2025-10-30T16:51:18Z","title":"Improving Classification of Occluded Objects through Scene Context","summary":"  The presence of occlusions has provided substantial challenges to\ntypically-powerful object recognition algorithms. Additional sources of\ninformation can be extremely valuable to reduce errors caused by occlusions.\nScene context is known to aid in object recognition in biological vision. In\nthis work, we attempt to add robustness into existing Region Proposal\nNetwork-Deep Convolutional Neural Network (RPN-DCNN) object detection networks\nthrough two distinct scene-based information fusion techniques. We present one\nalgorithm under each methodology: the first operates prior to prediction,\nselecting a custom object network to use based on the identified background\nscene, and the second operates after detection, fusing scene knowledge into\ninitial object scores output by the RPN. We demonstrate our algorithms on\nchallenging datasets featuring partial occlusions, which show overall\nimprovement in both recall and precision against baseline methods. In addition,\nour experiments contrast multiple training methodologies for occlusion\nhandling, finding that training on a combination of both occluded and\nunoccluded images demonstrates an improvement over the others. Our method is\ninterpretable and can easily be adapted to other datasets, offering many future\ndirections for research and practical applications.\n","authors":["Courtney M. King","Daniel D. Leeds","Damian Lyons","George Kalaitzis"],"pdf_url":"https://arxiv.org/pdf/2510.26681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17434v5","updated":"2025-10-30T16:47:32Z","published":"2023-11-29T08:26:18Z","title":"GSE: Group-wise Sparse and Explainable Adversarial Attacks","summary":"  Sparse adversarial attacks fool deep neural networks (DNNs) through minimal\npixel perturbations, often regularized by the $\\ell_0$ norm. Recent efforts\nhave replaced this norm with a structural sparsity regularizer, such as the\nnuclear group norm, to craft group-wise sparse adversarial attacks. The\nresulting perturbations are thus explainable and hold significant practical\nrelevance, shedding light on an even greater vulnerability of DNNs. However,\ncrafting such attacks poses an optimization challenge, as it involves computing\nnorms for groups of pixels within a non-convex objective. We address this by\npresenting a two-phase algorithm that generates group-wise sparse attacks\nwithin semantically meaningful areas of an image. Initially, we optimize a\nquasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored\nfor non-convex programming. Subsequently, the algorithm transitions to a\nprojected Nesterov's accelerated gradient descent with $2-$norm regularization\napplied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and\nImageNet datasets demonstrate a remarkable increase in group-wise sparsity,\ne.g., $50.9\\%$ on CIFAR-10 and $38.4\\%$ on ImageNet (average case, targeted\nattack). This performance improvement is accompanied by significantly faster\ncomputation times, improved explainability, and a $100\\%$ attack success rate.\n","authors":["Shpresim Sadiku","Moritz Wagner","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2311.17434v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15389v3","updated":"2025-10-30T16:42:33Z","published":"2024-12-19T20:43:22Z","title":"Resource Efficient Multi-stain Kidney Glomeruli Segmentation via\n  Self-supervision","summary":"  Semantic segmentation under domain shift remains a fundamental challenge in\ncomputer vision, particularly when labelled training data is scarce. This\nchallenge is particularly exemplified in histopathology image analysis, where\nthe same tissue structures must be segmented across images captured under\ndifferent imaging conditions (stains), each representing a distinct visual\ndomain. Traditional deep learning methods like UNet require extensive labels,\nwhich is both costly and time-consuming, particularly when dealing with\nmultiple domains (or stains). To mitigate this, various unsupervised domain\nadaptation based methods such as UDAGAN have been proposed, which reduce the\nneed for labels by requiring only one (source) stain to be labelled.\nNonetheless, obtaining source stain labels can still be challenging. This\narticle shows that through self-supervised pre-training -- including SimCLR,\nBYOL, and a novel approach, HR-CS-CO -- the performance of these segmentation\nmethods (UNet, and UDAGAN) can be retained even with 95% fewer labels. Notably,\nwith self-supervised pre-training and using only 5% labels, the performance\ndrops are minimal: 5.9% for UNet and 6.2% for UDAGAN, averaged over all stains,\ncompared to their respective fully supervised counterparts (without\npre-training, using 100% labels). Furthermore, these findings are shown to\ngeneralise beyond their training distribution to public benchmark datasets.\nImplementations and pre-trained models are publicly available\n\\href{https://github.com/zeeshannisar/resource-effecient-multi-stain-kidney-glomeruli-segmentation.git}{online}.\n","authors":["Zeeshan Nisar","Friedrich Feuerhake","Thomas Lampert"],"pdf_url":"https://arxiv.org/pdf/2412.15389v3.pdf","comment":"39 pages, 10 figures, 4 Tables"},{"id":"http://arxiv.org/abs/2506.19816v2","updated":"2025-10-30T16:38:19Z","published":"2025-06-24T17:30:27Z","title":"CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame\n  Vision-Language-Action Modeling","summary":"  Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong performance in robotic\nmanipulation. However, these models remain constrained by the single-frame\nimage paradigm and fail to fully leverage the temporal information offered by\nmulti-frame histories, as directly feeding multiple frames into VLM backbones\nincurs substantial computational overhead and inference latency. We propose\nCronusVLA, a unified framework that extends single-frame VLA models to the\nmulti-frame paradigm. CronusVLA follows a two-stage process: (1) Single-frame\npretraining on large-scale embodied datasets with autoregressive prediction of\naction tokens, establishing an effective embodied vision-language foundation;\n(2) Multi-frame post-training, which adapts the prediction of the\nvision-language backbone from discrete tokens to learnable features, and\naggregates historical information via feature chunking. CronusVLA effectively\naddresses the existing challenges of multi-frame modeling while enhancing\nperformance and observational robustness. To evaluate the robustness under\ntemporal and spatial disturbances, we introduce SimplerEnv-OR, a novel\nbenchmark featuring 24 types of observational disturbances and 120 severity\nlevels. Experiments across three embodiments in simulated and real-world\nenvironments demonstrate that CronusVLA achieves leading performance and\nsuperior robustness, with a 70.9% success rate on SimplerEnv, a 26.8%\nimprovement over OpenVLA on LIBERO, and the highest robustness score on\nSimplerEnv-OR. These results highlight the potential of efficient multi-frame\nadaptation in VLA models for more powerful and robust real-world deployment.\n","authors":["Hao Li","Shuai Yang","Yilun Chen","Xinyi Chen","Xiaoda Yang","Yang Tian","Hanqing Wang","Tai Wang","Dahua Lin","Feng Zhao","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2506.19816v2.pdf","comment":"39 pages, 24 figures"},{"id":"http://arxiv.org/abs/2510.26661v1","updated":"2025-10-30T16:29:09Z","published":"2025-10-30T16:29:09Z","title":"BRIQA: Balanced Reweighting in Image Quality Assessment of Pediatric\n  Brain MRI","summary":"  Assessing the severity of artifacts in pediatric brain Magnetic Resonance\nImaging (MRI) is critical for diagnostic accuracy, especially in low-field\nsystems where the signal-to-noise ratio is reduced. Manual quality assessment\nis time-consuming and subjective, motivating the need for robust automated\nsolutions. In this work, we propose BRIQA (Balanced Reweighting in Image\nQuality Assessment), which addresses class imbalance in artifact severity\nlevels. BRIQA uses gradient-based loss reweighting to dynamically adjust\nper-class contributions and employs a rotating batching scheme to ensure\nconsistent exposure to underrepresented classes. Through experiments, no single\narchitecture performs best across all artifact types, emphasizing the\nimportance of architectural diversity. The rotating batching configuration\nimproves performance across metrics by promoting balanced learning when\ncombined with cross-entropy loss. BRIQA improves average macro F1 score from\n0.659 to 0.706, with notable gains in Noise (0.430), Zipper (0.098),\nPositioning (0.097), Contrast (0.217), Motion (0.022), and Banding (0.012)\nartifact severity classification. The code is available at\nhttps://github.com/BioMedIA-MBZUAI/BRIQA.\n","authors":["Alya Almsouti","Ainur Khamitova","Darya Taratynova","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2510.26661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26653v1","updated":"2025-10-30T16:20:28Z","published":"2025-10-30T16:20:28Z","title":"Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning\n  Optical Flow on RADARSAT-2","summary":"  Accurate estimation of sea ice drift is critical for Arctic navigation,\nclimate research, and operational forecasting. While optical flow, a computer\nvision technique for estimating pixel wise motion between consecutive images,\nhas advanced rapidly in computer vision, its applicability to geophysical\nproblems and to satellite SAR imagery remains underexplored. Classical optical\nflow methods rely on mathematical models and strong assumptions about motion,\nwhich limit their accuracy in complex scenarios. Recent deep learning based\napproaches have substantially improved performance and are now the standard in\ncomputer vision, motivating their application to sea ice drift estimation. We\npresent the first large scale benchmark of 48 deep learning optical flow models\non RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and\nFl all metrics against GNSS tracked buoys. Several models achieve sub kilometer\naccuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the\nspatial scales of sea ice motion and typical navigation requirements in the\nArctic. Our results demonstrate that the models are capable of capturing\nconsistent regional drift patterns and that recent deep learning based optical\nflow methods, which have substantially improved motion estimation accuracy\ncompared to classical methods, can be effectively transferred to polar remote\nsensing. Optical flow produces spatially continuous drift fields, providing\nmotion estimates for every image pixel rather than at sparse buoy locations,\noffering new opportunities for navigation and climate modeling.\n","authors":["Daniela Martin","Joseph Gallego"],"pdf_url":"https://arxiv.org/pdf/2510.26653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26641v1","updated":"2025-10-30T16:08:25Z","published":"2025-10-30T16:08:25Z","title":"All You Need for Object Detection: From Pixels, Points, and Prompts to\n  Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles","summary":"  Autonomous Vehicles (AVs) are transforming the future of transportation\nthrough advances in intelligent perception, decision-making, and control\nsystems. However, their success is tied to one core capability, reliable object\ndetection in complex and multimodal environments. While recent breakthroughs in\nComputer Vision (CV) and Artificial Intelligence (AI) have driven remarkable\nprogress, the field still faces a critical challenge as knowledge remains\nfragmented across multimodal perception, contextual reasoning, and cooperative\nintelligence. This survey bridges that gap by delivering a forward-looking\nanalysis of object detection in AVs, emphasizing emerging paradigms such as\nVision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI\nrather than re-examining outdated techniques. We begin by systematically\nreviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,\nand Radar) and their fusion strategies, highlighting not only their\ncapabilities and limitations in dynamic driving environments but also their\npotential to integrate with recent advances in LLM/VLM-driven perception\nframeworks. Next, we introduce a structured categorization of AV datasets that\nmoves beyond simple collections, positioning ego-vehicle, infrastructure-based,\nand cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a\ncross-analysis of data structures and characteristics. Ultimately, we analyze\ncutting-edge detection methodologies, ranging from 2D and 3D pipelines to\nhybrid sensor fusion, with particular attention to emerging transformer-driven\napproaches powered by Vision Transformers (ViTs), Large and Small Language\nModels (SLMs), and VLMs. By synthesizing these perspectives, our survey\ndelivers a clear roadmap of current capabilities, open challenges, and future\nopportunities.\n","authors":["Sayed Pedram Haeri Boroujeni","Niloufar Mehrabi","Hazim Alzorgan","Ahmad Sarlak","Mahlagha Fazeli","Abolfazl Razi"],"pdf_url":"https://arxiv.org/pdf/2510.26641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26635v1","updated":"2025-10-30T16:04:00Z","published":"2025-10-30T16:04:00Z","title":"SAMRI: Segment Anything Model for MRI","summary":"  Accurate magnetic resonance imaging (MRI) segmentation is crucial for\nclinical decision-making, but remains labor-intensive when performed manually.\nConvolutional neural network (CNN)-based methods can be accurate and efficient,\nbut often generalize poorly to MRI's variable contrast, intensity\ninhomogeneity, and protocols. Although the transformer-based Segment Anything\nModel (SAM) has demonstrated remarkable generalizability in natural images,\nexisting adaptations often treat MRI as another imaging modality, overlooking\nthese modality-specific challenges. We present SAMRI, an MRI-specialized SAM\ntrained and validated on 1.1 million labeled MR slices spanning whole-body\norgans and pathologies. We demonstrate that SAM can be effectively adapted to\nMRI by simply fine-tuning its mask decoder using a two-stage strategy, reducing\ntraining time by 94% and trainable parameters by 96% versus full-model\nretraining. Across diverse MRI segmentation tasks, SAMRI achieves a mean Dice\nof 0.87, delivering state-of-the-art accuracy across anatomical regions and\nrobust generalization on unseen structures, particularly small and clinically\nimportant structures.\n","authors":["Zhao Wang","Wei Dai","Thuy Thanh Dao","Steffen Bollmann","Hongfu Sun","Craig Engstrom","Shekhar S. Chandra"],"pdf_url":"https://arxiv.org/pdf/2510.26635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.16556v2","updated":"2025-10-30T16:01:55Z","published":"2025-10-18T16:00:10Z","title":"Fit for Purpose? Deepfake Detection in the Real World","summary":"  The rapid proliferation of AI-generated content, driven by advances in\ngenerative adversarial networks, diffusion models, and multimodal large\nlanguage models, has made the creation and dissemination of synthetic media\neffortless, heightening the risks of misinformation, particularly political\ndeepfakes that distort truth and undermine trust in political institutions. In\nturn, governments, research institutions, and industry have strongly promoted\ndeepfake detection initiatives as solutions. Yet, most existing models are\ntrained and validated on synthetic, laboratory-controlled datasets, limiting\ntheir generalizability to the kinds of real-world political deepfakes\ncirculating on social platforms that affect the public. In this work, we\nintroduce the first systematic benchmark based on the Political Deepfakes\nIncident Database, a curated collection of real-world political deepfakes\nshared on social media since 2018. Our study includes a systematic evaluation\nof state-of-the-art deepfake detectors across academia, government, and\nindustry. We find that the detectors from academia and government perform\nrelatively poorly. While paid detection tools achieve relatively higher\nperformance than free-access models, all evaluated detectors struggle to\ngeneralize effectively to authentic political deepfakes, and are vulnerable to\nsimple manipulations, especially in the video domain. Results urge the need for\npolitically contextualized deepfake detection frameworks to better safeguard\nthe public in real-world settings.\n","authors":["Guangyu Lin","Li Lin","Christina P. Walker","Daniel S. Schiff","Shu Hu"],"pdf_url":"https://arxiv.org/pdf/2510.16556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26630v1","updated":"2025-10-30T15:57:20Z","published":"2025-10-30T15:57:20Z","title":"PT-DETR: Small Target Detection Based on Partially-Aware Detail Focus","summary":"  To address the challenges in UAV object detection, such as complex\nbackgrounds, severe occlusion, dense small objects, and varying lighting\nconditions,this paper proposes PT-DETR based on RT-DETR, a novel detection\nalgorithm specifically designed for small objects in UAV imagery. In the\nbackbone network, we introduce the Partially-Aware Detail Focus (PADF) Module\nto enhance feature extraction for small objects. Additionally,we design the\nMedian-Frequency Feature Fusion (MFFF) module,which effectively improves the\nmodel's ability to capture small-object details and contextual information.\nFurthermore,we incorporate Focaler-SIoU to strengthen the model's bounding box\nmatching capability and increase its sensitivity to small-object features,\nthereby further enhancing detection accuracy and robustness. Compared with\nRT-DETR, our PT-DETR achieves mAP improvements of 1.6% and 1.7% on the\nVisDrone2019 dataset with lower computational complexity and fewer parameters,\ndemonstrating its robustness and feasibility for small-object detection tasks.\n","authors":["Bingcong Huo","Zhiming Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.23292v2","updated":"2025-10-30T15:53:26Z","published":"2025-06-29T15:29:03Z","title":"DDL: A Large-Scale Datasets for Deepfake Detection and Localization in\n  Diversified Real-World Scenarios","summary":"  Recent advances in AIGC have exacerbated the misuse of malicious deepfake\ncontent, making the development of reliable deepfake detection methods an\nessential means to address this challenge. Although existing deepfake detection\nmodels demonstrate outstanding performance in detection metrics, most methods\nonly provide simple binary classification results, lacking interpretability.\nRecent studies have attempted to enhance the interpretability of classification\nresults by providing spatial manipulation masks or temporal forgery segments.\nHowever, due to the limitations of forgery datasets, the practical\neffectiveness of these methods remains suboptimal. The primary reason lies in\nthe fact that most existing deepfake datasets contain only binary labels, with\nlimited variety in forgery scenarios, insufficient diversity in deepfake types,\nand relatively small data scales, making them inadequate for complex real-world\nscenarios.To address this predicament, we construct a novel large-scale\ndeepfake detection and localization (\\textbf{DDL}) dataset containing over\n$\\textbf{1.4M+}$ forged samples and encompassing up to $\\textbf{80}$ distinct\ndeepfake methods. The DDL design incorporates four key innovations: (1)\n\\textbf{Comprehensive Deepfake Methods} (covering 7 different generation\narchitectures and a total of 80 methods), (2) \\textbf{Varied Manipulation\nModes} (incorporating 7 classic and 3 novel forgery modes), (3) \\textbf{Diverse\nForgery Scenarios and Modalities} (including 3 scenarios and 3 modalities), and\n(4) \\textbf{Fine-grained Forgery Annotations} (providing 1.18M+ precise spatial\nmasks and 0.23M+ precise temporal segments).Through these improvements, our DDL\nnot only provides a more challenging benchmark for complex real-world forgeries\nbut also offers crucial support for building next-generation deepfake\ndetection, localization, and interpretability methods.\n","authors":["Changtao Miao","Yi Zhang","Weize Gao","Zhiya Tan","Weiwei Feng","Man Luo","Jianshu Li","Ajian Liu","Yunfeng Diao","Qi Chu","Tao Gong","Zhe Li","Weibin Yao","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.23292v2.pdf","comment":"This paper is a preliminary version, with an extended and\n  comprehensive version currently under development"},{"id":"http://arxiv.org/abs/2508.10566v2","updated":"2025-10-30T15:42:29Z","published":"2025-08-14T12:01:52Z","title":"HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head\n  Synthesis","summary":"  Audio-driven talking head video generation enhances user engagement in\nhuman-computer interaction. However, current methods frequently produce videos\nwith motion blur and lip jitter, primarily due to their reliance on implicit\nmodeling of audio-facial motion correlations--an approach lacking explicit\narticulatory priors (i.e., anatomical guidance for speech-related facial\nmovements). To overcome this limitation, we propose HM-Talker, a novel\nframework for generating high-fidelity, temporally coherent talking heads.\nHM-Talker leverages a hybrid motion representation combining both implicit and\nexplicit motion cues. Explicit cues use Action Units (AUs), anatomically\ndefined facial muscle movements, alongside implicit features to minimize\nphoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement\nModule (CMDM) extracts complementary implicit/explicit motion features while\npredicting AUs directly from audio input aligned to visual cues. To mitigate\nidentity-dependent biases in explicit features and enhance cross-subject\ngeneralization, we introduce the Hybrid Motion Modeling Module (HMMM). This\nmodule dynamically merges randomly paired implicit/explicit features, enforcing\nidentity-agnostic learning. Together, these components enable robust lip\nsynchronization across diverse identities, advancing personalized talking head\nsynthesis. Extensive experiments demonstrate HM-Talker's superiority over\nstate-of-the-art methods in visual quality and lip-sync accuracy.\n","authors":["Shiyu Liu","Kui Jiang","Xianming Liu","Hongxun Yao","Xiaocheng Feng"],"pdf_url":"https://arxiv.org/pdf/2508.10566v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26614v1","updated":"2025-10-30T15:40:34Z","published":"2025-10-30T15:40:34Z","title":"Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event\n  Cameras","summary":"  We propose tokenization of events and present a tokenizer, Spiking Patches,\nspecifically designed for event cameras. Given a stream of asynchronous and\nspatially sparse events, our goal is to discover an event representation that\npreserves these properties. Prior works have represented events as frames or as\nvoxels. However, while these representations yield high accuracy, both frames\nand voxels are synchronous and decrease the spatial sparsity. Spiking Patches\ngives the means to preserve the unique properties of event cameras and we show\nin our experiments that this comes without sacrificing accuracy. We evaluate\nour tokenizer using a GNN, PCN, and a Transformer on gesture recognition and\nobject detection. Tokens from Spiking Patches yield inference times that are up\nto 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We\nachieve this while matching their accuracy and even surpassing in some cases\nwith absolute improvements up to 3.8 for gesture recognition and up to 1.4 for\nobject detection. Thus, tokenization constitutes a novel direction in\nevent-based vision and marks a step towards methods that preserve the\nproperties of event cameras.\n","authors":["Christoffer Koo Øhrstrøm","Ronja Güldenring","Lazaros Nalpantidis"],"pdf_url":"https://arxiv.org/pdf/2510.26614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.14904v2","updated":"2025-10-30T15:39:25Z","published":"2025-10-16T17:20:22Z","title":"MaskCaptioner: Learning to Jointly Segment and Caption Object\n  Trajectories in Videos","summary":"  Dense Video Object Captioning (DVOC) is the task of jointly detecting,\ntracking, and captioning object trajectories in a video, requiring the ability\nto understand spatio-temporal details and describe them in natural language.\nDue to the complexity of the task and the high cost associated with manual\nannotation, previous approaches resort to disjoint training strategies,\npotentially leading to suboptimal performance. To circumvent this issue, we\npropose to generate captions about spatio-temporally localized entities\nleveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets\nwith our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an\nend-to-end model capable of jointly detecting, segmenting, tracking and\ncaptioning object trajectories. Moreover, with pretraining on LVISCap and\nLV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three\nexisting benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are\navailable at https://www.gabriel.fiastre.fr/maskcaptioner/.\n","authors":["Gabriel Fiastre","Antoine Yang","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2510.14904v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.26609v1","updated":"2025-10-30T15:37:40Z","published":"2025-10-30T15:37:40Z","title":"CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for\n  Satellite Sensing","summary":"  Accurate and timely crop yield prediction is crucial for global food security\nand modern agricultural management. Traditional methods often lack the\nscalability and granularity required for precision farming. This paper\nintroduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder\nfor Satellite Sensing), a deep learning model designed for high-resolution,\nintra-field canola yield prediction. CYPRESS leverages a pre-trained,\nlarge-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for\na continuous regression task, transforming multi-temporal satellite imagery\ninto dense, pixel-level yield maps. Evaluated on a comprehensive dataset from\nthe Canadian Prairies, CYPRESS demonstrates superior performance over existing\ndeep learning-based yield prediction models, highlighting the effectiveness of\nfine-tuning foundation models for specialized agricultural applications. By\nproviding a continuous, high-resolution output, CYPRESS offers a more\nactionable tool for precision agriculture than conventional classification or\ncounty-level aggregation methods. This work validates a novel approach that\nbridges the gap between large-scale Earth observation and on-farm\ndecision-making, offering a scalable solution for detailed agricultural\nmonitoring.\n","authors":["Shayan Nejadshamsi","Yuanyuan Zhang","Shadi Zaki","Brock Porth","Lysa Porth","Vahab Khoshdel"],"pdf_url":"https://arxiv.org/pdf/2510.26609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26601v1","updated":"2025-10-30T15:29:20Z","published":"2025-10-30T15:29:20Z","title":"ResMatching: Noise-Resilient Computational Super-Resolution via Guided\n  Conditional Flow Matching","summary":"  Computational Super-Resolution (CSR) in fluorescence microscopy has, despite\nbeing an ill-posed problem, a long history. At its very core, CSR is about\nfinding a prior that can be used to extrapolate frequencies in a micrograph\nthat have never been imaged by the image-generating microscope. It stands to\nreason that, with the advent of better data-driven machine learning techniques,\nstronger prior can be learned and hence CSR can lead to better results. Here,\nwe present ResMatching, a novel CSR method that uses guided conditional flow\nmatching to learn such improved data-priors. We evaluate ResMatching on 4\ndiverse biological structures from the BioSR dataset and compare its results\nagainst 7 baselines. ResMatching consistently achieves competitive results,\ndemonstrating in all cases the best trade-off between data fidelity and\nperceptual realism. We observe that CSR using ResMatching is particularly\neffective in cases where a strong prior is hard to learn, e.g. when the given\nlow-resolution images contain a lot of noise. Additionally, we show that\nResMatching can be used to sample from an implicitly learned posterior\ndistribution and that this distribution is calibrated for all tested use-cases,\nenabling our method to deliver a pixel-wise data-uncertainty term that can\nguide future users to reject uncertain predictions.\n","authors":["Anirban Ray","Vera Galinova","Florian Jug"],"pdf_url":"https://arxiv.org/pdf/2510.26601v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.26583v1","updated":"2025-10-30T15:11:16Z","published":"2025-10-30T15:11:16Z","title":"Emu3.5: Native Multimodal Models are World Learners","summary":"  We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.\n","authors":["Yufeng Cui","Honghao Chen","Haoge Deng","Xu Huang","Xinghang Li","Jirong Liu","Yang Liu","Zhuoyan Luo","Jinsheng Wang","Wenxuan Wang","Yueze Wang","Chengyuan Wang","Fan Zhang","Yingli Zhao","Ting Pan","Xianduo Li","Zecheng Hao","Wenxuan Ma","Zhuo Chen","Yulong Ao","Tiejun Huang","Zhongyuan Wang","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26583v1.pdf","comment":"project page: https://emu.world"},{"id":"http://arxiv.org/abs/2510.26582v1","updated":"2025-10-30T15:10:02Z","published":"2025-10-30T15:10:02Z","title":"CATCH: A Modular Cross-domain Adaptive Template with Hook","summary":"  Recent advances in Visual Question Answering (VQA) have demonstrated\nimpressive performance in natural image domains, with models like LLaVA\nleveraging large language models (LLMs) for open-ended reasoning. However,\ntheir generalization degrades significantly when transferred to out-of-domain\nscenarios such as remote sensing, medical imaging, or math diagrams, due to\nlarge distributional shifts and the lack of effective domain adaptation\nmechanisms. Existing approaches typically rely on per-domain fine-tuning or\nbespoke pipelines, which are costly, inflexible, and not scalable across\ndiverse tasks. In this paper, we propose CATCH, a plug-and-play framework for\ncross-domain adaptation that improves the generalization of VQA models while\nrequiring minimal changes to their core architecture. Our key idea is to\ndecouple visual and linguistic adaptation by introducing two lightweight\nmodules: a domain classifier to identify the input image type, and a dual\nadapter mechanism comprising a Prompt Adapter for language modulation and a\nVisual Adapter for vision feature adjustment. Both modules are dynamically\ninjected via a unified hook interface, requiring no retraining of the backbone\nmodel. Experimental results across four domain-specific VQA benchmarks\ndemonstrate that our framework achieves consistent performance gains without\nretraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on\nMedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH\nprovides a scalable and extensible approach to multi-domain VQA, enabling\npractical deployment across diverse application domains.\n","authors":["Xinjin Li","Yulie Lu","Jinghan Cao","Yu Ma","Zhenglin Li","Yeyang Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26580v1","updated":"2025-10-30T15:07:55Z","published":"2025-10-30T15:07:55Z","title":"Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in\n  Zero-Shot Real-World Scenarios","summary":"  In real-world environments, AI systems often face unfamiliar scenarios\nwithout labeled data, creating a major challenge for conventional scene\nunderstanding models. The inability to generalize across unseen contexts limits\nthe deployment of vision-based applications in dynamic, unstructured settings.\nThis work introduces a Dynamic Context-Aware Scene Reasoning framework that\nleverages Vision-Language Alignment to address zero-shot real-world scenarios.\nThe goal is to enable intelligent systems to infer and adapt to new\nenvironments without prior task-specific training. The proposed approach\nintegrates pre-trained vision transformers and large language models to align\nvisual semantics with natural language descriptions, enhancing contextual\ncomprehension. A dynamic reasoning module refines predictions by combining\nglobal scene cues and object-level interactions guided by linguistic priors.\nExtensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and\nOpen Images demonstrate up to 18% improvement in scene understanding accuracy\nover baseline models in complex and unseen environments. Results also show\nrobust performance in ambiguous or cluttered scenes due to the synergistic\nfusion of vision and language. This framework offers a scalable and\ninterpretable approach for context-aware reasoning, advancing zero-shot\ngeneralization in dynamic real-world settings.\n","authors":["Manjunath Prasad Holenarasipura Rajiv","B. M. Vidyavathi"],"pdf_url":"https://arxiv.org/pdf/2510.26580v1.pdf","comment":"Preprint under review at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI), 2025"},{"id":"http://arxiv.org/abs/2510.26573v1","updated":"2025-10-30T15:00:50Z","published":"2025-10-30T15:00:50Z","title":"Comparative Analysis of Deep Learning Models for Olive Tree Crown and\n  Shadow Segmentation Towards Biovolume Estimation","summary":"  Olive tree biovolume estimation is a key task in precision agriculture,\nsupporting yield prediction and resource management, especially in\nMediterranean regions severely impacted by climate-induced stress. This study\npresents a comparative analysis of three deep learning models U-Net,\nYOLOv11m-seg, and Mask RCNN for segmenting olive tree crowns and their shadows\nin ultra-high resolution UAV imagery. The UAV dataset, acquired over\nVicopisano, Italy, includes manually annotated crown and shadow masks. Building\non these annotations, the methodology emphasizes spatial feature extraction and\nrobust segmentation; per-tree biovolume is then estimated by combining crown\nprojected area with shadow-derived height using solar geometry. In testing,\nMask R-CNN achieved the best overall accuracy (F1 = 0.86; mIoU = 0.72), while\nYOLOv11m-seg provided the fastest throughput (0.12 second per image). The\nestimated biovolumes spanned from approximately 4 to 24 cubic meters,\nreflecting clear structural differences among trees. These results indicate\nMask R-CNN is preferable when biovolume accuracy is paramount, whereas\nYOLOv11m-seg suits large-area deployments where speed is critical; U-Net\nremains a lightweight, high-sensitivity option. The framework enables accurate,\nscalable orchard monitoring and can be further strengthened with DEM or DSM\nintegration and field calibration for operational decision support.\n","authors":["Wondimagegn Abebe Demissie","Stefano Roccella","Rudy Rossetto","Antonio Minnocci","Andrea Vannini","Luca Sebastiani"],"pdf_url":"https://arxiv.org/pdf/2510.26573v1.pdf","comment":"6 pages, 2025 IEEE International Workshop on Metrology for\n  Agriculture and Forestry (MetroAgriFor)"},{"id":"http://arxiv.org/abs/2510.26569v1","updated":"2025-10-30T14:59:37Z","published":"2025-10-30T14:59:37Z","title":"AdSum: Two-stream Audio-visual Summarization for Automated Video\n  Advertisement Clipping","summary":"  Advertisers commonly need multiple versions of the same advertisement (ad) at\nvarying durations for a single campaign. The traditional approach involves\nmanually selecting and re-editing shots from longer video ads to create shorter\nversions, which is labor-intensive and time-consuming. In this paper, we\nintroduce a framework for automated video ad clipping using video summarization\ntechniques. We are the first to frame video clipping as a shot selection\nproblem, tailored specifically for advertising. Unlike existing general video\nsummarization methods that primarily focus on visual content, our approach\nemphasizes the critical role of audio in advertising. To achieve this, we\ndevelop a two-stream audio-visual fusion model that predicts the importance of\nvideo frames, where importance is defined as the likelihood of a frame being\nselected in the firm-produced short ad. To address the lack of ad-specific\ndatasets, we present AdSum204, a novel dataset comprising 102 pairs of\n30-second and 15-second ads from real advertising campaigns. Extensive\nexperiments demonstrate that our model outperforms state-of-the-art methods\nacross various metrics, including Average Precision, Area Under Curve,\nSpearman, and Kendall.\n","authors":["Wen Xie","Yanjun Zhu","Gijs Overgoor","Yakov Bart","Agata Lapedriza Garcia","Sarah Ostadabbas"],"pdf_url":"https://arxiv.org/pdf/2510.26569v1.pdf","comment":"Accepted at 32nd International Conference on MultiMedia Modeling"},{"id":"http://arxiv.org/abs/2510.26568v1","updated":"2025-10-30T14:58:16Z","published":"2025-10-30T14:58:16Z","title":"SA$^{2}$Net: Scale-Adaptive Structure-Affinity Transformation for Spine\n  Segmentation from Ultrasound Volume Projection Imaging","summary":"  Spine segmentation, based on ultrasound volume projection imaging (VPI),\nplays a vital role for intelligent scoliosis diagnosis in clinical\napplications. However, this task faces several significant challenges. Firstly,\nthe global contextual knowledge of spines may not be well-learned if we neglect\nthe high spatial correlation of different bone features. Secondly, the spine\nbones contain rich structural knowledge regarding their shapes and positions,\nwhich deserves to be encoded into the segmentation process. To address these\nchallenges, we propose a novel scale-adaptive structure-aware network\n(SA$^{2}$Net) for effective spine segmentation. First, we propose a\nscale-adaptive complementary strategy to learn the cross-dimensional\nlong-distance correlation features for spinal images. Second, motivated by the\nconsistency between multi-head self-attention in Transformers and semantic\nlevel affinity, we propose structure-affinity transformation to transform\nsemantic features with class-specific affinity and combine it with a\nTransformer decoder for structure-aware reasoning. In addition, we adopt a\nfeature mixing loss aggregation method to enhance model training. This method\nimproves the robustness and accuracy of the segmentation process. The\nexperimental results demonstrate that our SA$^{2}$Net achieves superior\nsegmentation performance compared to other state-of-the-art methods. Moreover,\nthe adaptability of SA$^{2}$Net to various backbones enhances its potential as\na promising tool for advanced scoliosis diagnosis using intelligent spinal\nimage analysis. The code and experimental demo are available at\nhttps://github.com/taetiseo09/SA2Net.\n","authors":["Hao Xie","Zixun Huang","Yushen Zuo","Yakun Ju","Frank H. F. Leung","N. F. Law","Kin-Man Lam","Yong-Ping Zheng","Sai Ho Ling"],"pdf_url":"https://arxiv.org/pdf/2510.26568v1.pdf","comment":"Accepted by Computerized Medical Imaging and Graphics (CMIG)"},{"id":"http://arxiv.org/abs/2510.08771v2","updated":"2025-10-30T14:46:21Z","published":"2025-10-09T19:41:51Z","title":"LinearSR: Unlocking Linear Attention for Stable and Efficient Image\n  Super-Resolution","summary":"  Generative models for Image Super-Resolution (SR) are increasingly powerful,\nyet their reliance on self-attention's quadratic complexity (O(N^2)) creates a\nmajor computational bottleneck. Linear Attention offers an O(N) solution, but\nits promise for photorealistic SR has remained largely untapped, historically\nhindered by a cascade of interrelated and previously unsolved challenges. This\npaper introduces LinearSR, a holistic framework that, for the first time,\nsystematically overcomes these critical hurdles. Specifically, we resolve a\nfundamental, training instability that causes catastrophic model divergence\nusing our novel \"knee point\"-based Early-Stopping Guided Fine-tuning (ESGF)\nstrategy. Furthermore, we mitigate the classic perception-distortion trade-off\nwith a dedicated SNR-based Mixture of Experts (MoE) architecture. Finally, we\nestablish an effective and lightweight guidance paradigm, TAG, derived from our\n\"precision-over-volume\" principle. Our resulting LinearSR model simultaneously\ndelivers state-of-the-art perceptual quality with exceptional efficiency. Its\ncore diffusion forward pass (1-NFE) achieves SOTA-level speed, while its\noverall multi-step inference time remains highly competitive. This work\nprovides the first robust methodology for applying Linear Attention in the\nphotorealistic SR domain, establishing a foundational paradigm for future\nresearch in efficient generative super-resolution.\n","authors":["Xiaohui Li","Shaobin Zhuang","Shuo Cao","Yang Yang","Yuandong Pu","Qi Qin","Siqi Luo","Bin Fu","Yihao Liu"],"pdf_url":"https://arxiv.org/pdf/2510.08771v2.pdf","comment":"19 pages, 9 figures, 6 tables"},{"id":"http://arxiv.org/abs/2501.05783v2","updated":"2025-10-30T14:04:15Z","published":"2025-01-10T08:33:31Z","title":"UV-Attack: Physical-World Adversarial Attacks for Person Detection via\n  Dynamic-NeRF-based UV Mapping","summary":"  In recent research, adversarial attacks on person detectors using patches or\nstatic 3D model-based texture modifications have struggled with low success\nrates due to the flexible nature of human movement. Modeling the 3D\ndeformations caused by various actions has been a major challenge. Fortunately,\nadvancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer\nnew possibilities. In this paper, we introduce UV-Attack, a groundbreaking\napproach that achieves high success rates even with extensive and unseen human\nactions. We address the challenge above by leveraging dynamic-NeRF-based UV\nmapping. UV-Attack can generate human images across diverse actions and\nviewpoints, and even create novel actions by sampling from the SMPL parameter\nspace. While dynamic NeRF models are capable of modeling human bodies,\nmodifying clothing textures is challenging because they are embedded in neural\nnetwork parameters. To tackle this, UV-Attack generates UV maps instead of RGB\nimages and modifies the texture stacks. This approach enables real-time texture\nedits and makes the attack more practical. We also propose a novel Expectation\nover Pose Transformation loss (EoPT) to improve the evasion success rate on\nunseen poses and views. Our experiments show that UV-Attack achieves a 92.7%\nattack success rate against the FastRCNN model across varied poses in dynamic\nvideo settings, significantly outperforming the state-of-the-art AdvCamou\nattack, which only had a 28.5% ASR. Moreover, we achieve 49.5% ASR on the\nlatest YOLOv8 detector in black-box settings. This work highlights the\npotential of dynamic NeRF-based UV mapping for creating more effective\nadversarial attacks on person detectors, addressing key challenges in modeling\nhuman movement and texture modification. The code is available at\nhttps://github.com/PolyLiYJ/UV-Attack.\n","authors":["Yanjie Li","Kaisheng Liang","Bin Xiao"],"pdf_url":"https://arxiv.org/pdf/2501.05783v2.pdf","comment":"23 pages, 22 figures, accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2510.26509v1","updated":"2025-10-30T14:03:09Z","published":"2025-10-30T14:03:09Z","title":"Analysis of the Robustness of an Edge Detector Based on Cellular\n  Automata Optimized by Particle Swarm","summary":"  The edge detection task is essential in image processing aiming to extract\nrelevant information from an image. One recurring problem in this task is the\nweaknesses found in some detectors, such as the difficulty in detecting loose\nedges and the lack of context to extract relevant information from specific\nproblems. To address these weaknesses and adapt the detector to the properties\nof an image, an adaptable detector described by two-dimensional cellular\nautomaton and optimized by meta-heuristic combined with transfer learning\ntechniques was developed. This study aims to analyze the impact of expanding\nthe search space of the optimization phase and the robustness of the\nadaptability of the detector in identifying edges of a set of natural images\nand specialized subsets extracted from the same image set. The results obtained\nprove that expanding the search space of the optimization phase was not\neffective for the chosen image set. The study also analyzed the adaptability of\nthe model through a series of experiments and validation techniques and found\nthat, regardless of the validation, the model was able to adapt to the input\nand the transfer learning techniques applied to the model showed no significant\nimprovements.\n","authors":["Vinícius Ferraria","Eurico Ruivo"],"pdf_url":"https://arxiv.org/pdf/2510.26509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21004v2","updated":"2025-10-30T13:43:39Z","published":"2024-10-28T13:28:21Z","title":"A Continuous and Interpretable Morphometric for Robust Quantification of\n  Dynamic Biological Shapes","summary":"  We introduce the Push-Forward Signed Distance Morphometric (PF-SDM) for shape\nquantification in biomedical imaging. The PF-SDM compactly encodes geometric\nand topological properties of closed shapes, including their skeleton and\nsymmetries. This provides robust and interpretable features for shape\ncomparison and machine learning. The PF-SDM is mathematically smooth, providing\naccess to gradients and differential-geometric quantities. It also extends to\ntemporal dynamics and allows fusing spatial intensity distributions, such as\ngenetic markers, with shape dynamics. We present the PF-SDM theory, benchmark\nit on synthetic data, and apply it to predicting body-axis formation in mouse\ngastruloids, outperforming a CNN baseline in both accuracy and speed.\n","authors":["Roua Rouatbi","Juan-Esteban Suarez Cardona","Alba Villaronga-Luque","Jesse V. Veenvliet","Ivo F. Sbalzarini"],"pdf_url":"https://arxiv.org/pdf/2410.21004v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08788v2","updated":"2025-10-30T13:41:48Z","published":"2024-01-30T09:05:38Z","title":"VerifIoU - Robustness of Object Detection to Perturbations","summary":"  We introduce a novel Interval Bound Propagation (IBP) approach for the formal\nverification of object detection models, specifically targeting the\nIntersection over Union (IoU) metric. The approach has been implemented in an\nopen source code, named IBP IoU, compatible with popular abstract\ninterpretation based verification tools. The resulting verifier is evaluated on\nlanding approach runway detection and handwritten digit recognition case\nstudies. Comparisons against a baseline (Vanilla IBP IoU) highlight the\nsuperior performance of IBP IoU in ensuring accuracy and stability,\ncontributing to more secure and robust machine learning applications.\n","authors":["Noémie Cohen","Mélanie Ducoffe","Ryma Boumazouza","Christophe Gabreau","Claire Pagetti","Xavier Pucel","Audrey Galametz"],"pdf_url":"https://arxiv.org/pdf/2403.08788v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.24325v2","updated":"2025-10-30T13:38:59Z","published":"2025-09-29T06:23:47Z","title":"ReCon-GS: Continuum-Preserved Gaussian Streaming for Fast and Compact\n  Reconstruction of Dynamic Scenes","summary":"  Online free-viewpoint video (FVV) reconstruction is challenged by slow\nper-frame optimization, inconsistent motion estimation, and unsustainable\nstorage demands. To address these challenges, we propose the Reconfigurable\nContinuum Gaussian Stream, dubbed ReCon-GS, a novel storage-aware framework\nthat enables high fidelity online dynamic scene reconstruction and real-time\nrendering. Specifically, we dynamically allocate multi-level Anchor Gaussians\nin a density-adaptive fashion to capture inter-frame geometric deformations,\nthereby decomposing scene motion into compact coarse-to-fine representations.\nThen, we design a dynamic hierarchy reconfiguration strategy that preserves\nlocalized motion expressiveness through on-demand anchor re-hierarchization,\nwhile ensuring temporal consistency through intra-hierarchical deformation\ninheritance that confines transformation priors to their respective hierarchy\nlevels. Furthermore, we introduce a storage-aware optimization mechanism that\nflexibly adjusts the density of Anchor Gaussians at different hierarchy levels,\nenabling a controllable trade-off between reconstruction fidelity and memory\nusage. Extensive experiments on three widely used datasets demonstrate that,\ncompared to state-of-the-art methods, ReCon-GS improves training efficiency by\napproximately 15% and achieves superior FVV synthesis quality with enhanced\nrobustness and stability. Moreover, at equivalent rendering quality, ReCon-GS\nslashes memory requirements by over 50% compared to leading state-of-the-art\nmethods.\n","authors":["Jiaye Fu","Qiankun Gao","Chengxiang Wen","Yanmin Wu","Siwei Ma","Jiaqi Zhang","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.24325v2.pdf","comment":"Published in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26474v1","updated":"2025-10-30T13:26:58Z","published":"2025-10-30T13:26:58Z","title":"Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing","summary":"  Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.\n","authors":["Xin Guo","Zhiheng Xi","Yiwen Ding","Yitao Zhai","Xiaowei Shi","Xunliang Cai","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26474v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.18766v2","updated":"2025-10-30T13:19:55Z","published":"2025-05-24T16:09:26Z","title":"StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks\n  by Style Perturbations","summary":"  Recently, text-to-image diffusion models have been widely used for style\nmimicry and personalized customization through methods such as DreamBooth and\nTextual Inversion. This has raised concerns about intellectual property\nprotection and the generation of deceptive content. Recent studies, such as\nGlaze and Anti-DreamBooth, have proposed using adversarial noise to protect\nimages from these attacks. However, recent purification-based methods, such as\nDiffPure and Noise Upscaling, have successfully attacked these latest defenses,\nshowing the vulnerabilities of these methods. Moreover, present methods show\nlimited transferability across models, making them less effective against\nunknown text-to-image models. To address these issues, we propose a novel\nanti-mimicry method, StyleGuard. We propose a novel style loss that optimizes\nthe style-related features in the latent space to make it deviate from the\noriginal image, which improves model-agnostic transferability. Additionally, to\nenhance the perturbation's ability to bypass diffusion-based purification, we\ndesigned a novel upscale loss that involves ensemble purifiers and upscalers\nduring training. Extensive experiments on the WikiArt and CelebA datasets\ndemonstrate that StyleGuard outperforms existing methods in robustness against\nvarious transformations and purifications, effectively countering style mimicry\nin various models. Moreover, StyleGuard is effective on different style mimicry\nmethods, including DreamBooth and Textual Inversion. The code is available at\nhttps://github.com/PolyLiYJ/StyleGuard.\n","authors":["Yanjie Li","Wenxuan Zhang","Xinqi Lyu","Yihao Liu","Bin Xiao"],"pdf_url":"https://arxiv.org/pdf/2505.18766v2.pdf","comment":"Accepted by NIPS2025"},{"id":"http://arxiv.org/abs/2510.26466v1","updated":"2025-10-30T13:11:23Z","published":"2025-10-30T13:11:23Z","title":"Representation-Level Counterfactual Calibration for Debiased Zero-Shot\n  Recognition","summary":"  Object-context shortcuts remain a persistent challenge in vision-language\nmodels, undermining zero-shot reliability when test-time scenes differ from\nfamiliar training co-occurrences. We recast this issue as a causal inference\nproblem and ask: Would the prediction remain if the object appeared in a\ndifferent environment? To answer this at inference time, we estimate object and\nbackground expectations within CLIP's representation space, and synthesize\ncounterfactual embeddings by recombining object features with diverse\nalternative contexts sampled from external datasets, batch neighbors, or\ntext-derived descriptions. By estimating the Total Direct Effect and simulating\nintervention, we further subtract background-only activation, preserving\nbeneficial object-context interactions while mitigating hallucinated scores.\nWithout retraining or prompt design, our method substantially improves both\nworst-group and average accuracy on context-sensitive benchmarks, establishing\na new zero-shot state of the art. Beyond performance, our framework provides a\nlightweight representation-level counterfactual approach, offering a practical\ncausal avenue for debiased and reliable multimodal reasoning.\n","authors":["Pei Peng","MingKun Xie","Hang Hao","Tong Jin","ShengJun Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26464v1","updated":"2025-10-30T13:09:00Z","published":"2025-10-30T13:09:00Z","title":"Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly\n  Detection","summary":"  Few-shot anomaly detection (FSAD) methods identify anomalous regions with few\nknown normal samples. Most existing methods rely on the generalization ability\nof pre-trained vision-language models (VLMs) to recognize potentially anomalous\nregions through feature similarity between text descriptions and images.\nHowever, due to the lack of detailed textual descriptions, these methods can\nonly pre-define image-level descriptions to match each visual patch token to\nidentify potential anomalous regions, which leads to the semantic misalignment\nbetween image descriptions and patch-level visual anomalies, achieving\nsub-optimal localization performance. To address the above issues, we propose\nthe Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and\nfine-grained textual descriptions for existing anomaly detection datasets with\nautomatic construction pipeline. Based on the MFSC, we propose a novel\nframework named FineGrainedAD to improve anomaly localization performance,\nwhich consists of two components: Multi-Level Learnable Prompt (MLLP) and\nMulti-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics\ninto multi-level learnable prompts through automatic replacement and\nconcatenation mechanism, while MLSA designs region aggregation strategy and\nmulti-level alignment training to facilitate learnable prompts better align\nwith corresponding visual regions. Experiments demonstrate that the proposed\nFineGrainedAD achieves superior overall performance in few-shot settings on\nMVTec-AD and VisA datasets.\n","authors":["Yuanting Fan","Jun Liu","Xiaochen Chen","Bin-Bin Gao","Jian Li","Yong Liu","Jinlong Peng","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26464v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2503.23722v3","updated":"2025-10-30T12:49:25Z","published":"2025-03-31T04:47:05Z","title":"LATex: Leveraging Attribute-based Text Knowledge for Aerial-Ground\n  Person Re-Identification","summary":"  As an important task in intelligent transportation systems, Aerial-Ground\nperson Re-IDentification (AG-ReID) aims to retrieve specific persons across\nheterogeneous cameras in different viewpoints. Previous methods typically adopt\ndeep learning-based models, focusing on extracting view-invariant features.\nHowever, they usually overlook the semantic information in person attributes.\nIn addition, existing training strategies often rely on full fine-tuning\nlarge-scale models, which significantly increases training costs. To address\nthese issues, we propose a novel framework named LATex for AG-ReID, which\nadopts prompt-tuning strategies to leverage attribute-based text knowledge.\nSpecifically, with the Contrastive Language-Image Pre-training (CLIP) model, we\nfirst propose an Attribute-aware Image Encoder (AIE) to extract both global\nsemantic features and attribute-aware features from input images. Then, with\nthese features, we propose a Prompted Attribute Classifier Group (PACG) to\npredict person attributes and obtain attribute representations. Finally, we\ndesign a Coupled Prompt Template (CPT) to transform attribute representations\nand view information into structured sentences. These sentences are processed\nby the text encoder of CLIP to generate more discriminative features. As a\nresult, our framework can fully leverage attribute-based text knowledge to\nimprove AG-ReID performance. Extensive experiments on three AG-ReID benchmarks\ndemonstrate the effectiveness of our proposed methods. The source code is\navailable at https://github.com/kevinhu314/LATex.\n","authors":["Pingping Zhang","Xiang Hu","Yuhao Wang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2503.23722v3.pdf","comment":"More modifications may be performed"},{"id":"http://arxiv.org/abs/2510.26443v1","updated":"2025-10-30T12:46:56Z","published":"2025-10-30T12:46:56Z","title":"PointSt3R: Point Tracking through 3D Grounded Correspondence","summary":"  Recent advances in foundational 3D reconstruction models, such as DUSt3R and\nMASt3R, have shown great potential in 2D and 3D correspondence in static\nscenes. In this paper, we propose to adapt them for the task of point tracking\nthrough 3D grounded correspondence. We first demonstrate that these models are\ncompetitive point trackers when focusing on static points, present in current\npoint tracking benchmarks ($+33.5\\%$ on EgoPoints vs. CoTracker2). We propose\nto combine the reconstruction loss with training for dynamic correspondence\nalong with a visibility head, and fine-tuning MASt3R for point tracking using a\nrelatively small amount of synthetic data. Importantly, we only train and\nevaluate on pairs of frames where one contains the query point, effectively\nremoving any temporal context. Using a mix of dynamic and static point\ncorrespondences, we achieve competitive or superior point tracking results on\nfour datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\\delta_{avg}$ / 85.8\\%\nocclusion acc. for PointSt3R compared to 75.7 / 88.3\\% for CoTracker2; and\nsignificantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs\n82.8). We also present results on 3D point tracking along with several\nablations on training datasets and percentage of dynamic correspondences.\n","authors":["Rhodri Guerrier","Adam W. Harley","Dima Damen"],"pdf_url":"https://arxiv.org/pdf/2510.26443v1.pdf","comment":"http://rhodriguerrier.github.io/PointSt3R"},{"id":"http://arxiv.org/abs/2510.26441v1","updated":"2025-10-30T12:45:24Z","published":"2025-10-30T12:45:24Z","title":"A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt\n  Tuning of Vision-Language Models","summary":"  Test-time prompt tuning (TPT) has emerged as a promising technique for\nadapting large vision-language models (VLMs) to unseen tasks without relying on\nlabeled data. However, the lack of dispersion between textual features can hurt\ncalibration performance, which raises concerns about VLMs' reliability,\ntrustworthiness, and safety. Current TPT approaches primarily focus on\nimproving prompt calibration by either maximizing average textual feature\ndispersion or enforcing orthogonality constraints to encourage angular\nseparation. However, these methods may not always have optimal angular\nseparation between class-wise textual features, which implies overlooking the\ncritical role of angular diversity. To address this, we propose A-TPT, a novel\nTPT framework that introduces angular diversity to encourage uniformity in the\ndistribution of normalized textual features induced by corresponding learnable\nprompts. This uniformity is achieved by maximizing the minimum pairwise angular\ndistance between features on the unit hypersphere. We show that our approach\nconsistently surpasses state-of-the-art TPT methods in reducing the aggregate\naverage calibration error while maintaining comparable accuracy through\nextensive experiments with various backbones on different datasets. Notably,\nour approach exhibits superior zero-shot calibration performance on natural\ndistribution shifts and generalizes well to medical datasets. We provide\nextensive analyses, including theoretical aspects, to establish the grounding\nof A-TPT. These results highlight the potency of promoting angular diversity to\nachieve well-dispersed textual features, significantly improving VLM\ncalibration during test-time adaptation. Our code will be made publicly\navailable.\n","authors":["Shihab Aaqil Ahamed","Udaya S. K. P. Miriya Thanthrige","Ranga Rodrigo","Muhammad Haris Khan"],"pdf_url":"https://arxiv.org/pdf/2510.26441v1.pdf","comment":"23 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.15863v3","updated":"2025-10-30T12:26:23Z","published":"2024-06-22T14:43:23Z","title":"EmoAttack: Emotion-to-Image Diffusion Models for Emotional Backdoor\n  Generation","summary":"  Text-to-image diffusion models can generate realistic images based on textual\ninputs, enabling users to convey their opinions visually through language.\nMeanwhile, within language, emotion plays a crucial role in expressing personal\nopinions in our daily lives and the inclusion of maliciously negative content\ncan lead users astray, exacerbating negative emotions. Recognizing the success\nof diffusion models and the significance of emotion, we investigate a\npreviously overlooked risk associated with text-to-image diffusion models, that\nis, utilizing emotion in the input texts to introduce negative content and\nprovoke unfavorable emotions in users. Specifically, we identify a new backdoor\nattack, i.e., emotion-aware backdoor attack (EmoAttack), which introduces\nmalicious negative content triggered by emotional texts during image\ngeneration. We formulate such an attack as a diffusion personalization problem\nto avoid extensive model retraining and propose the EmoBooth. Unlike existing\npersonalization methods, our approach fine-tunes a pre-trained diffusion model\nby establishing a mapping between a cluster of emotional words and a given\nreference image containing malicious negative content. To validate the\neffectiveness of our method, we built a dataset and conducted extensive\nanalysis and discussion about its effectiveness. Given consumers' widespread\nuse of diffusion models, uncovering this threat is critical for society.\n","authors":["Tianyu Wei","Shanmin Pang","Qi Guo","Yizhuo Ma","Xiaofeng Cao","Qing Guo"],"pdf_url":"https://arxiv.org/pdf/2406.15863v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.08083v3","updated":"2025-10-30T12:07:37Z","published":"2022-08-17T05:42:59Z","title":"Two Heads are Better than One: Robust Learning Meets Multi-branch Models","summary":"  Deep neural networks (DNNs) are vulnerable to adversarial examples, in which\nDNNs are misled to false outputs due to inputs containing imperceptible\nperturbations. Adversarial training, a reliable and effective method of\ndefense, may significantly reduce the vulnerability of neural networks and\nbecomes the de facto standard for robust learning. While many recent works\npractice the data-centric philosophy, such as how to generate better\nadversarial examples or use generative models to produce additional training\ndata, we look back to the models themselves and revisit the adversarial\nrobustness from the perspective of deep feature distribution as an insightful\ncomplementarity. In this paper, we propose \\textit{Branch Orthogonality\nadveRsarial Training} (BORT) to obtain state-of-the-art performance with solely\nthe original dataset for adversarial training. To practice our design idea of\nintegrating multiple orthogonal solution spaces, we leverage a simple and\nstraightforward multi-branch neural network that eclipses adversarial attacks\nwith no increase in inference time. We heuristically propose a corresponding\nloss function, branch-orthogonal loss, to make each solution space of the\nmulti-branch model orthogonal. We evaluate our approach on CIFAR-10, CIFAR-100\nand SVHN against $\\ell_{\\infty}$ norm-bounded perturbations of size $\\epsilon =\n8/255$, respectively. Exhaustive experiments are conducted to show that our\nmethod goes beyond all state-of-the-art methods without any tricks. Compared to\nall methods that do not use additional data for training, our models achieve\n67.3\\% and 41.5\\% robust accuracy on CIFAR-10 and CIFAR-100 (improving upon the\nstate-of-the-art by +7.23\\% and +9.07\\%). We also outperform methods using a\ntraining set with a far larger scale than ours.\n","authors":["Zongyuan Zhang","Qingwen Bu","Tianyang Duan","Zheng Lin","Yuhao Qing","Zihan Fang","Heming Cui","Dong Huang"],"pdf_url":"https://arxiv.org/pdf/2208.08083v3.pdf","comment":"Camera-ready version for ICPADS 2025"},{"id":"http://arxiv.org/abs/2409.06154v3","updated":"2025-10-30T12:04:13Z","published":"2024-09-10T01:57:57Z","title":"Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial\n  Expressions Using Static Expression Data","summary":"  Dynamic facial expression recognition (DFER) infers emotions from the\ntemporal evolution of expressions, unlike static facial expression recognition\n(SFER), which relies solely on a single snapshot. This temporal analysis\nprovides richer information and promises greater recognition capability.\nHowever, current DFER methods often exhibit unsatisfied performance largely due\nto fewer training samples compared to SFER. Given the inherent correlation\nbetween static and dynamic expressions, we hypothesize that leveraging the\nabundant SFER data can enhance DFER. To this end, we propose Static-for-Dynamic\n(S4D), a unified dual-modal learning framework that integrates SFER data as a\ncomplementary resource for DFER. Specifically, S4D employs dual-modal\nself-supervised pre-training on facial images and videos using a shared Vision\nTransformer (ViT) encoder-decoder architecture, yielding improved\nspatiotemporal representations. The pre-trained encoder is then fine-tuned on\nstatic and dynamic expression datasets in a multi-task learning setup to\nfacilitate emotional information interaction. Unfortunately, vanilla multi-task\nlearning in our study results in negative transfer. To address this, we propose\nan innovative Mixture of Adapter Experts (MoAE) module that facilitates\ntask-specific knowledge acquisition while effectively extracting shared\nknowledge from both static and dynamic expression data. Extensive experiments\ndemonstrate that S4D achieves a deeper understanding of DFER, setting new\nstate-of-the-art performance on FERV39K, MAFW, and DFEW benchmarks, with\nweighted average recall (WAR) of 53.65\\%, 58.44\\%, and 76.68\\%, respectively.\nAdditionally, a systematic correlation analysis between SFER and DFER tasks is\npresented, which further elucidates the potential benefits of leveraging SFER.\n","authors":["Yin Chen","Jia Li","Yu Zhang","Zhenzhen Hu","Shiguang Shan","Meng Wang","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2409.06154v3.pdf","comment":"The code and model are publicly available here\n  https://github.com/MSA-LMC/S4D"},{"id":"http://arxiv.org/abs/2509.23885v2","updated":"2025-10-30T12:02:27Z","published":"2025-09-28T13:50:29Z","title":"Tunable-Generalization Diffusion Powered by Self-Supervised Contextual\n  Sub-Data for Low-Dose CT Reconstruction","summary":"  Current models based on deep learning for low-dose CT denoising rely heavily\non paired data and generalize poorly. Even the more concerned diffusion models\nneed to learn the distribution of clean data for reconstruction, which is\ndifficult to satisfy in medical clinical applications. At the same time,\nself-supervised-based methods face the challenge of significant degradation of\ngeneralizability of models pre-trained for the current dose to expand to other\ndoses. To address these issues, this work proposes a novel method of\nTUnable-geneRalizatioN Diffusion (TurnDiff) powered by self-supervised\ncontextual sub-data for low-dose CT reconstruction. Firstly, a contextual\nsubdata self-enhancing similarity strategy is designed for denoising centered\non the LDCT projection domain, which provides an initial prior for the\nsubsequent progress. Subsequently, the initial prior is used to combine\nknowledge distillation with a deep combination of latent diffusion models for\noptimizing image details. The pre-trained model is used for inference\nreconstruction, and the pixel-level self-correcting fusion technique is\nproposed for fine-grained reconstruction of the image domain to enhance the\nimage fidelity, using the initial prior and the LDCT image as a guide. In\naddition, the technique is flexibly applied to the generalization of upper and\nlower doses or even unseen doses. Dual-domain strategy cascade for\nself-supervised LDCT denoising, TurnDiff requires only LDCT projection domain\ndata for training and testing. Comprehensive evaluation on both benchmark\ndatasets and real-world data demonstrates that TurnDiff consistently\noutperforms state-of-the-art methods in both reconstruction and generalization.\n","authors":["Guoquan Wei","Liu Shi","Zekun Zhou","Wenzhe Shan","Qiegen Liu"],"pdf_url":"https://arxiv.org/pdf/2509.23885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23117v2","updated":"2025-10-30T12:02:18Z","published":"2025-10-27T08:38:17Z","title":"Seeing Structural Failure Before it Happens: An Image-Based\n  Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction","summary":"  Physics Informed Neural Networks (PINNs) are gaining attention for their\nability to embed physical laws into deep learning models, which is particularly\nuseful in structural engineering tasks with limited data. This paper aims to\nexplore the use of PINNs to predict the weight of small scale spaghetti\nbridges, a task relevant to understanding load limits and potential failure\nmodes in simplified structural models. Our proposed framework incorporates\nphysics-based constraints to the prediction model for improved performance. In\naddition to standard PINNs, we introduce a novel architecture named Physics\nInformed Kolmogorov Arnold Network (PIKAN), which blends universal function\napproximation theory with physical insights. The structural parameters provided\nas input to the model are collected either manually or through computer vision\nmethods. Our dataset includes 15 real bridges, augmented to 100 samples, and\nour best model achieves an $R^2$ score of 0.9603 and a mean absolute error\n(MAE) of 10.50 units. From applied perspective, we also provide a web based\ninterface for parameter entry and prediction. These results show that PINNs can\noffer reliable estimates of structural weight, even with limited data, and may\nhelp inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at\nhttps://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.\n","authors":["Omer Jauhar Khan","Sudais Khan","Hafeez Anwar","Shahzeb Khan","Shams Ul Arifeen"],"pdf_url":"https://arxiv.org/pdf/2510.23117v2.pdf","comment":"12 pages, 17 figures. Preprint"},{"id":"http://arxiv.org/abs/2510.26412v1","updated":"2025-10-30T12:00:46Z","published":"2025-10-30T12:00:46Z","title":"LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video\n  Generation","summary":"  Recently text-to-video generation has made impressive progress in producing\nshort, high-quality clips, but evaluating long-form outputs remains a major\nchallenge especially when processing complex prompts. Existing benchmarks\nmostly rely on simplified prompts and focus on low-level metrics, overlooking\nfine-grained alignment with prompts and abstract dimensions such as narrative\ncoherence and thematic expression. To address these gaps, we propose\nLoCoT2V-Bench, a benchmark specifically designed for long video generation\n(LVG) under complex input conditions. Based on various real-world videos,\nLoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating\nelements like scene transitions and event dynamics. Moreover, it constructs a\nmulti-dimensional evaluation framework that includes our newly proposed metrics\nsuch as event-level alignment, fine-grained temporal consistency, content\nclarity, and the Human Expectation Realization Degree (HERD) that focuses on\nmore abstract attributes like narrative flow, emotional response, and character\ndevelopment. Using this framework, we conduct a comprehensive evaluation of\nnine representative LVG models, finding that while current methods perform well\non basic visual and temporal aspects, they struggle with inter-event\nconsistency, fine-grained alignment, and high-level thematic adherence, etc.\nOverall, LoCoT2V-Bench provides a comprehensive and reliable platform for\nevaluating long-form complex text-to-video generation and highlights critical\ndirections for future method improvement.\n","authors":["Xiangqing Zheng","Chengyue Wu","Kehai Chen","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09549v2","updated":"2025-10-30T12:00:18Z","published":"2025-04-13T12:44:50Z","title":"SD-ReID: View-aware Stable Diffusion for Aerial-Ground Person\n  Re-Identification","summary":"  Aerial-Ground Person Re-IDentification (AG-ReID) aims to retrieve specific\npersons across cameras with different viewpoints. Previous works focus on\ndesigning discriminative models to maintain the identity consistency despite\ndrastic changes in camera viewpoints. The core idea behind these methods is\nquite natural, but designing a view-robust model is a very challenging task.\nMoreover, they overlook the contribution of view-specific features in enhancing\nthe model's ability to represent persons. To address these issues, we propose a\nnovel generative framework named SD-ReID for AG-ReID, which leverages\ngenerative models to mimic the feature distribution of different views while\nextracting robust identity representations. More specifically, we first train a\nViT-based model to extract person representations along with controllable\nconditions, including identity and view conditions. We then fine-tune the\nStable Diffusion (SD) model to enhance person representations guided by these\ncontrollable conditions. Furthermore, we introduce the View-Refined Decoder\n(VRD) to bridge the gap between instance-level and global-level features.\nFinally, both person representations and all-view features are employed to\nretrieve target persons. Extensive experiments on five AG-ReID benchmarks\n(i.e., CARGO, AG-ReIDv1, AG-ReIDv2, LAGPeR and G2APS-ReID) demonstrate the\neffectiveness of our proposed method. The source code will be available.\n","authors":["Yuhao Wang","Xiang Hu","Lixin Wang","Pingping Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2504.09549v2.pdf","comment":"More modifications may performed"},{"id":"http://arxiv.org/abs/2510.26391v1","updated":"2025-10-30T11:34:37Z","published":"2025-10-30T11:34:37Z","title":"EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models","summary":"  Existing EEG-driven image reconstruction methods often overlook spatial\nattention mechanisms, limiting fidelity and semantic coherence. To address\nthis, we propose a dual-conditioning framework that combines EEG embeddings\nwith spatial saliency maps to enhance image generation. Our approach leverages\nthe Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes\nStable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals\nwith visual semantics, while a ControlNet branch conditions generation on\nsaliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves\na significant improvement in the quality of low- and high-level image features\nover existing approaches. Simultaneously, strongly aligning with human visual\nattention. The results demonstrate that attentional priors resolve EEG\nambiguities, enabling high-fidelity reconstructions with applications in\nmedical diagnostics and neuroadaptive interfaces, advancing neural decoding\nthrough efficient adaptation of pre-trained diffusion models.\n","authors":["Igor Abramov","Ilya Makarov"],"pdf_url":"https://arxiv.org/pdf/2510.26391v1.pdf","comment":"Demo paper"},{"id":"http://arxiv.org/abs/2510.26390v1","updated":"2025-10-30T11:33:29Z","published":"2025-10-30T11:33:29Z","title":"SPG-CDENet: Spatial Prior-Guided Cross Dual Encoder Network for\n  Multi-Organ Segmentation","summary":"  Multi-organ segmentation is a critical task in computer-aided diagnosis.\nWhile recent deep learning methods have achieved remarkable success in image\nsegmentation, huge variations in organ size and shape challenge their\neffectiveness in multi-organ segmentation. To address these challenges, we\npropose a Spatial Prior-Guided Cross Dual Encoder Network (SPG-CDENet), a novel\ntwo-stage segmentation paradigm designed to improve multi-organ segmentation\naccuracy. Our SPG-CDENet consists of two key components: a spatial prior\nnetwork and a cross dual encoder network. The prior network generates coarse\nlocalization maps that delineate the approximate ROI, serving as spatial\nguidance for the dual encoder network. The cross dual encoder network comprises\nfour essential components: a global encoder, a local encoder, a symmetric\ncross-attention module, and a flow-based decoder. The global encoder captures\nglobal semantic features from the entire image, while the local encoder focuses\non features from the prior network. To enhance the interaction between the\nglobal and local encoders, a symmetric cross-attention module is proposed\nacross all layers of the encoders to fuse and refine features. Furthermore, the\nflow-based decoder directly propagates high-level semantic features from the\nfinal encoder layer to all decoder layers, maximizing feature preservation and\nutilization. Extensive qualitative and quantitative experiments on two public\ndatasets demonstrate the superior performance of SPG-CDENet compared to\nexisting segmentation methods. Furthermore, ablation studies further validate\nthe effectiveness of the proposed modules in improving segmentation accuracy.\n","authors":["Xizhi Tian","Changjun Zhou","Yulin. Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26369v1","updated":"2025-10-30T11:14:17Z","published":"2025-10-30T11:14:17Z","title":"CorVS: Person Identification via Video Trajectory-Sensor Correspondence\n  in a Real-World Warehouse","summary":"  Worker location data is key to higher productivity in industrial sites.\nCameras are a promising tool for localization in logistics warehouses since\nthey also offer valuable environmental contexts such as package status.\nHowever, identifying individuals with only visual data is often impractical.\nAccordingly, several prior studies identified people in videos by comparing\ntheir trajectories and wearable sensor measurements. While this approach has\nadvantages such as independence from appearance, the existing methods may break\ndown under real-world conditions. To overcome this challenge, we propose CorVS,\na novel data-driven person identification method based on correspondence\nbetween visual tracking trajectories and sensor measurements. Firstly, our deep\nlearning model predicts correspondence probabilities and reliabilities for\nevery pair of a trajectory and sensor measurements. Secondly, our algorithm\nmatches the trajectories and sensor measurements over time using the predicted\nprobabilities and reliabilities. We developed a dataset with actual warehouse\noperations and demonstrated the method's effectiveness for real-world\napplications.\n","authors":["Kazuma Kano","Yuki Mori","Shin Katayama","Kenta Urano","Takuro Yonezawa","Nobuo Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2510.26369v1.pdf","comment":"7 pages, 3 figures, accepted to IPIN 2025"},{"id":"http://arxiv.org/abs/2510.26358v1","updated":"2025-10-30T11:08:23Z","published":"2025-10-30T11:08:23Z","title":"AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian\n  Splatting SLAM","summary":"  Autonomous robots in orchards require real-time 3D scene understanding\ndespite repetitive row geometry, seasonal appearance changes, and wind-driven\nfoliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that\ncouples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian\nSplatting (3DGS) rendering. Batch rasterization across complementary viewpoints\nrecovers orchard structure under occlusions, while a unified gradient-driven\nmap lifecycle executed between keyframes preserves fine details and bounds\nmemory. Pose refinement is guided by a probabilistic LiDAR-based depth\nconsistency term, back-propagated through the camera projection to tighten\ngeometry-appearance coupling. We deploy the system on a field platform in apple\nand pear orchards across dormancy, flowering, and harvesting, using a\nstandardized trajectory protocol that evaluates both training-view and\nnovel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons\nand sites, AgriGS-SLAM delivers sharper, more stable reconstructions and\nsteadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while\nmaintaining real-time performance on-tractor. While demonstrated in orchard\nmonitoring, the approach can be applied to other outdoor domains requiring\nrobust multimodal perception.\n","authors":["Mirko Usuelli","David Rapado-Rincon","Gert Kootstra","Matteo Matteucci"],"pdf_url":"https://arxiv.org/pdf/2510.26358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04448v2","updated":"2025-10-30T10:58:04Z","published":"2025-09-04T17:59:43Z","title":"TRUST-VL: An Explainable News Assistant for General Multimodal\n  Misinformation Detection","summary":"  Multimodal misinformation, encompassing textual, visual, and cross-modal\ndistortions, poses an increasing societal threat that is amplified by\ngenerative AI. Existing methods typically focus on a single type of distortion\nand struggle to generalize to unseen scenarios. In this work, we observe that\ndifferent distortion types share common reasoning capabilities while also\nrequiring task-specific skills. We hypothesize that joint training across\ndistortion types facilitates knowledge sharing and enhances the model's ability\nto generalize. To this end, we introduce TRUST-VL, a unified and explainable\nvision-language model for general multimodal misinformation detection. TRUST-VL\nincorporates a novel Question-Aware Visual Amplifier module, designed to\nextract task-specific visual features. To support training, we also construct\nTRUST-Instruct, a large-scale instruction dataset containing 198K samples\nfeaturing structured reasoning chains aligned with human fact-checking\nworkflows. Extensive experiments on both in-domain and zero-shot benchmarks\ndemonstrate that TRUST-VL achieves state-of-the-art performance, while also\noffering strong generalization and interpretability.\n","authors":["Zehong Yan","Peng Qi","Wynne Hsu","Mong Li Lee"],"pdf_url":"https://arxiv.org/pdf/2509.04448v2.pdf","comment":"EMNLP 2025 Oral; Project Homepage:\n  https://yanzehong.github.io/trust-vl/"},{"id":"http://arxiv.org/abs/2505.21497v2","updated":"2025-10-30T10:49:28Z","published":"2025-05-27T17:58:49Z","title":"Paper2Poster: Towards Multimodal Poster Automation from Scientific\n  Papers","summary":"  Academic poster generation is a crucial yet challenging task in scientific\ncommunication, requiring the compression of long-context interleaved documents\ninto a single, visually coherent page. To address this challenge, we introduce\nthe first benchmark and metric suite for poster generation, which pairs recent\nconference papers with author-designed posters and evaluates outputs on\n(i)Visual Quality-semantic alignment with human posters, (ii)Textual\nCoherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic\nand informational criteria scored by a VLM-as-judge, and notably\n(iv)PaperQuiz-the poster's ability to convey core paper content as measured by\nVLMs answering generated quizzes. Building on this benchmark, we propose\nPosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser\ndistills the paper into a structured asset library; the (b)Planner aligns\ntext-visual pairs into a binary-tree layout that preserves reading order and\nspatial balance; and the (c)Painter-Commenter loop refines each panel by\nexecuting rendering code and using VLM feedback to eliminate overflow and\nensure alignment. In our comprehensive evaluation, we find that GPT-4o\noutputs-though visually appealing at first glance-often exhibit noisy text and\npoor PaperQuiz scores, and we find that reader engagement is the primary\naesthetic bottleneck, as human-designed posters rely largely on visual\nsemantics to convey meaning. Our fully open-source variants (e.g. based on the\nQwen-2.5 series) outperform existing 4o-driven multi-agent systems across\nnearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper\ninto a finalized yet editable .pptx poster - all for just $0.005. These\nfindings chart clear directions for the next generation of fully automated\nposter-generation models. The code and datasets are available at\nhttps://github.com/Paper2Poster/Paper2Poster.\n","authors":["Wei Pang","Kevin Qinghong Lin","Xiangru Jian","Xi He","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2505.21497v2.pdf","comment":"Project Page: https://github.com/Paper2Poster/Paper2Poster"},{"id":"http://arxiv.org/abs/2510.26339v1","updated":"2025-10-30T10:46:28Z","published":"2025-10-30T10:46:28Z","title":"GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and\n  High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?","summary":"  Image super-resolution(SR) is fundamental to many vision system-from\nsurveillance and autonomy to document analysis and retail analytics-because\nrecovering high-frequency details, especially scene-text, enables reliable\ndownstream perception. Scene-text, i.e., text embedded in natural images such\nas signs, product labels, and storefronts, often carries the most actionable\ninformation; when characters are blurred or hallucinated, optical character\nrecognition(OCR) and subsequent decisions fail even if the rest of the image\nappears sharp. Yet previous SR research has often been tuned to distortion\n(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that\nare largely insensitive to character-level errors. Furthermore, studies that do\naddress text SR often focus on simplified benchmarks with isolated characters,\noverlooking the challenges of text within complex natural scenes. As a result,\nscene-text is effectively treated as generic texture. For SR to be effective in\npractical deployments, it is therefore essential to explicitly optimize for\nboth text legibility and perceptual quality. We present GLYPH-SR, a\nvision-language-guided diffusion framework that aims to achieve both objectives\njointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by\nOCR data, and a ping-pong scheduler that alternates between text- and\nscene-centric guidance. To enable targeted text restoration, we train these\ncomponents on a synthetic corpus while keeping the main SR branch frozen.\nAcross SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by\nup to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)\nwhile maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed\nto satisfy both objectives simultaneously-high readability and high visual\nrealism-delivering SR that looks right and reds right.\n","authors":["Mingyu Sung","Seungjae Ham","Kangwoo Kim","Yeokyoung Yoon","Sangseok Yun","Il-Min Kim","Jae-Mo Kang"],"pdf_url":"https://arxiv.org/pdf/2510.26339v1.pdf","comment":"11 pages, 6 figures. Includes supplementary material. Under review as\n  a conference paper at ICLR 2026"},{"id":"http://arxiv.org/abs/2503.09499v3","updated":"2025-10-30T10:21:42Z","published":"2025-03-12T16:03:03Z","title":"MindGYM: What Matters in Question Synthesis for Thinking-Centric\n  Fine-Tuning?","summary":"  Large foundation models face challenges in acquiring transferable, structured\nthinking abilities, especially when supervised with rigid templates or\ncrowd-annotated instruction datasets. Unlike prior approaches, we focus on a\nthinking-centric data synthesis paradigm that enables models to evolve through\nself-generated, cognitively guided data. We propose MindGYM, a structured and\nscalable framework for question synthesis, composed of: (1) Cognitive Thinking\nProcess Injection, which infuses high-level reasoning objectives to shape the\nmodel's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating\natomic questions from diverse semantic types to encourage broader thinking; and\n(3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop\nquestions based on QA seeds for deeper reasoning. Detailed analysis shows that\nsynthetic data generated by our method achieves 16.7% higher average quality\nand 67.91% lower quality variance compared to baseline sources, highlighting\nthat both high-quality and self-contained data are essential for effective,\nthinking-oriented fine-tuning. MindGYM improves performance on six reasoning\nbenchmarks, achieving gains of up to 16% on MathVision using only 400 data\nsamples, and generalizable improvements across different model sizes and\narchitectures. MindGYM underscores the viability of self-challenging mechanisms\nin refining large model capabilities while minimizing human intervention and\nresource demands. Code and data are released to promote data-centric research\ninto self-evolving foundation models driven by their internal reasoning\ncapabilities.\n","authors":["Zhe Xu","Daoyuan Chen","Zhenqing Ling","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2503.09499v3.pdf","comment":"Accepted by NeurIPS'25. 30 pages, 2 figures, 13 tables"},{"id":"http://arxiv.org/abs/2509.06771v2","updated":"2025-10-30T10:15:05Z","published":"2025-09-08T14:55:16Z","title":"D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning -\n  A Benchmark Dataset and Method","summary":"  Dark humor in online memes poses unique challenges due to its reliance on\nimplicit, sensitive, and culturally contextual cues. To address the lack of\nresources and methods for detecting dark humor in multimodal content, we\nintroduce a novel dataset of 4,379 Reddit memes annotated for dark humor,\ntarget category (gender, mental health, violence, race, disability, and other),\nand a three-level intensity rating (mild, moderate, severe). Building on this\nresource, we propose a reasoning-augmented framework that first generates\nstructured explanations for each meme using a Large Vision-Language Model\n(VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective\nto iteratively refine its explanations, ensuring completeness and alignment. We\nthen extract textual features from both the OCR transcript and the self-refined\nreasoning via a text encoder, while visual features are obtained using a vision\ntransformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three\nstreams, text, image, and reasoning, via pairwise attention mechanisms,\nproducing a unified representation for classification. Experimental results\ndemonstrate that our approach outperforms strong baselines across three tasks:\ndark humor detection, target identification, and intensity prediction. The\ndataset, annotations, and code are released to facilitate further research in\nmultimodal humor understanding and content moderation. Code and Dataset are\navailable at:\nhttps://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning\n","authors":["Sai Kartheek Reddy Kasu","Mohammad Zia Ur Rehman","Shahid Shafi Dar","Rishi Bharat Junghare","Dhanvin Sanjay Namboodiri","Nagendra Kumar"],"pdf_url":"https://arxiv.org/pdf/2509.06771v2.pdf","comment":"Accepted at IEEE International Conference on Data Mining (ICDM) 2025"},{"id":"http://arxiv.org/abs/2510.26315v1","updated":"2025-10-30T10:08:06Z","published":"2025-10-30T10:08:06Z","title":"A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for\n  Diabetic Retinopathy Grading","summary":"  Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged\nand elderly people, which significantly impacts their daily lives and mental\nhealth. To improve the efficiency of clinical screening and enable the early\ndetection of DR, a variety of automated DR diagnosis systems have been recently\nestablished based on convolutional neural network (CNN) or vision Transformer\n(ViT). However, due to the own shortages of CNN / ViT, the performance of\nexisting methods using single-type backbone has reached a bottleneck. One\npotential way for the further improvements is integrating different kinds of\nbackbones, which can fully leverage the respective strengths of them\n(\\emph{i.e.,} the local feature extraction capability of CNN and the global\nfeature capturing ability of ViT). To this end, we propose a novel paradigm to\neffectively fuse the features extracted by different backbones based on the\ntheory of evidence. Specifically, the proposed evidential fusion paradigm\ntransforms the features from different backbones into supporting evidences via\na set of deep evidential networks. With the supporting evidences, the\naggregated opinion can be accordingly formed, which can be used to adaptively\ntune the fusion pattern between different backbones and accordingly boost the\nperformance of our hybrid model. We evaluated our method on two publicly\navailable DR grading datasets. The experimental results demonstrate that our\nhybrid model not only improves the accuracy of DR grading, compared to the\nstate-of-the-art frameworks, but also provides the excellent interpretability\nfor feature fusion and decision-making.\n","authors":["Junlai Qiu","Yunzhu Chen","Hao Zheng","Yawen Huang","Yuexiang Li"],"pdf_url":"https://arxiv.org/pdf/2510.26315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22159v3","updated":"2025-10-30T10:00:04Z","published":"2025-03-28T05:46:02Z","title":"Disentangled 4D Gaussian Splatting: Rendering High-Resolution Dynamic\n  World at 343 FPS","summary":"  While dynamic novel view synthesis from 2D videos has seen progress,\nachieving efficient reconstruction and rendering of dynamic scenes remains a\nchallenging task. In this paper, we introduce Disentangled 4D Gaussian\nSplatting (Disentangled4DGS), a novel representation and rendering pipeline\nthat achieves real-time performance without compromising visual fidelity.\nDisentangled4DGS decouples the temporal and spatial components of 4D Gaussians,\navoiding the need for slicing first and four-dimensional matrix calculations in\nprior methods. By projecting temporal and spatial deformations into dynamic 2D\nGaussians and deferring temporal processing, we minimize redundant computations\nof 4DGS. Our approach also features a gradient-guided flow loss and temporal\nsplitting strategy to reduce artifacts. Experiments demonstrate a significant\nimprovement in rendering speed and quality, achieving 343 FPS when render\n1352*1014 resolution images on a single RTX3090 while reducing storage\nrequirements by at least 4.5%. Our approach sets a new benchmark for dynamic\nnovel view synthesis, outperforming existing methods on both multi-view and\nmonocular dynamic scene datasets.\n","authors":["Hao Feng","Hao Sun","Wei Xie","Zhi Zuo","Zhengzhe Liu"],"pdf_url":"https://arxiv.org/pdf/2503.22159v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26304v1","updated":"2025-10-30T09:43:56Z","published":"2025-10-30T09:43:56Z","title":"Exploring the correlation between the type of music and the emotions\n  evoked: A study using subjective questionnaires and EEG","summary":"  The subject of this work is to check how different types of music affect\nhuman emotions. While listening to music, a subjective survey and brain\nactivity measurements were carried out using an EEG helmet. The aim is to\ndemonstrate the impact of different music genres on emotions. The research\ninvolved a diverse group of participants of different gender and musical\npreferences. This had the effect of capturing a wide range of emotional\nresponses to music. After the experiment, a relationship analysis of the\nrespondents' questionnaires with EEG signals was performed. The analysis\nrevealed connections between emotions and observed brain activity.\n","authors":["Jelizaveta Jankowska","Bożena Kostek","Fernando Alonso-Fernandez","Prayag Tiwari"],"pdf_url":"https://arxiv.org/pdf/2510.26304v1.pdf","comment":"Published at IWAIPR 2025 conference"},{"id":"http://arxiv.org/abs/2510.22319v2","updated":"2025-10-30T09:33:15Z","published":"2025-10-25T14:51:17Z","title":"GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via\n  Regulated Clipping","summary":"  Recently, GRPO-based reinforcement learning has shown remarkable progress in\noptimizing flow-matching models, effectively improving their alignment with\ntask-specific rewards. Within these frameworks, the policy update relies on\nimportance-ratio clipping to constrain overconfident positive and negative\ngradients. However, in practice, we observe a systematic shift in the\nimportance-ratio distribution-its mean falls below 1 and its variance differs\nsubstantially across timesteps. This left-shifted and inconsistent distribution\nprevents positive-advantage samples from entering the clipped region, causing\nthe mechanism to fail in constraining overconfident positive updates. As a\nresult, the policy model inevitably enters an implicit over-optimization\nstage-while the proxy reward continues to increase, essential metrics such as\nimage quality and text-prompt alignment deteriorate sharply, ultimately making\nthe learned policy impractical for real-world use. To address this issue, we\nintroduce GRPO-Guard, a simple yet effective enhancement to existing GRPO\nframeworks. Our method incorporates ratio normalization, which restores a\nbalanced and step-consistent importance ratio, ensuring that PPO clipping\nproperly constrains harmful updates across denoising timesteps. In addition, a\ngradient reweighting strategy equalizes policy gradients over noise conditions,\npreventing excessive updates from particular timestep regions. Together, these\ndesigns act as a regulated clipping mechanism, stabilizing optimization and\nsubstantially mitigating implicit over-optimization without relying on heavy KL\nregularization. Extensive experiments on multiple diffusion backbones (e.g.,\nSD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard\nsignificantly reduces over-optimization while maintaining or even improving\ngeneration quality.\n","authors":["Jing Wang","Jiajun Liang","Jie Liu","Henglin Liu","Gongye Liu","Jun Zheng","Wanyuan Pang","Ao Ma","Zhenyu Xie","Xintao Wang","Meng Wang","Pengfei Wan","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2510.22319v2.pdf","comment":"Project Page: https://jingw193.github.io/GRPO-Guard/"},{"id":"http://arxiv.org/abs/2510.26297v1","updated":"2025-10-30T09:31:47Z","published":"2025-10-30T09:31:47Z","title":"Towards Realistic Earth-Observation Constellation Scheduling: Benchmark\n  and Methodology","summary":"  Agile Earth Observation Satellites (AEOSs) constellations offer unprecedented\nflexibility for monitoring the Earth's surface, but their scheduling remains\nchallenging under large-scale scenarios, dynamic environments, and stringent\nconstraints. Existing methods often simplify these complexities, limiting their\nreal-world performance. We address this gap with a unified framework\nintegrating a standardized benchmark suite and a novel scheduling model. Our\nbenchmark suite, AEOS-Bench, contains $3,907$ finely tuned satellite assets and\n$16,410$ scenarios. Each scenario features $1$ to $50$ satellites and $50$ to\n$300$ imaging tasks. These scenarios are generated via a high-fidelity\nsimulation platform, ensuring realistic satellite behavior such as orbital\ndynamics and resource constraints. Ground truth scheduling annotations are\nprovided for each scenario. To our knowledge, AEOS-Bench is the first\nlarge-scale benchmark suite tailored for realistic constellation scheduling.\nBuilding upon this benchmark, we introduce AEOS-Former, a Transformer-based\nscheduling model that incorporates a constraint-aware attention mechanism. A\ndedicated internal constraint module explicitly models the physical and\noperational limits of each satellite. Through simulation-based iterative\nlearning, AEOS-Former adapts to diverse scenarios, offering a robust solution\nfor AEOS constellation scheduling. Experimental results demonstrate that\nAEOS-Former outperforms baseline models in task completion and energy\nefficiency, with ablation studies highlighting the contribution of each\ncomponent. Code and data are provided in\nhttps://github.com/buaa-colalab/AEOSBench.\n","authors":["Luting Wang","Yinghao Xiang","Hongliang Huang","Dongjun Li","Chen Gao","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02393v3","updated":"2025-10-30T09:31:07Z","published":"2025-06-03T03:18:17Z","title":"RRCANet: Recurrent Reusable-Convolution Attention Network for Infrared\n  Small Target Detection","summary":"  Infrared small target detection is a challenging task due to its unique\ncharacteristics (e.g., small, dim, shapeless and changeable). Recently\npublished CNN-based methods have achieved promising performance with heavy\nfeature extraction and fusion modules. To achieve efficient and effective\ndetection, we propose a recurrent reusable-convolution attention network\n(RRCA-Net) for infrared small target detection. Specifically, RRCA-Net\nincorporates reusable-convolution block (RuCB) in a recurrent manner without\nintroducing extra parameters. With the help of the repetitive iteration in\nRuCB, the high-level information of small targets in the deep layers can be\nwell maintained and further refined. Then, a dual interactive attention\naggregation module (DIAAM) is proposed to promote the mutual enhancement and\nfusion of refined information. In this way, RRCA-Net can both achieve\nhigh-level feature refinement and enhance the correlation of contextual\ninformation between adjacent layers. Moreover, to achieve steady convergence,\nwe design a target characteristic inspired loss function (DpT-k loss) by\nintegrating physical and mathematical constraints. Experimental results on\nthree benchmark datasets (e.g. NUAA-SIRST, IRSTD-1k, DenseSIRST) demonstrate\nthat our RRCA-Net can achieve comparable performance to the state-of-the-art\nmethods while maintaining a small number of parameters, and act as a plug and\nplay module to introduce consistent performance improvement for several popular\nIRSTD methods.\n","authors":["Yongxian Liu","Boyang Li","Ting Liu","Zaiping Lin","Wei An"],"pdf_url":"https://arxiv.org/pdf/2506.02393v3.pdf","comment":"We have updated the journal reference and DOI"},{"id":"http://arxiv.org/abs/2510.26294v1","updated":"2025-10-30T09:28:48Z","published":"2025-10-30T09:28:48Z","title":"Leveraging Large-Scale Face Datasets for Deep Periocular Recognition via\n  Ocular Cropping","summary":"  We focus on ocular biometrics, specifically the periocular region (the area\naround the eye), which offers high discrimination and minimal acquisition\nconstraints. We evaluate three Convolutional Neural Network architectures of\nvarying depth and complexity to assess their effectiveness for periocular\nrecognition. The networks are trained on 1,907,572 ocular crops extracted from\nthe large-scale VGGFace2 database. This significantly contrasts with existing\nworks, which typically rely on small-scale periocular datasets for training\nhaving only a few thousand images. Experiments are conducted with ocular images\nfrom VGGFace2-Pose, a subset of VGGFace2 containing in-the-wild face images,\nand the UFPR-Periocular database, which consists of selfies captured via mobile\ndevices with user guidance on the screen. Due to the uncontrolled conditions of\nVGGFace2, the Equal Error Rates (EERs) obtained with ocular crops range from\n9-15%, noticeably higher than the 3-6% EERs achieved using full-face images. In\ncontrast, UFPR-Periocular yields significantly better performance (EERs of\n1-2%), thanks to higher image quality and more consistent acquisition\nprotocols. To the best of our knowledge, these are the lowest reported EERs on\nthe UFPR dataset to date.\n","authors":["Fernando Alonso-Fernandez","Kevin Hernandez-Diaz","Jose Maria Buades Rubio","Josef Bigun"],"pdf_url":"https://arxiv.org/pdf/2510.26294v1.pdf","comment":"Published at IWAIPR 2025 conference"},{"id":"http://arxiv.org/abs/2510.26292v1","updated":"2025-10-30T09:24:34Z","published":"2025-10-30T09:24:34Z","title":"Beyond Imitation: Constraint-Aware Trajectory Generation with Flow\n  Matching For End-to-End Autonomous Driving","summary":"  Planning is a critical component of end-to-end autonomous driving. However,\nprevailing imitation learning methods often suffer from mode collapse, failing\nto produce diverse trajectory hypotheses. Meanwhile, existing generative\napproaches struggle to incorporate crucial safety and physical constraints\ndirectly into the generative process, necessitating an additional optimization\nstage to refine their outputs. To address these limitations, we propose CATG, a\nnovel planning framework that leverages Constrained Flow Matching. Concretely,\nCATG explicitly models the flow matching process, which inherently mitigates\nmode collapse and allows for flexible guidance from various conditioning\nsignals. Our primary contribution is the novel imposition of explicit\nconstraints directly within the flow matching process, ensuring that the\ngenerated trajectories adhere to vital safety and kinematic rules. Secondly,\nCATG parameterizes driving aggressiveness as a control signal during\ngeneration, enabling precise manipulation of trajectory style. Notably, on the\nNavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and\nwas honored with the Innovation Award.\n","authors":["Lin Liu","Guanyi Yu","Ziying Song","Junqiao Li","Caiyan Jia","Feiyang Jia","Peiliang Wu","Yandan Luo"],"pdf_url":"https://arxiv.org/pdf/2510.26292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26282v1","updated":"2025-10-30T09:07:36Z","published":"2025-10-30T09:07:36Z","title":"Exploring Complementarity and Explainability in CNNs for Periocular\n  Verification Across Acquisition Distances","summary":"  We study the complementarity of different CNNs for periocular verification at\ndifferent distances on the UBIPr database. We train three architectures of\nincreasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of\neye crops from VGGFace2. We analyse performance with cosine and chi2 metrics,\ncompare different network initialisations, and apply score-level fusion via\nlogistic regression. In addition, we use LIME heatmaps and Jensen-Shannon\ndivergence to compare attention patterns of the CNNs. While ResNet50\nconsistently performs best individually, the fusion provides substantial gains,\nespecially when combining all three networks. Heatmaps show that networks\nusually focus on distinct regions of a given image, which explains their\ncomplementarity. Our method significantly outperforms previous works on UBIPr,\nachieving a new state-of-the-art.\n","authors":["Fernando Alonso-Fernandez","Kevin Hernandez Diaz","Jose M. Buades","Kiran Raja","Josef Bigun"],"pdf_url":"https://arxiv.org/pdf/2510.26282v1.pdf","comment":"Accepted at BIOSIG 2025 conference"},{"id":"http://arxiv.org/abs/2510.26268v1","updated":"2025-10-30T08:53:13Z","published":"2025-10-30T08:53:13Z","title":"Revisiting Generative Infrared and Visible Image Fusion Based on Human\n  Cognitive Laws","summary":"  Existing infrared and visible image fusion methods often face the dilemma of\nbalancing modal information. Generative fusion methods reconstruct fused images\nby learning from data distributions, but their generative capabilities remain\nlimited. Moreover, the lack of interpretability in modal information selection\nfurther affects the reliability and consistency of fusion results in complex\nscenarios. This manuscript revisits the essence of generative image fusion\nunder the inspiration of human cognitive laws and proposes a novel infrared and\nvisible image fusion method, termed HCLFuse. First, HCLFuse investigates the\nquantification theory of information mapping in unsupervised fusion networks,\nwhich leads to the design of a multi-scale mask-regulated variational\nbottleneck encoder. This encoder applies posterior probability modeling and\ninformation decomposition to extract accurate and concise low-level modal\ninformation, thereby supporting the generation of high-fidelity structural\ndetails. Furthermore, the probabilistic generative capability of the diffusion\nmodel is integrated with physical laws, forming a time-varying physical\nguidance mechanism that adaptively regulates the generation process at\ndifferent stages, thereby enhancing the ability of the model to perceive the\nintrinsic structure of data and reducing dependence on data quality.\nExperimental results show that the proposed method achieves state-of-the-art\nfusion performance in qualitative and quantitative evaluations across multiple\ndatasets and significantly improves semantic segmentation metrics. This fully\ndemonstrates the advantages of this generative image fusion method, drawing\ninspiration from human cognition, in enhancing structural consistency and\ndetail quality.\n","authors":["Lin Guo","Xiaoqing Luo","Wei Xie","Zhancheng Zhang","Hui Li","Rui Wang","Zhenhua Feng","Xiaoning Song"],"pdf_url":"https://arxiv.org/pdf/2510.26268v1.pdf","comment":"NeurIPS 2025 spotlight"},{"id":"http://arxiv.org/abs/2503.11094v4","updated":"2025-10-30T08:44:27Z","published":"2025-03-14T05:35:38Z","title":"Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with\n  Multimodal Large Language Model in Open Space","summary":"  Spatial reasoning is a fundamental capability of multimodal large language\nmodels (MLLMs), yet their performance in open aerial environments remains\nunderexplored. In this work, we present Open3D-VQA, a novel benchmark for\nevaluating MLLMs' ability to reason about complex spatial relationships from an\naerial perspective. The benchmark comprises 73k QA pairs spanning 7 general\nspatial reasoning tasks, including multiple-choice, true/false, and\nshort-answer formats, and supports both visual and point cloud modalities. The\nquestions are automatically generated from spatial relations extracted from\nboth real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs\nreveals that: 1) Models are generally better at answering questions about\nrelative spatial relations than absolute distances, 2) 3D LLMs fail to\ndemonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on\nthe simulated dataset can significantly improve the model's spatial reasoning\nperformance in real-world scenarios. We release our benchmark, data generation\npipeline, and evaluation toolkit to support further research:\nhttps://github.com/EmbodiedCity/Open3D-VQA.code.\n","authors":["Weichen Zhang","Zile Zhou","Xin Zeng","Xuchen Liu","Jianjie Fang","Chen Gao","Yong Li","Jinqiang Cui","Xinlei Chen","Xiao-Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11094v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26241v1","updated":"2025-10-30T08:21:50Z","published":"2025-10-30T08:21:50Z","title":"Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for\n  Vision-Language Models","summary":"  Modern vision-language models (VLMs) excel at many multimodal tasks, yet\ntheir grasp of temporal information in video remains weak and, crucially,\nunder-evaluated. We probe this gap with a deceptively simple but revealing\nchallenge: judging the arrow of time (AoT)-whether a short clip is played\nforward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated\nbenchmark that tests whether VLMs can infer temporal direction in natural\nvideos using the same stimuli and behavioral baselines established for humans.\nOur comprehensive evaluation of open-weight and proprietary, reasoning and\nnon-reasoning VLMs reveals that most models perform near chance, and even the\nbest lag far behind human accuracy on physically irreversible processes (e.g.,\nfree fall, diffusion/explosion) and causal manual actions (division/addition)\nthat humans recognize almost instantly. These results highlight a fundamental\ngap in current multimodal systems: while they capture rich visual-semantic\ncorrelations, they lack the inductive biases required for temporal continuity\nand causal understanding. We release the code and data for AoT-PsyPhyBENCH to\nencourage further progress in the physical and temporal reasoning capabilities\nof VLMs.\n","authors":["Shiho Matta","Lis Kanashiro Pereira","Peitao Han","Fei Cheng","Shigeru Kitazawa"],"pdf_url":"https://arxiv.org/pdf/2510.26241v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2509.06159v3","updated":"2025-10-30T08:10:05Z","published":"2025-09-07T17:59:09Z","title":"FASL-Seg: Anatomy and Tool Segmentation of Surgical Scenes","summary":"  The growing popularity of robotic minimally invasive surgeries has made deep\nlearning-based surgical training a key area of research. A thorough\nunderstanding of the surgical scene components is crucial, which semantic\nsegmentation models can help achieve. However, most existing work focuses on\nsurgical tools and overlooks anatomical objects. Additionally, current\nstate-of-the-art (SOTA) models struggle to balance capturing high-level\ncontextual features and low-level edge features. We propose a Feature-Adaptive\nSpatial Localization model (FASL-Seg), designed to capture features at multiple\nlevels of detail through two distinct processing streams, namely a Low-Level\nFeature Projection (LLFP) and a High-Level Feature Projection (HLFP) stream,\nfor varying feature resolutions - enabling precise segmentation of anatomy and\nsurgical instruments. We evaluated FASL-Seg on surgical segmentation benchmark\ndatasets EndoVis18 and EndoVis17 on three use cases. The FASL-Seg model\nachieves a mean Intersection over Union (mIoU) of 72.71% on parts and anatomy\nsegmentation in EndoVis18, improving on SOTA by 5%. It further achieves a mIoU\nof 85.61% and 72.78% in EndoVis18 and EndoVis17 tool type segmentation,\nrespectively, outperforming SOTA overall performance, with comparable per-class\nSOTA results in both datasets and consistent performance in various classes for\nanatomy and instruments, demonstrating the effectiveness of distinct processing\nstreams for varying feature resolutions.\n","authors":["Muraam Abdel-Ghani","Mahmoud Ali","Mohamed Ali","Fatmaelzahraa Ahmed","Muhammad Arsalan","Abdulaziz Al-Ali","Shidin Balakrishnan"],"pdf_url":"https://arxiv.org/pdf/2509.06159v3.pdf","comment":"8 pages, 6 figures, In Proceedings of European Conference on\n  Artificial Intelligence (ECAI) 2025 <https://doi.org/10.3233/FAIA250908>"},{"id":"http://arxiv.org/abs/2508.07981v3","updated":"2025-10-30T08:09:13Z","published":"2025-08-11T13:41:24Z","title":"Omni-Effects: Unified and Spatially-Controllable Visual Effects\n  Generation","summary":"  Visual effects (VFX) are essential visual enhancements fundamental to modern\ncinematic production. Although video generation models offer cost-efficient\nsolutions for VFX production, current methods are constrained by per-effect\nLoRA training, which limits generation to single effects. This fundamental\nlimitation impedes applications that require spatially controllable composite\neffects, i.e., the concurrent generation of multiple effects at designated\nlocations. However, integrating diverse effects into a unified framework faces\nmajor challenges: interference from effect variations and spatial\nuncontrollability during multi-VFX joint training. To tackle these challenges,\nwe propose Omni-Effects, a first unified framework capable of generating\nprompt-guided effects and spatially controllable composite effects. The core of\nour framework comprises two key innovations: (1) LoRA-based Mixture of Experts\n(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects\nwithin a unified model while effectively mitigating cross-task interference.\n(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the\ntext token, enabling precise spatial control. Furthermore, we introduce an\nIndependent-Information Flow (IIF) module integrated within the SAP, isolating\nthe control signals corresponding to individual effects to prevent any unwanted\nblending. To facilitate this research, we construct a comprehensive VFX dataset\nOmni-VFX via a novel data collection pipeline combining image editing and\nFirst-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX\nevaluation framework for validating model performance. Extensive experiments\ndemonstrate that Omni-Effects achieves precise spatial control and diverse\neffect generation, enabling users to specify both the category and location of\ndesired effects.\n","authors":["Fangyuan Mao","Aiming Hao","Jintao Chen","Dongxia Liu","Xiaokun Feng","Jiashu Zhu","Meiqi Wu","Chubin Chen","Jiahong Wu","Xiangxiang Chu"],"pdf_url":"https://arxiv.org/pdf/2508.07981v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20392v4","updated":"2025-10-30T07:48:58Z","published":"2024-12-29T08:09:20Z","title":"Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning","summary":"  Multimodal contrastive learning models (e.g., CLIP) can learn high-quality\nrepresentations from large-scale image-text datasets, while they exhibit\nsignificant vulnerabilities to backdoor attacks, raising serious safety\nconcerns. In this paper, we reveal that CLIP's vulnerabilities primarily stem\nfrom its tendency to encode features beyond in-dataset predictive patterns,\ncompromising its visual feature resistivity to input perturbations. This makes\nits encoded features highly susceptible to being reshaped by backdoor triggers.\nTo address this challenge, we propose Repulsive Visual Prompt Tuning (RVPT), a\nnovel defense approach that employs deep visual prompt tuning with a specially\ndesigned feature-repelling loss. Specifically, RVPT adversarially repels the\nencoded features from deeper layers while optimizing the standard cross-entropy\nloss, ensuring that only predictive features in downstream tasks are encoded,\nthereby enhancing CLIP's visual feature resistivity against input perturbations\nand mitigating its susceptibility to backdoor attacks. Unlike existing\nmultimodal backdoor defense methods that typically require the availability of\npoisoned data or involve fine-tuning the entire model, RVPT leverages few-shot\ndownstream clean samples and only tunes a small number of parameters. Empirical\nresults demonstrate that RVPT tunes only 0.27\\% of the parameters in CLIP, yet\nit significantly outperforms state-of-the-art defense methods, reducing the\nattack success rate from 89.70\\% to 2.76\\% against the most advanced multimodal\nattacks on ImageNet and effectively generalizes its defensive capabilities\nacross multiple datasets.\n","authors":["Zhifang Zhang","Shuo He","Haobo Wang","Bingquan Shen","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2412.20392v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26213v1","updated":"2025-10-30T07:39:54Z","published":"2025-10-30T07:39:54Z","title":"OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal\n  Document Layout Generation","summary":"  Document AI has advanced rapidly and is attracting increasing attention. Yet,\nwhile most efforts have focused on document layout analysis (DLA), its\ngenerative counterpart, document layout generation, remains underexplored. A\nmajor obstacle lies in the scarcity of diverse layouts: academic papers with\nManhattan-style structures dominate existing studies, while open-world genres\nsuch as newspapers and magazines remain severely underrepresented. To address\nthis gap, we curate OmniLayout-1M, the first million-scale dataset of diverse\ndocument layouts, covering six common document types and comprising\ncontemporary layouts collected from multiple sources. Moreover, since existing\nmethods struggle in complex domains and often fail to arrange long sequences\ncoherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage\nCoarse-to-Fine learning paradigm: 1) learning universal layout principles from\nOmniLayout-1M with coarse category definitions, and 2) transferring the\nknowledge to a specific domain with fine-grained annotations. Extensive\nexperiments demonstrate that our approach achieves strong performance on\nmultiple domains in M$^{6}$Doc dataset, substantially surpassing both existing\nlayout generation experts and several latest general-purpose LLMs. Our code,\nmodels, and dataset will be publicly released.\n","authors":["Hengrui Kang","Zhuangcheng Gu","Zhiyuan Zhao","Zichen Wen","Bin Wang","Weijia Li","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2510.26213v1.pdf","comment":"TL;DR: With OmniLayout-1M dataset and LLM-based coarse-to-fine\n  learning, we enable universal and diverse document layout generation"},{"id":"http://arxiv.org/abs/2510.23588v2","updated":"2025-10-30T07:38:54Z","published":"2025-10-27T17:54:08Z","title":"FARMER: Flow AutoRegressive Transformer over Pixels","summary":"  Directly modeling the explicit likelihood of the raw data distribution is key\ntopic in the machine learning area, which achieves the scaling successes in\nLarge Language Models by autoregressive modeling. However, continuous AR\nmodeling over visual pixel data suffer from extremely long sequences and\nhigh-dimensional spaces. In this paper, we present FARMER, a novel end-to-end\ngenerative framework that unifies Normalizing Flows (NF) and Autoregressive\n(AR) models for tractable likelihood estimation and high-quality image\nsynthesis directly from raw pixels. FARMER employs an invertible autoregressive\nflow to transform images into latent sequences, whose distribution is modeled\nimplicitly by an autoregressive model. To address the redundancy and complexity\nin pixel-level modeling, we propose a self-supervised dimension reduction\nscheme that partitions NF latent channels into informative and redundant\ngroups, enabling more effective and efficient AR modeling. Furthermore, we\ndesign a one-step distillation scheme to significantly accelerate inference\nspeed and introduce a resampling-based classifier-free guidance algorithm to\nboost image generation quality. Extensive experiments demonstrate that FARMER\nachieves competitive performance compared to existing pixel-based generative\nmodels while providing exact likelihoods and scalable training.\n","authors":["Guangting Zheng","Qinyu Zhao","Tao Yang","Fei Xiao","Zhijie Lin","Jie Wu","Jiajun Deng","Yanyong Zhang","Rui Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.23588v2.pdf","comment":"Bytedance Seed Technical Report"},{"id":"http://arxiv.org/abs/2510.26203v1","updated":"2025-10-30T07:26:18Z","published":"2025-10-30T07:26:18Z","title":"Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain\n  Sustainability and Risk Management","summary":"  The sustainability of supply chain plays a key role in achieving optimal\nperformance in controlling the supply chain. The management of risks that occur\nin a supply chain is a fundamental problem for the purpose of developing the\nsustainability of the network and elevating the performance efficiency of the\nsupply chain. The correct classification of products is another essential\nelement in a sustainable supply chain. Acknowledging recent breakthroughs in\nthe context of deep networks, several architectural options have been deployed\nto analyze supply chain datasets. A novel geometric deep network is used to\npropose an ensemble deep network. The proposed Chebyshev ensemble geometric\nnetwork (Ch-EGN) is a hybrid convolutional and geometric deep learning. This\nnetwork is proposed to leverage the information dependencies in supply chain to\nderive invisible states of samples in the database. The functionality of the\nproposed deep network is assessed on the two different databases. The\nSupplyGraph Dataset and DataCo are considered in this research. The prediction\nof delivery status of DataCo supply chain is done for risk administration. The\nproduct classification and edge classification are performed using the\nSupplyGraph database to enhance the sustainability of the supply network. An\naverage accuracy of 98.95% is obtained for the ensemble network for risk\nmanagement. The average accuracy of 100% and 98.07% are obtained for\nsustainable supply chain in terms of 5 product group classification and 4\nproduct relation classification, respectively. The average accuracy of 92.37%\nis attained for 25 company relation classification. The results confirm an\naverage improvement and efficiency of the proposed method compared to the\nstate-of-the-art approaches.\n","authors":["Mehdi Khaleghi","Nastaran Khaleghi","Sobhan Sheykhivand","Sebelan Danishvar"],"pdf_url":"https://arxiv.org/pdf/2510.26203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10173v2","updated":"2025-10-30T07:25:20Z","published":"2025-06-11T20:53:45Z","title":"SPARKE: Scalable Prompt-Aware Diversity and Novelty Guidance in\n  Diffusion Models via RKE Score","summary":"  Diffusion models have demonstrated remarkable success in high-fidelity image\nsynthesis and prompt-guided generative modeling. However, ensuring adequate\ndiversity in generated samples of prompt-guided diffusion models remains a\nchallenge, particularly when the prompts span a broad semantic spectrum and the\ndiversity of generated data needs to be evaluated in a prompt-aware fashion\nacross semantically similar prompts. Recent methods have introduced guidance\nvia diversity measures to encourage more varied generations. In this work, we\nextend the diversity measure-based approaches by proposing the Scalable\nPrompt-Aware R\\'eny Kernel Entropy Diversity Guidance (SPARKE) method for\nprompt-aware diversity guidance. SPARKE utilizes conditional entropy for\ndiversity guidance, which dynamically conditions diversity measurement on\nsimilar prompts and enables prompt-aware diversity control. While the\nentropy-based guidance approach enhances prompt-aware diversity, its reliance\non the matrix-based entropy scores poses computational challenges in\nlarge-scale generation settings. To address this, we focus on the special case\nof Conditional latent RKE Score Guidance, reducing entropy computation and\ngradient-based optimization complexity from the $O(n^3)$ of general entropy\nmeasures to $O(n)$. The reduced computational complexity allows for\ndiversity-guided sampling over potentially thousands of generation rounds on\ndifferent prompts. We numerically test the SPARKE method on several\ntext-to-image diffusion models, demonstrating that the proposed method improves\nthe prompt-aware diversity of the generated data without incurring significant\ncomputational costs. We release our code on the project page:\nhttps://mjalali.github.io/SPARKE\n","authors":["Mohammad Jalali","Haoyu Lei","Amin Gohari","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2506.10173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.14431v3","updated":"2025-10-30T07:17:56Z","published":"2025-10-16T08:31:44Z","title":"Real-Time Neural Video Compression with Unified Intra and Inter Coding","summary":"  Neural video compression (NVC) technologies have advanced rapidly in recent\nyears, yielding state-of-the-art schemes such as DCVC-RT that offer superior\ncompression efficiency to H.266/VVC and real-time encoding/decoding\ncapabilities. Nonetheless, existing NVC schemes have several limitations,\nincluding inefficiency in dealing with disocclusion and new content, interframe\nerror propagation and accumulation, among others. To eliminate these\nlimitations, we borrow the idea from classic video coding schemes, which allow\nintra coding within inter-coded frames. With the intra coding tool enabled,\ndisocclusion and new content are properly handled, and interframe error\npropagation is naturally intercepted without the need for manual refresh\nmechanisms. We present an NVC framework with unified intra and inter coding,\nwhere every frame is processed by a single model that is trained to perform\nintra/inter coding adaptively. Moreover, we propose a simultaneous two-frame\ncompression design to exploit interframe redundancy not only forwardly but also\nbackwardly. Experimental results show that our scheme outperforms DCVC-RT by an\naverage of 12.1% BD-rate reduction, delivers more stable bitrate and quality\nper frame, and retains real-time encoding/decoding performances. Code and\nmodels will be released.\n","authors":["Hui Xiang","Yifan Bian","Li Li","Jingran Wu","Xianguo Zhang","Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.14431v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2510.26196v1","updated":"2025-10-30T07:13:46Z","published":"2025-10-30T07:13:46Z","title":"Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose\n  Prediction","summary":"  3D human pose estimation from sketches has broad applications in computer\nanimation and film production. Unlike traditional human pose estimation, this\ntask presents unique challenges due to the abstract and disproportionate nature\nof sketches. Previous sketch-to-pose methods, constrained by the lack of\nlarge-scale sketch-3D pose annotations, primarily relied on optimization with\nheuristic rules-an approach that is both time-consuming and limited in\ngeneralizability. To address these challenges, we propose a novel approach\nleveraging a \"learn from synthesis\" strategy. First, a diffusion model is\ntrained to synthesize sketch images from 2D poses projected from 3D human\nposes, mimicking disproportionate human structures in sketches. This process\nenables the creation of a synthetic dataset, SKEP-120K, consisting of 120k\naccurate sketch-3D pose annotation pairs across various sketch styles. Building\non this synthetic dataset, we introduce an end-to-end data-driven framework for\nestimating human poses and shapes from diverse sketch styles. Our framework\ncombines existing 2D pose detectors and generative diffusion priors for sketch\nfeature extraction with a feed-forward neural network for efficient 2D pose\nestimation. Multiple heuristic loss functions are incorporated to guarantee\ngeometric coherence between the derived 3D poses and the detected 2D poses\nwhile preserving accurate self-contacts. Qualitative, quantitative, and\nsubjective evaluations collectively show that our model substantially surpasses\nprevious ones in both estimation accuracy and speed for sketch-to-pose tasks.\n","authors":["Li Wang","Yiyu Zhuang","Yanwen Wang","Xun Cao","Chuan Guo","Xinxin Zuo","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.26196v1.pdf","comment":"SIGGRAPH Asia 2025"},{"id":"http://arxiv.org/abs/2510.23981v2","updated":"2025-10-30T07:09:32Z","published":"2025-10-28T01:24:24Z","title":"TeleEgo: Benchmarking Egocentric AI Assistants in the Wild","summary":"  Egocentric AI assistants in real-world settings must process multi-modal\ninputs (video, audio, text), respond in real time, and retain evolving\nlong-term memory. However, existing benchmarks typically evaluate these\nabilities in isolation, lack realistic streaming scenarios, or support only\nshort-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming,\nomni-modal benchmark for evaluating egocentric AI assistants in realistic daily\ncontexts. The dataset features over 14 hours per participant of synchronized\negocentric video, audio, and text across four domains: work \\& study, lifestyle\n\\& routines, social activities, and outings \\& culture. All data is aligned on\na unified global timeline and includes high-quality visual narrations and\nspeech transcripts, curated through human refinement.TeleEgo defines 12\ndiagnostic subtasks across three core capabilities: Memory (recalling past\nevents), Understanding (interpreting the current moment), and Cross-Memory\nReasoning (linking distant events). It contains 3,291 human-verified QA items\nspanning multiple question formats (single-choice, binary, multi-choice, and\nopen-ended), evaluated strictly in a streaming setting. We propose two key\nmetrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess\ncorrectness, temporal responsiveness, and long-term retention. TeleEgo provides\na realistic and comprehensive evaluation to advance the development of\npractical AI assistants.\n","authors":["Jiaqi Yan","Ruilong Ren","Jingren Liu","Shuning Xu","Ling Wang","Yiheng Wang","Yun Wang","Long Zhang","Xiangyu Chen","Changzhi Sun","Jixiang Luo","Dell Zhang","Hao Sun","Chi Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2510.23981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26186v1","updated":"2025-10-30T06:46:17Z","published":"2025-10-30T06:46:17Z","title":"ConceptScope: Characterizing Dataset Bias via Disentangled Visual\n  Concepts","summary":"  Dataset bias, where data points are skewed to certain concepts, is ubiquitous\nin machine learning datasets. Yet, systematically identifying these biases is\nchallenging without costly, fine-grained attribute annotations. We present\nConceptScope, a scalable and automated framework for analyzing visual datasets\nby discovering and quantifying human-interpretable concepts using Sparse\nAutoencoders trained on representations from vision foundation models.\nConceptScope categorizes concepts into target, context, and bias types based on\ntheir semantic relevance and statistical correlation to class labels, enabling\nclass-level dataset characterization, bias identification, and robustness\nevaluation through concept-based subgrouping. We validate that ConceptScope\ncaptures a wide range of visual concepts, including objects, textures,\nbackgrounds, facial attributes, emotions, and actions, through comparisons with\nannotated datasets. Furthermore, we show that concept activations produce\nspatial attributions that align with semantically meaningful image regions.\nConceptScope reliably detects known biases (e.g., background bias in\nWaterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects\nin ImageNet), offering a practical tool for dataset auditing and model\ndiagnostics.\n","authors":["Jinho Choi","Hyesu Lim","Steffen Schneider","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2510.26186v1.pdf","comment":"Published in the Thirty-Ninth Conference on Neural Information\n  Processing Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2505.16239v2","updated":"2025-10-30T06:40:44Z","published":"2025-05-22T05:16:45Z","title":"DOVE: Efficient One-Step Diffusion Model for Real-World Video\n  Super-Resolution","summary":"  Diffusion models have demonstrated promising performance in real-world video\nsuper-resolution (VSR). However, the dozens of sampling steps they require,\nmake inference extremely slow. Sampling acceleration techniques, particularly\nsingle-step, provide a potential solution. Nonetheless, achieving one step in\nVSR remains challenging, due to the high training overhead on video data and\nstringent fidelity demands. To tackle the above issues, we propose DOVE, an\nefficient one-step diffusion model for real-world VSR. DOVE is obtained by\nfine-tuning a pretrained video diffusion model (i.e., CogVideoX). To\neffectively train DOVE, we introduce the latent-pixel training strategy. The\nstrategy employs a two-stage scheme to gradually adapt the model to the video\nsuper-resolution task. Meanwhile, we design a video processing pipeline to\nconstruct a high-quality dataset tailored for VSR, termed HQ-VSR. Fine-tuning\non this dataset further enhances the restoration capability of DOVE. Extensive\nexperiments show that DOVE exhibits comparable or superior performance to\nmulti-step diffusion-based VSR methods. It also offers outstanding inference\nefficiency, achieving up to a 28$\\times$ speed-up over existing methods such as\nMGLD-VSR. Code is available at: https://github.com/zhengchen1999/DOVE.\n","authors":["Zheng Chen","Zichen Zou","Kewei Zhang","Xiongfei Su","Xin Yuan","Yong Guo","Yulun Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.16239v2.pdf","comment":"Accepted to NeurIPS 2025. Code is available at:\n  https://github.com/zhengchen1999/DOVE"},{"id":"http://arxiv.org/abs/2510.26173v1","updated":"2025-10-30T06:24:02Z","published":"2025-10-30T06:24:02Z","title":"MoTDiff: High-resolution Motion Trajectory estimation from a single\n  blurred image using Diffusion models","summary":"  Accurate estimation of motion information is crucial in diverse computational\nimaging and computer vision applications. Researchers have investigated various\nmethods to extract motion information from a single blurred image, including\nblur kernels and optical flow. However, existing motion representations are\noften of low quality, i.e., coarse-grained and inaccurate. In this paper, we\npropose the first high-resolution (HR) Motion Trajectory estimation framework\nusing Diffusion models (MoTDiff). Different from existing motion\nrepresentations, we aim to estimate an HR motion trajectory with high-quality\nfrom a single motion-blurred image. The proposed MoTDiff consists of two key\ncomponents: 1) a new conditional diffusion framework that uses multi-scale\nfeature maps extracted from a single blurred image as a condition, and 2) a new\ntraining method that can promote precise identification of a fine-grained\nmotion trajectory, consistent estimation of overall shape and position of a\nmotion path, and pixel connectivity along a motion trajectory. Our experiments\ndemonstrate that the proposed MoTDiff can outperform state-of-the-art methods\nin both blind image deblurring and coded exposure photography applications.\n","authors":["Wontae Choi","Jaelin Lee","Hyung Sup Yun","Byeungwoo Jeon","Il Yong Chun"],"pdf_url":"https://arxiv.org/pdf/2510.26173v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.13160v2","updated":"2025-10-30T06:21:47Z","published":"2025-03-17T13:31:19Z","title":"Language-guided Open-world Video Anomaly Detection under Weak\n  Supervision","summary":"  Video anomaly detection (VAD) aims to detect anomalies that deviate from what\nis expected. In open-world scenarios, the expected events may change as\nrequirements change. For example, not wearing a mask may be considered abnormal\nduring a flu outbreak but normal otherwise. However, existing methods assume\nthat the definition of anomalies is invariable, and thus are not applicable to\nthe open world. To address this, we propose a novel open-world VAD paradigm\nwith variable definitions, allowing guided detection through user-provided\nnatural language at inference time. This paradigm necessitates establishing a\nrobust mapping from video and textual definition to anomaly scores. Therefore,\nwe propose LaGoVAD (Language-guided Open-world Video Anomaly Detector), a model\nthat dynamically adapts anomaly definitions under weak supervision with two\nregularization strategies: diversifying the relative durations of anomalies via\ndynamic video synthesis, and enhancing feature robustness through contrastive\nlearning with negative mining. Training such adaptable models requires diverse\nanomaly definitions, but existing datasets typically provide labels without\nsemantic descriptions. To bridge this gap, we collect PreVAD (Pre-training\nVideo Anomaly Dataset), the largest and most diverse video anomaly dataset to\ndate, featuring 35,279 annotated videos with multi-level category labels and\ndescriptions that explicitly define anomalies. Zero-shot experiments on seven\ndatasets demonstrate LaGoVAD's SOTA performance. Our dataset and code will be\nreleased at https://github.com/Kamino666/LaGoVAD-PreVAD.\n","authors":["Zihao Liu","Xiaoyu Wu","Jianqin Wu","Xuxu Wang","Linlin Yang"],"pdf_url":"https://arxiv.org/pdf/2503.13160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26170v1","updated":"2025-10-30T06:14:22Z","published":"2025-10-30T06:14:22Z","title":"Self-localization on a 3D map by fusing global and local features from a\n  monocular camera","summary":"  Self-localization on a 3D map by using an inexpensive monocular camera is\nrequired to realize autonomous driving. Self-localization based on a camera\noften uses a convolutional neural network (CNN) that can extract local features\nthat are calculated by nearby pixels. However, when dynamic obstacles, such as\npeople, are present, CNN does not work well. This study proposes a new method\ncombining CNN with Vision Transformer, which excels at extracting global\nfeatures that show the relationship of patches on whole image. Experimental\nresults showed that, compared to the state-of-the-art method (SOTA), the\naccuracy improvement rate in a CG dataset with dynamic obstacles is 1.5 times\nhigher than that without dynamic obstacles. Moreover, the self-localization\nerror of our method is 20.1% smaller than that of SOTA on public datasets.\nAdditionally, our robot using our method can localize itself with 7.51cm error\non average, which is more accurate than SOTA.\n","authors":["Satoshi Kikuch","Masaya Kato","Tsuyoshi Tasaki"],"pdf_url":"https://arxiv.org/pdf/2510.26170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26160v1","updated":"2025-10-30T05:50:48Z","published":"2025-10-30T05:50:48Z","title":"CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark","summary":"  Wearable devices such as smart glasses are transforming the way people\ninteract with their surroundings, enabling users to seek information regarding\nentities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)\nplays a key role in supporting such questions, yet there is still no\ncomprehensive benchmark for this task, especially regarding wearables\nscenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG\nbenchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse\nset of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn\nconversations across 13 domains, including 6.2K egocentric images designed to\nmimic captures from wearable devices. We carefully constructed the questions to\nreflect real-world scenarios and challenges, including five types of\nimage-quality issues, six question types, varying entity popularity, differing\ninformation dynamism, and different conversation turns. We design three tasks:\nsingle-source augmentation, multi-source augmentation, and multi-turn\nconversations -- each paired with an associated retrieval corpus and APIs for\nboth image-KG retrieval and webpage retrieval. Our evaluation shows that\nstraightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM\nsingle- and multi-turn QA, respectively, whereas state-of-the-art industry\nsolutions have similar quality (32%/45%), underscoring ample room for\nimprovement. The benchmark has hosted KDD Cup 2025, attracting about 1K\nparticipants and 5K submissions, with winning solutions improving baseline\nperformance by 28%, highlighting its early impact on advancing the field.\n","authors":["Jiaqi Wang","Xiao Yang","Kai Sun","Parth Suresh","Sanat Sharma","Adam Czyzewski","Derek Andersen","Surya Appini","Arkav Banerjee","Sajal Choudhary","Shervin Ghasemlou","Ziqiang Guan","Akil Iyer","Haidar Khan","Lingkun Kong","Roy Luo","Tiffany Ma","Zhen Qiao","David Tran","Wenfang Xu","Skyler Yeatman","Chen Zhou","Gunveer Gujral","Yinglong Xia","Shane Moon","Nicolas Scheffer","Nirav Shah","Eun Chang","Yue Liu","Florian Metze","Tammy Stark","Zhaleh Feizollahi","Andrea Jessee","Mangesh Pujari","Ahmed Aly","Babak Damavandi","Rakesh Wanga","Anuj Kumar","Rohit Patel","Wen-tau Yih","Xin Luna Dong"],"pdf_url":"https://arxiv.org/pdf/2510.26160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26154v1","updated":"2025-10-30T05:20:36Z","published":"2025-10-30T05:20:36Z","title":"Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A\n  Case Study on Bangladesh","summary":"  Modes of transportation vary across countries depending on geographical\nlocation and cultural context. In South Asian countries rickshaws are among the\nmost common means of local transport. Based on their mode of operation,\nrickshaws in cities across Bangladesh can be broadly classified into non-auto\n(pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of\nauto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from\naccessing certain routes. However, existing surveillance systems make it quite\ndifficult to monitor them due to their similarity to other vehicles, especially\nnon-auto rickshaws whereas manual video analysis is too time-consuming. This\npaper presents a machine learning-based approach to automatically detect\nauto-rickshaws in traffic images. In this system, we used real-time object\ndetection using the YOLOv8 model. For training purposes, we prepared a set of\n1,730 annotated images that were captured under various traffic conditions. The\nresults show that our proposed model performs well in real-time auto-rickshaw\ndetection and offers an mAP50 of 83.447% and binary precision and recall values\nabove 78%, demonstrating its effectiveness in handling both dense and sparse\ntraffic scenarios. The dataset has been publicly released for further research.\n","authors":["Sudipto Das Sukanto","Diponker Roy","Fahim Shakil","Nirjhar Singha","Abdullah Asik","Aniket Joarder","Mridha Md Nafis Fuad","Muhammad Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2510.26154v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2510.21271v2","updated":"2025-10-30T05:16:33Z","published":"2025-10-24T09:12:59Z","title":"Buffer layers for Test-Time Adaptation","summary":"  In recent advancements in Test Time Adaptation (TTA), most existing\nmethodologies focus on updating normalization layers to adapt to the test\ndomain. However, the reliance on normalization-based adaptation presents key\nchallenges. First, normalization layers such as Batch Normalization (BN) are\nhighly sensitive to small batch sizes, leading to unstable and inaccurate\nstatistics. Moreover, normalization-based adaptation is inherently constrained\nby the structure of the pre-trained model, as it relies on training-time\nstatistics that may not generalize well to unseen domains. These issues limit\nthe effectiveness of normalization-based TTA approaches, especially under\nsignificant domain shift. In this paper, we introduce a novel paradigm based on\nthe concept of a Buffer layer, which addresses the fundamental limitations of\nnormalization layer updates. Unlike existing methods that modify the core\nparameters of the model, our approach preserves the integrity of the\npre-trained backbone, inherently mitigating the risk of catastrophic forgetting\nduring online adaptation. Through comprehensive experimentation, we demonstrate\nthat our approach not only outperforms traditional methods in mitigating domain\nshift and enhancing model robustness, but also exhibits strong resilience to\nforgetting. Furthermore, our Buffer layer is modular and can be seamlessly\nintegrated into nearly all existing TTA frameworks, resulting in consistent\nperformance improvements across various architectures. These findings validate\nthe effectiveness and versatility of the proposed solution in real-world domain\nadaptation scenarios. The code is available at\nhttps://github.com/hyeongyu-kim/Buffer_TTA.\n","authors":["Hyeongyu Kim","Geonhui Han","Dosik Hwang"],"pdf_url":"https://arxiv.org/pdf/2510.21271v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26151v1","updated":"2025-10-30T05:12:29Z","published":"2025-10-30T05:12:29Z","title":"MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer\n  Diagnosis and Risk Prediction","summary":"  Large annotated datasets are essential for training robust Computer-Aided\nDiagnosis (CAD) models for breast cancer detection or risk prediction. However,\nacquiring such datasets with fine-detailed annotation is both costly and\ntime-consuming. Vision-Language Models (VLMs), such as CLIP, which are\npre-trained on large image-text pairs, offer a promising solution by enhancing\nrobustness and data efficiency in medical imaging tasks. This paper introduces\na novel Multi-View Mammography and Language Model for breast cancer\nclassification and risk prediction, trained on a dataset of paired mammogram\nimages and synthetic radiology reports. Our MV-MLM leverages multi-view\nsupervision to learn rich representations from extensive radiology data by\nemploying cross-modal self-supervision across image-text pairs. This includes\nmultiple views and the corresponding pseudo-radiology reports. We propose a\nnovel joint visual-textual learning strategy to enhance generalization and\naccuracy performance over different data types and tasks to distinguish breast\ntissues or cancer characteristics(calcification, mass) and utilize these\npatterns to understand mammography images and predict cancer risk. We evaluated\nour method on both private and publicly available datasets, demonstrating that\nthe proposed model achieves state-of-the-art performance in three\nclassification tasks: (1) malignancy classification, (2) subtype\nclassification, and (3) image-based cancer risk prediction. Furthermore, the\nmodel exhibits strong data efficiency, outperforming existing fully supervised\nor VLM baselines while trained on synthetic text reports and without the need\nfor actual radiology reports.\n","authors":["Shunjie-Fabian Zheng","Hyeonjun Lee","Thijs Kooi","Ali Diba"],"pdf_url":"https://arxiv.org/pdf/2510.26151v1.pdf","comment":"Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)\n  Workshop at ICCV 2025"},{"id":"http://arxiv.org/abs/2510.26149v1","updated":"2025-10-30T05:08:45Z","published":"2025-10-30T05:08:45Z","title":"BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and\n  Enhanced Motion Compensation","summary":"  Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution\nof video frames, potentially at various scaling factors, which presents several\nchallenges regarding spatial detail reproduction, temporal consistency, and\ncomputational complexity. In this paper, we propose a strong baseline BasicAVSR\nfor AVSR by integrating four key components: 1) adaptive multi-scale frequency\npriors generated from image Laplacian pyramids, 2) a flow-guided propagation\nunit to aggregate spatiotemporal information from adjacent frames, 3) a\nsecond-order motion compensation unit for more accurate spatial alignment of\nadjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and\ncontent-independent upsampling kernels. To meet diverse application demands, we\ninstantiate three propagation variants: (i) a unidirectional RNN unit for\nstrictly online inference, (ii) a unidirectional RNN unit empowered with a\nlimited lookahead that tolerates a small output delay, and (iii) a\nbidirectional RNN unit designed for offline tasks where computational resources\nare less constrained. Experimental results demonstrate the effectiveness and\nadaptability of our model across these different scenarios. Through extensive\nexperiments, we show that BasicAVSR significantly outperforms existing methods\nin terms of super-resolution quality, generalization ability, and inference\nspeed. Our work not only advances the state-of-the-art in AVSR but also extends\nits core components to multiple frameworks for diverse scenarios. The code is\navailable at https://github.com/shangwei5/BasicAVSR.\n","authors":["Wei Shang","Wanying Zhang","Shuhang Gu","Pengfei Zhu","Qinghua Hu","Dongwei Ren"],"pdf_url":"https://arxiv.org/pdf/2510.26149v1.pdf","comment":"13 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2506.00871v2","updated":"2025-10-30T05:04:19Z","published":"2025-06-01T07:18:47Z","title":"Towards Predicting Any Human Trajectory In Context","summary":"  Predicting accurate future trajectories of pedestrians is essential for\nautonomous systems but remains a challenging task due to the need for\nadaptability in different environments and domains. A common approach involves\ncollecting scenario-specific data and performing fine-tuning via\nbackpropagation. However, the need to fine-tune for each new scenario is often\nimpractical for deployment on edge devices. To address this challenge, we\nintroduce \\paper, an In-Context Learning (ICL) framework for pedestrian\ntrajectory prediction that enables adaptation without fine-tuning on the\nscenario-specific data at inference time without requiring weight updates. We\npropose a spatio-temporal similarity-based example selection (STES) method that\nselects relevant examples from previously observed trajectories within the same\nscene by identifying similar motion patterns at corresponding locations. To\nfurther refine this selection, we introduce prediction-guided example selection\n(PG-ES), which selects examples based on both the past trajectory and the\npredicted future trajectory, rather than relying solely on the past trajectory.\nThis approach allows the model to account for long-term dynamics when selecting\nexamples. Finally, instead of relying on small real-world datasets with limited\nscenario diversity, we train our model on a large-scale synthetic dataset to\nenhance its prediction ability by leveraging in-context examples. Extensive\nexperiments demonstrate that TrajICL achieves remarkable adaptation across both\nin-domain and cross-domain scenarios, outperforming even fine-tuned approaches\nacross multiple public benchmarks. Project Page:\nhttps://fujiry0.github.io/TrajICL-project-page/.\n","authors":["Ryo Fujii","Hideo Saito","Ryo Hachiuma"],"pdf_url":"https://arxiv.org/pdf/2506.00871v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.16396v3","updated":"2025-10-30T04:59:32Z","published":"2025-10-18T08:19:49Z","title":"SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation","summary":"  With the increasing ubiquity of AR/VR devices, the deployment of deep\nlearning models on edge devices has become a critical challenge. These devices\nrequire real-time inference, low power consumption, and minimal latency. Many\nframework designers face the conundrum of balancing efficiency and performance.\nWe design a light framework that adopts an encoder-decoder architecture and\nintroduces several key contributions aimed at improving both efficiency and\naccuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the\ninherent sparsity in hand pose images, achieving a 42% end-to-end efficiency\nimprovement. Moreover, we propose our SPLite decoder. This new architecture\nsignificantly boosts the decoding process's frame rate by 3.1x on the Raspberry\nPi 5, while maintaining accuracy on par. To further optimize performance, we\napply quantization-aware training, reducing memory usage while preserving\naccuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on\nFreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5\nCPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on\ncompound benchmark datasets, demonstrating comparable accuracy to\nstate-of-the-art approaches while significantly enhancing computational\nefficiency.\n","authors":["Yeh Keng Hao","Hsu Tzu Wei","Sun Min"],"pdf_url":"https://arxiv.org/pdf/2510.16396v3.pdf","comment":"Accepted to AICCC 2025"},{"id":"http://arxiv.org/abs/2510.26141v1","updated":"2025-10-30T04:52:12Z","published":"2025-10-30T04:52:12Z","title":"StructLayoutFormer:Conditional Structured Layout Generation via\n  Structure Serialization and Disentanglement","summary":"  Structured layouts are preferable in many 2D visual contents (\\eg, GUIs,\nwebpages) since the structural information allows convenient layout editing.\nComputational frameworks can help create structured layouts but require heavy\nlabor input. Existing data-driven approaches are effective in automatically\ngenerating fixed layouts but fail to produce layout structures. We present\nStructLayoutFormer, a novel Transformer-based approach for conditional\nstructured layout generation. We use a structure serialization scheme to\nrepresent structured layouts as sequences. To better control the structures of\ngenerated layouts, we disentangle the structural information from the element\nplacements. Our approach is the first data-driven approach that achieves\nconditional structured layout generation and produces realistic layout\nstructures explicitly. We compare our approach with existing data-driven layout\ngeneration approaches by including post-processing for structure extraction.\nExtensive experiments have shown that our approach exceeds these baselines in\nconditional structured layout generation. We also demonstrate that our approach\nis effective in extracting and transferring layout structures. The code is\npublicly available at %\\href{https://github.com/Teagrus/StructLayoutFormer}\n{https://github.com/Teagrus/StructLayoutFormer}.\n","authors":["Xin Hu","Pengfei Xu","Jin Zhou","Hongbo Fu","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26140v1","updated":"2025-10-30T04:51:05Z","published":"2025-10-30T04:51:05Z","title":"FullPart: Generating each 3D Part at Full Resolution","summary":"  Part-based 3D generation holds great potential for various applications.\nPrevious part generators that represent parts using implicit vector-set tokens\noften suffer from insufficient geometric details. Another line of work adopts\nan explicit voxel representation but shares a global voxel grid among all\nparts; this often causes small parts to occupy too few voxels, leading to\ndegraded quality. In this paper, we propose FullPart, a novel framework that\ncombines both implicit and explicit paradigms. It first derives the bounding\nbox layout through an implicit box vector-set diffusion process, a task that\nimplicit diffusion handles effectively since box tokens contain little\ngeometric detail. Then, it generates detailed parts, each within its own fixed\nfull-resolution voxel grid. Instead of sharing a global low-resolution space,\neach part in our method - even small ones - is generated at full resolution,\nenabling the synthesis of intricate details. We further introduce a\ncenter-point encoding strategy to address the misalignment issue when\nexchanging information between parts of different actual sizes, thereby\nmaintaining global coherence. Moreover, to tackle the scarcity of reliable part\ndata, we present PartVerse-XL, the largest human-annotated 3D part dataset to\ndate with 40K objects and 320K parts. Extensive experiments demonstrate that\nFullPart achieves state-of-the-art results in 3D part generation. We will\nrelease all code, data, and model to benefit future research in 3D part\ngeneration.\n","authors":["Lihe Ding","Shaocong Dong","Yaokun Li","Chenjian Gao","Xiao Chen","Rui Han","Yihao Kuang","Hong Zhang","Bo Huang","Zhanpeng Huang","Zibin Wang","Dan Xu","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2510.26140v1.pdf","comment":"Project page: https://fullpart3d.github.io"},{"id":"http://arxiv.org/abs/2510.26131v1","updated":"2025-10-30T04:31:56Z","published":"2025-10-30T04:31:56Z","title":"Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM","summary":"  Attention models have recently emerged as a powerful approach, demonstrating\nsignificant progress in various fields. Visualization techniques, such as class\nactivation mapping, provide visual insights into the reasoning of convolutional\nneural networks (CNNs). Using network gradients, it is possible to identify\nregions where the network pays attention during image recognition tasks.\nFurthermore, these gradients can be combined with CNN features to localize more\ngeneralizable, task-specific attentive (salient) regions within scenes.\nHowever, explicit use of this gradient-based attention information integrated\ndirectly into CNN representations for semantic object understanding remains\nlimited. Such integration is particularly beneficial for visual tasks like\nsimultaneous localization and mapping (SLAM), where CNN representations\nenriched with spatially attentive object locations can enhance performance. In\nthis work, we propose utilizing task-specific network attention for RGB-D\nindoor SLAM. Specifically, we integrate layer-wise attention information\nderived from network gradients with CNN feature representations to improve\nframe association performance. Experimental results indicate improved\nperformance compared to baseline methods, particularly for large environments.\n","authors":["Ali Caglayan","Nevrez Imamoglu","Oguzhan Guclu","Ali Osman Serhatoglu","Ahmet Burak Can","Ryosuke Nakamura"],"pdf_url":"https://arxiv.org/pdf/2510.26131v1.pdf","comment":"double-column 5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2510.26125v1","updated":"2025-10-30T04:25:33Z","published":"2025-10-30T04:25:33Z","title":"WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging\n  Long-tail Scenarios","summary":"  Vision-based end-to-end (E2E) driving has garnered significant interest in\nthe research community due to its scalability and synergy with multimodal large\nlanguage models (MLLMs). However, current E2E driving benchmarks primarily\nfeature nominal scenarios, failing to adequately test the true potential of\nthese systems. Furthermore, existing open-loop evaluation metrics often fall\nshort in capturing the multi-modal nature of driving or effectively evaluating\nperformance in long-tail scenarios. To address these gaps, we introduce the\nWaymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021\ndriving segments (approximately 12 hours), specifically curated for challenging\nlong-tail scenarios that that are rare in daily life with an occurring\nfrequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the\nhigh-level routing information, ego states, and 360-degree camera views from 8\nsurrounding cameras. To evaluate the E2E driving performance on these long-tail\nsituations, we propose a novel open-loop evaluation metric: Rater Feedback\nScore (RFS). Unlike conventional metrics that measure the distance between\npredicted way points and the logs, RFS measures how closely the predicted\ntrajectory matches rater-annotated trajectory preference labels. We have\nreleased rater preference labels for all WOD-E2E validation set segments, while\nthe held out test set labels have been used for the 2025 WOD-E2E Challenge.\nThrough our work, we aim to foster state of the art research into\ngeneralizable, robust, and safe end-to-end autonomous driving agents capable of\nhandling complex real-world situations.\n","authors":["Runsheng Xu","Hubert Lin","Wonseok Jeon","Hao Feng","Yuliang Zou","Liting Sun","John Gorman","Kate Tolstaya","Sarah Tang","Brandyn White","Ben Sapp","Mingxing Tan","Jyh-Jing Hwang","Drago Anguelov"],"pdf_url":"https://arxiv.org/pdf/2510.26125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08772v2","updated":"2025-10-30T04:25:25Z","published":"2025-07-11T17:33:18Z","title":"From One to More: Contextual Part Latents for 3D Generation","summary":"  Recent advances in 3D generation have transitioned from multi-view 2D\nrendering approaches to 3D-native latent diffusion frameworks that exploit\ngeometric priors in ground truth data. Despite progress, three key limitations\npersist: (1) Single-latent representations fail to capture complex multi-part\ngeometries, causing detail degradation; (2) Holistic latent coding neglects\npart independence and interrelationships critical for compositional design; (3)\nGlobal conditioning mechanisms lack fine-grained controllability. Inspired by\nhuman 3D design workflows, we propose CoPart - a part-aware diffusion framework\nthat decomposes 3D objects into contextual part latents for coherent multi-part\ngeneration. This paradigm offers three advantages: i) Reduces encoding\ncomplexity through part decomposition; ii) Enables explicit part relationship\nmodeling; iii) Supports part-level conditioning. We further develop a mutual\nguidance strategy to fine-tune pre-trained diffusion models for joint part\nlatent denoising, ensuring both geometric coherence and foundation model\npriors. To enable large-scale training, we construct Partverse - a novel 3D\npart dataset derived from Objaverse through automated mesh segmentation and\nhuman-verified annotations. Extensive experiments demonstrate CoPart's superior\ncapabilities in part-level editing, articulated object generation, and scene\ncomposition with unprecedented controllability.\n","authors":["Shaocong Dong","Lihe Ding","Xiao Chen","Yaokun Li","Yuxin Wang","Yucheng Wang","Qi Wang","Jaehyeok Kim","Chenjian Gao","Zhanpeng Huang","Zibin Wang","Tianfan Xue","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2507.08772v2.pdf","comment":"Project page: https://copart3d.github.io/"},{"id":"http://arxiv.org/abs/2510.26117v1","updated":"2025-10-30T04:00:07Z","published":"2025-10-30T04:00:07Z","title":"JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting","summary":"  Traditional novel view synthesis methods heavily rely on external camera pose\nestimation tools such as COLMAP, which often introduce computational\nbottlenecks and propagate errors. To address these challenges, we propose a\nunified framework that jointly optimizes 3D Gaussian points and camera poses\nwithout requiring pre-calibrated inputs. Our approach iteratively refines 3D\nGaussian parameters and updates camera poses through a novel co-optimization\nstrategy, ensuring simultaneous improvements in scene reconstruction fidelity\nand pose accuracy. The key innovation lies in decoupling the joint optimization\ninto two interleaved phases: first, updating 3D Gaussian parameters via\ndifferentiable rendering with fixed poses, and second, refining camera poses\nusing a customized 3D optical flow algorithm that incorporates geometric and\nphotometric constraints. This formulation progressively reduces projection\nerrors, particularly in challenging scenarios with large viewpoint variations\nand sparse feature distributions, where traditional methods struggle. Extensive\nevaluations on multiple datasets demonstrate that our approach significantly\noutperforms existing COLMAP-free techniques in reconstruction quality, and also\nsurpasses the standard COLMAP-based baseline in general.\n","authors":["Yuxuan Li","Tao Wang","Xianben Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26114v1","updated":"2025-10-30T03:54:53Z","published":"2025-10-30T03:54:53Z","title":"OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script\n  Research","summary":"  As one of the earliest writing systems, Oracle Bone Script (OBS) preserves\nthe cultural and intellectual heritage of ancient civilizations. However,\ncurrent OBS research faces two major challenges: (1) the interpretation of OBS\ninvolves a complex workflow comprising multiple serial and parallel sub-tasks,\nand (2) the efficiency of OBS information organization and retrieval remains a\ncritical bottleneck, as scholars often spend substantial effort searching for,\ncompiling, and managing relevant resources. To address these challenges, we\npresent OracleAgent, the first agent system designed for the structured\nmanagement and retrieval of OBS-related information. OracleAgent seamlessly\nintegrates multiple OBS analysis tools, empowered by large language models\n(LLMs), and can flexibly orchestrate these components. Additionally, we\nconstruct a comprehensive domain-specific multimodal knowledge base for OBS,\nwhich is built through a rigorous multi-year process of data collection,\ncleaning, and expert annotation. The knowledge base comprises over 1.4M\nsingle-character rubbing images and 80K interpretation texts. OracleAgent\nleverages this resource through its multimodal tools to assist experts in\nretrieval tasks of character, document, interpretation text, and rubbing image.\nExtensive experiments demonstrate that OracleAgent achieves superior\nperformance across a range of multimodal reasoning and generation tasks,\nsurpassing leading mainstream multimodal large language models (MLLMs) (e.g.,\nGPT-4o). Furthermore, our case study illustrates that OracleAgent can\neffectively assist domain experts, significantly reducing the time cost of OBS\nresearch. These results highlight OracleAgent as a significant step toward the\npractical deployment of OBS-assisted research and automated interpretation\nsystems.\n","authors":["Caoshuo Li","Zengmao Ding","Xiaobin Hu","Bang Li","Donghao Luo","Xu Peng","Taisong Jin","Yongge Liu","Shengwei Han","Jing Yang","Xiaoping He","Feng Gao","AndyPian Wu"," SevenShu","Chaoyang Wang","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26113v1","updated":"2025-10-30T03:53:22Z","published":"2025-10-30T03:53:22Z","title":"EgoExo-Con: Exploring View-Invariant Video Temporal Understanding","summary":"  Can Video-LLMs achieve consistent temporal understanding when videos capture\nthe same event from different viewpoints? To study this, we introduce\nEgoExo-Con (Consistency), a benchmark of comprehensively synchronized\negocentric and exocentric video pairs with human-refined queries in natural\nlanguage. EgoExo-Con emphasizes two temporal understanding tasks: Temporal\nVerification and Temporal Grounding. It evaluates not only correctness but\nconsistency across viewpoints. Our analysis reveals two critical limitations of\nexisting Video-LLMs: (1) models often fail to maintain consistency, with\nresults far worse than their single-view performances. (2) When naively\nfinetuned with synchronized videos of both viewpoints, the models show improved\nconsistency but often underperform those trained on a single view. For\nimprovements, we propose View-GRPO, a novel reinforcement learning framework\nthat effectively strengthens view-specific temporal reasoning while encouraging\nconsistent comprehension across viewpoints. Our method demonstrates its\nsuperiority over naive SFT and GRPO, especially for improving cross-view\nconsistency. All resources will be made publicly available.\n","authors":["Minjoon Jung","Junbin Xiao","Junghyun Kim","Byoung-Tak Zhang","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2510.26113v1.pdf","comment":"project page:\n  \\url{https://minjoong507.github.io/projects/EgoExo-Con/}"},{"id":"http://arxiv.org/abs/2510.26105v1","updated":"2025-10-30T03:31:20Z","published":"2025-10-30T03:31:20Z","title":"Security Risk of Misalignment between Text and Image in Multi-modal\n  Model","summary":"  Despite the notable advancements and versatility of multi-modal diffusion\nmodels, such as text-to-image models, their susceptibility to adversarial\ninputs remains underexplored. Contrary to expectations, our investigations\nreveal that the alignment between textual and Image modalities in existing\ndiffusion models is inadequate. This misalignment presents significant risks,\nespecially in the generation of inappropriate or Not-Safe-For-Work (NSFW)\ncontent. To this end, we propose a novel attack called Prompt-Restricted\nMulti-modal Attack (PReMA) to manipulate the generated content by modifying the\ninput image in conjunction with any specified prompt, without altering the\nprompt itself. PReMA is the first attack that manipulates model outputs by\nsolely creating adversarial images, distinguishing itself from prior methods\nthat primarily generate adversarial prompts to produce NSFW content.\nConsequently, PReMA poses a novel threat to the integrity of multi-modal\ndiffusion models, particularly in image-editing applications that operate with\nfixed prompts. Comprehensive evaluations conducted on image inpainting and\nstyle transfer tasks across various models confirm the potent efficacy of\nPReMA.\n","authors":["Xiaosen Wang","Zhijin Ge","Shaokang Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.24267v2","updated":"2025-10-30T03:29:32Z","published":"2025-09-29T04:24:13Z","title":"Cycle Diffusion Model for Counterfactual Image Generation","summary":"  Deep generative models have demonstrated remarkable success in medical image\nsynthesis. However, ensuring conditioning faithfulness and high-quality\nsynthetic images for direct or counterfactual generation remains a challenge.\nIn this work, we introduce a cycle training framework to fine-tune diffusion\nmodels for improved conditioning adherence and enhanced synthetic image\nrealism. Our approach, Cycle Diffusion Model (CDM), enforces consistency\nbetween generated and original images by incorporating cycle constraints,\nenabling more reliable direct and counterfactual generation. Experiments on a\ncombined 3D brain MRI dataset (from ABCD, HCP aging & young adults, ADNI, and\nPPMI) show that our method improves conditioning accuracy and enhances image\nquality as measured by FID and SSIM. The results suggest that the cycle\nstrategy used in CDM can be an effective method for refining diffusion-based\nmedical image generation, with applications in data augmentation,\ncounterfactual, and disease progression modeling.\n","authors":["Fangrui Huang","Alan Wang","Binxu Li","Bailey Trang","Ridvan Yesiloglu","Tianyu Hua","Wei Peng","Ehsan Adeli"],"pdf_url":"https://arxiv.org/pdf/2509.24267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00254v4","updated":"2025-10-30T03:12:42Z","published":"2025-05-01T02:40:23Z","title":"Empowering Agentic Video Analytics Systems with Video Language Models","summary":"  AI-driven video analytics has become increasingly important across diverse\ndomains. However, existing systems are often constrained to specific,\npredefined tasks, limiting their adaptability in open-ended analytical\nscenarios. The recent emergence of Vision Language Models (VLMs) as\ntransformative technologies offers significant potential for enabling\nopen-ended video understanding, reasoning, and analytics. Nevertheless, their\nlimited context windows present challenges when processing ultra-long video\ncontent, which is prevalent in real-world applications. To address this, we\nintroduce AVA, a VLM-powered system designed for open-ended, advanced video\nanalytics. AVA incorporates two key innovations: (1) the near real-time\nconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long or\ncontinuous video streams, and (2) an agentic retrieval-generation mechanism\nthat leverages EKGs to handle complex and diverse queries. Comprehensive\nevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that\nAVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,\nrespectively-significantly surpassing existing VLM and video\nRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video\nanalytics in ultra-long and open-world video scenarios, we introduce a new\nbenchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours\nin duration, along with 120 manually annotated, diverse, and complex\nquestion-answer pairs. On AVA-100, AVA achieves top-tier performance with an\naccuracy of 75.8%. The source code of AVA is available at\nhttps://github.com/I-ESC/Project-Ava. The AVA-100 benchmark can be accessed at\nhttps://huggingface.co/datasets/iesc/Ava-100.\n","authors":["Yuxuan Yan","Shiqi Jiang","Ting Cao","Yifan Yang","Qianqian Yang","Yuanchao Shu","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2505.00254v4.pdf","comment":"Accepted to NDSI 2026, 19pages, 12 figures, complementary evaluations\n  and appendix"},{"id":"http://arxiv.org/abs/2505.18584v2","updated":"2025-10-30T02:59:44Z","published":"2025-05-24T08:20:36Z","title":"Unleashing Diffusion Transformers for Visual Correspondence by\n  Modulating Massive Activations","summary":"  Pre-trained stable diffusion models (SD) have shown great advances in visual\ncorrespondence. In this paper, we investigate the capabilities of Diffusion\nTransformers (DiTs) for accurate dense correspondence. Distinct from SD, DiTs\nexhibit a critical phenomenon in which very few feature activations exhibit\nsignificantly larger values than others, known as \\textit{massive activations},\nleading to uninformative representations and significant performance\ndegradation for DiTs. The massive activations consistently concentrate at very\nfew fixed dimensions across all image patch tokens, holding little local\ninformation. We trace these dimension-concentrated massive activations and find\nthat such concentration can be effectively localized by the zero-initialized\nAdaptive Layer Norm (AdaLN-zero). Building on these findings, we propose\nDiffusion Transformer Feature (DiTF), a training-free framework designed to\nextract semantic-discriminative features from DiTs. Specifically, DiTF employs\nAdaLN to adaptively localize and normalize massive activations with\nchannel-wise modulation. In addition, we develop a channel discard strategy to\nfurther eliminate the negative impacts from massive activations. Experimental\nresults demonstrate that our DiTF outperforms both DINO and SD-based models and\nestablishes a new state-of-the-art performance for DiTs in different visual\ncorrespondence tasks (\\eg, with +9.4\\% on Spair-71k and +4.4\\% on AP-10K-C.S.).\n","authors":["Chaofan Gan","Yuanpeng Tu","Xi Chen","Tieyuan Chen","Yuxi Li","Mehrtash Harandi","Weiyao Lin"],"pdf_url":"https://arxiv.org/pdf/2505.18584v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25327v2","updated":"2025-10-30T02:51:38Z","published":"2025-10-29T09:41:03Z","title":"MMEdge: Accelerating On-device Multimodal Inference via Pipelined\n  Sensing and Encoding","summary":"  Real-time multimodal inference on resource-constrained edge devices is\nessential for applications such as autonomous driving, human-computer\ninteraction, and mobile health. However, prior work often overlooks the tight\ncoupling between sensing dynamics and model execution, as well as the complex\ninter-modality dependencies. In this paper, we propose MMEdge, an new on-device\nmulti-modal inference framework based on pipelined sensing and encoding.\nInstead of waiting for complete sensor inputs, MMEdge decomposes the entire\ninference process into a sequence of fine-grained sensing and encoding units,\nallowing computation to proceed incrementally as data arrive. MMEdge also\nintroduces a lightweight but effective temporal aggregation module that\ncaptures rich temporal dynamics across different pipelined units to maintain\naccuracy performance. Such pipelined design also opens up opportunities for\nfine-grained cross-modal optimization and early decision-making during\ninference. To further enhance system performance under resource variability and\ninput data complexity, MMEdge incorporates an adaptive multimodal configuration\noptimizer that dynamically selects optimal sensing and model configurations for\neach modality under latency constraints, and a cross-modal speculative skipping\nmechanism that bypasses future units of slower modalities when early\npredictions reach sufficient confidence. We evaluate MMEdge using two public\nmultimodal datasets and deploy it on a real-world unmanned aerial vehicle\n(UAV)-based multimodal testbed. The results show that MMEdge significantly\nreduces end-to-end latency while maintaining high task accuracy across various\nsystem and data dynamics.\n","authors":["Runxi Huang","Mingxuan Yu","Mingyu Tsoi","Xiaomin Ouyang"],"pdf_url":"https://arxiv.org/pdf/2510.25327v2.pdf","comment":"Code available at: https://github.com/HKUST-MINSys-Lab/MMEdge.\n  Accepted by SenSys 2026"},{"id":"http://arxiv.org/abs/2506.21046v2","updated":"2025-10-30T02:36:15Z","published":"2025-06-26T06:47:51Z","title":"Boosting Generative Adversarial Transferability with Self-supervised\n  Vision Transformer Features","summary":"  The ability of deep neural networks (DNNs) come from extracting and\ninterpreting features from the data provided. By exploiting intermediate\nfeatures in DNNs instead of relying on hard labels, we craft adversarial\nperturbation that generalize more effectively, boosting black-box\ntransferability. These features ubiquitously come from supervised learning in\nprevious work. Inspired by the exceptional synergy between self-supervised\nlearning and the Transformer architecture, this paper explores whether\nexploiting self-supervised Vision Transformer (ViT) representations can improve\nadversarial transferability. We present dSVA -- a generative dual\nself-supervised ViT features attack, that exploits both global structural\nfeatures from contrastive learning (CL) and local textural features from masked\nimage modeling (MIM), the self-supervised learning paradigm duo for ViTs. We\ndesign a novel generative training framework that incorporates a generator to\ncreate black-box adversarial examples, and strategies to train the generator by\nexploiting joint features and the attention mechanism of self-supervised ViTs.\nOur findings show that CL and MIM enable ViTs to attend to distinct feature\ntendencies, which, when exploited in tandem, boast great adversarial\ngeneralizability. By disrupting dual deep features distilled by self-supervised\nViTs, we are rewarded with remarkable black-box transferability to models of\nvarious architectures that outperform state-of-the-arts. Code available at\nhttps://github.com/spencerwooo/dSVA.\n","authors":["Shangbo Wu","Yu-an Tan","Ruinan Ma","Wencong Ma","Dehua Zhu","Yuanzhang Li"],"pdf_url":"https://arxiv.org/pdf/2506.21046v2.pdf","comment":"14 pages, 9 figures, accepted at ICCV 2025"},{"id":"http://arxiv.org/abs/2408.01701v6","updated":"2025-10-30T02:28:16Z","published":"2024-08-03T07:47:16Z","title":"Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action\n  Recognition via Learning Temporal-Frequency Dynamics","summary":"  For multimodal skeleton-based action recognition, Graph Convolutional\nNetworks (GCNs) are effective models. Still, their reliance on floating-point\ncomputations leads to high energy consumption, limiting their applicability in\nbattery-powered devices. While energy-efficient, Spiking Neural Networks (SNNs)\nstruggle to model skeleton dynamics, leading to suboptimal solutions. We\npropose Signal-SGN (Spiking Graph Convolutional Network), which utilizes the\ntemporal dimension of skeleton sequences as the spike time steps and represents\nfeatures as multi-dimensional discrete stochastic signals for\ntemporal-frequency domain feature extraction. It combines the 1D Spiking Graph\nConvolution (1D-SGC) module and the Frequency Spiking Convolution (FSC) module\nto extract features from the skeleton represented as spiking form.\nAdditionally, the Multi-Scale Wavelet Transform Feature Fusion (MWTF) module is\nproposed to extract dynamic spiking features and capture frequency-specific\ncharacteristics, enhancing classification performance. Experiments across three\nlarge-scale datasets reveal Signal-SGN exceeding state-of-the-art SNN-based\nmethods in accuracy and computational efficiency while attaining comparable\nperformance with GCN methods and significantly reducing theoretical energy\nconsumption.\n","authors":["Naichuan Zheng","Yuchen Du","Hailun Xia","Zeyu Liang"],"pdf_url":"https://arxiv.org/pdf/2408.01701v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17148v3","updated":"2025-10-30T01:44:58Z","published":"2025-10-20T04:49:14Z","title":"DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through\n  Metric-Guided Alignment","summary":"  Conventional end-to-end (E2E) driving models are effective at generating\nphysically plausible trajectories, but often fail to generalize to long-tail\nscenarios due to the lack of essential world knowledge to understand and reason\nabout surrounding environments. In contrast, Vision-Language-Action (VLA)\nmodels leverage world knowledge to handle challenging cases, but their limited\n3D reasoning capability can lead to physically infeasible actions. In this work\nwe introduce DiffVLA++, an enhanced autonomous driving framework that\nexplicitly bridges cognitive reasoning and E2E planning through metric-guided\nalignment. First, we build a VLA module directly generating semantically\ngrounded driving trajectories. Second, we design an E2E module with a dense\ntrajectory vocabulary that ensures physical feasibility. Third, and most\ncritically, we introduce a metric-guided trajectory scorer that guides and\naligns the outputs of the VLA and E2E modules, thereby integrating their\ncomplementary strengths. The experiment on the ICCV 2025 Autonomous Grand\nChallenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.\n","authors":["Yu Gao","Anqing Jiang","Yiru Wang","Wang Jijun","Hao Jiang","Zhigang Sun","Heng Yuwen","Wang Shuo","Hao Zhao","Sun Hao"],"pdf_url":"https://arxiv.org/pdf/2510.17148v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13444v2","updated":"2025-10-30T01:42:07Z","published":"2025-05-19T17:59:27Z","title":"ChartMuseum: Testing Visual Reasoning Capabilities of Large\n  Vision-Language Models","summary":"  Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs.\n","authors":["Liyan Tang","Grace Kim","Xinyu Zhao","Thom Lake","Wenxuan Ding","Fangcong Yin","Prasann Singhal","Manya Wadhwa","Zeyu Leo Liu","Zayne Sprague","Ramya Namuduri","Bodun Hu","Juan Diego Rodriguez","Puyuan Peng","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2505.13444v2.pdf","comment":"NeurIPS 2025 Datasets & Benchmarks"},{"id":"http://arxiv.org/abs/2510.25077v2","updated":"2025-10-30T01:37:51Z","published":"2025-10-29T01:24:49Z","title":"Neighborhood Feature Pooling for Remote Sensing Image Classification","summary":"  In this work, we propose neighborhood feature pooling (NFP) as a novel\ntexture feature extraction method for remote sensing image classification. The\nNFP layer captures relationships between neighboring inputs and efficiently\naggregates local similarities across feature dimensions. Implemented using\nconvolutional layers, NFP can be seamlessly integrated into any network.\nResults comparing the baseline models and the NFP method indicate that NFP\nconsistently improves performance across diverse datasets and architectures\nwhile maintaining minimal parameter overhead.\n","authors":["Fahimeh Orvati Nia","Amirmohammad Mohammadi","Salim Al Kharsa","Pragati Naikare","Zigfried Hampel-Arias","Joshua Peeples"],"pdf_url":"https://arxiv.org/pdf/2510.25077v2.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.02781v2","updated":"2025-10-30T01:23:43Z","published":"2025-10-03T07:27:55Z","title":"GCVAMD: A Modified CausalVAE Model for Causal Age-related Macular\n  Degeneration Risk Factor Detection and Prediction","summary":"  Age Related Macular Degeneration(AMD) has been one of the most leading causes\nof permanent vision impairment in ophthalmology. Though treatments, such as\nanti VEGF drugs or photodynamic therapies, were developed to slow down the\ndegenerative process of AMD, there is still no specific cure to reverse vision\nloss caused by AMD. Thus, for AMD, detecting existence of risk factors of AMD\nor AMD itself within the patient retina in early stages is a crucial task to\nreduce the possibility of vision impairment. Apart from traditional approaches,\ndeep learning based methods, especially attention mechanism based CNNs and\nGradCAM based XAI analysis on OCT scans, exhibited successful performance in\ndistinguishing AMD retina from normal retinas, making it possible to use AI\ndriven models to aid medical diagnosis and analysis by ophthalmologists\nregarding AMD. However, though having significant success, previous works\nmostly focused on prediction performance itself, not pathologies or underlying\ncausal mechanisms of AMD, which can prohibit intervention analysis on specific\nfactors or even lead to less reliable decisions. Thus, this paper introduces a\nnovel causal AMD analysis model: GCVAMD, which incorporates a modified\nCausalVAE approach that can extract latent causal factors from only raw OCT\nimages. By considering causality in AMD detection, GCVAMD enables causal\ninference such as treatment simulation or intervention analysis regarding major\nrisk factors: drusen and neovascularization, while returning informative latent\ncausal features that can enhance downstream tasks. Results show that through\nGCVAMD, drusen status and neovascularization status can be identified with AMD\ncausal mechanisms in GCVAMD latent spaces, which can in turn be used for\nvarious tasks from AMD detection(classification) to intervention analysis.\n","authors":["Daeyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2510.02781v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26052v1","updated":"2025-10-30T01:10:25Z","published":"2025-10-30T01:10:25Z","title":"Dynamic VLM-Guided Negative Prompting for Diffusion Models","summary":"  We propose a novel approach for dynamic negative prompting in diffusion\nmodels that leverages Vision-Language Models (VLMs) to adaptively generate\nnegative prompts during the denoising process. Unlike traditional Negative\nPrompting methods that use fixed negative prompts, our method generates\nintermediate image predictions at specific denoising steps and queries a VLM to\nproduce contextually appropriate negative prompts. We evaluate our approach on\nvarious benchmark datasets and demonstrate the trade-offs between negative\nguidance strength and text-image alignment.\n","authors":["Hoyeon Chang","Seungjin Kim","Yoonseok Choi"],"pdf_url":"https://arxiv.org/pdf/2510.26052v1.pdf","comment":"39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: The First Workshop on Generative and Protective AI for\n  Content Creation"},{"id":"http://arxiv.org/abs/2510.26049v1","updated":"2025-10-30T00:53:26Z","published":"2025-10-30T00:53:26Z","title":"FlexICL: A Flexible Visual In-context Learning Framework for Elbow and\n  Wrist Ultrasound Segmentation","summary":"  Elbow and wrist fractures are the most common fractures in pediatric\npopulations. Automatic segmentation of musculoskeletal structures in ultrasound\n(US) can improve diagnostic accuracy and treatment planning. Fractures appear\nas cortical defects but require expert interpretation. Deep learning (DL) can\nprovide real-time feedback and highlight key structures, helping lightly\ntrained users perform exams more confidently. However, pixel-wise expert\nannotations for training remain time-consuming and costly. To address this\nchallenge, we propose FlexICL, a novel and flexible in-context learning (ICL)\nframework for segmenting bony regions in US images. We apply it to an\nintra-video segmentation setting, where experts annotate only a small subset of\nframes, and the model segments unseen frames. We systematically investigate\nvarious image concatenation techniques and training strategies for visual ICL\nand introduce novel concatenation methods that significantly enhance model\nperformance with limited labeled data. By integrating multiple augmentation\nstrategies, FlexICL achieves robust segmentation performance across four wrist\nand elbow US datasets while requiring only 5% of the training images. It\noutperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and\nconventional segmentation models like U-Net and TransUNet by 1-27% Dice\ncoefficient on 1,252 US sweeps. These initial results highlight the potential\nof FlexICL as an efficient and scalable solution for US image segmentation well\nsuited for medical imaging use cases where labeled data is scarce.\n","authors":["Yuyue Zhou","Jessica Knight","Shrimanti Ghosh","Banafshe Felfeliyan","Jacob L. Jaremko","Abhilash R. Hareendranathan"],"pdf_url":"https://arxiv.org/pdf/2510.26049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26038v1","updated":"2025-10-30T00:34:16Z","published":"2025-10-30T00:34:16Z","title":"Do Students Debias Like Teachers? On the Distillability of Bias\n  Mitigation Methods","summary":"  Knowledge distillation (KD) is an effective method for model compression and\ntransferring knowledge between models. However, its effect on model's\nrobustness against spurious correlations that degrade performance on\nout-of-distribution data remains underexplored. This study investigates the\neffect of knowledge distillation on the transferability of ``debiasing''\ncapabilities from teacher models to student models on natural language\ninference (NLI) and image classification tasks. Through extensive experiments,\nwe illustrate several key findings: (i) overall the debiasing capability of a\nmodel is undermined post-KD; (ii) training a debiased model does not benefit\nfrom injecting teacher knowledge; (iii) although the overall robustness of a\nmodel may remain stable post-distillation, significant variations can occur\nacross different types of biases; and (iv) we pin-point the internal attention\npattern and circuit that causes the distinct behavior post-KD. Given the above\nfindings, we propose three effective solutions to improve the distillability of\ndebiasing methods: developing high quality data for augmentation, implementing\niterative knowledge distillation, and initializing student models with weights\nobtained from teacher models. To the best of our knowledge, this is the first\nstudy on the effect of KD on debiasing and its interenal mechanism at scale.\nOur findings provide understandings on how KD works and how to design better\ndebiasing methods.\n","authors":["Jiali Cheng","Chirag Agarwal","Hadi Amiri"],"pdf_url":"https://arxiv.org/pdf/2510.26038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08325v4","updated":"2025-10-30T00:33:00Z","published":"2025-01-14T18:57:21Z","title":"GameFactory: Creating New Games with Generative Interactive Videos","summary":"  Generative videos have the potential to revolutionize game development by\nautonomously creating new content. In this paper, we present GameFactory, a\nframework for action-controlled scene-generalizable game video generation. We\nfirst address the fundamental challenge of action controllability by\nintroducing GF-Minecraft, an action-annotated game video dataset without human\nbias, and developing an action control module that enables precise control over\nboth keyboard and mouse inputs. We further extend to support autoregressive\ngeneration for unlimited-length interactive videos. More importantly,\nGameFactory tackles the critical challenge of scene-generalizable action\ncontrol, which most existing methods fail to address. To enable the creation of\nentirely new and diverse games beyond fixed styles and scenes, we leverage the\nopen-domain generative priors from pre-trained video diffusion models. To\nbridge the domain gap between open-domain priors and small-scale game datasets,\nwe propose a multi-phase training strategy with a domain adapter that decouples\ngame style learning from action control. This decoupling ensures that action\ncontrol learning is no longer bound to specific game styles, thereby achieving\nscene-generalizable action control. Experimental results demonstrate that\nGameFactory effectively generates open-domain action-controllable game videos,\nrepresenting a significant step forward in AI-driven game generation.\n","authors":["Jiwen Yu","Yiran Qin","Xintao Wang","Pengfei Wan","Di Zhang","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08325v4.pdf","comment":"ICCV 2025 Highlight, Project Page:\n  https://yujiwen.github.io/gamefactory"},{"id":"http://arxiv.org/abs/2510.23968v2","updated":"2025-10-30T00:14:35Z","published":"2025-10-28T00:48:00Z","title":"Reasoning Visual Language Model for Chest X-Ray Analysis","summary":"  Vision-language models (VLMs) have shown strong promise for medical image\nanalysis, but most remain opaque, offering predictions without the transparent,\nstepwise reasoning clinicians rely on. We present a framework that brings\nchain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by\nreasoning-first training paradigms, our approach is designed to learn how\nexperts reason, not just what they conclude, by aligning intermediate steps\nwith observable image evidence and radiology workflow. Beyond accuracy, the\nexplicit reasoning traces support clinical auditability: they reveal why a\nconclusion was reached, which alternatives were considered, and where\nuncertainty remains, enabling quality assurance, error analysis, and safer\nhuman-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training\nrecipe: a reasoning-style supervised fine-tuning (SFT) followed by\nreinforcement learning (RL) that uses verifiable rewards over a list of X-ray\nabnormalities. The model outputs reasoning that mirrors radiologists systematic\nthought process, uncertainty, and differential diagnosis. In\nout-of-distribution evaluation, the approach achieves competitive multi-label\nclassification while improving interpretability. In a reader study with expert\nradiologists, full reasoning traces increased confidence, supported error\nauditing, and reduced time to finalize reports. We release code and the model\nNV-Reason-CXR-3B to support community progress toward trustworthy, explainable\nAI in chest radiography and other medical imaging tasks where reasoning quality\nis as critical as prediction quality.\n","authors":["Andriy Myronenko","Dong Yang","Baris Turkbey","Mariam Aboian","Sena Azamat","Esra Akcicek","Hongxu Yin","Pavlo Molchanov","Marc Edgar","Yufan He","Pengfei Guo","Yucheng Tang","Daguang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.23968v2.pdf","comment":"NV-Reason-CXR-3B"},{"id":"http://arxiv.org/abs/2403.08788v2","updated":"2025-10-30T13:41:48Z","published":"2024-01-30T09:05:38Z","title":"VerifIoU -- Robustness of Object Detection to Perturbations","summary":"  We introduce a novel Interval Bound Propagation (IBP) approach for the formal\nverification of object detection models, specifically targeting the\nIntersection over Union (IoU) metric. The approach has been implemented in an\nopen source code, named IBP IoU, compatible with popular abstract\ninterpretation based verification tools. The resulting verifier is evaluated on\nlanding approach runway detection and handwritten digit recognition case\nstudies. Comparisons against a baseline (Vanilla IBP IoU) highlight the\nsuperior performance of IBP IoU in ensuring accuracy and stability,\ncontributing to more secure and robust machine learning applications.\n","authors":["Noémie Cohen","Mélanie Ducoffe","Ryma Boumazouza","Christophe Gabreau","Claire Pagetti","Xavier Pucel","Audrey Galametz"],"pdf_url":"https://arxiv.org/pdf/2403.08788v2.pdf","comment":"44th Digital Avionics Systems Conference (DASC), Sep 2025, Montreal,\n  Canada"},{"id":"http://arxiv.org/abs/2509.06771v2","updated":"2025-10-30T10:15:05Z","published":"2025-09-08T14:55:16Z","title":"D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning --\n  A Benchmark Dataset and Method","summary":"  Dark humor in online memes poses unique challenges due to its reliance on\nimplicit, sensitive, and culturally contextual cues. To address the lack of\nresources and methods for detecting dark humor in multimodal content, we\nintroduce a novel dataset of 4,379 Reddit memes annotated for dark humor,\ntarget category (gender, mental health, violence, race, disability, and other),\nand a three-level intensity rating (mild, moderate, severe). Building on this\nresource, we propose a reasoning-augmented framework that first generates\nstructured explanations for each meme using a Large Vision-Language Model\n(VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective\nto iteratively refine its explanations, ensuring completeness and alignment. We\nthen extract textual features from both the OCR transcript and the self-refined\nreasoning via a text encoder, while visual features are obtained using a vision\ntransformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three\nstreams, text, image, and reasoning, via pairwise attention mechanisms,\nproducing a unified representation for classification. Experimental results\ndemonstrate that our approach outperforms strong baselines across three tasks:\ndark humor detection, target identification, and intensity prediction. The\ndataset, annotations, and code are released to facilitate further research in\nmultimodal humor understanding and content moderation. Code and Dataset are\navailable at:\nhttps://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning\n","authors":["Sai Kartheek Reddy Kasu","Mohammad Zia Ur Rehman","Shahid Shafi Dar","Rishi Bharat Junghare","Dhanvin Sanjay Namboodiri","Nagendra Kumar"],"pdf_url":"https://arxiv.org/pdf/2509.06771v2.pdf","comment":"Accepted at IEEE International Conference on Data Mining (ICDM) 2025"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2510.26800v1","updated":"2025-10-30T17:59:51Z","published":"2025-10-30T17:59:51Z","title":"OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes","summary":"  There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.\n","authors":["Yukun Huang","Jiwen Yu","Yanning Zhou","Jianan Wang","Xintao Wang","Pengfei Wan","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26800v1.pdf","comment":"Project page: https://yukun-huang.github.io/OmniX/"},{"id":"http://arxiv.org/abs/2506.03237v2","updated":"2025-10-30T17:59:46Z","published":"2025-06-03T17:49:41Z","title":"UniSite: The First Cross-Structure Dataset and Learning Framework for\n  End-to-End Ligand Binding Site Detection","summary":"  The detection of ligand binding sites for proteins is a fundamental step in\nStructure-Based Drug Design. Despite notable advances in recent years, existing\nmethods, datasets, and evaluation metrics are confronted with several key\nchallenges: (1) current datasets and methods are centered on individual\nprotein-ligand complexes and neglect that diverse binding sites may exist\nacross multiple complexes of the same protein, introducing significant\nstatistical bias; (2) ligand binding site detection is typically modeled as a\ndiscontinuous workflow, employing binary segmentation and subsequent clustering\nalgorithms; (3) traditional evaluation metrics do not adequately reflect the\nactual performance of different binding site prediction methods. To address\nthese issues, we first introduce UniSite-DS, the first UniProt (Unique\nProtein)-centric ligand binding site dataset, which contains 4.81 times more\nmulti-site data and 2.08 times more overall data compared to the previously\nmost widely used datasets. We then propose UniSite, the first end-to-end ligand\nbinding site detection framework supervised by set prediction loss with\nbijective matching. In addition, we introduce Average Precision based on\nIntersection over Union (IoU) as a more accurate evaluation metric for ligand\nbinding site prediction. Extensive experiments on UniSite-DS and several\nrepresentative benchmark datasets demonstrate that IoU-based Average Precision\nprovides a more accurate reflection of prediction quality, and that UniSite\noutperforms current state-of-the-art methods in ligand binding site detection.\nThe dataset and codes will be made publicly available at\nhttps://github.com/quanlin-wu/unisite.\n","authors":["Jigang Fan","Quanlin Wu","Shengjie Luo","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2506.03237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26795v1","updated":"2025-10-30T17:59:35Z","published":"2025-10-30T17:59:35Z","title":"Scaling Image Geo-Localization to Continent Level","summary":"  Determining the precise geographic location of an image at a global scale\nremains an unsolved challenge. Standard image retrieval techniques are\ninefficient due to the sheer volume of images (>100M) and fail when coverage is\ninsufficient. Scalable solutions, however, involve a trade-off: global\nclassification typically yields coarse results (10+ kilometers), while\ncross-view retrieval between ground and aerial imagery suffers from a domain\ngap and has been primarily studied on smaller regions. This paper introduces a\nhybrid approach that achieves fine-grained geo-localization across a large\ngeographic expanse the size of a continent. We leverage a proxy classification\ntask during training to learn rich feature representations that implicitly\nencode precise location information. We combine these learned prototypes with\nembeddings of aerial imagery to increase robustness to the sparsity of\nground-level data. This enables direct, fine-grained retrieval over areas\nspanning multiple countries. Our extensive evaluation demonstrates that our\napproach can localize within 200m more than 68\\% of queries of a dataset\ncovering a large part of Europe. The code is publicly available at\nhttps://scaling-geoloc.github.io.\n","authors":["Philipp Lindenberger","Paul-Edouard Sarlin","Jan Hosang","Matteo Balice","Marc Pollefeys","Simon Lynen","Eduard Trulls"],"pdf_url":"https://arxiv.org/pdf/2510.26795v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26792v1","updated":"2025-10-30T17:59:09Z","published":"2025-10-30T17:59:09Z","title":"Learning Pseudorandom Numbers with Transformers: Permuted Congruential\n  Generators, Curricula, and Interpretability","summary":"  We study the ability of Transformer models to learn sequences generated by\nPermuted Congruential Generators (PCGs), a widely used family of pseudo-random\nnumber generators (PRNGs). PCGs introduce substantial additional difficulty\nover linear congruential generators (LCGs) by applying a series of bit-wise\nshifts, XORs, rotations and truncations to the hidden state. We show that\nTransformers can nevertheless successfully perform in-context prediction on\nunseen sequences from diverse PCG variants, in tasks that are beyond published\nclassical attacks. In our experiments we scale moduli up to $2^{22}$ using up\nto $50$ million model parameters and datasets with up to $5$ billion tokens.\nSurprisingly, we find even when the output is truncated to a single bit, it can\nbe reliably predicted by the model. When multiple distinct PRNGs are presented\ntogether during training, the model can jointly learn them, identifying\nstructures from different permutations. We demonstrate a scaling law with\nmodulus $m$: the number of in-context sequence elements required for\nnear-perfect prediction grows as $\\sqrt{m}$. For larger moduli, optimization\nenters extended stagnation phases; in our experiments, learning moduli $m \\geq\n2^{20}$ requires incorporating training data from smaller moduli, demonstrating\na critical necessity for curriculum learning. Finally, we analyze embedding\nlayers and uncover a novel clustering phenomenon: the model spontaneously\ngroups the integer inputs into bitwise rotationally-invariant clusters,\nrevealing how representations can transfer from smaller to larger moduli.\n","authors":["Tao Tao","Maissam Barkeshli"],"pdf_url":"https://arxiv.org/pdf/2510.26792v1.pdf","comment":"10+13 pages, 8+19 figures"},{"id":"http://arxiv.org/abs/2510.26788v1","updated":"2025-10-30T17:58:11Z","published":"2025-10-30T17:58:11Z","title":"Defeating the Training-Inference Mismatch via FP16","summary":"  Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.\n","authors":["Penghui Qi","Zichen Liu","Xiangxin Zhou","Tianyu Pang","Chao Du","Wee Sun Lee","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2510.26788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26787v1","updated":"2025-10-30T17:58:04Z","published":"2025-10-30T17:58:04Z","title":"Remote Labor Index: Measuring AI Automation of Remote Work","summary":"  AIs have made rapid progress on research-oriented benchmarks of knowledge and\nreasoning, but it remains unclear how these gains translate into economic value\nand automation. To measure this, we introduce the Remote Labor Index (RLI), a\nbroadly multi-sector benchmark comprising real-world, economically valuable\nprojects designed to evaluate end-to-end agent performance in practical\nsettings. AI agents perform near the floor on RLI, with the highest-performing\nagent achieving an automation rate of 2.5%. These results help ground\ndiscussions of AI automation in empirical evidence, setting a common basis for\ntracking AI impacts and enabling stakeholders to proactively navigate AI-driven\nlabor automation.\n","authors":["Mantas Mazeika","Alice Gatti","Cristina Menghini","Udari Madhushani Sehwag","Shivam Singhal","Yury Orlovskiy","Steven Basart","Manasi Sharma","Denis Peskoff","Elaine Lau","Jaehyuk Lim","Lachlan Carroll","Alice Blair","Vinaya Sivakumar","Sumana Basu","Brad Kenstler","Yuntao Ma","Julian Michael","Xiaoke Li","Oliver Ingebretsen","Aditya Mehta","Jean Mottola","John Teichmann","Kevin Yu","Zaina Shaik","Adam Khoja","Richard Ren","Jason Hausenloy","Long Phan","Ye Htet","Ankit Aich","Tahseen Rabbani","Vivswan Shah","Andriy Novykov","Felix Binder","Kirill Chugunov","Luis Ramirez","Matias Geralnik","Hernán Mesura","Dean Lee","Ed-Yeremai Hernandez Cardona","Annette Diamond","Summer Yue","Alexandr Wang","Bing Liu","Ernesto Hernandez","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2510.26787v1.pdf","comment":"Website: https://www.remotelabor.ai"},{"id":"http://arxiv.org/abs/2510.26786v1","updated":"2025-10-30T17:57:40Z","published":"2025-10-30T17:57:40Z","title":"HEIR: Learning Graph-Based Motion Hierarchies","summary":"  Hierarchical structures of motion exist across research fields, including\ncomputer vision, graphics, and robotics, where complex dynamics typically arise\nfrom coordinated interactions among simpler motion components. Existing methods\nto model such dynamics typically rely on manually-defined or heuristic\nhierarchies with fixed motion primitives, limiting their generalizability\nacross different tasks. In this work, we propose a general hierarchical motion\nmodeling method that learns structured, interpretable motion relationships\ndirectly from data. Our method represents observed motions using graph-based\nhierarchies, explicitly decomposing global absolute motions into\nparent-inherited patterns and local motion residuals. We formulate hierarchy\ninference as a differentiable graph learning problem, where vertices represent\nelemental motions and directed edges capture learned parent-child dependencies\nthrough graph neural networks. We evaluate our hierarchical reconstruction\napproach on three examples: 1D translational motion, 2D rotational motion, and\ndynamic 3D scene deformation via Gaussian splatting. Experimental results show\nthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,\nand produces more realistic and interpretable deformations compared to the\nbaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,\ndata-driven hierarchical modeling paradigm, our method offers a formulation\napplicable to a broad range of motion-centric tasks. Project Page:\nhttps://light.princeton.edu/HEIR/\n","authors":["Cheng Zheng","William Koch","Baiang Li","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2510.26786v1.pdf","comment":"Code link: https://github.com/princeton-computational-imaging/HEIR"},{"id":"http://arxiv.org/abs/2510.26783v1","updated":"2025-10-30T17:56:47Z","published":"2025-10-30T17:56:47Z","title":"A Unified Theory for Causal Inference: Direct Debiased Machine Learning\n  via Bregman-Riesz Regression","summary":"  This note introduces a unified theory for causal inference that integrates\nRiesz regression, covariate balancing, density-ratio estimation (DRE), targeted\nmaximum likelihood estimation (TMLE), and the matching estimator in average\ntreatment effect (ATE) estimation. In ATE estimation, the balancing weights and\nthe regression functions of the outcome play important roles, where the\nbalancing weights are referred to as the Riesz representer, bias-correction\nterm, and clever covariates, depending on the context. Riesz regression,\ncovariate balancing, DRE, and the matching estimator are methods for estimating\nthe balancing weights, where Riesz regression is essentially equivalent to DRE\nin the ATE context, the matching estimator is a special case of DRE, and DRE is\nin a dual relationship with covariate balancing. TMLE is a method for\nconstructing regression function estimators such that the leading bias term\nbecomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density\nRatio Estimation and Riesz Regression.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2510.26783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26782v1","updated":"2025-10-30T17:56:43Z","published":"2025-10-30T17:56:43Z","title":"Clone Deterministic 3D Worlds with Geometrically-Regularized World\n  Models","summary":"  A world model is an internal model that simulates how the world evolves.\nGiven past observations and actions, it predicts the future of both the\nembodied agent and its environment. Accurate world models are essential for\nenabling agents to think, plan, and reason effectively in complex, dynamic\nsettings. Despite rapid progress, current world models remain brittle and\ndegrade over long horizons. We argue that a central cause is representation\nquality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or\nentangled latents make dynamics learning unnecessarily hard. We therefore ask\nwhether improving representation learning alone can substantially improve\nworld-model performance. In this work, we take a step toward building a truly\naccurate world model by addressing a fundamental yet open problem: constructing\na model that can fully clone and overfit to a deterministic 3D world. We\npropose Geometrically-Regularized World Models (GRWM), which enforces that\nconsecutive points along a natural sensory trajectory remain close in latent\nrepresentation space. This approach yields significantly improved latent\nrepresentations that align closely with the true topology of the environment.\nGRWM is plug-and-play, requires only minimal architectural modification, scales\nwith trajectory length, and is compatible with diverse latent generative\nbackbones. Across deterministic 3D settings and long-horizon prediction tasks,\nGRWM significantly increases rollout fidelity and stability. Analyses show that\nits benefits stem from learning a latent manifold with superior geometric\nstructure. These findings support a clear takeaway: improving representation\nlearning is a direct and useful path to robust world models, delivering\nreliable long-horizon predictions without enlarging the dynamics module.\n","authors":["Zaishuo Xia","Yukuan Lu","Xinyi Li","Yifan Xu","Yubei Chen"],"pdf_url":"https://arxiv.org/pdf/2510.26782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15723v7","updated":"2025-10-30T17:56:10Z","published":"2024-10-21T07:42:43Z","title":"S-CFE: Simple Counterfactual Explanations","summary":"  We study the problem of finding optimal sparse, manifold-aligned\ncounterfactual explanations for classifiers. Canonically, this can be\nformulated as an optimization problem with multiple non-convex components,\nincluding classifier loss functions and manifold alignment (or\n\\emph{plausibility}) metrics. The added complexity of enforcing\n\\emph{sparsity}, or shorter explanations, complicates the problem further.\nExisting methods often focus on specific models and plausibility measures,\nrelying on convex $\\ell_1$ regularizers to enforce sparsity. In this paper, we\ntackle the canonical formulation using the accelerated proximal gradient (APG)\nmethod, a simple yet efficient first-order procedure capable of handling smooth\nnon-convex objectives and non-smooth $\\ell_p$ (where $0 \\leq p < 1$)\nregularizers. This enables our approach to seamlessly incorporate various\nclassifiers and plausibility measures while producing sparser solutions. Our\nalgorithm only requires differentiable data-manifold regularizers and supports\nbox constraints for bounded feature ranges, ensuring the generated\ncounterfactuals remain \\emph{actionable}. Finally, experiments on real-world\ndatasets demonstrate that our approach effectively produces sparse,\nmanifold-aligned counterfactual explanations while maintaining proximity to the\nfactual data and computational efficiency.\n","authors":["Shpresim Sadiku","Moritz Wagner","Sai Ganesh Nagarajan","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2410.15723v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26778v1","updated":"2025-10-30T17:55:46Z","published":"2025-10-30T17:55:46Z","title":"Surpassing state of the art on AMD area estimation from RGB fundus\n  images through careful selection of U-Net architectures and loss functions\n  for class imbalance","summary":"  Age-related macular degeneration (AMD) is one of the leading causes of\nirreversible vision impairment in people over the age of 60. This research\nfocuses on semantic segmentation for AMD lesion detection in RGB fundus images,\na non-invasive and cost-effective imaging technique. The results of the ADAM\nchallenge - the most comprehensive AMD detection from RGB fundus images\nresearch competition and open dataset to date - serve as a benchmark for our\nevaluation. Taking the U-Net connectivity as a base of our framework, we\nevaluate and compare several approaches to improve the segmentation model's\narchitecture and training pipeline, including pre-processing techniques,\nencoder (backbone) deep network types of varying complexity, and specialized\nloss functions to mitigate class imbalances on image and pixel levels. The main\noutcome of this research is the final configuration of the AMD detection\nframework, which outperforms all the prior ADAM challenge submissions on the\nmulti-class segmentation of different AMD lesion types in non-invasive RGB\nfundus images. The source code used to conduct the experiments presented in\nthis paper is made freely available.\n","authors":["Valentyna Starodub","Mantas Lukoševičius"],"pdf_url":"https://arxiv.org/pdf/2510.26778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23534v2","updated":"2025-10-30T17:55:38Z","published":"2025-10-27T17:10:43Z","title":"Direct Debiased Machine Learning via Bregman Divergence Minimization","summary":"  We develop a direct debiased machine learning framework comprising Neyman\ntargeted estimation and generalized Riesz regression. Our framework unifies\nRiesz regression for automatic debiased machine learning, covariate balancing,\ntargeted maximum likelihood estimation (TMLE), and density-ratio estimation. In\nmany problems involving causal effects or structural models, the parameters of\ninterest depend on regression functions. Plugging regression functions\nestimated by machine learning methods into the identifying equations can yield\npoor performance because of first-stage bias. To reduce such bias, debiased\nmachine learning employs Neyman orthogonal estimating equations. Debiased\nmachine learning typically requires estimation of the Riesz representer and the\nregression function. For this problem, we develop a direct debiased machine\nlearning framework with an end-to-end algorithm. We formulate estimation of the\nnuisance parameters, the regression function and the Riesz representer, as\nminimizing the discrepancy between Neyman orthogonal scores computed with known\nand unknown nuisance parameters, which we refer to as Neyman targeted\nestimation. Neyman targeted estimation includes Riesz representer estimation,\nand we measure discrepancies using the Bregman divergence. The Bregman\ndivergence encompasses various loss functions as special cases, where the\nsquared loss yields Riesz regression and the Kullback-Leibler divergence yields\nentropy balancing. We refer to this Riesz representer estimation as generalized\nRiesz regression. Neyman targeted estimation also yields TMLE as a special case\nfor regression function estimation. Furthermore, for specific pairs of models\nand Riesz representer estimation methods, we can automatically obtain the\ncovariate balancing property without explicitly solving the covariate balancing\nobjective.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2510.23534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26777v1","updated":"2025-10-30T17:55:23Z","published":"2025-10-30T17:55:23Z","title":"Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for\n  Time Series Classification","summary":"  Recent research on time series foundation models has primarily focused on\nforecasting, leaving it unclear how generalizable their learned representations\nare. In this study, we examine whether frozen pre-trained forecasting models\ncan provide effective representations for classification. To this end, we\ncompare different representation extraction strategies and introduce two\nmodel-agnostic embedding augmentations. Our experiments show that the best\nforecasting models achieve classification accuracy that matches or even\nsurpasses that of state-of-the-art models pre-trained specifically for\nclassification. Moreover, we observe a positive correlation between forecasting\nand classification performance. These findings challenge the assumption that\ntask-specific pre-training is necessary, and suggest that learning to forecast\nmay provide a powerful route toward constructing general-purpose time series\nfoundation models.\n","authors":["Andreas Auer","Daniel Klotz","Sebastinan Böck","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2510.26777v1.pdf","comment":"NeurIPS 2025 Workshop on Recent Advances in Time Series Foundation\n  Models (BERT2S)"},{"id":"http://arxiv.org/abs/2510.26776v1","updated":"2025-10-30T17:55:19Z","published":"2025-10-30T17:55:19Z","title":"Faithful and Fast Influence Function via Advanced Sampling","summary":"  How can we explain the influence of training data on black-box models?\nInfluence functions (IFs) offer a post-hoc solution by utilizing gradients and\nHessians. However, computing the Hessian for an entire dataset is\nresource-intensive, necessitating a feasible alternative. A common approach\ninvolves randomly sampling a small subset of the training data, but this method\noften results in highly inconsistent IF estimates due to the high variance in\nsample configurations. To address this, we propose two advanced sampling\ntechniques based on features and logits. These samplers select a small yet\nrepresentative subset of the entire dataset by considering the stochastic\ndistribution of features or logits, thereby enhancing the accuracy of IF\nestimations. We validate our approach through class removal experiments, a\ntypical application of IFs, using the F1-score to measure how effectively the\nmodel forgets the removed class while maintaining inference consistency on the\nremaining classes. Our method reduces computation time by 30.1% and memory\nusage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.\n","authors":["Jungyeon Koh","Hyeonsu Lyu","Jonggyu Jang","Hyun Jong Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26771v1","updated":"2025-10-30T17:53:42Z","published":"2025-10-30T17:53:42Z","title":"STaMP: Sequence Transformation and Mixed Precision for Low-Precision\n  Activation Quantization","summary":"  Quantization is the key method for reducing inference latency, power and\nmemory footprint of generative AI models. However, accuracy often degrades\nsharply when activations are quantized below eight bits. Recent work suggests\nthat invertible linear transformations (e.g. rotations) can aid quantization,\nby reparameterizing feature channels and weights. In this paper, we propose\n\\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a\nnovel strategy that applies linear transformations along the \\textit{sequence}\ndimension to exploit the strong local correlation in language and visual data.\nBy keeping a small number of tokens in each intermediate activation at higher\nprecision, we can maintain model accuracy at lower (average) activations\nbit-widths. We evaluate STaMP on recent LVM and LLM architectures,\ndemonstrating that it significantly improves low bit width activation\nquantization and complements established activation and weight quantization\nmethods including recent feature transformations.\n","authors":["Marco Federici","Riccardo Del Chiaro","Boris van Breugel","Paul Whatmough","Markus Nagel"],"pdf_url":"https://arxiv.org/pdf/2510.26771v1.pdf","comment":"10 pages main text, 8 pages supplementary material"},{"id":"http://arxiv.org/abs/2510.26769v1","updated":"2025-10-30T17:52:39Z","published":"2025-10-30T17:52:39Z","title":"SteerVLM: Robust Model Control through Lightweight Activation Steering\n  for Vision Language Models","summary":"  This work introduces SteerVLM, a lightweight steering module designed to\nguide Vision-Language Models (VLMs) towards outputs that better adhere to\ndesired instructions. Our approach learns from the latent embeddings of paired\nprompts encoding target and converse behaviors to dynamically adjust\nactivations connecting the language modality with image context. This allows\nfor fine-grained, inference-time control over complex output semantics without\nmodifying model weights while preserving performance on off-target tasks. Our\nsteering module requires learning parameters equal to 0.14% of the original\nVLM's size. Our steering module gains model control through dimension-wise\nactivation modulation and adaptive steering across layers without requiring\npre-extracted static vectors or manual tuning of intervention points.\nFurthermore, we introduce VNIA (Visual Narrative Intent Alignment), a\nmultimodal dataset specifically created to facilitate the development and\nevaluation of VLM steering techniques. Our method outperforms existing\nintervention techniques on steering and hallucination mitigation benchmarks for\nVLMs and proposes a robust solution for multimodal model control through\nactivation engineering.\n","authors":["Anushka Sivakumar","Andrew Zhang","Zaber Hakim","Chris Thomas"],"pdf_url":"https://arxiv.org/pdf/2510.26769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.15370v3","updated":"2025-10-30T17:51:51Z","published":"2025-09-18T19:17:07Z","title":"Adversarial generalization of unfolding (model-based) networks","summary":"  Unfolding networks are interpretable networks emerging from iterative\nalgorithms, incorporate prior knowledge of data structure, and are designed to\nsolve inverse problems like compressed sensing, which deals with recovering\ndata from noisy, missing observations. Compressed sensing finds applications in\ncritical domains, from medical imaging to cryptography, where adversarial\nrobustness is crucial to prevent catastrophic failures. However, a solid\ntheoretical understanding of the performance of unfolding networks in the\npresence of adversarial attacks is still in its infancy. In this paper, we\nstudy the adversarial generalization of unfolding networks when perturbed with\n$l_2$-norm constrained attacks, generated by the fast gradient sign method.\nParticularly, we choose a family of state-of-the-art overaparameterized\nunfolding networks and deploy a new framework to estimate their adversarial\nRademacher complexity. Given this estimate, we provide adversarial\ngeneralization error bounds for the networks under study, which are tight with\nrespect to the attack level. To our knowledge, this is the first theoretical\nanalysis on the adversarial generalization of unfolding networks. We further\npresent a series of experiments on real-world data, with results corroborating\nour derived theory, consistently for all data. Finally, we observe that the\nfamily's overparameterization can be exploited to promote adversarial\nrobustness, shedding light on how to efficiently robustify neural networks.\n","authors":["Vicky Kouni"],"pdf_url":"https://arxiv.org/pdf/2509.15370v3.pdf","comment":"Accepted at NeurIPS2025"},{"id":"http://arxiv.org/abs/2510.26752v1","updated":"2025-10-30T17:46:49Z","published":"2025-10-30T17:46:49Z","title":"The Oversight Game: Learning to Cooperatively Balance an AI Agent's\n  Safety and Autonomy","summary":"  As increasingly capable agents are deployed, a central safety question is how\nto retain meaningful human control without modifying the underlying system. We\nstudy a minimal control interface where an agent chooses whether to act\nautonomously (play) or defer (ask), while a human simultaneously chooses\nwhether to be permissive (trust) or to engage in oversight (oversee). If the\nagent defers, the human's choice determines the outcome, potentially leading to\na corrective action or a system shutdown. We model this interaction as a\ntwo-player Markov Game. Our analysis focuses on cases where this game qualifies\nas a Markov Potential Game (MPG), a class of games where we can provide an\nalignment guarantee: under a structural assumption on the human's value\nfunction, any decision by the agent to act more autonomously that benefits\nitself cannot harm the human's value. We also analyze extensions to this MPG\nframework. Theoretically, this perspective provides conditions for a specific\nform of intrinsic alignment. If the reward structures of the human-agent game\nmeet these conditions, we have a formal guarantee that the agent improving its\nown outcome will not harm the human's. Practically, this model motivates a\ntransparent control layer with predictable incentives where the agent learns to\ndefer when risky and act when safe, while its pretrained policy and the\nenvironment's reward structure remain untouched. Our gridworld simulation shows\nthat through independent learning, the agent and human discover their optimal\noversight roles. The agent learns to ask when uncertain and the human learns\nwhen to oversee, leading to an emergent collaboration that avoids safety\nviolations introduced post-training. This demonstrates a practical method for\nmaking misaligned models safer after deployment.\n","authors":["William Overman","Mohsen Bayati"],"pdf_url":"https://arxiv.org/pdf/2510.26752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26745v1","updated":"2025-10-30T17:40:22Z","published":"2025-10-30T17:40:22Z","title":"Deep sequence models tend to memorize geometrically; it is unclear why","summary":"  In sequence modeling, the parametric memory of atomic facts has been\npredominantly abstracted as a brute-force lookup of co-occurrences between\nentities. We contrast this associative view against a geometric view of how\nmemory is stored. We begin by isolating a clean and analyzable instance of\nTransformer reasoning that is incompatible with memory as strictly a storage of\nthe local co-occurrences specified during training. Instead, the model must\nhave somehow synthesized its own geometry of atomic facts, encoding global\nrelationships between all entities, including non-co-occurring ones. This in\nturn has simplified a hard reasoning task involving an $\\ell$-fold composition\ninto an easy-to-learn 1-step geometric task.\n  From this phenomenon, we extract fundamental aspects of neural embedding\ngeometries that are hard to explain. We argue that the rise of such a geometry,\ndespite optimizing over mere local associations, cannot be straightforwardly\nattributed to typical architectural or optimizational pressures.\nCounterintuitively, an elegant geometry is learned even when it is not more\nsuccinct than a brute-force lookup of associations.\n  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry\nstems from a spectral bias that -- in contrast to prevailing theories -- indeed\narises naturally despite the lack of various pressures. This analysis also\npoints to practitioners a visible headroom to make Transformer memory more\nstrongly geometric. We hope the geometric view of parametric memory encourages\nrevisiting the default intuitions that guide researchers in areas like\nknowledge acquisition, capacity, discovery and unlearning.\n","authors":["Shahriar Noroozizadeh","Vaishnavh Nagarajan","Elan Rosenfeld","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2510.26745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19419v2","updated":"2025-10-30T17:32:12Z","published":"2025-04-28T02:10:18Z","title":"Advancing Local Clustering on Graphs via Compressive Sensing:\n  Semi-supervised and Unsupervised Methods","summary":"  Local clustering aims to identify specific substructures within a large graph\nwithout any additional structural information of the graph. These substructures\nare typically small compared to the overall graph, enabling the problem to be\napproached by finding a sparse solution to a linear system associated with the\ngraph Laplacian. In this work, we first propose a method for identifying\nspecific local clusters when very few labeled data are given, which we term\nsemi-supervised local clustering. We then extend this approach to the\nunsupervised setting when no prior information on labels is available. The\nproposed methods involve randomly sampling the graph, applying diffusion\nthrough local cluster extraction, then examining the overlap among the results\nto find each cluster. We establish the co-membership conditions for any pair of\nnodes, and rigorously prove the correctness of our methods. Additionally, we\nconduct extensive experiments to demonstrate that the proposed methods achieve\nstate of the art results in the low-label rates regime.\n","authors":["Zhaiming Shen","Sung Ha Kang"],"pdf_url":"https://arxiv.org/pdf/2504.19419v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05567v2","updated":"2025-10-30T17:31:52Z","published":"2025-06-05T20:26:18Z","title":"Partially-Supervised Neural Network Model For Quadratic Multiparametric\n  Programming","summary":"  Neural Networks (NN) with ReLU activation functions are used to model\nmultiparametric quadratic optimization problems (mp-QP) in diverse engineering\napplications. Researchers have suggested leveraging the piecewise affine\nproperty of deep NN models to solve mp-QP with linear constraints, which also\nexhibit piecewise affine behaviour. However, traditional deep NN applications\nto mp-QP fall short of providing optimal and feasible predictions, even when\ntrained on large datasets. This study proposes a partially-supervised NN (PSNN)\narchitecture that directly represents the mathematical structure of the global\nsolution function. In contrast to generic NN training approaches, the proposed\nPSNN method derives a large proportion of model weights directly from the\nmathematical properties of the optimization problem, producing more accurate\nsolutions despite significantly smaller training data sets. Many energy\nmanagement problems are formulated as QP, so we apply the proposed approach to\nenergy systems (specifically DC optimal power flow) to demonstrate proof of\nconcept. Model performance in terms of solution accuracy and speed of\npredictions was compared against a commercial solver and a generic Deep NN\nmodel based on classical training. Results show KKT sufficient conditions for\nPSNN consistently outperform generic NN architectures with classical training\nusing far less data, including when tested on extreme, out-of-training\ndistribution test data. Given its speed advantages over traditional solvers,\nthe PSNN model can quickly produce optimal and feasible solutions within a\nsecond for millions of input parameters sampled from a distribution of\nstochastic demands and renewable generator dispatches, which can be used for\nsimulations and long term planning.\n","authors":["Fuat Can Beylunioglu","Mehrdad Pirnia","P. Robert Duimering"],"pdf_url":"https://arxiv.org/pdf/2506.05567v2.pdf","comment":"36 pages including references and appendix"},{"id":"http://arxiv.org/abs/2410.01755v3","updated":"2025-10-30T17:28:23Z","published":"2024-10-02T17:05:48Z","title":"Integrating Protein Sequence and Expression Level to Analysis Molecular\n  Characterization of Breast Cancer Subtypes","summary":"  Breast cancer's complexity and variability pose significant challenges in\nunderstanding its progression and guiding effective treatment. This study aims\nto integrate protein sequence data with expression levels to improve the\nmolecular characterization of breast cancer subtypes and predict clinical\noutcomes. Using ProtGPT2, a language model specifically designed for protein\nsequences, we generated embeddings that capture the functional and structural\nproperties of proteins. These embeddings were integrated with protein\nexpression levels to form enriched biological representations, which were\nanalyzed using machine learning methods, such as ensemble K-means for\nclustering and XGBoost for classification. Our approach enabled the successful\nclustering of patients into biologically distinct groups and accurately\npredicted clinical outcomes such as survival and biomarker status, achieving\nhigh performance metrics, notably an F1 score of 0.88 for survival and 0.87 for\nbiomarker status prediction. Feature importance analysis identified KMT2C,\nCLASP2, and MYO1B as key proteins involved in hormone signaling, cytoskeletal\nremodeling, and therapy resistance in hormone receptor-positive and\ntriple-negative breast cancer, with potential influence on breast cancer\nsubtype behavior and progression. Furthermore, protein-protein interaction\nnetworks and correlation analyses revealed functional interdependencies among\nproteins that may influence the behavior and progression of breast cancer\nsubtypes. These findings suggest that integrating protein sequence and\nexpression data provides valuable insights into tumor biology and has\nsignificant potential to enhance personalized treatment strategies in breast\ncancer care.\n","authors":["Hossein Sholehrasa","Majid Jaberi-Douraki"],"pdf_url":"https://arxiv.org/pdf/2410.01755v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26723v1","updated":"2025-10-30T17:23:40Z","published":"2025-10-30T17:23:40Z","title":"Bridging the Gap between Empirical Welfare Maximization and Conditional\n  Average Treatment Effect Estimation in Policy Learning","summary":"  The goal of policy learning is to train a policy function that recommends a\ntreatment given covariates to maximize population welfare. There are two major\napproaches in policy learning: the empirical welfare maximization (EWM)\napproach and the plug-in approach. The EWM approach is analogous to a\nclassification problem, where one first builds an estimator of the population\nwelfare, which is a functional of policy functions, and then trains a policy by\nmaximizing the estimated welfare. In contrast, the plug-in approach is based on\nregression, where one first estimates the conditional average treatment effect\n(CATE) and then recommends the treatment with the highest estimated outcome.\nThis study bridges the gap between the two approaches by showing that both are\nbased on essentially the same optimization problem. In particular, we prove an\nexact equivalence between EWM and least squares over a reparameterization of\nthe policy class. As a consequence, the two approaches are interchangeable in\nseveral respects and share the same theoretical guarantees under common\nconditions. Leveraging this equivalence, we propose a novel regularization\nmethod for policy learning. Our findings yield a convex and computationally\nefficient training procedure that avoids the NP-hard combinatorial step\ntypically required in EWM.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2510.26723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26722v1","updated":"2025-10-30T17:22:57Z","published":"2025-10-30T17:22:57Z","title":"Non-Convex Over-the-Air Heterogeneous Federated Learning: A\n  Bias-Variance Trade-off","summary":"  Over-the-air (OTA) federated learning (FL) has been well recognized as a\nscalable paradigm that exploits the waveform superposition of the wireless\nmultiple-access channel to aggregate model updates in a single use. Existing\nOTA-FL designs largely enforce zero-bias model updates by either assuming\n\\emph{homogeneous} wireless conditions (equal path loss across devices) or\nforcing zero-bias updates to guarantee convergence. Under \\emph{heterogeneous}\nwireless scenarios, however, such designs are constrained by the weakest device\nand inflate the update variance. Moreover, prior analyses of biased OTA-FL\nlargely address convex objectives, while most modern AI models are highly\nnon-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient\ndescent (SGD) for general smooth non-convex objectives under wireless\nheterogeneity. We develop novel OTA-FL SGD updates that allow a structured,\ntime-invariant model bias while facilitating reduced variance updates. We\nderive a finite-time stationarity bound (expected time average squared gradient\nnorm) that explicitly reveals a bias-variance trade-off. To optimize this\ntrade-off, we pose a non-convex joint OTA power-control design and develop an\nefficient successive convex approximation (SCA) algorithm that requires only\nstatistical CSI at the base station. Experiments on a non-convex image\nclassification task validate the approach: the SCA-based design accelerates\nconvergence via an optimized bias and improves generalization over prior OTA-FL\nbaselines.\n","authors":["Muhammad Faraz Ul Abrar","Nicolò Michelusi"],"pdf_url":"https://arxiv.org/pdf/2510.26722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26717v1","updated":"2025-10-30T17:18:53Z","published":"2025-10-30T17:18:53Z","title":"On Purely Private Covariance Estimation","summary":"  We present a simple perturbation mechanism for the release of $d$-dimensional\ncovariance matrices $\\Sigma$ under pure differential privacy. For large\ndatasets with at least $n\\geq d^2/\\varepsilon$ elements, our mechanism recovers\nthe provably optimal Frobenius norm error guarantees of\n\\cite{nikolov2023private}, while simultaneously achieving best known error for\nall other $p$-Schatten norms, with $p\\in [1,\\infty]$. Our error is\ninformation-theoretically optimal for all $p\\ge 2$, in particular, our\nmechanism is the first purely private covariance estimator that achieves\noptimal error in spectral norm.\n  For small datasets $n< d^2/\\varepsilon$, we further show that by projecting\nthe output onto the nuclear norm ball of appropriate radius, our algorithm\nachieves the optimal Frobenius norm error $O(\\sqrt{d\\;\\text{Tr}(\\Sigma) /n})$,\nimproving over the known bounds of $O(\\sqrt{d/n})$ of \\cite{nikolov2023private}\nand ${O}\\big(d^{3/4}\\sqrt{\\text{Tr}(\\Sigma)/n}\\big)$ of\n\\cite{dong2022differentially}.\n","authors":["Tommaso d'Orsi","Gleb Novikov"],"pdf_url":"https://arxiv.org/pdf/2510.26717v1.pdf","comment":"equal contribution"},{"id":"http://arxiv.org/abs/2506.08645v2","updated":"2025-10-30T17:15:23Z","published":"2025-06-10T09:57:58Z","title":"When Kernels Multiply, Clusters Unify: Fusing Embeddings with the\n  Kronecker Product","summary":"  State-of-the-art embeddings often capture distinct yet complementary\ndiscriminative features: For instance, one image embedding model may excel at\ndistinguishing fine-grained textures, while another focuses on object-level\nstructure. Motivated by this observation, we propose a principled approach to\nfuse such complementary representations through kernel multiplication.\nMultiplying the kernel similarity functions of two embeddings allows their\ndiscriminative structures to interact, producing a fused representation whose\nkernel encodes the union of the clusters identified by each parent embedding.\nThis formulation also provides a natural way to construct joint kernels for\npaired multi-modal data (e.g., image-text tuples), where the product of\nmodality-specific kernels inherits structure from both domains. We highlight\nthat this kernel product is mathematically realized via the Kronecker product\nof the embedding feature maps, yielding our proposed KrossFuse framework for\nembedding fusion. To address the computational cost of the resulting\nhigh-dimensional Kronecker space, we further develop RP-KrossFuse, a scalable\nvariant that leverages random projections for efficient approximation. As a key\napplication, we use this framework to bridge the performance gap between\ncross-modal embeddings (e.g., CLIP, BLIP) and unimodal experts (e.g., DINOv2,\nE5). Experiments show that RP-KrossFuse effectively integrates these models,\nenhancing modality-specific performance while preserving cross-modal alignment.\nThe project code is available at https://github.com/yokiwuuu/KrossFuse.\n","authors":["Youqi Wu","Jingwei Zhang","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2506.08645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26715v1","updated":"2025-10-30T17:13:58Z","published":"2025-10-30T17:13:58Z","title":"LSM-MS2: A Foundation Model Bridging Spectral Identification and\n  Biological Interpretation","summary":"  A vast majority of mass spectrometry data remains uncharacterized, leaving\nmuch of its biological and chemical information untapped. Recent advances in\nmachine learning have begun to address this gap, particularly for tasks such as\nspectral identification in tandem mass spectrometry data. Here, we present the\nlatest generation of LSM-MS2, a large-scale deep learning foundation model\ntrained on millions of spectra to learn a semantic chemical space. LSM-MS2\nachieves state-of-the-art performance in spectral identification, improving on\nexisting methods by 30% in accuracy of identifying challenging isomeric\ncompounds, yielding 42% more correct identifications in complex biological\nsamples, and maintaining robustness under low-concentration conditions.\nFurthermore, LSM-MS2 produces rich spectral embeddings that enable direct\nbiological interpretation from minimal downstream data, successfully\ndifferentiating disease states and predicting clinical outcomes across diverse\ntranslational applications.\n","authors":["Gabriel Asher","Devesh Shah","Amy A. Caudy","Luke Ferro","Lea Amar","Ana S. H. Costa","Thomas Patton","Niall O'Connor","Jennifer M. Campbell","Jack Geremia"],"pdf_url":"https://arxiv.org/pdf/2510.26715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26714v1","updated":"2025-10-30T17:13:42Z","published":"2025-10-30T17:13:42Z","title":"On the limitation of evaluating machine unlearning using only a single\n  training seed","summary":"  Machine unlearning (MU) aims to remove the influence of certain data points\nfrom a trained model without costly retraining. Most practical MU algorithms\nare only approximate and their performance can only be assessed empirically.\nCare must therefore be taken to make empirical comparisons as representative as\npossible. A common practice is to run the MU algorithm multiple times\nindependently starting from the same trained model. In this work, we\ndemonstrate that this practice can give highly non-representative results\nbecause -- even for the same architecture and same dataset -- some MU methods\ncan be highly sensitive to the choice of random number seed used for model\ntraining. We therefore recommend that empirical\ncomphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should\nalso reflect the variability across different model training seeds.\n","authors":["Jamie Lanyon","Axel Finke","Petros Andreou","Georgina Cosma"],"pdf_url":"https://arxiv.org/pdf/2510.26714v1.pdf","comment":"mini paper, 2 figures"},{"id":"http://arxiv.org/abs/2510.26709v1","updated":"2025-10-30T17:11:01Z","published":"2025-10-30T17:11:01Z","title":"An All-Reduce Compatible Top-K Compressor for Communication-Efficient\n  Distributed Learning","summary":"  Communication remains a central bottleneck in large-scale distributed machine\nlearning, and gradient sparsification has emerged as a promising strategy to\nalleviate this challenge. However, existing gradient compressors face notable\nlimitations: Rand-$K$\\ discards structural information and performs poorly in\npractice, while Top-$K$\\ preserves informative entries but loses the\ncontraction property and requires costly All-Gather operations. In this paper,\nwe propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that\naligns sparsity patterns across nodes using a lightweight sketch of the\ngradient, enabling index-free All-Reduce while preserving globally significant\ninformation. ARC-Top-$K$\\ is provably contractive and, when combined with\nmomentum error feedback (EF21M), achieves linear speedup and sharper\nconvergence rates than the original EF21M under standard assumptions.\nEmpirically, ARC-Top-$K$\\ matches the accuracy of Top-$K$\\ while reducing\nwall-clock training time by up to 60.7\\%, offering an efficient and scalable\nsolution that combines the robustness of Rand-$K$\\ with the strong performance\nof Top-$K$.\n","authors":["Chuyan Chen","Chenyang Ma","Zhangxin Li","Yutong He","Yanjie Dong","Kun Yuan"],"pdf_url":"https://arxiv.org/pdf/2510.26709v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2509.21319v2","updated":"2025-10-30T17:09:54Z","published":"2025-09-25T16:19:06Z","title":"RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards","summary":"  Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models:\nhttps://huggingface.co/collections/nvidia/reward-models-10-2025\n","authors":["Zhilin Wang","Jiaqi Zeng","Olivier Delalleau","Ellie Evans","Daniel Egert","Hoo-Chang Shin","Felipe Soares","Yi Dong","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2509.21319v2.pdf","comment":"Added link to access models:\n  https://huggingface.co/collections/nvidia/reward-models-10-2025"},{"id":"http://arxiv.org/abs/2510.26707v1","updated":"2025-10-30T17:09:09Z","published":"2025-10-30T17:09:09Z","title":"Value Drifts: Tracing Value Alignment During LLM Post-Training","summary":"  As LLMs occupy an increasingly important role in society, they are more and\nmore confronted with questions that require them not only to draw on their\ngeneral knowledge but also to align with certain human value systems.\nTherefore, studying the alignment of LLMs with human values has become a\ncrucial field of inquiry. Prior work, however, mostly focuses on evaluating the\nalignment of fully trained models, overlooking the training dynamics by which\nmodels learn to express human values. In this work, we investigate how and at\nwhich stage value alignment arises during the course of a model's\npost-training. Our analysis disentangles the effects of post-training\nalgorithms and datasets, measuring both the magnitude and time of value drifts\nduring training. Experimenting with Llama-3 and Qwen-3 models of different\nsizes and popular supervised fine-tuning (SFT) and preference optimization\ndatasets and algorithms, we find that the SFT phase generally establishes a\nmodel's values, and subsequent preference optimization rarely re-aligns these\nvalues. Furthermore, using a synthetic preference dataset that enables\ncontrolled manipulation of values, we find that different preference\noptimization algorithms lead to different value alignment outcomes, even when\npreference data is held constant. Our findings provide actionable insights into\nhow values are learned during post-training and help to inform data curation,\nas well as the selection of models and algorithms for preference optimization\nto improve model alignment to human values.\n","authors":["Mehar Bhatia","Shravan Nayak","Gaurav Kamath","Marius Mosbach","Karolina Stańczak","Vered Shwartz","Siva Reddy"],"pdf_url":"https://arxiv.org/pdf/2510.26707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26706v1","updated":"2025-10-30T17:08:52Z","published":"2025-10-30T17:08:52Z","title":"Budgeted Multiple-Expert Deferral","summary":"  Learning to defer uncertain predictions to costly experts offers a powerful\nstrategy for improving the accuracy and efficiency of machine learning systems.\nHowever, standard training procedures for deferral algorithms typically require\nquerying all experts for every training instance, an approach that becomes\nprohibitively expensive when expert queries incur significant computational or\nresource costs. This undermines the core goal of deferral: to limit unnecessary\nexpert usage. To overcome this challenge, we introduce the budgeted deferral\nframework, which aims to train effective deferral algorithms while minimizing\nexpert query costs during training. We propose new algorithms for both\ntwo-stage and single-stage multiple-expert deferral settings that selectively\nquery only a subset of experts per training example. While inspired by active\nlearning, our setting is fundamentally different: labels are already known, and\nthe core challenge is to decide which experts to query in order to balance cost\nand predictive performance. We establish theoretical guarantees for both of our\nalgorithms, including generalization bounds and label complexity analyses.\nEmpirical results across several domains show that our algorithms substantially\nreduce training costs without sacrificing prediction accuracy, demonstrating\nthe practical value of our budget-aware deferral algorithms.\n","authors":["Giulia DeSalvo","Clara Mohri","Mehryar Mohri","Yutao Zhong"],"pdf_url":"https://arxiv.org/pdf/2510.26706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26704v1","updated":"2025-10-30T17:07:14Z","published":"2025-10-30T17:07:14Z","title":"How Regularization Terms Make Invertible Neural Networks Bayesian Point\n  Estimators","summary":"  Can regularization terms in the training of invertible neural networks lead\nto known Bayesian point estimators in reconstruction? Invertible networks are\nattractive for inverse problems due to their inherent stability and\ninterpretability. Recently, optimization strategies for invertible neural\nnetworks that approximate either a reconstruction map or the forward operator\nhave been studied from a Bayesian perspective, but each has limitations. To\naddress this, we introduce and analyze two regularization terms for the network\ntraining that, upon inversion of the network, recover properties of classical\nBayesian point estimators: while the first can be connected to the posterior\nmean, the second resembles the MAP estimator. Our theoretical analysis\ncharacterizes how each loss shapes both the learned forward operator and its\ninverse reconstruction map. Numerical experiments support our findings and\ndemonstrate how these loss-term regularizers introduce data-dependence in a\nstable and interpretable way.\n","authors":["Nick Heilenkötter"],"pdf_url":"https://arxiv.org/pdf/2510.26704v1.pdf","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2505.12275v2","updated":"2025-10-30T17:06:21Z","published":"2025-05-18T07:27:35Z","title":"Curriculum Abductive Learning","summary":"  Abductive Learning (ABL) integrates machine learning with logical reasoning\nin a loop: a learning model predicts symbolic concept labels from raw inputs,\nwhich are revised through abduction using domain knowledge and then fed back\nfor retraining. However, due to the nondeterminism of abduction, the training\nprocess often suffers from instability, especially when the knowledge base is\nlarge and complex, resulting in a prohibitively large abduction space. While\nprior works focus on improving candidate selection within this space, they\ntypically treat the knowledge base as a static black box. In this work, we\npropose Curriculum Abductive Learning (C-ABL), a method that explicitly\nleverages the internal structure of the knowledge base to address the ABL\ntraining challenges. C-ABL partitions the knowledge base into a sequence of\nsub-bases, progressively introduced during training. This reduces the abduction\nspace throughout training and enables the model to incorporate logic in a\nstepwise, smooth way. Experiments across multiple tasks show that C-ABL\noutperforms previous ABL implementations, significantly improves training\nstability, convergence speed, and final accuracy, especially under complex\nknowledge setting.\n","authors":["Wen-Chao Hu","Qi-Jie Li","Lin-Han Jia","Cunjing Ge","Yu-Feng Li","Yuan Jiang","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.12275v2.pdf","comment":"Accepted by NeurIPS 2025, 22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2510.26700v1","updated":"2025-10-30T17:05:57Z","published":"2025-10-30T17:05:57Z","title":"Assessment of the conditional exchangeability assumption in causal\n  machine learning models: a simulation study","summary":"  Observational studies developing causal machine learning (ML) models for the\nprediction of individualized treatment effects (ITEs) seldom conduct empirical\nevaluations to assess the conditional exchangeability assumption. We aimed to\nevaluate the performance of these models under conditional exchangeability\nviolations and the utility of negative control outcomes (NCOs) as a diagnostic.\nWe conducted a simulation study to examine confounding bias in ITE estimates\ngenerated by causal forest and X-learner models under varying conditions,\nincluding the presence or absence of true heterogeneity. We simulated data to\nreflect real-world scenarios with differing levels of confounding, sample size,\nand NCO confounding structures. We then estimated and compared subgroup-level\ntreatment effects on the primary outcome and NCOs across settings with and\nwithout unmeasured confounding. When conditional exchangeability was violated,\ncausal forest and X-learner models failed to recover true treatment effect\nheterogeneity and, in some cases, falsely indicated heterogeneity when there\nwas none. NCOs successfully identified subgroups affected by unmeasured\nconfounding. Even when NCOs did not perfectly satisfy its ideal assumptions, it\nremained informative, flagging potential bias in subgroup level estimates,\nthough not always pinpointing the subgroup with the largest confounding.\nViolations of conditional exchangeability substantially limit the validity of\nITE estimates from causal ML models in routinely collected observational data.\nNCOs serve a useful empirical diagnostic tool for detecting subgroup-specific\nunmeasured confounding and should be incorporated into causal ML workflows to\nsupport the credibility of individualized inference.\n","authors":["Gerard T. Portela","Jason B. Gibbons","Sebastian Schneeweiss","Rishi J. Desai"],"pdf_url":"https://arxiv.org/pdf/2510.26700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20138v2","updated":"2025-10-30T17:04:50Z","published":"2025-03-26T01:00:35Z","title":"Guided Model Merging for Hybrid Data Learning: Leveraging Centralized\n  Data to Refine Decentralized Models","summary":"  Current network training paradigms primarily focus on either centralized or\ndecentralized data regimes. However, in practice, data availability often\nexhibits a hybrid nature, where both regimes coexist. This hybrid setting\npresents new opportunities for model training, as the two regimes offer\ncomplementary trade-offs: decentralized data is abundant but subject to\nheterogeneity and communication constraints, while centralized data, though\nlimited in volume and potentially unrepresentative, enables better curation and\nhigh-throughput access. Despite its potential, effectively combining these\nparadigms remains challenging, and few frameworks are tailored to hybrid data\nregimes. To address this, we propose a novel framework that constructs a model\natlas from decentralized models and leverages centralized data to refine a\nglobal model within this structured space. The refined model is then used to\nreinitialize the decentralized models. Our method synergizes federated learning\n(to exploit decentralized data) and model merging (to utilize centralized\ndata), enabling effective training under hybrid data availability.\nTheoretically, we show that our approach achieves faster convergence than\nmethods relying solely on decentralized data, due to variance reduction in the\nmerging process. Extensive experiments demonstrate that our framework\nconsistently outperforms purely centralized, purely decentralized, and existing\nhybrid-adaptable methods. Notably, our method remains robust even when the\ncentralized and decentralized data domains differ or when decentralized data\ncontains noise, significantly broadening its applicability.\n","authors":["Junyi Zhu","Ruicong Yao","Taha Ceritli","Savas Ozkan","Matthew B. Blaschko","Eunchung Noh","Jeongwon Min","Cho Jung Min","Mete Ozay"],"pdf_url":"https://arxiv.org/pdf/2503.20138v2.pdf","comment":"Accepted at WACV 2026"},{"id":"http://arxiv.org/abs/2510.26692v1","updated":"2025-10-30T16:59:43Z","published":"2025-10-30T16:59:43Z","title":"Kimi Linear: An Expressive, Efficient Attention Architecture","summary":"  We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.\n","authors":[" Kimi Team","Yu Zhang","Zongyu Lin","Xingcheng Yao","Jiaxi Hu","Fanqing Meng","Chengyin Liu","Xin Men","Songlin Yang","Zhiyuan Li","Wentao Li","Enzhe Lu","Weizhou Liu","Yanru Chen","Weixin Xu","Longhui Yu","Yejie Wang","Yu Fan","Longguang Zhong","Enming Yuan","Dehao Zhang","Yizhi Zhang","T. Y. Liu","Haiming Wang","Shengjun Fang","Weiran He","Shaowei Liu","Yiwei Li","Jianlin Su","Jiezhong Qiu","Bo Pang","Junjie Yan","Zhejun Jiang","Weixiao Huang","Bohong Yin","Jiacheng You","Chu Wei","Zhengtao Wang","Chao Hong","Yutian Chen","Guanduo Chen","Yucheng Wang","Huabin Zheng","Feng Wang","Yibo Liu","Mengnan Dong","Zheng Zhang","Siyuan Pan","Wenhao Wu","Yuhao Wu","Longyu Guan","Jiawen Tao","Guohong Fu","Xinran Xu","Yuzhi Wang","Guokun Lai","Yuxin Wu","Xinyu Zhou","Zhilin Yang","Yulun Du"],"pdf_url":"https://arxiv.org/pdf/2510.26692v1.pdf","comment":"Kimi Linear tech report"},{"id":"http://arxiv.org/abs/2510.26690v1","updated":"2025-10-30T16:59:22Z","published":"2025-10-30T16:59:22Z","title":"LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits","summary":"  Low-Rank Adaptation (LoRA) has become a popular technique for\nparameter-efficient fine-tuning of large language models (LLMs). In many\nreal-world scenarios, multiple adapters are loaded simultaneously to enable LLM\ncustomization for personalized user experiences or to support a diverse range\nof tasks. Although each adapter is lightweight in isolation, their aggregate\ncost becomes substantial at scale. To address this, we propose LoRAQuant, a\nmixed-precision post-training quantization method tailored to LoRA.\nSpecifically, LoRAQuant reparameterizes each adapter by singular value\ndecomposition (SVD) to concentrate the most important information into specific\nrows and columns. This makes it possible to quantize the important components\nto higher precision, while quantizing the rest to ultra-low bitwidth. We\nconduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B\nmodels on mathematical reasoning, coding, and summarization tasks. Results show\nthat our LoRAQuant uses significantly lower bits than other quantization\nmethods, but achieves comparable or even higher performance.\n","authors":["Amir Reza Mirzaei","Yuqiao Wen","Yanshuai Cao","Lili Mou"],"pdf_url":"https://arxiv.org/pdf/2510.26690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26688v1","updated":"2025-10-30T16:57:13Z","published":"2025-10-30T16:57:13Z","title":"FlowQ-Net: A Generative Framework for Automated Quantum Circuit Design","summary":"  Designing efficient quantum circuits is a central bottleneck to exploring the\npotential of quantum computing, particularly for noisy intermediate-scale\nquantum (NISQ) devices, where circuit efficiency and resilience to errors are\nparamount. The search space of gate sequences grows combinatorially, and\nhandcrafted templates often waste scarce qubit and depth budgets. We introduce\n\\textsc{FlowQ-Net} (Flow-based Quantum design Network), a generative framework\nfor automated quantum circuit synthesis based on Generative Flow Networks\n(GFlowNets). This framework learns a stochastic policy to construct circuits\nsequentially, sampling them in proportion to a flexible, user-defined reward\nfunction that can encode multiple design objectives such as performance, depth,\nand gate count. This approach uniquely enables the generation of a diverse\nensemble of high-quality circuits, moving beyond single-solution optimization.\nWe demonstrate the efficacy of \\textsc{FlowQ-Net} through an extensive set of\nsimulations. We apply our method to Variational Quantum Algorithm (VQA) ansatz\ndesign for molecular ground state estimation, Max-Cut, and image\nclassification, key challenges in near-term quantum computing. Circuits\ndesigned by \\textsc{FlowQ-Net} achieve significant improvements, yielding\ncircuits that are 10$\\times$-30$\\times$ more compact in terms of parameters,\ngates, and depth compared to commonly used unitary baselines, without\ncompromising accuracy. This trend holds even when subjected to error profiles\nfrom real-world quantum devices. Our results underline the potential of\ngenerative models as a general-purpose methodology for automated quantum\ncircuit design, offering a promising path towards more efficient quantum\nalgorithms and accelerating scientific discovery in the quantum domain.\n","authors":["Jun Dai","Michael Rizvi-Martel","Guillaume Rabusseau"],"pdf_url":"https://arxiv.org/pdf/2510.26688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17434v5","updated":"2025-10-30T16:47:32Z","published":"2023-11-29T08:26:18Z","title":"GSE: Group-wise Sparse and Explainable Adversarial Attacks","summary":"  Sparse adversarial attacks fool deep neural networks (DNNs) through minimal\npixel perturbations, often regularized by the $\\ell_0$ norm. Recent efforts\nhave replaced this norm with a structural sparsity regularizer, such as the\nnuclear group norm, to craft group-wise sparse adversarial attacks. The\nresulting perturbations are thus explainable and hold significant practical\nrelevance, shedding light on an even greater vulnerability of DNNs. However,\ncrafting such attacks poses an optimization challenge, as it involves computing\nnorms for groups of pixels within a non-convex objective. We address this by\npresenting a two-phase algorithm that generates group-wise sparse attacks\nwithin semantically meaningful areas of an image. Initially, we optimize a\nquasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored\nfor non-convex programming. Subsequently, the algorithm transitions to a\nprojected Nesterov's accelerated gradient descent with $2-$norm regularization\napplied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and\nImageNet datasets demonstrate a remarkable increase in group-wise sparsity,\ne.g., $50.9\\%$ on CIFAR-10 and $38.4\\%$ on ImageNet (average case, targeted\nattack). This performance improvement is accompanied by significantly faster\ncomputation times, improved explainability, and a $100\\%$ attack success rate.\n","authors":["Shpresim Sadiku","Moritz Wagner","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2311.17434v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26679v1","updated":"2025-10-30T16:47:26Z","published":"2025-10-30T16:47:26Z","title":"Tight Differentially Private PCA via Matrix Coherence","summary":"  We revisit the task of computing the span of the top $r$ singular vectors\n$u_1, \\ldots, u_r$ of a matrix under differential privacy. We show that a\nsimple and efficient algorithm -- based on singular value decomposition and\nstandard perturbation mechanisms -- returns a private rank-$r$ approximation\nwhose error depends only on the \\emph{rank-$r$ coherence} of $u_1, \\ldots, u_r$\nand the spectral gap $\\sigma_r - \\sigma_{r+1}$. This resolves a question posed\nby Hardt and Roth~\\cite{hardt2013beyond}. Our estimator outperforms the state\nof the art -- significantly so in some regimes. In particular, we show that in\nthe dense setting, it achieves the same guarantees for single-spike PCA in the\nWishart model as those attained by optimal non-private algorithms, whereas\nprior private algorithms failed to do so.\n  In addition, we prove that (rank-$r$) coherence does not increase under\nGaussian perturbations. This implies that any estimator based on the Gaussian\nmechanism -- including ours -- preserves the coherence of the input. We\nconjecture that similar behavior holds for other structured models, including\nplanted problems in graphs.\n  We also explore applications of coherence to graph problems. In particular,\nwe present a differentially private algorithm for Max-Cut and other constraint\nsatisfaction problems under low coherence assumptions.\n","authors":["Tommaso d'Orsi","Gleb Novikov"],"pdf_url":"https://arxiv.org/pdf/2510.26679v1.pdf","comment":"SODA 2026; equal contribution"},{"id":"http://arxiv.org/abs/2510.26672v1","updated":"2025-10-30T16:42:09Z","published":"2025-10-30T16:42:09Z","title":"Action-Driven Processes for Continuous-Time Control","summary":"  At the heart of reinforcement learning are actions - decisions made in\nresponse to observations of the environment. Actions are equally fundamental in\nthe modeling of stochastic processes, as they trigger discontinuous state\ntransitions and enable the flow of information through large, complex systems.\nIn this paper, we unify the perspectives of stochastic processes and\nreinforcement learning through action- driven processes, and illustrate their\napplication to spiking neural networks. Leveraging ideas from\ncontrol-as-inference, we show that minimizing the Kullback-Leibler divergence\nbetween a policy-driven true distribution and a reward-driven model\ndistribution for a suitably defined action-driven process is equivalent to\nmaximum entropy reinforcement learning.\n","authors":["Ruimin He","Shaowei Lin"],"pdf_url":"https://arxiv.org/pdf/2510.26672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26656v1","updated":"2025-10-30T16:23:46Z","published":"2025-10-30T16:23:46Z","title":"Heuristic Adaptation of Potentially Misspecified Domain Support for\n  Likelihood-Free Inference in Stochastic Dynamical Systems","summary":"  In robotics, likelihood-free inference (LFI) can provide the domain\ndistribution that adapts a learnt agent in a parametric set of deployment\nconditions. LFI assumes an arbitrary support for sampling, which remains\nconstant as the initial generic prior is iteratively refined to more\ndescriptive posteriors. However, a potentially misspecified support can lead to\nsuboptimal, yet falsely certain, posteriors. To address this issue, we propose\nthree heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the\nposterior mode shift over inference steps in its own way and, when integrated\ninto an LFI step, adapts the support alongside posterior inference. We first\nexpose the support misspecification issue and evaluate our heuristics using\nstochastic dynamical benchmarks. We then evaluate the impact of heuristic\nsupport adaptation on parameter inference and policy learning for a dynamic\ndeformable linear object (DLO) manipulation task. Inference results in a finer\nlength and stiffness classification for a parametric set of DLOs. When the\nresulting posteriors are used as domain distributions for sim-based policy\nlearning, they lead to more robust object-centric agent performance.\n","authors":["Georgios Kamaras","Craig Innes","Subramanian Ramamoorthy"],"pdf_url":"https://arxiv.org/pdf/2510.26656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26646v1","updated":"2025-10-30T16:12:01Z","published":"2025-10-30T16:12:01Z","title":"Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in\n  Dynamic Environments","summary":"  This paper presents a hierarchical path-planning and control framework that\ncombines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with\na low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller\nfor continuous actuation. The high-level module selects behaviors and\nsub-goals; the low-level module executes smooth velocity commands. We design a\npractical reward shaping scheme (direction, distance, obstacle avoidance,\naction smoothness, collision penalty, time penalty, and progress), together\nwith a LiDAR-based safety gate that prevents unsafe motions. The system is\nimplemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,\nincluding success rate, collision rate, path efficiency, and re-planning\nefficiency, in dynamic and partially observable environments. Experiments show\nimproved success rate and sample efficiency over single-algorithm baselines\n(DQN or TD3 alone) and rule-based planners, with better generalization to\nunseen obstacle configurations and reduced abrupt control changes. Code and\nevaluation scripts are available at the project repository.\n","authors":["Xiaoyi He","Danggui Chen","Zhenshuo Zhang","Zimeng Bai"],"pdf_url":"https://arxiv.org/pdf/2510.26646v1.pdf","comment":"6 pages, 5 figures; ROS+Gazebo (TurtleBot3) implementation;\n  evaluation with PathBench metrics; code (primary):\n  https://github.com/MayaCHEN-github/HierarchicalRL-robot-navigation; mirror\n  (for reproducibility): https://github.com/ShowyHe/DRL-robot-navigation"},{"id":"http://arxiv.org/abs/2510.26645v1","updated":"2025-10-30T16:11:39Z","published":"2025-10-30T16:11:39Z","title":"Curly Flow Matching for Learning Non-gradient Field Dynamics","summary":"  Modeling the transport dynamics of natural processes from population-level\nobservations is a ubiquitous problem in the natural sciences. Such models rely\non key assumptions about the underlying process in order to enable faithful\nlearning of governing dynamics that mimic the actual system behavior. The de\nfacto assumption in current approaches relies on the principle of least action\nthat results in gradient field dynamics and leads to trajectories minimizing an\nenergy functional between two probability measures. However, many real-world\nsystems, such as cell cycles in single-cell RNA, are known to exhibit\nnon-gradient, periodic behavior, which fundamentally cannot be captured by\ncurrent state-of-the-art methods such as flow and bridge matching. In this\npaper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is\ncapable of learning non-gradient field dynamics by designing and solving a\nSchr\\\"odinger bridge problem with a non-zero drift reference process -- in\nstark contrast to typical zero-drift reference processes -- which is\nconstructed using inferred velocities in addition to population snapshot data.\nWe showcase Curly-FM by solving the trajectory inference problems for single\ncells, computational fluid dynamics, and ocean currents with approximate\nvelocities. We demonstrate that Curly-FM can learn trajectories that better\nmatch both the reference process and population marginals. Curly-FM expands\nflow matching models beyond the modeling of populations and towards the\nmodeling of known periodic behavior in physical systems. Our code repository is\naccessible at: https://github.com/kpetrovicc/curly-flow-matching.git\n","authors":["Katarina Petrović","Lazar Atanackovic","Viggo Moro","Kacper Kapuśniak","İsmail İlkan Ceylan","Michael Bronstein","Avishek Joey Bose","Alexander Tong"],"pdf_url":"https://arxiv.org/pdf/2510.26645v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26643v1","updated":"2025-10-30T16:09:51Z","published":"2025-10-30T16:09:51Z","title":"MSAD: A Deep Dive into Model Selection for Time series Anomaly Detection","summary":"  Anomaly detection is a fundamental task for time series analytics with\nimportant implications for the downstream performance of many applications.\nDespite increasing academic interest and the large number of methods proposed\nin the literature, recent benchmarks and evaluation studies demonstrated that\nno overall best anomaly detection methods exist when applied to very\nheterogeneous time series datasets. Therefore, the only scalable and viable\nsolution to solve anomaly detection over very different time series collected\nfrom diverse domains is to propose a model selection method that will select,\nbased on time series characteristics, the best anomaly detection methods to\nrun. Existing AutoML solutions are, unfortunately, not directly applicable to\ntime series anomaly detection, and no evaluation of time series-based\napproaches for model selection exists. Towards that direction, this paper\nstudies the performance of time series classification methods used as model\nselection for anomaly detection. In total, we evaluate 234 model configurations\nderived from 16 base classifiers across more than 1980 time series, and we\npropose the first extensive experimental evaluation of time series\nclassification as model selection for anomaly detection. Our results\ndemonstrate that model selection methods outperform every single anomaly\ndetection method while being in the same order of magnitude regarding execution\ntime. This evaluation is the first step to demonstrate the accuracy and\nefficiency of time series classification algorithms for anomaly detection, and\nrepresents a strong baseline that can then be used to guide the model selection\nstep in general AutoML pipelines. Preprint version of an article accepted at\nthe VLDB Journal.\n","authors":["Emmanouil Sylligardos","John Paparrizos","Themis Palpanas","Pierre Senellart","Paul Boniol"],"pdf_url":"https://arxiv.org/pdf/2510.26643v1.pdf","comment":"25 pages, 13 figures, VLDB Journal"},{"id":"http://arxiv.org/abs/2507.00927v3","updated":"2025-10-30T16:05:04Z","published":"2025-07-01T16:34:19Z","title":"Understanding Generalization in Node and Link Prediction","summary":"  Using message-passing graph neural networks (MPNNs) for node and link\nprediction is crucial in various scientific and industrial domains, which has\nled to the development of diverse MPNN architectures. Besides working well in\npractical settings, their ability to generalize beyond the training set remains\npoorly understood. While some studies have explored MPNNs' generalization in\ngraph-level prediction tasks, much less attention has been given to node- and\nlink-level predictions. Existing works often rely on unrealistic i.i.d.\\@\nassumptions, overlooking possible correlations between nodes or links, and\nassuming fixed aggregation and impractical loss functions while neglecting the\ninfluence of graph structure. In this work, we introduce a unified framework to\nanalyze the generalization properties of MPNNs in inductive and transductive\nnode and link prediction settings, incorporating diverse architectural\nparameters and loss functions and quantifying the influence of graph structure.\nAdditionally, our proposed generalization framework can be applied beyond\ngraphs to any classification task under the inductive or transductive setting.\nOur empirical study supports our theoretical insights, deepening our\nunderstanding of MPNNs' generalization capabilities in these tasks.\n","authors":["Antonis Vasileiou","Timo Stoll","Christopher Morris"],"pdf_url":"https://arxiv.org/pdf/2507.00927v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2412.07106"},{"id":"http://arxiv.org/abs/2510.26633v1","updated":"2025-10-30T16:02:53Z","published":"2025-10-30T16:02:53Z","title":"Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian\n  Optimization","summary":"  Bayesian Optimization (BO) has the potential to solve various combinatorial\ntasks, ranging from materials science to neural architecture search. However,\nBO requires specialized kernels to effectively model combinatorial domains.\nRecent efforts have introduced several combinatorial kernels, but the\nrelationships among them are not well understood. To bridge this gap, we\ndevelop a unifying framework based on heat kernels, which we derive in a\nsystematic way and express as simple closed-form expressions. Using this\nframework, we prove that many successful combinatorial kernels are either\nrelated or equivalent to heat kernels, and validate this theoretical claim in\nour experiments. Moreover, our analysis confirms and extends the results\npresented in Bounce: certain algorithms' performance decreases substantially\nwhen the unknown optima of the function do not have a certain structure. In\ncontrast, heat kernels are not sensitive to the location of the optima. Lastly,\nwe show that a fast and simple pipeline, relying on heat kernels, is able to\nachieve state-of-the-art results, matching or even outperforming certain slow\nor complex algorithms.\n","authors":["Colin Doumont","Victor Picheny","Viacheslav Borovitskiy","Henry Moss"],"pdf_url":"https://arxiv.org/pdf/2510.26633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.21258v2","updated":"2025-10-30T16:01:21Z","published":"2025-08-28T23:09:54Z","title":"RelP: Faithful and Efficient Circuit Discovery in Language Models via\n  Relevance Patching","summary":"  Activation patching is a standard method in mechanistic interpretability for\nlocalizing the components of a model responsible for specific behaviors, but it\nis computationally expensive to apply at scale. Attribution patching offers a\nfaster, gradient-based approximation, yet suffers from noise and reduced\nreliability in deep, highly non-linear networks. In this work, we introduce\nRelevance Patching (RelP), which replaces the local gradients in attribution\npatching with propagation coefficients derived from Layer-wise Relevance\nPropagation (LRP). LRP propagates the network's output backward through the\nlayers, redistributing relevance to lower-level components according to local\npropagation rules that ensure properties such as relevance conservation or\nimproved signal-to-noise ratio. Like attribution patching, RelP requires only\ntwo forward passes and one backward pass, maintaining computational efficiency\nwhile improving faithfulness. We validate RelP across a range of models and\ntasks, showing that it more accurately approximates activation patching than\nstandard attribution patching, particularly when analyzing residual stream and\nMLP outputs in the Indirect Object Identification (IOI) task. For instance, for\nMLP outputs in GPT-2 Large, attribution patching achieves a Pearson correlation\nof 0.006, whereas RelP reaches 0.956, highlighting the improvement offered by\nRelP. Additionally, we compare the faithfulness of sparse feature circuits\nidentified by RelP and Integrated Gradients (IG), showing that RelP achieves\ncomparable faithfulness without the extra computational cost associated with\nIG.\n","authors":["Farnoush Rezaei Jafari","Oliver Eberle","Ashkan Khakzar","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2508.21258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.01988v3","updated":"2025-10-30T15:47:25Z","published":"2025-10-02T13:07:37Z","title":"PepCompass: Navigating peptide embedding spaces using Riemannian\n  Geometry","summary":"  Antimicrobial peptide discovery is challenged by the astronomical size of\npeptide space and the relative scarcity of active peptides. Generative models\nprovide continuous latent \"maps\" of peptide space, but conventionally ignore\ndecoder-induced geometry and rely on flat Euclidean metrics, rendering\nexploration and optimization distorted and inefficient. Prior manifold-based\nremedies assume fixed intrinsic dimensionality, which critically fails in\npractice for peptide data. Here, we introduce PepCompass, a geometry-aware\nframework for peptide exploration and optimization. At its core, we define a\nUnion of $\\kappa$-Stable Riemannian Manifolds $\\mathbb{M}^{\\kappa}$, a family\nof decoder-induced manifolds that captures local geometry while ensuring\ncomputational stability. We propose two local exploration methods: Second-Order\nRiemannian Brownian Efficient Sampling, which provides a convergent\nsecond-order approximation to Riemannian Brownian motion, and Mutation\nEnumeration in Tangent Space, which reinterprets tangent directions as discrete\namino-acid substitutions. Combining these yields Local Enumeration Bayesian\nOptimization (LE-BO), an efficient algorithm for local activity optimization.\nFinally, we introduce Potential-minimizing Geodesic Search (PoGS), which\ninterpolates between prototype embeddings along property-enriched geodesics,\nbiasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro\nvalidation confirms the effectiveness of PepCompass: PoGS yields four novel\nseeds, and subsequent optimization with LE-BO discovers 25 highly active\npeptides with broad-spectrum activity, including against resistant bacterial\nstrains. These results demonstrate that geometry-informed exploration provides\na powerful new paradigm for antimicrobial peptide design.\n","authors":["Marcin Możejko","Adam Bielecki","Jurand Prądzyński","Marcin Traskowski","Antoni Janowski","Hyun-Su Lee","Marcelo Der Torossian Torres","Michał Kmicikiewicz","Paulina Szymczak","Karol Jurasz","Michał Kucharczyk","Cesar de la Fuente-Nunez","Ewa Szczurek"],"pdf_url":"https://arxiv.org/pdf/2510.01988v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17773v3","updated":"2025-10-30T15:43:22Z","published":"2025-05-23T11:44:02Z","title":"C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in\n  Large Language Models","summary":"  Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning\nlarge language models (LLMs), but it often produces overconfident predictions\nin data-scarce few-shot settings. To address this issue, several classical\nstatistical learning approaches have been repurposed for scalable\nuncertainty-aware LoRA fine-tuning. However, these approaches neglect how input\ncharacteristics affect the predictive uncertainty estimates. To address this\nlimitation, we propose Contextual Low-Rank Adaptation (C-LoRA) as a novel\nuncertainty-aware and parameter efficient fine-tuning approach, by developing\nnew lightweight LoRA modules contextualized to each input data sample to\ndynamically adapt uncertainty estimates. Incorporating data-driven contexts\ninto the parameter posteriors, C-LoRA mitigates overfitting, achieves\nwell-calibrated uncertainties, and yields robust predictions. Extensive\nexperiments on LLaMA2-7B models demonstrate that C-LoRA consistently\noutperforms the state-of-the-art uncertainty-aware LoRA methods in both\nuncertainty quantification and model generalization. Ablation studies further\nconfirm the critical role of our contextual modules in capturing\nsample-specific uncertainties. C-LoRA sets a new standard for robust,\nuncertainty-aware LLM fine-tuning in few-shot regimes. Although our experiments\nare limited to 7B models, our method is architecture-agnostic and, in\nprinciple, applies beyond this scale; studying its scaling to larger models\nremains an open problem. Our code is available at\nhttps://github.com/ahra99/c_lora.\n","authors":["Amir Hossein Rahmati","Sanket Jantre","Weifeng Zhang","Yucheng Wang","Byung-Jun Yoon","Nathan M. Urban","Xiaoning Qian"],"pdf_url":"https://arxiv.org/pdf/2505.17773v3.pdf","comment":"Conference on Neural Information Processing Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.26616v1","updated":"2025-10-30T15:41:43Z","published":"2025-10-30T15:41:43Z","title":"Aeolus: A Multi-structural Flight Delay Dataset","summary":"  We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed\nto advance research on flight delay prediction and support the development of\nfoundation models for tabular data. Existing datasets in this domain are\ntypically limited to flat tabular structures and fail to capture the\nspatiotemporal dynamics inherent in delay propagation. Aeolus addresses this\nlimitation by providing three aligned modalities: (i) a tabular dataset with\nrich operational, meteorological, and airportlevel features for over 50 million\nflights; (ii) a flight chain module that models delay propagation along\nsequential flight legs, capturing upstream and downstream dependencies; and\n(iii) a flight network graph that encodes shared aircraft, crew, and airport\nresource connections, enabling cross-flight relational reasoning. The dataset\nis carefully constructed with temporal splits, comprehensive features, and\nstrict leakage prevention to support realistic and reproducible machine\nlearning evaluation. Aeolus supports a broad range of tasks, including\nregression, classification, temporal structure modeling, and graph learning,\nserving as a unified benchmark across tabular, sequential, and graph\nmodalities. We release baseline experiments and preprocessing tools to\nfacilitate adoption. Aeolus fills a key gap for both domain-specific modeling\nand general-purpose structured data research.Our source code and data can be\naccessed at https://github.com/Flnny/Delay-data\n","authors":["Lin Xu","Xinyun Yuan","Yuxuan Liang","Suwan Yin","Yuankai Wu"],"pdf_url":"https://arxiv.org/pdf/2510.26616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.14904v2","updated":"2025-10-30T15:39:25Z","published":"2025-10-16T17:20:22Z","title":"MaskCaptioner: Learning to Jointly Segment and Caption Object\n  Trajectories in Videos","summary":"  Dense Video Object Captioning (DVOC) is the task of jointly detecting,\ntracking, and captioning object trajectories in a video, requiring the ability\nto understand spatio-temporal details and describe them in natural language.\nDue to the complexity of the task and the high cost associated with manual\nannotation, previous approaches resort to disjoint training strategies,\npotentially leading to suboptimal performance. To circumvent this issue, we\npropose to generate captions about spatio-temporally localized entities\nleveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets\nwith our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an\nend-to-end model capable of jointly detecting, segmenting, tracking and\ncaptioning object trajectories. Moreover, with pretraining on LVISCap and\nLV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three\nexisting benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are\navailable at https://www.gabriel.fiastre.fr/maskcaptioner/.\n","authors":["Gabriel Fiastre","Antoine Yang","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2510.14904v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.26609v1","updated":"2025-10-30T15:37:40Z","published":"2025-10-30T15:37:40Z","title":"CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for\n  Satellite Sensing","summary":"  Accurate and timely crop yield prediction is crucial for global food security\nand modern agricultural management. Traditional methods often lack the\nscalability and granularity required for precision farming. This paper\nintroduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder\nfor Satellite Sensing), a deep learning model designed for high-resolution,\nintra-field canola yield prediction. CYPRESS leverages a pre-trained,\nlarge-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for\na continuous regression task, transforming multi-temporal satellite imagery\ninto dense, pixel-level yield maps. Evaluated on a comprehensive dataset from\nthe Canadian Prairies, CYPRESS demonstrates superior performance over existing\ndeep learning-based yield prediction models, highlighting the effectiveness of\nfine-tuning foundation models for specialized agricultural applications. By\nproviding a continuous, high-resolution output, CYPRESS offers a more\nactionable tool for precision agriculture than conventional classification or\ncounty-level aggregation methods. This work validates a novel approach that\nbridges the gap between large-scale Earth observation and on-farm\ndecision-making, offering a scalable solution for detailed agricultural\nmonitoring.\n","authors":["Shayan Nejadshamsi","Yuanyuan Zhang","Shadi Zaki","Brock Porth","Lysa Porth","Vahab Khoshdel"],"pdf_url":"https://arxiv.org/pdf/2510.26609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26607v1","updated":"2025-10-30T15:36:39Z","published":"2025-10-30T15:36:39Z","title":"Wasserstein Regression as a Variational Approximation of Probabilistic\n  Trajectories through the Bernstein Basis","summary":"  This paper considers the problem of regression over distributions, which is\nbecoming increasingly important in machine learning. Existing approaches often\nignore the geometry of the probability space or are computationally expensive.\nTo overcome these limitations, a new method is proposed that combines the\nparameterization of probability trajectories using a Bernstein basis and the\nminimization of the Wasserstein distance between distributions. The key idea is\nto model a conditional distribution as a smooth probability trajectory defined\nby a weighted sum of Gaussian components whose parameters -- the mean and\ncovariance -- are functions of the input variable constructed using Bernstein\npolynomials. The loss function is the averaged squared Wasserstein distance\nbetween the predicted Gaussian distributions and the empirical data, which\ntakes into account the geometry of the distributions. An autodiff-based\noptimization method is used to train the model. Experiments on synthetic\ndatasets that include complex trajectories demonstrated that the proposed\nmethod provides competitive approximation quality in terms of the Wasserstein\ndistance, Energy Distance, and RMSE metrics, especially in cases of pronounced\nnonlinearity. The model demonstrates trajectory smoothness that is better than\nor comparable to alternatives and robustness to changes in data structure,\nwhile maintaining high interpretability due to explicit parameterization via\ncontrol points. The developed approach represents a balanced solution that\ncombines geometric accuracy, computational practicality, and interpretability.\nProspects for further research include extending the method to non-Gaussian\ndistributions, applying entropy regularization to speed up computations, and\nadapting the approach to working with high-dimensional data for approximating\nsurfaces and more complex structures.\n","authors":["Maksim Maslov","Alexander Kugaevskikh","Matthew Ivanov"],"pdf_url":"https://arxiv.org/pdf/2510.26607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.08604v2","updated":"2025-10-30T15:33:58Z","published":"2025-10-07T09:40:20Z","title":"LatentBreak: Jailbreaking Large Language Models through Latent Space\n  Feedback","summary":"  Jailbreaks are adversarial attacks designed to bypass the built-in safety\nmechanisms of large language models. Automated jailbreaks typically optimize an\nadversarial suffix or adapt long prompt templates by forcing the model to\ngenerate the initial part of a restricted or harmful response. In this work, we\nshow that existing jailbreak attacks that leverage such mechanisms to unlock\nthe model response can be detected by a straightforward perplexity-based\nfiltering on the input prompt. To overcome this issue, we propose LatentBreak,\na white-box jailbreak attack that generates natural adversarial prompts with\nlow perplexity capable of evading such defenses. LatentBreak substitutes words\nin the input prompt with semantically-equivalent ones, preserving the initial\nintent of the prompt, instead of adding high-perplexity adversarial suffixes or\nlong templates. These words are chosen by minimizing the distance in the latent\nspace between the representation of the adversarial prompt and that of harmless\nrequests. Our extensive evaluation shows that LatentBreak leads to shorter and\nlow-perplexity prompts, thus outperforming competing jailbreak algorithms\nagainst perplexity-based filters on multiple safety-aligned models.\n","authors":["Raffaele Mura","Giorgio Piras","Kamilė Lukošiūtė","Maura Pintor","Amin Karbasi","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2510.08604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17197v2","updated":"2025-10-30T15:26:13Z","published":"2025-09-21T18:54:54Z","title":"SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal\n  Processing","summary":"  Modern signal processing (SP) pipelines, whether model-based or data-driven,\noften constrained by complex and fragmented workflow, rely heavily on expert\nknowledge and manual engineering, and struggle with adaptability and\ngeneralization under limited data. In contrast, Large Language Models (LLMs)\noffer strong reasoning capabilities, broad general-purpose knowledge,\nin-context learning, and cross-modal transfer abilities, positioning them as\npowerful tools for automating and generalizing SP workflows. Motivated by these\npotentials, we introduce SignalLLM, the first general-purpose LLM-based agent\nframework for general SP tasks. Unlike prior LLM-based SP approaches that are\nlimited to narrow applications or tricky prompting, SignalLLM introduces a\nprincipled, modular architecture. It decomposes high-level SP goals into\nstructured subtasks via in-context learning and domain-specific retrieval,\nfollowed by hierarchical planning through adaptive retrieval-augmented\ngeneration (RAG) and refinement; these subtasks are then executed through\nprompt-based reasoning, cross-modal reasoning, code synthesis, model\ninvocation, or data-driven LLM-assisted modeling. Its generalizable design\nenables the flexible selection of problem solving strategies across different\nsignal modalities, task types, and data conditions. We demonstrate the\nversatility and effectiveness of SignalLLM through five representative tasks in\ncommunication and sensing, such as radar target detection, human activity\nrecognition, and text compression. Experimental results show superior\nperformance over traditional and existing LLM-based methods, particularly in\nfew-shot and zero-shot settings.\n","authors":["Junlong Ke","Qiying Hu","Shenghai Yuan","Yuecong Xu","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2509.17197v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2510.26593v1","updated":"2025-10-30T15:18:07Z","published":"2025-10-30T15:18:07Z","title":"Hybrid Physical-Neural Simulator for Fast Cosmological Hydrodynamics","summary":"  Cosmological field-level inference requires differentiable forward models\nthat solve the challenging dynamics of gas and dark matter under hydrodynamics\nand gravity. We propose a hybrid approach where gravitational forces are\ncomputed using a differentiable particle-mesh solver, while the hydrodynamics\nare parametrized by a neural network that maps local quantities to an effective\npressure field. We demonstrate that our method improves upon alternative\napproaches, such as an Enthalpy Gradient Descent baseline, both at the field\nand summary-statistic level. The approach is furthermore highly data efficient,\nwith a single reference simulation of cosmological structure formation being\nsufficient to constrain the neural pressure model. This opens the door for\nfuture applications where the model is fit directly to observational data,\nrather than a training set of simulations.\n","authors":["Arne Thomsen","Tilman Tröster","François Lanusse"],"pdf_url":"https://arxiv.org/pdf/2510.26593v1.pdf","comment":"Accepted to the NeurIPS 2025 Workshop on Machine Learning and the\n  Physical Sciences"},{"id":"http://arxiv.org/abs/2510.26586v1","updated":"2025-10-30T15:13:25Z","published":"2025-10-30T15:13:25Z","title":"Physics-Informed Mixture Models and Surrogate Models for Precision\n  Additive Manufacturing","summary":"  In this study, we leverage a mixture model learning approach to identify\ndefects in laser-based Additive Manufacturing (AM) processes. By incorporating\nphysics based principles, we also ensure that the model is sensitive to\nmeaningful physical parameter variations. The empirical evaluation was\nconducted by analyzing real-world data from two AM processes: Directed Energy\nDeposition and Laser Powder Bed Fusion. In addition, we also studied the\nperformance of the developed framework over public datasets with different\nalloy type and experimental parameter information. The results show the\npotential of physics-guided mixture models to examine the underlying physical\nbehavior of an AM system.\n","authors":["Sebastian Basterrech","Shuo Shan","Debabrata Adhikari","Sankhya Mohanty"],"pdf_url":"https://arxiv.org/pdf/2510.26586v1.pdf","comment":"Five pages, four figures, to be presented at the AI in Science\n  Summit, Denmark, November, 2025"},{"id":"http://arxiv.org/abs/2410.06324v4","updated":"2025-10-30T15:07:47Z","published":"2024-10-08T20:01:39Z","title":"Differentiation Through Black-Box Quadratic Programming Solvers","summary":"  Differentiable optimization has attracted significant research interest,\nparticularly for quadratic programming (QP). Existing approaches for\ndifferentiating the solution of a QP with respect to its defining parameters\noften rely on specific integrated solvers. This integration limits their\napplicability, including their use in neural network architectures and bi-level\noptimization tasks, restricting users to a narrow selection of solver choices.\nTo address this limitation, we introduce dQP, a modular and solver-agnostic\nframework for plug-and-play differentiation of virtually any QP solver. A key\ninsight we leverage to achieve modularity is that, once the active set of\ninequality constraints is known, both the solution and its derivative can be\nexpressed using simplified linear systems that share the same matrix. This\nformulation fully decouples the computation of the QP solution from its\ndifferentiation. Building on this result, we provide a minimal-overhead,\nopen-source implementation ( https://github.com/cwmagoon/dQP ) that seamlessly\nintegrates with over 15 state-of-the-art solvers. Comprehensive benchmark\nexperiments demonstrate dQP's robustness and scalability, particularly\nhighlighting its advantages in large-scale sparse problems.\n","authors":["Connor W. Magoon","Fengyu Yang","Noam Aigerman","Shahar Z. Kovalsky"],"pdf_url":"https://arxiv.org/pdf/2410.06324v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26577v1","updated":"2025-10-30T15:04:36Z","published":"2025-10-30T15:04:36Z","title":"Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference\n  in Large Language Models","summary":"  Large Language Models (LLMs) face significant inference latency challenges\nstemming from their autoregressive design and large size. To address this,\nspeculative decoding emerges as a solution, enabling the simultaneous\ngeneration and validation of multiple tokens. While recent approaches like\nEAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,\nthey often neglect the impact of crucial system variables such as GPU devices\nand batch sizes.\n  Therefore, we introduce a new dynamic tree decoding approach called CAST that\ntakes into account inference costs, including factors such as GPU\nconfigurations and batch sizes, to dynamically refine the tree structure.\nThrough comprehensive experimentation across six diverse tasks and utilizing\nsix distinct LLMs, our methodology demonstrates remarkable results, achieving\nspeeds up to 5.2 times faster than conventional decoding methods. Moreover, it\ngenerally outperforms existing state-of-the-art techniques from 5% to 20%.\n","authors":["Yinrong Hong","Zhiquan Tan","Kai Hu"],"pdf_url":"https://arxiv.org/pdf/2510.26577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05978v2","updated":"2025-10-30T15:02:26Z","published":"2025-04-08T12:33:38Z","title":"Smart Exploration in Reinforcement Learning using Bounded Uncertainty\n  Models","summary":"  Reinforcement learning (RL) is a powerful framework for decision-making in\nuncertain environments, but it often requires large amounts of data to learn an\noptimal policy. We address this challenge by incorporating prior model\nknowledge to guide exploration and accelerate the learning process.\nSpecifically, we assume access to a model set that contains the true transition\nkernel and reward function. We optimize over this model set to obtain upper and\nlower bounds on the Q-function, which are then used to guide the exploration of\nthe agent. We provide theoretical guarantees on the convergence of the\nQ-function to the optimal Q-function under the proposed class of exploring\npolicies. Furthermore, we also introduce a data-driven regularized version of\nthe model set optimization problem that ensures the convergence of the class of\nexploring policies to the optimal policy. Lastly, we show that when the model\nset has a specific structure, namely the bounded-parameter MDP (BMDP)\nframework, the regularized model set optimization problem becomes convex and\nsimple to implement. In this setting, we also prove finite-time convergence to\nthe optimal policy under mild assumptions. We demonstrate the effectiveness of\nthe proposed exploration strategy, which we call BUMEX (Bounded Uncertainty\nModel-based Exploration), in a simulation study. The results indicate that the\nproposed method can significantly accelerate learning in benchmark examples. A\ntoolbox is available at https://github.com/JvHulst/BUMEX.\n","authors":["J. S. van Hulst","W. P. M. H. Heemels","D. J. Antunes"],"pdf_url":"https://arxiv.org/pdf/2504.05978v2.pdf","comment":"Accepted for Presentation at 64th IEEE Conference on Decision and\n  Control, CDC 2025, Rio de Janeiro, Brazil, 2025"},{"id":"http://arxiv.org/abs/2510.26566v1","updated":"2025-10-30T14:56:07Z","published":"2025-10-30T14:56:07Z","title":"Multiclass Local Calibration With the Jensen-Shannon Distance","summary":"  Developing trustworthy Machine Learning (ML) models requires their predicted\nprobabilities to be well-calibrated, meaning they should reflect true-class\nfrequencies. Among calibration notions in multiclass classification, strong\ncalibration is the most stringent, as it requires all predicted probabilities\nto be simultaneously calibrated across all classes. However, existing\napproaches to multiclass calibration lack a notion of distance among inputs,\nwhich makes them vulnerable to proximity bias: predictions in sparse regions of\nthe feature space are systematically miscalibrated. This is especially relevant\nin high-stakes settings, such as healthcare, where the sparse instances are\nexactly those most at risk of biased treatment. In this work, we address this\nmain shortcoming by introducing a local perspective on multiclass calibration.\nFirst, we formally define multiclass local calibration and establish its\nrelationship with strong calibration. Second, we theoretically analyze the\npitfalls of existing evaluation metrics when applied to multiclass local\ncalibration. Third, we propose a practical method for enhancing local\ncalibration in Neural Networks, which enforces alignment between predicted\nprobabilities and local estimates of class frequencies using the Jensen-Shannon\ndistance. Finally, we empirically validate our approach against existing\nmulticlass calibration techniques.\n","authors":["Cesare Barbera","Lorenzo Perini","Giovanni De Toni","Andrea Passerini","Andrea Pugnana"],"pdf_url":"https://arxiv.org/pdf/2510.26566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.04226v4","updated":"2025-10-30T14:52:48Z","published":"2025-10-05T14:29:15Z","title":"Epistemic Diversity and Knowledge Collapse in Large Language Models","summary":"  Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation\n","authors":["Dustin Wright","Sarah Masud","Jared Moore","Srishti Yadav","Maria Antoniak","Chan Young Park","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2510.04226v4.pdf","comment":"16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for\n  table 3, random effect is the model version; v3 changelog: Fixed minor\n  formatting issues in tables 2 and 3; v4 changelog: Fixed some typos and model\n  description"},{"id":"http://arxiv.org/abs/2510.26560v1","updated":"2025-10-30T14:51:03Z","published":"2025-10-30T14:51:03Z","title":"On Measuring Localization of Shortcuts in Deep Networks","summary":"  Shortcuts, spurious rules that perform well during training but fail to\ngeneralize, present a major challenge to the reliability of deep networks\n(Geirhos et al., 2020). However, the impact of shortcuts on feature\nrepresentations remains understudied, obstructing the design of principled\nshortcut-mitigation methods. To overcome this limitation, we investigate the\nlayer-wise localization of shortcuts in deep models. Our novel experiment\ndesign quantifies the layer-wise contribution to accuracy degradation caused by\na shortcut-inducing skew by counterfactual training on clean and skewed\ndatasets. We employ our design to study shortcuts on CIFAR-10, Waterbirds, and\nCelebA datasets across VGG, ResNet, DeiT, and ConvNeXt architectures. We find\nthat shortcut learning is not localized in specific layers but distributed\nthroughout the network. Different network parts play different roles in this\nprocess: shallow layers predominantly encode spurious features, while deeper\nlayers predominantly forget core features that are predictive on clean data. We\nalso analyze the differences in localization and describe its principal axes of\nvariation. Finally, our analysis of layer-wise shortcut-mitigation strategies\nsuggests the hardness of designing general methods, supporting dataset- and\narchitecture-specific approaches instead.\n","authors":["Nikita Tsoy","Nikola Konstantinov"],"pdf_url":"https://arxiv.org/pdf/2510.26560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26557v1","updated":"2025-10-30T14:47:57Z","published":"2025-10-30T14:47:57Z","title":"Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices","summary":"  Deploying machine learning models on compute-constrained devices has become a\nkey building block of modern IoT applications. In this work, we present a\ncompression scheme for boosted decision trees, addressing the growing need for\nlightweight machine learning models. Specifically, we provide techniques for\ntraining compact boosted decision tree ensembles that exhibit a reduced memory\nfootprint by rewarding, among other things, the reuse of features and\nthresholds during training. Our experimental evaluation shows that models\nachieved the same performance with a compression ratio of 4-16x compared to\nLightGBM models using an adapted training process and an alternative memory\nlayout. Once deployed, the corresponding IoT devices can operate independently\nof constant communication or external energy supply, and, thus, autonomously,\nrequiring only minimal computing power and energy. This capability opens the\ndoor to a wide range of IoT applications, including remote monitoring, edge\nanalytics, and real-time decision making in isolated or power-limited\nenvironments.\n","authors":["Jan Stenkamp","Nina Herrmann","Benjamin Karic","Stefan Oehmcke","Fabian Gieseke"],"pdf_url":"https://arxiv.org/pdf/2510.26557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01369v2","updated":"2025-10-30T14:45:40Z","published":"2025-06-02T06:54:29Z","title":"Incentivizing LLMs to Self-Verify Their Answers","summary":"  Large Language Models (LLMs) have demonstrated remarkable progress in complex\nreasoning tasks through both post-training and test-time scaling laws. While\nprevalent test-time scaling approaches are often realized by using external\nreward models to guide the model generation process, we find that only marginal\ngains can be acquired when scaling a model post-trained on specific reasoning\ntasks. We identify that the limited improvement stems from distribution\ndiscrepancies between the specific post-trained generator and the general\nreward model. To address this, we propose a framework that incentivizes LLMs to\nself-verify their own answers. By unifying answer generation and verification\nwithin a single reinforcement learning (RL) process, we train models that can\neffectively assess the correctness of their own solutions. The trained model\ncan further scale its performance at inference time by verifying its\ngenerations, without the need for external verifiers. We train our\nself-verification models based on Qwen2.5-Math-7B and\nDeepSeek-R1-Distill-Qwen-1.5B, demonstrating their capabilities across varying\nreasoning context lengths. Experiments on multiple mathematical reasoning\nbenchmarks show that our models can not only improve post-training performance\nbut also enable effective test-time scaling.\n","authors":["Fuxiang Zhang","Jiacheng Xu","Chaojie Wang","Ce Cui","Yang Liu","Bo An"],"pdf_url":"https://arxiv.org/pdf/2506.01369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23216v3","updated":"2025-10-30T14:45:38Z","published":"2025-10-27T11:06:00Z","title":"Human-Like Goalkeeping in a Realistic Football Simulation: a\n  Sample-Efficient Reinforcement Learning Approach","summary":"  While several high profile video games have served as testbeds for Deep\nReinforcement Learning (DRL), this technique has rarely been employed by the\ngame industry for crafting authentic AI behaviors. Previous research focuses on\ntraining super-human agents with large models, which is impractical for game\nstudios with limited resources aiming for human-like agents. This paper\nproposes a sample-efficient DRL method tailored for training and fine-tuning\nagents in industrial settings such as the video game industry. Our method\nimproves sample efficiency of value-based DRL by leveraging pre-collected data\nand increasing network plasticity. We evaluate our method training a goalkeeper\nagent in EA SPORTS FC 25, one of the best-selling football simulations today.\nOur agent outperforms the game's built-in AI by 10% in ball saving rate.\nAblation studies show that our method trains agents 50% faster compared to\nstandard DRL methods. Finally, qualitative evaluation from domain experts\nindicates that our approach creates more human-like gameplay compared to\nhand-crafted agents. As a testament to the impact of the approach, the method\nhas been adopted for use in the most recent release of the series.\n","authors":["Alessandro Sestini","Joakim Bergdahl","Jean-Philippe Barrette-LaPierre","Florian Fuchs","Brady Chen","Michael Jones","Linus Gisslén"],"pdf_url":"https://arxiv.org/pdf/2510.23216v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26551v1","updated":"2025-10-30T14:44:24Z","published":"2025-10-30T14:44:24Z","title":"Adaptive Inverse Kinematics Framework for Learning Variable-Length Tool\n  Manipulation in Robotics","summary":"  Conventional robots possess a limited understanding of their kinematics and\nare confined to preprogrammed tasks, hindering their ability to leverage tools\nefficiently. Driven by the essential components of tool usage - grasping the\ndesired outcome, selecting the most suitable tool, determining optimal tool\norientation, and executing precise manipulations - we introduce a pioneering\nframework. Our novel approach expands the capabilities of the robot's inverse\nkinematics solver, empowering it to acquire a sequential repertoire of actions\nusing tools of varying lengths. By integrating a simulation-learned action\ntrajectory with the tool, we showcase the practicality of transferring acquired\nskills from simulation to real-world scenarios through comprehensive\nexperimentation. Remarkably, our extended inverse kinematics solver\ndemonstrates an impressive error rate of less than 1 cm. Furthermore, our\ntrained policy achieves a mean error of 8 cm in simulation. Noteworthy, our\nmodel achieves virtually indistinguishable performance when employing two\ndistinct tools of different lengths. This research provides an indication of\npotential advances in the exploration of all four fundamental aspects of tool\nusage, enabling robots to master the intricate art of tool manipulation across\ndiverse tasks.\n","authors":["Prathamesh Kothavale","Sravani Boddepalli"],"pdf_url":"https://arxiv.org/pdf/2510.26551v1.pdf","comment":"10 pages, 5 figures. Demonstrates a reinforcement learning framework\n  for adaptive tool manipulation with variable-length extensions"},{"id":"http://arxiv.org/abs/2510.26543v1","updated":"2025-10-30T14:36:09Z","published":"2025-10-30T14:36:09Z","title":"The Structure of Relation Decoding Linear Operators in Large Language\n  Models","summary":"  This paper investigates the structure of linear operators introduced in\nHernandez et al. [2023] that decode specific relational facts in transformer\nlanguage models. We extend their single-relation findings to a collection of\nrelations and systematically chart their organization. We show that such\ncollections of relation decoders can be highly compressed by simple order-3\ntensor networks without significant loss in decoding accuracy. To explain this\nsurprising redundancy, we develop a cross-evaluation protocol, in which we\napply each linear decoder operator to the subjects of every other relation. Our\nresults reveal that these linear maps do not encode distinct relations, but\nextract recurring, coarse-grained semantic properties (e.g., country of capital\ncity and country of food are both in the country-of-X property). This\nproperty-centric structure clarifies both the operators' compressibility and\nhighlights why they generalize only to new relations that are semantically\nclose. Our findings thus interpret linear relational decoding in transformer\nlanguage models as primarily property-based, rather than relation-specific.\n","authors":["Miranda Anna Christ","Adrián Csiszárik","Gergely Becsó","Dániel Varga"],"pdf_url":"https://arxiv.org/pdf/2510.26543v1.pdf","comment":"NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.26541v1","updated":"2025-10-30T14:30:53Z","published":"2025-10-30T14:30:53Z","title":"A Three-Stage Bayesian Transfer Learning Framework to Improve\n  Predictions in Data-Scarce Domains","summary":"  The use of ML in engineering has grown steadily to support a wide array of\napplications. Among these methods, deep neural networks have been widely\nadopted due to their performance and accessibility, but they require large,\nhigh-quality datasets. Experimental data are often sparse, noisy, or\ninsufficient to build resilient data-driven models. Transfer learning, which\nleverages relevant data-abundant source domains to assist learning in\ndata-scarce target domains, has shown efficacy. Parameter transfer, where\npretrained weights are reused, is common but degrades under large domain\nshifts. Domain-adversarial neural networks (DANNs) help address this issue by\nlearning domain-invariant representations, thereby improving transfer under\ngreater domain shifts in a semi-supervised setting. However, DANNs can be\nunstable during training and lack a native means for uncertainty\nquantification. This study introduces a fully-supervised three-stage framework,\nthe staged Bayesian domain-adversarial neural network (staged B-DANN), that\ncombines parameter transfer and shared latent space adaptation. In Stage 1, a\ndeterministic feature extractor is trained on the source domain. This feature\nextractor is then adversarially refined using a DANN in Stage 2. In Stage 3, a\nBayesian neural network is built on the adapted feature extractor for\nfine-tuning on the target domain to handle conditional shifts and yield\ncalibrated uncertainty estimates. This staged B-DANN approach was first\nvalidated on a synthetic benchmark, where it was shown to significantly\noutperform standard transfer techniques. It was then applied to the task of\npredicting critical heat flux in rectangular channels, leveraging data from\ntube experiments as the source domain. The results of this study show that the\nstaged B-DANN method can improve predictive accuracy and generalization,\npotentially assisting other domains in nuclear engineering.\n","authors":["Aidan Furlong","Robert Salko","Xingang Zhao","Xu Wu"],"pdf_url":"https://arxiv.org/pdf/2510.26541v1.pdf","comment":"Submitted to Engineering Applications of Artificial Intelligence"},{"id":"http://arxiv.org/abs/2406.08525v2","updated":"2025-10-30T14:25:46Z","published":"2024-06-12T07:33:38Z","title":"A mathematical certification for positivity conditions in Neural\n  Networks with applications to partial monotonicity and Trustworthy AI","summary":"  Artificial Neural Networks (ANNs) have become a powerful tool for modeling\ncomplex relationships in large-scale datasets. However, their black-box nature\nposes trustworthiness challenges. In certain situations, ensuring trust in\npredictions might require following specific partial monotonicity constraints.\nHowever, certifying if an already-trained ANN is partially monotonic is\nchallenging. Therefore, ANNs are often disregarded in some critical\napplications, such as credit scoring, where partial monotonicity is required.\nTo address this challenge, this paper presents a novel algorithm (LipVor) that\ncertifies if a black-box model, such as an ANN, is positive based on a finite\nnumber of evaluations. Consequently, since partial monotonicity can be\nexpressed as a positivity condition on partial derivatives, LipVor can certify\nwhether an ANN is partially monotonic. To do so, for every positively evaluated\npoint, the Lipschitzianity of the black-box model is used to construct a\nspecific neighborhood where the function remains positive. Next, based on the\nVoronoi diagram of the evaluated points, a sufficient condition is stated to\ncertify if the function is positive in the domain. Unlike prior methods, our\napproach certifies partial monotonicity without constrained architectures or\npiece-wise linear activations. Therefore, LipVor could open up the possibility\nof using unconstrained ANN in some critical fields. Moreover, some other\nproperties of an ANN, such as convexity, can be posed as positivity conditions,\nand therefore, LipVor could also be applied.\n","authors":["Alejandro Polo-Molina","David Alfaya","Jose Portela"],"pdf_url":"https://arxiv.org/pdf/2406.08525v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.26533v1","updated":"2025-10-30T14:22:57Z","published":"2025-10-30T14:22:57Z","title":"Higher-Order Regularization Learning on Hypergraphs","summary":"  Higher-Order Hypergraph Learning (HOHL) was recently introduced as a\nprincipled alternative to classical hypergraph regularization, enforcing\nhigher-order smoothness via powers of multiscale Laplacians induced by the\nhypergraph structure. Prior work established the well- and ill-posedness of\nHOHL through an asymptotic consistency analysis in geometric settings. We\nextend this theoretical foundation by proving the consistency of a truncated\nversion of HOHL and deriving explicit convergence rates when HOHL is used as a\nregularizer in fully supervised learning. We further demonstrate its strong\nempirical performance in active learning and in datasets lacking an underlying\ngeometric structure, highlighting HOHL's versatility and robustness across\ndiverse learning settings.\n","authors":["Adrien Weihs","Andrea Bertozzi","Matthew Thorpe"],"pdf_url":"https://arxiv.org/pdf/2510.26533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26527v1","updated":"2025-10-30T14:20:24Z","published":"2025-10-30T14:20:24Z","title":"Polybasic Speculative Decoding Through a Theoretical Perspective","summary":"  Inference latency stands as a critical bottleneck in the large-scale\ndeployment of Large Language Models (LLMs). Speculative decoding methods have\nrecently shown promise in accelerating inference without compromising the\noutput distribution. However, existing work typically relies on a dualistic\ndraft-verify framework and lacks rigorous theoretical grounding. In this paper,\nwe introduce a novel \\emph{polybasic} speculative decoding framework,\nunderpinned by a comprehensive theoretical analysis. Specifically, we prove a\nfundamental theorem that characterizes the optimal inference time for\nmulti-model speculative decoding systems, shedding light on how to extend\nbeyond the dualistic approach to a more general polybasic paradigm. Through our\ntheoretical investigation of multi-model token generation, we expose and\noptimize the interplay between model capabilities, acceptance lengths, and\noverall computational cost. Our framework supports both standalone\nimplementation and integration with existing speculative techniques, leading to\naccelerated performance in practice. Experimental results across multiple model\nfamilies demonstrate that our approach yields speedup ratios ranging from\n$3.31\\times$ to $4.01\\times$ for LLaMA2-Chat 7B, up to $3.87 \\times$ for\nLLaMA3-8B, up to $4.43 \\times$ for Vicuna-7B and up to $3.85 \\times$ for\nQwen2-7B -- all while preserving the original output distribution. We release\nour theoretical proofs and implementation code to facilitate further\ninvestigation into polybasic speculative decoding.\n","authors":["Ruilin Wang","Huixia Li","Yuexiao Ma","Xiawu Zheng","Fei Chao","Xuefeng Xiao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2510.26527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26519v1","updated":"2025-10-30T14:14:15Z","published":"2025-10-30T14:14:15Z","title":"Think Outside the Policy: In-Context Steered Policy Optimization","summary":"  Existing Reinforcement Learning from Verifiable Rewards (RLVR) methods, such\nas Group Relative Policy Optimization (GRPO), have achieved remarkable progress\nin improving the reasoning capabilities of Large Reasoning Models (LRMs).\nHowever, they exhibit limited exploration due to reliance on on-policy rollouts\nwhere confined to the current policy's distribution, resulting in narrow\ntrajectory diversity. Recent approaches attempt to expand policy coverage by\nincorporating trajectories generated from stronger expert models, yet this\nreliance increases computational cost and such advaned models are often\ninaccessible. To address these issues, we propose In-Context Steered Policy\nOptimization (ICPO), a unified framework that leverages the inherent in-context\nlearning capability of LRMs to provide expert guidance using existing datasets.\nICPO introduces Mixed-Policy GRPO with Implicit Expert Forcing, which expands\nexploration beyond the current policy distribution without requiring advanced\nLRM trajectories. To further stabilize optimization, ICPO integrates Expert\nRegion Reject Sampling to filter unreliable off-policy trajectories and\nAnnealed Expert-Bonus Reward Shaping to balance early expert guidance with\nlater autonomous improvement. Results demonstrate that ICPO consistently\nenhances reinforcement learning performance and training stability on\nmathematical reasoning benchmarks, revealing a scalable and effective RLVR\nparadigm for LRMs.\n","authors":["Hsiu-Yuan Huang","Chenming Tang","Weijie Liu","Saiyong Yang","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2510.26519v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2510.26512v1","updated":"2025-10-30T14:05:55Z","published":"2025-10-30T14:05:55Z","title":"Inside CORE-KG: Evaluating Structured Prompting and Coreference\n  Resolution for Knowledge Graphs","summary":"  Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer critical insights but are often unstructured,\nlexically dense, and filled with ambiguous or shifting references, which pose\nsignificant challenges for automated knowledge graph (KG) construction. While\nrecent LLM-based approaches improve over static templates, they still generate\nnoisy, fragmented graphs with duplicate nodes due to the absence of guided\nextraction and coreference resolution. The recently proposed CORE-KG framework\naddresses these limitations by integrating a type-aware coreference module and\ndomain-guided structured prompts, significantly reducing node duplication and\nlegal noise. In this work, we present a systematic ablation study of CORE-KG to\nquantify the individual contributions of its two key components. Our results\nshow that removing coreference resolution results in a 28.32% increase in node\nduplication and a 4.32% increase in noisy nodes, while removing structured\nprompts leads to a 4.34% increase in node duplication and a 73.33% increase in\nnoisy nodes. These findings offer empirical insights for designing robust\nLLM-based pipelines for extracting structured representations from complex\nlegal texts.\n","authors":["Dipak Meher","Carlotta Domeniconi"],"pdf_url":"https://arxiv.org/pdf/2510.26512v1.pdf","comment":"ICDM 2025 Workshop"},{"id":"http://arxiv.org/abs/2510.26510v1","updated":"2025-10-30T14:04:25Z","published":"2025-10-30T14:04:25Z","title":"LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection","summary":"  Model and hyperparameter selection are critical but challenging in machine\nlearning, typically requiring expert intuition or expensive automated search.\nWe investigate whether large language models (LLMs) can act as in-context\nmeta-learners for this task. By converting each dataset into interpretable\nmetadata, we prompt an LLM to recommend both model families and\nhyperparameters. We study two prompting strategies: (1) a zero-shot mode\nrelying solely on pretrained knowledge, and (2) a meta-informed mode augmented\nwith examples of models and their performance on past tasks. Across synthetic\nand real-world benchmarks, we show that LLMs can exploit dataset metadata to\nrecommend competitive models and hyperparameters without search, and that\nimprovements from meta-informed prompting demonstrate their capacity for\nin-context meta-learning. These results highlight a promising new role for LLMs\nas lightweight, general-purpose assistants for model selection and\nhyperparameter optimization.\n","authors":["Youssef Attia El Hili","Albert Thomas","Malik Tiomoko","Abdelhakim Benechehab","Corentin Léger","Corinne Ancourt","Balázs Kégl"],"pdf_url":"https://arxiv.org/pdf/2510.26510v1.pdf","comment":"27 pages, 6 figures"},{"id":"http://arxiv.org/abs/2508.06576v2","updated":"2025-10-30T13:59:28Z","published":"2025-08-07T14:03:23Z","title":"GFlowNets for Learning Better Drug-Drug Interaction Representations","summary":"  Drug-drug interactions pose a significant challenge in clinical pharmacology,\nwith severe class imbalance among interaction types limiting the effectiveness\nof predictive models. Common interactions dominate datasets, while rare but\ncritical interactions remain underrepresented, leading to poor model\nperformance on infrequent cases. Existing methods often treat DDI prediction as\na binary problem, ignoring class-specific nuances and exacerbating bias toward\nfrequent interactions. To address this, we propose a framework combining\nGenerative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)\nto generate synthetic samples for rare classes, improving model balance and\ngenerate effective and novel DDI pairs. Our approach enhances predictive\nperformance across interaction types, ensuring better clinical reliability.\n","authors":["Azmine Toushik Wasi"],"pdf_url":"https://arxiv.org/pdf/2508.06576v2.pdf","comment":"Accepted to ICANN 2025:AIDD and NeurIPS 2025 Workshop on Structured\n  Probabilistic Inference & Generative Modeling\n  (https://openreview.net/forum?id=LZW1jSgfCI)"},{"id":"http://arxiv.org/abs/2509.22018v2","updated":"2025-10-30T13:57:35Z","published":"2025-09-26T07:51:59Z","title":"Exploring the Early Universe with Deep Learning","summary":"  Hydrogen is the most abundant element in our Universe. The first generation\nof stars and galaxies produced photons that ionized hydrogen gas, driving a\ncosmological event known as the Epoch of Reionization (EoR). The upcoming\nSquare Kilometre Array Observatory (SKAO) will map the distribution of neutral\nhydrogen during this era, aiding in the study of the properties of these\nfirst-generation objects. Extracting astrophysical information will be\nchallenging, as SKAO will produce a tremendous amount of data where the\nhydrogen signal will be contaminated with undesired foreground contamination\nand instrumental systematics. To address this, we develop the latest deep\nlearning techniques to extract information from the 2D power spectra of the\nhydrogen signal expected from SKAO. We apply a series of neural network models\nto these measurements and quantify their ability to predict the history of\ncosmic hydrogen reionization, which is connected to the increasing number and\nefficiency of early photon sources. We show that the study of the early\nUniverse benefits from modern deep learning technology. In particular, we\ndemonstrate that dedicated machine learning algorithms can achieve more than a\n$0.95$ $R^2$ score on average in recovering the reionization history. This\nenables accurate and precise cosmological and astrophysical inference of\nstructure formation in the early Universe.\n","authors":["Emmanuel de Salis","Massimo De Santis","Davide Piras","Sambit K. Giri","Michele Bianco","Nicolas Cerardi","Philipp Denzel","Merve Selcuk-Simsek","Kelley M. Hess","M. Carmen Toribio","Franz Kirsten","Hatem Ghorbel"],"pdf_url":"https://arxiv.org/pdf/2509.22018v2.pdf","comment":"EPIA 2025 preprint version, 12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2510.26501v1","updated":"2025-10-30T13:54:37Z","published":"2025-10-30T13:54:37Z","title":"Enhancing ECG Classification Robustness with Lightweight Unsupervised\n  Anomaly Detection Filters","summary":"  Continuous electrocardiogram (ECG) monitoring via wearables offers\nsignificant potential for early cardiovascular disease (CVD) detection.\nHowever, deploying deep learning models for automated analysis in\nresource-constrained environments faces reliability challenges due to\ninevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen\npathologies or noisecorrupted signals, often cause erroneous, high-confidence\npredictions by standard classifiers, compromising patient safety. Existing OOD\ndetection methods either neglect computational constraints or address noise and\nunseen classes separately. This paper explores Unsupervised Anomaly Detection\n(UAD) as an independent, upstream filtering mechanism to improve robustness. We\nbenchmark six UAD approaches, including Deep SVDD, reconstruction-based models,\nMasked Anomaly Detection, normalizing flows, and diffusion models, optimized\nvia Neural Architecture Search (NAS) under strict resource constraints (at most\n512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection\nof OOD CVD classes and signals unsuitable for analysis due to noise. Results\nshow Deep SVDD consistently achieves the best trade-off between detection and\nefficiency. In a realistic deployment simulation, integrating the optimized\nDeep SVDD filter with a diagnostic classifier improved accuracy by up to 21\npercentage points over a classifier-only baseline. This study demonstrates that\noptimized UAD filters can safeguard automated ECG analysis, enabling safer,\nmore reliable continuous cardiovascular monitoring on wearables.\n","authors":["Mustafa Fuad Rifet Ibrahim","Maurice Meijer","Alexander Schlaefer","Peer Stelldinger"],"pdf_url":"https://arxiv.org/pdf/2510.26501v1.pdf","comment":"Submitted to the 24th International Conference on Pervasive Computing\n  and Communications (PerCom 2026)"},{"id":"http://arxiv.org/abs/2505.11730v2","updated":"2025-10-30T13:52:37Z","published":"2025-05-16T22:24:48Z","title":"Rethinking Optimal Verification Granularity for Compute-Efficient\n  Test-Time Scaling","summary":"  Test-time scaling (TTS) has proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Verification plays a key role in\nTTS, simultaneously influencing (1) reasoning performance and (2) compute\nefficiency, due to the quality and computational cost of verification. In this\nwork, we challenge the conventional paradigms of verification, and make the\nfirst attempt toward systematically investigating the impact of verification\ngranularity-that is, how frequently the verifier is invoked during generation,\nbeyond verifying only the final output or individual generation steps. To this\nend, we introduce Variable Granularity Search (VG-Search), a unified algorithm\nthat generalizes beam search and Best-of-N sampling via a tunable granularity\nparameter g. Extensive experiments with VG-Search under varying compute\nbudgets, generator-verifier configurations, and task attributes reveal that\ndynamically selecting g can improve the compute efficiency and scaling\nbehavior. Building on these findings, we propose adaptive VG-Search strategies\nthat achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over\nBest-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to\nsupport future research.\n","authors":["Hao Mark Chen","Guanxi Lu","Yasuyuki Okoshi","Zhiwen Mo","Masato Motomura","Hongxiang Fan"],"pdf_url":"https://arxiv.org/pdf/2505.11730v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.17475v3","updated":"2025-10-30T13:42:29Z","published":"2025-06-20T20:46:01Z","title":"A geometric framework for momentum-based optimizers for low-rank\n  training","summary":"  Low-rank pre-training and fine-tuning have recently emerged as promising\ntechniques for reducing the computational and storage costs of large neural\nnetworks. Training low-rank parameterizations typically relies on conventional\noptimizers such as heavy ball momentum methods or Adam. In this work, we\nidentify and analyze potential difficulties that these training methods\nencounter when used to train low-rank parameterizations of weights. In\nparticular, we show that classical momentum methods can struggle to converge to\na local optimum due to the geometry of the underlying optimization landscape.\nTo address this, we introduce novel training strategies derived from dynamical\nlow-rank approximation, which explicitly account for the underlying geometric\nstructure. Our approach leverages and combines tools from dynamical low-rank\napproximation and momentum-based optimization to design optimizers that respect\nthe intrinsic geometry of the parameter space. We validate our methods through\nnumerical experiments, demonstrating faster convergence, and stronger\nvalidation metrics at given parameter budgets.\n","authors":["Steffen Schotthöfer","Timon Klein","Jonas Kusch"],"pdf_url":"https://arxiv.org/pdf/2506.17475v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26491v1","updated":"2025-10-30T13:40:52Z","published":"2025-10-30T13:40:52Z","title":"Data-Efficient RLVR via Off-Policy Influence Guidance","summary":"  Data selection is a critical aspect of Reinforcement Learning with Verifiable\nRewards (RLVR) for enhancing the reasoning capabilities of large language\nmodels (LLMs). Current data selection methods are largely heuristic-based,\nlacking theoretical guarantees and generalizability. This work proposes a\ntheoretically-grounded approach using influence functions to estimate the\ncontribution of each data point to the learning objective. To overcome the\nprohibitive computational cost of policy rollouts required for online influence\nestimation, we introduce an off-policy influence estimation method that\nefficiently approximates data influence using pre-collected offline\ntrajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we\nemploy sparse random projection to reduce dimensionality and improve storage\nand computation efficiency. Leveraging these techniques, we develop\n\\textbf{C}urriculum \\textbf{R}L with \\textbf{O}ff-\\textbf{P}olicy\n\\text{I}nfluence guidance (\\textbf{CROPI}), a multi-stage RL framework that\niteratively selects the most influential data for the current policy.\nExperiments on models up to 7B parameters demonstrate that CROPI significantly\naccelerates training. On a 1.5B model, it achieves a 2.66x step-level\nacceleration while using only 10\\% of the data per stage compared to\nfull-dataset training. Our results highlight the substantial potential of\ninfluence-based data selection for efficient RLVR.\n","authors":["Erle Zhu","Dazhi Jiang","Yuan Wang","Xujun Li","Jiale Cheng","Yuxian Gu","Yilin Niu","Aohan Zeng","Jie Tang","Minlie Huang","Hongning Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26487v1","updated":"2025-10-30T13:39:44Z","published":"2025-10-30T13:39:44Z","title":"Quantum Gated Recurrent GAN with Gaussian Uncertainty for Network\n  Anomaly Detection","summary":"  Anomaly detection in time-series data is a critical challenge with\nsignificant implications for network security. Recent quantum machine learning\napproaches, such as quantum kernel methods and variational quantum circuits,\nhave shown promise in capturing complex data distributions for anomaly\ndetection but remain constrained by limited qubit counts. We introduce in this\nwork a novel Quantum Gated Recurrent Unit (QGRU)-based Generative Adversarial\nNetwork (GAN) employing Successive Data Injection (SuDaI) and a multi-metric\ngating strategy for robust network anomaly detection. Our model uniquely\nutilizes a quantum-enhanced generator that outputs parameters (mean and\nlog-variance) of a Gaussian distribution via reparameterization, combined with\na Wasserstein critic to stabilize adversarial training. Anomalies are\nidentified through a novel gating mechanism that initially flags potential\nanomalies based on Gaussian uncertainty estimates and subsequently verifies\nthem using a composite of critic scores and reconstruction errors. Evaluated on\nbenchmark datasets, our method achieves a high time-series aware F1 score\n(TaF1) of 89.43% demonstrating superior capability in detecting anomalies\naccurately and promptly as compared to existing classical and quantum models.\nFurthermore, the trained QGRU-WGAN was deployed on real IBM Quantum hardware,\nwhere it retained high anomaly detection performance, confirming its robustness\nand practical feasibility on current noisy intermediate-scale quantum (NISQ)\ndevices.\n","authors":["Wajdi Hammami","Soumaya Cherkaoui","Jean-Frederic Laprade","Ola Ahmad","Shengrui Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26486v1","updated":"2025-10-30T13:39:08Z","published":"2025-10-30T13:39:08Z","title":"LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks","summary":"  Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.\n","authors":["Dipak Meher","Carlotta Domeniconi","Guadalupe Correa-Cabrera"],"pdf_url":"https://arxiv.org/pdf/2510.26486v1.pdf","comment":"Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures"},{"id":"http://arxiv.org/abs/2507.02843v2","updated":"2025-10-30T13:29:54Z","published":"2025-07-03T17:52:27Z","title":"LLM-Driven Treatment Effect Estimation Under Inference Time Text\n  Confounding","summary":"  Estimating treatment effects is crucial for personalized decision-making in\nmedicine, but this task faces unique challenges in clinical practice. At\ntraining time, models for estimating treatment effects are typically trained on\nwell-structured medical datasets that contain detailed patient information.\nHowever, at inference time, predictions are often made using textual\ndescriptions (e.g., descriptions with self-reported symptoms), which are\nincomplete representations of the original patient information. In this work,\nwe make three contributions. (1) We show that the discrepancy between the data\navailable during training time and inference time can lead to biased estimates\nof treatment effects. We formalize this issue as an inference time text\nconfounding problem, where confounders are fully observed during training time\nbut only partially available through text at inference time. (2) To address\nthis problem, we propose a novel framework for estimating treatment effects\nthat explicitly accounts for inference time text confounding. Our framework\nleverages large language models together with a custom doubly robust learner to\nmitigate biases caused by the inference time text confounding. (3) Through a\nseries of experiments, we demonstrate the effectiveness of our framework in\nreal-world applications.\n","authors":["Yuchen Ma","Dennis Frauen","Jonas Schweisthal","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2507.02843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26475v1","updated":"2025-10-30T13:27:42Z","published":"2025-10-30T13:27:42Z","title":"ReSpec: Towards Optimizing Speculative Decoding in Reinforcement\n  Learning Systems","summary":"  Adapting large language models (LLMs) via reinforcement learning (RL) is\noften bottlenecked by the generation stage, which can consume over 75\\% of the\ntraining time. Speculative decoding (SD) accelerates autoregressive generation\nin serving systems, but its behavior under RL training remains largely\nunexplored. We identify three critical gaps that hinder the naive integration\nof SD into RL systems: diminishing speedups at large batch sizes, drafter\nstaleness under continual actor updates, and drafter-induced policy\ndegradation.\n  To address these gaps, we present ReSpec, a system that adapts SD to RL\nthrough three complementary mechanisms: dynamically tuning SD configurations,\nevolving the drafter via knowledge distillation, and weighting updates by\nrollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup\nwhile preserving reward convergence and training stability, providing a\npractical solution for efficient RL-based LLM adaptation.\n","authors":["Qiaoling Chen","Zijun Liu","Peng Sun","Shenggui Li","Guoteng Wang","Ziming Liu","Yonggang Wen","Siyuan Feng","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12371v2","updated":"2025-10-30T13:27:07Z","published":"2025-05-18T11:28:17Z","title":"MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional\n  Methods for Diverse Medical Tasks","summary":"  The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/.\n","authors":["Yinghao Zhu","Ziyi He","Haoran Hu","Xiaochen Zheng","Xichen Zhang","Zixiang Wang","Junyi Gao","Liantao Ma","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2505.12371v2.pdf","comment":"Accepted by NeurIPS 2025 Datasets & Benchmarks Track"},{"id":"http://arxiv.org/abs/2510.26474v1","updated":"2025-10-30T13:26:58Z","published":"2025-10-30T13:26:58Z","title":"Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing","summary":"  Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.\n","authors":["Xin Guo","Zhiheng Xi","Yiwen Ding","Yitao Zhai","Xiaowei Shi","Xunliang Cai","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26474v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2508.08606v3","updated":"2025-10-30T13:25:58Z","published":"2025-08-12T03:39:07Z","title":"Distributed optimization: designed for federated learning","summary":"  Federated learning (FL), as a distributed collaborative machine learning (ML)\nframework under privacy-preserving constraints, has garnered increasing\nresearch attention in cross-organizational data collaboration scenarios. This\npaper proposes a class of distributed optimization algorithms based on the\naugmented Lagrangian technique, designed to accommodate diverse communication\ntopologies in both centralized and decentralized FL settings. Furthermore, we\ndevelop multiple termination criteria and parameter update mechanisms to\nenhance computational efficiency, accompanied by rigorous theoretical\nguarantees of convergence. By generalizing the augmented Lagrangian relaxation\nthrough the incorporation of proximal relaxation and quadratic approximation,\nour framework systematically recovers a broad of classical unconstrained\noptimization methods, including proximal algorithm, classic gradient descent,\nand stochastic gradient descent, among others. Notably, the convergence\nproperties of these methods can be naturally derived within the proposed\ntheoretical framework. Numerical experiments demonstrate that the proposed\nalgorithm exhibits strong performance in large-scale settings with significant\nstatistical heterogeneity across clients.\n","authors":["Wenyou Guo","Ting Qu","Chunrong Pan","George Q. Huang"],"pdf_url":"https://arxiv.org/pdf/2508.08606v3.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.02331v2","updated":"2025-10-30T13:17:24Z","published":"2025-02-04T14:06:27Z","title":"On the Impact of Performative Risk Minimization for Binary Random\n  Variables","summary":"  Performativity, the phenomenon where outcomes are influenced by predictions,\nis particularly prevalent in social contexts where individuals strategically\nrespond to a deployed model. In order to preserve the high accuracy of machine\nlearning models under distribution shifts caused by performativity, Perdomo et\nal. (2020) introduced the concept of performative risk minimization (PRM).\nWhile this framework ensures model accuracy, it overlooks the impact of the PRM\non the underlying distributions and the predictions of the model. In this\npaper, we initiate the analysis of the impact of PRM, by studying\nperformativity for a sequential performative risk minimization problem with\nbinary random variables and linear performative shifts. We formulate two\nnatural measures of impact. In the case of full information, where the\ndistribution dynamics are known, we derive explicit formulas for the PRM\nsolution and our impact measures. In the case of partial information, we\nprovide performative-aware statistical estimators, as well as simulations. Our\nanalysis contrasts PRM to alternatives that do not model data shift and\nindicates that PRM can have amplified side effects compared to such methods.\n","authors":["Nikita Tsoy","Ivan Kirev","Negin Rahimiyazdi","Nikola Konstantinov"],"pdf_url":"https://arxiv.org/pdf/2502.02331v2.pdf","comment":"ICML 2025 camera-ready"},{"id":"http://arxiv.org/abs/2509.09695v2","updated":"2025-10-30T13:14:13Z","published":"2025-08-27T09:31:50Z","title":"Machine-learning competition to grade EEG background patterns in\n  newborns with hypoxic-ischaemic encephalopathy","summary":"  Machine learning (ML) has the potential to support and improve expert\nperformance in monitoring the brain function of at-risk newborns. Developing\naccurate and reliable ML models depends on access to high-quality, annotated\ndata, a resource in short supply. ML competitions address this need by\nproviding researchers access to expertly annotated datasets, fostering shared\nlearning through direct model comparisons, and leveraging the benefits of\ncrowdsourcing diverse expertise. We compiled a retrospective dataset containing\n353 hours of EEG from 102 individual newborns from a multi-centre study. The\ndata was fully anonymised and divided into training, testing, and held-out\nvalidation datasets. EEGs were graded for the severity of abnormal background\npatterns. Next, we created a web-based competition platform and hosted a\nmachine learning competition to develop ML models for classifying the severity\nof EEG background patterns in newborns. After the competition closed, the top 4\nperforming models were evaluated offline on a separate held-out validation\ndataset. Although a feature-based model ranked first on the testing dataset,\ndeep learning models generalised better on the validation sets. All methods had\na significant decline in validation performance compared to the testing\nperformance. This highlights the challenges for model generalisation on unseen\ndata, emphasising the need for held-out validation datasets in ML studies with\nneonatal EEG. The study underscores the importance of training ML models on\nlarge and diverse datasets to ensure robust generalisation. The competition's\noutcome demonstrates the potential for open-access data and collaborative ML\ndevelopment to foster a collaborative research environment and expedite the\ndevelopment of clinical decision-support tools for neonatal neuromonitoring.\n","authors":["Fabio Magarelli","Geraldine B. Boylan","Saeed Montazeri","Feargal O'Sullivan","Dominic Lightbody","Minoo Ashoori","Tamara Skoric","John M. O'Toole"],"pdf_url":"https://arxiv.org/pdf/2509.09695v2.pdf","comment":"29 pages, supplementary materials: \"supplementary materials ML\n  Comp.docx\""},{"id":"http://arxiv.org/abs/2510.26466v1","updated":"2025-10-30T13:11:23Z","published":"2025-10-30T13:11:23Z","title":"Representation-Level Counterfactual Calibration for Debiased Zero-Shot\n  Recognition","summary":"  Object-context shortcuts remain a persistent challenge in vision-language\nmodels, undermining zero-shot reliability when test-time scenes differ from\nfamiliar training co-occurrences. We recast this issue as a causal inference\nproblem and ask: Would the prediction remain if the object appeared in a\ndifferent environment? To answer this at inference time, we estimate object and\nbackground expectations within CLIP's representation space, and synthesize\ncounterfactual embeddings by recombining object features with diverse\nalternative contexts sampled from external datasets, batch neighbors, or\ntext-derived descriptions. By estimating the Total Direct Effect and simulating\nintervention, we further subtract background-only activation, preserving\nbeneficial object-context interactions while mitigating hallucinated scores.\nWithout retraining or prompt design, our method substantially improves both\nworst-group and average accuracy on context-sensitive benchmarks, establishing\na new zero-shot state of the art. Beyond performance, our framework provides a\nlightweight representation-level counterfactual approach, offering a practical\ncausal avenue for debiased and reliable multimodal reasoning.\n","authors":["Pei Peng","MingKun Xie","Hang Hao","Tong Jin","ShengJun Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26461v1","updated":"2025-10-30T13:07:39Z","published":"2025-10-30T13:07:39Z","title":"Vectorized Context-Aware Embeddings for GAT-Based Collaborative\n  Filtering","summary":"  Recommender systems often struggle with data sparsity and cold-start\nscenarios, limiting their ability to provide accurate suggestions for new or\ninfrequent users. This paper presents a Graph Attention Network (GAT) based\nCollaborative Filtering (CF) framework enhanced with Large Language Model (LLM)\ndriven context aware embeddings. Specifically, we generate concise textual user\nprofiles and unify item metadata (titles, genres, overviews) into rich textual\nembeddings, injecting these as initial node features in a bipartite user item\ngraph. To further optimize ranking performance, we introduce a hybrid loss\nfunction that combines Bayesian Personalized Ranking (BPR) with a cosine\nsimilarity term and robust negative sampling, ensuring explicit negative\nfeedback is distinguished from unobserved data. Experiments on the MovieLens\n100k and 1M datasets show consistent improvements over state-of-the-art\nbaselines in Precision, NDCG, and MAP while demonstrating robustness for users\nwith limited interaction history. Ablation studies confirm the critical role of\nLLM-augmented embeddings and the cosine similarity term in capturing nuanced\nsemantic relationships. Our approach effectively mitigates sparsity and\ncold-start limitations by integrating LLM-derived contextual understanding into\ngraph-based architectures. Future directions include balancing recommendation\naccuracy with coverage and diversity, and introducing fairness-aware\nconstraints and interpretability features to enhance system performance\nfurther.\n","authors":["Danial Ebrat","Sepideh Ahmadian","Luis Rueda"],"pdf_url":"https://arxiv.org/pdf/2510.26461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21513v2","updated":"2025-10-30T13:03:25Z","published":"2025-10-24T14:39:23Z","title":"Wisdom and Delusion of LLM Ensembles for Code Generation and Repair","summary":"  Today's pursuit of a single Large Language Model (LMM) for all software\nengineering tasks is resource-intensive and overlooks the potential benefits of\ncomplementarity, where different models contribute unique strengths. However,\nthe degree to which coding LLMs complement each other and the best strategy for\nmaximizing an ensemble's potential are unclear, leaving practitioners without a\nclear path to move beyond single-model systems. To address this gap, we\nempirically compare ten individual LLMs from five families, and three ensembles\nof these LLMs across three software engineering benchmarks covering code\ngeneration and program repair. We assess the complementarity between models and\nthe performance gap between the best individual model and the ensembles. Next,\nwe evaluate various selection heuristics to identify correct solutions from an\nensemble's candidate pool. We find that the theoretical upperbound for an\nensemble's performance can be 83% above the best single model. Our results show\nthat consensus-based strategies for selecting solutions fall into a \"popularity\ntrap,\" amplifying common but incorrect outputs. In contrast, a diversity-based\nstrategy realizes up to 95% of this theoretical potential, and proves effective\neven in small two-model ensembles, enabling a cost-efficient way to enhance\nperformance by leveraging multiple LLMs.\n","authors":["Fernando Vallecillos-Ruiz","Max Hort","Leon Moonen"],"pdf_url":"https://arxiv.org/pdf/2510.21513v2.pdf","comment":"Added Acknowledgments section and hyphenated last names"},{"id":"http://arxiv.org/abs/2510.26451v1","updated":"2025-10-30T12:55:21Z","published":"2025-10-30T12:55:21Z","title":"Robust Graph Condensation via Classification Complexity Mitigation","summary":"  Graph condensation (GC) has gained significant attention for its ability to\nsynthesize smaller yet informative graphs. However, existing studies often\noverlook the robustness of GC in scenarios where the original graph is\ncorrupted. In such cases, we observe that the performance of GC deteriorates\nsignificantly, while existing robust graph learning technologies offer only\nlimited effectiveness. Through both empirical investigation and theoretical\nanalysis, we reveal that GC is inherently an intrinsic-dimension-reducing\nprocess, synthesizing a condensed graph with lower classification complexity.\nAlthough this property is critical for effective GC performance, it remains\nhighly vulnerable to adversarial perturbations. To tackle this vulnerability\nand improve GC robustness, we adopt the geometry perspective of graph data\nmanifold and propose a novel Manifold-constrained Robust Graph Condensation\nframework named MRGC. Specifically, we introduce three graph data manifold\nlearning modules that guide the condensed graph to lie within a smooth,\nlow-dimensional manifold with minimal class ambiguity, thereby preserving the\nclassification complexity reduction capability of GC and ensuring robust\nperformance under universal adversarial attacks. Extensive experiments\ndemonstrate the robustness of \\ModelName\\ across diverse attack scenarios.\n","authors":["Jiayi Luo","Qingyun Sun","Beining Yang","Haonan Yuan","Xingcheng Fu","Yanbiao Ma","Jianxin Li","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2510.26451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26444v1","updated":"2025-10-30T12:50:12Z","published":"2025-10-30T12:50:12Z","title":"Personalized Treatment Outcome Prediction from Scarce Data via\n  Dual-Channel Knowledge Distillation and Adaptive Fusion","summary":"  Personalized treatment outcome prediction based on trial data for\nsmall-sample and rare patient groups is critical in precision medicine.\nHowever, the costly trial data limit the prediction performance. To address\nthis issue, we propose a cross-fidelity knowledge distillation and adaptive\nfusion network (CFKD-AFN), which leverages abundant but low-fidelity simulation\ndata to enhance predictions on scarce but high-fidelity trial data. CFKD-AFN\nincorporates a dual-channel knowledge distillation module to extract\ncomplementary knowledge from the low-fidelity model, along with an\nattention-guided fusion module to dynamically integrate multi-source\ninformation. Experiments on treatment outcome prediction for the chronic\nobstructive pulmonary disease demonstrates significant improvements of CFKD-AFN\nover state-of-the-art methods in prediction accuracy, ranging from 6.67\\% to\n74.55\\%, and strong robustness to varying high-fidelity dataset sizes.\nFurthermore, we extend CFKD-AFN to an interpretable variant, enabling the\nexploration of latent medical semantics to support clinical decision-making.\n","authors":["Wenjie Chen","Li Zhuang","Ziying Luo","Yu Liu","Jiahao Wu","Shengcai Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11542v2","updated":"2025-10-30T12:48:34Z","published":"2025-05-14T13:18:12Z","title":"Cybersecurity threat detection based on a UEBA framework using Deep\n  Autoencoders","summary":"  User and Entity Behaviour Analytics (UEBA) is a broad branch of data\nanalytics that attempts to build a normal behavioural profile in order to\ndetect anomalous events. Among the techniques used to detect anomalies, Deep\nAutoencoders constitute one of the most promising deep learning models on UEBA\ntasks, allowing explainable detection of security incidents that could lead to\nthe leak of personal data, hijacking of systems, or access to sensitive\nbusiness information. In this study, we introduce the first implementation of\nan explainable UEBA-based anomaly detection framework that leverages Deep\nAutoencoders in combination with Doc2Vec to process both numerical and textual\nfeatures. Additionally, based on the theoretical foundations of neural\nnetworks, we offer a novel proof demonstrating the equivalence of two widely\nused definitions for fully-connected neural networks. The experimental results\ndemonstrate the proposed framework capability to detect real and synthetic\nanomalies effectively generated from real attack data, showing that the models\nprovide not only correct identification of anomalies but also explainable\nresults that enable the reconstruction of the possible origin of the anomaly.\nOur findings suggest that the proposed UEBA framework can be seamlessly\nintegrated into enterprise environments, complementing existing security\nsystems for explainable threat detection.\n","authors":["Jose Fuentes","Ines Ortega-Fernandez","Nora M. Villanueva","Marta Sestelo"],"pdf_url":"https://arxiv.org/pdf/2505.11542v2.pdf","comment":"Published in AIMS Mathematics (2025), 10(10): 23496-23517. DOI:\n  10.3934/math.20251043"},{"id":"http://arxiv.org/abs/2510.26433v1","updated":"2025-10-30T12:28:40Z","published":"2025-10-30T12:28:40Z","title":"Co-Evolving Latent Action World Models","summary":"  Adapting pre-trained video generation models into controllable world models\nvia latent actions is a promising step towards creating generalist world\nmodels. The dominant paradigm adopts a two-stage approach that trains latent\naction model (LAM) and the world model separately, resulting in redundant\ntraining and limiting their potential for co-adaptation. A conceptually simple\nand appealing idea is to directly replace the forward dynamic model in LAM with\na powerful world model and training them jointly, but it is non-trivial and\nprone to representational collapse. In this work, we propose CoLA-World, which\nfor the first time successfully realizes this synergistic paradigm, resolving\nthe core challenge in joint learning through a critical warm-up phase that\neffectively aligns the representations of the from-scratch LAM with the\npre-trained world model. This unlocks a co-evolution cycle: the world model\nacts as a knowledgeable tutor, providing gradients to shape a high-quality LAM,\nwhile the LAM offers a more precise and adaptable control interface to the\nworld model. Empirically, CoLA-World matches or outperforms prior two-stage\nmethods in both video simulation quality and downstream visual planning,\nestablishing a robust and efficient new paradigm for the field.\n","authors":["Yucen Wang","Fengming Zhang","De-Chuan Zhan","Li Zhao","Kaixin Wang","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2510.26433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25080v2","updated":"2025-10-30T12:16:59Z","published":"2025-10-29T01:38:19Z","title":"Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response\n  Games","summary":"  Card games are widely used to study sequential decision-making under\nuncertainty, with real-world analogues in negotiation, finance, and\ncybersecurity. These games typically fall into three categories based on the\nflow of control: strictly sequential (players alternate single actions),\ndeterministic response (some actions trigger a fixed outcome), and unbounded\nreciprocal response (alternating counterplays are permitted). A less-explored\nbut strategically rich structure is the bounded one-sided response, where a\nplayer's action briefly transfers control to the opponent, who must satisfy a\nfixed condition through one or more moves before the turn resolves. We term\ngames featuring this mechanism Bounded One-Sided Response Games (BORGs). We\nintroduce a modified version of Monopoly Deal as a benchmark environment that\nisolates this dynamic, where a Rent action forces the opponent to choose\npayment assets. The gold-standard algorithm, Counterfactual Regret Minimization\n(CFR), converges on effective strategies without novel algorithmic extensions.\nA lightweight full-stack research platform unifies the environment, a\nparallelized CFR runtime, and a human-playable web interface. The trained CFR\nagent and source code are available at https://monopolydeal.ai.\n","authors":["Will Wolf"],"pdf_url":"https://arxiv.org/pdf/2510.25080v2.pdf","comment":"24 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.17883v2","updated":"2025-10-30T12:06:31Z","published":"2024-12-23T06:22:03Z","title":"In Defence of Post-hoc Explainability","summary":"  This position paper defends post-hoc explainability methods as legitimate\ntools for scientific knowledge production in machine learning. Addressing\ncriticism of these methods' reliability and epistemic status, we develop a\nphilosophical framework grounded in mediated understanding and bounded\nfactivity. We argue that scientific insights can emerge through structured\ninterpretation of model behaviour without requiring complete mechanistic\ntransparency, provided explanations acknowledge their approximative nature and\nundergo rigorous empirical validation. Through analysis of recent biomedical ML\napplications, we demonstrate how post-hoc methods, when properly integrated\ninto scientific practice, generate novel hypotheses and advance phenomenal\nunderstanding.\n","authors":["Nick Oh"],"pdf_url":"https://arxiv.org/pdf/2412.17883v2.pdf","comment":"v1 presented at the Interpretable AI: Past, Present, and Future\n  Workshop at NeurIPS 2024 (non-archival)"},{"id":"http://arxiv.org/abs/2510.17670v2","updated":"2025-10-30T12:05:58Z","published":"2025-10-20T15:41:55Z","title":"On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active\n  Marginal-Samples Exploration","summary":"  Open-vocabulary object detection (OVD) models offer remarkable flexibility by\ndetecting objects from arbitrary text queries. However, their zero-shot\nperformance in specialized domains like Remote Sensing (RS) is often\ncompromised by the inherent ambiguity of natural language, limiting critical\ndownstream applications. For instance, an OVD model may struggle to distinguish\nbetween fine-grained classes such as \"fishing boat\" and \"yacht\" since their\nembeddings are similar and often inseparable. This can hamper specific user\ngoals, such as monitoring illegal fishing, by producing irrelevant detections.\nTo address this, we propose a cascaded approach that couples the broad\ngeneralization of a large pre-trained OVD model with a lightweight few-shot\nclassifier. Our method first employs the zero-shot model to generate\nhigh-recall object proposals. These proposals are then refined for high\nprecision by a compact classifier trained in real-time on only a handful of\nuser-annotated examples - drastically reducing the high costs of RS imagery\nannotation.The core of our framework is FLAME, a one-step active learning\nstrategy that selects the most informative samples for training. FLAME\nidentifies, on the fly, uncertain marginal candidates near the decision\nboundary using density estimation, followed by clustering to ensure sample\ndiversity. This efficient sampling technique achieves high accuracy without\ncostly full-model fine-tuning and enables instant adaptation, within less then\na minute, which is significantly faster than state-of-the-art alternatives.Our\nmethod consistently surpasses state-of-the-art performance on RS benchmarks,\nestablishing a practical and resource-efficient framework for adapting\nfoundation models to specific user needs.\n","authors":["Yehonathan Refael","Amit Aides","Aviad Barzilai","George Leifman","Genady Beryozkin","Vered Silverman","Bolous Jaber","Tomer Shekel"],"pdf_url":"https://arxiv.org/pdf/2510.17670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23117v2","updated":"2025-10-30T12:02:18Z","published":"2025-10-27T08:38:17Z","title":"Seeing Structural Failure Before it Happens: An Image-Based\n  Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction","summary":"  Physics Informed Neural Networks (PINNs) are gaining attention for their\nability to embed physical laws into deep learning models, which is particularly\nuseful in structural engineering tasks with limited data. This paper aims to\nexplore the use of PINNs to predict the weight of small scale spaghetti\nbridges, a task relevant to understanding load limits and potential failure\nmodes in simplified structural models. Our proposed framework incorporates\nphysics-based constraints to the prediction model for improved performance. In\naddition to standard PINNs, we introduce a novel architecture named Physics\nInformed Kolmogorov Arnold Network (PIKAN), which blends universal function\napproximation theory with physical insights. The structural parameters provided\nas input to the model are collected either manually or through computer vision\nmethods. Our dataset includes 15 real bridges, augmented to 100 samples, and\nour best model achieves an $R^2$ score of 0.9603 and a mean absolute error\n(MAE) of 10.50 units. From applied perspective, we also provide a web based\ninterface for parameter entry and prediction. These results show that PINNs can\noffer reliable estimates of structural weight, even with limited data, and may\nhelp inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at\nhttps://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.\n","authors":["Omer Jauhar Khan","Sudais Khan","Hafeez Anwar","Shahzeb Khan","Shams Ul Arifeen"],"pdf_url":"https://arxiv.org/pdf/2510.23117v2.pdf","comment":"12 pages, 17 figures. Preprint"},{"id":"http://arxiv.org/abs/2510.26402v1","updated":"2025-10-30T11:41:50Z","published":"2025-10-30T11:41:50Z","title":"Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback\n  in Programming Education","summary":"  The rapid growth of programming education has outpaced traditional assessment\ntools, leaving faculty with limited means to provide meaningful, scalable\nfeedback. Conventional autograders, while efficient, act as black-box systems\nthat simply return pass/fail results, offering little insight into student\nthinking or learning needs.\n  Autograder+ is designed to shift autograding from a purely summative process\nto a formative learning experience. It introduces two key capabilities:\nautomated feedback generation using a fine-tuned Large Language Model, and\nvisualization of student code submissions to uncover learning patterns. The\nmodel is fine-tuned on curated student code and expert feedback to ensure\npedagogically aligned, context-aware guidance.\n  In evaluation across 600 student submissions from multiple programming tasks,\nthe system produced feedback with strong semantic alignment to instructor\ncomments. For visualization, contrastively learned code embeddings trained on\n1,000 annotated submissions enable grouping solutions into meaningful clusters\nbased on functionality and approach. The system also supports prompt-pooling,\nallowing instructors to guide feedback style through selected prompt templates.\n  By integrating AI-driven feedback, semantic clustering, and interactive\nvisualization, Autograder+ reduces instructor workload while supporting\ntargeted instruction and promoting stronger learning outcomes.\n","authors":["Vikrant Sahu","Gagan Raj Gupta","Raghav Borikar","Nitin Mane"],"pdf_url":"https://arxiv.org/pdf/2510.26402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26401v1","updated":"2025-10-30T11:41:19Z","published":"2025-10-30T11:41:19Z","title":"Multi-Output Robust and Conjugate Gaussian Processes","summary":"  Multi-output Gaussian process (MOGP) regression allows modelling dependencies\namong multiple correlated response variables. Similarly to standard Gaussian\nprocesses, MOGPs are sensitive to model misspecification and outliers, which\ncan distort predictions within individual outputs. This situation can be\nfurther exacerbated by multiple anomalous response variables whose errors\npropagate due to correlations between outputs. To handle this situation, we\nextend and generalise the robust and conjugate Gaussian process (RCGP)\nframework introduced by Altamirano et al. (2024). This results in the\nmulti-output RCGP (MO-RCGP): a provably robust MOGP that is conjugate, and\njointly captures correlations across outputs. We thoroughly evaluate our\napproach through applications in finance and cancer research.\n","authors":["Joshua Rooijakkers","Leiv Rønneberg","François-Xavier Briol","Jeremias Knoblauch","Matias Altamirano"],"pdf_url":"https://arxiv.org/pdf/2510.26401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26392v1","updated":"2025-10-30T11:35:05Z","published":"2025-10-30T11:35:05Z","title":"Multi-Task Learning Based on Support Vector Machines and Twin Support\n  Vector Machines: A Comprehensive Survey","summary":"  Multi-task learning (MTL) enables simultaneous training across related tasks,\nleveraging shared information to improve generalization, efficiency, and\nrobustness, especially in data-scarce or high-dimensional scenarios. While deep\nlearning dominates recent MTL research, Support Vector Machines (SVMs) and Twin\nSVMs (TWSVMs) remain relevant due to their interpretability, theoretical rigor,\nand effectiveness with small datasets.\n  This chapter surveys MTL approaches based on SVM and TWSVM, highlighting\nshared representations, task regularization, and structural coupling\nstrategies. Special attention is given to emerging TWSVM extensions for\nmulti-task settings, which show promise but remain underexplored. We compare\nthese models in terms of theoretical properties, optimization strategies, and\nempirical performance, and discuss applications in fields such as computer\nvision, natural language processing, and bioinformatics.\n  Finally, we identify research gaps and outline future directions for building\nscalable, interpretable, and reliable margin-based MTL frameworks. This work\nprovides a comprehensive resource for researchers and practitioners interested\nin SVM- and TWSVM-based multi-task learning.\n","authors":["Fatemeh Bazikar","Hossein Moosaei","Atefeh Hemmati","Panos M. Pardalos"],"pdf_url":"https://arxiv.org/pdf/2510.26392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11329v4","updated":"2025-10-30T11:34:01Z","published":"2025-05-16T14:53:50Z","title":"TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM\n  Inference","summary":"  Distributed inference of large language models (LLMs) can introduce overheads\nof up to 20% even over GPUs connected via high-speed interconnects such as\nNVLink. Multiple techniques have been proposed to mitigate these overheads by\ndecomposing computations into finer-grained tasks and overlapping communication\nwith sub-tasks as they complete. However, fine-grained decomposition of a large\ncomputation into many smaller computations on GPUs results in overheads.\nFurthermore, the communication itself uses many streaming multiprocessors\n(SMs), adding to the overhead.\n  We present TokenWeave to address these challenges. TokenWeave proposes a\nToken-Splitting technique that divides the tokens in the inference batch into\ntwo approximately equal subsets in a wave-aware manner. The communication of\none subset is then overlapped with the computation of the other. In addition,\nTokenWeave optimizes the order of the layer normalization computation with\nrespect to communication operations and implements a novel fused\nAllReduce--RMSNorm kernel that carefully leverages Multimem instruction support\navailable on Hopper and Blackwell NVIDIA GPUs. These optimizations allow\nTokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover,\nour kernel enables the memory-bound RMSNorm to be overlapped with the other\nbatch's computation, providing additional gains.\n  Our evaluations demonstrate up to 1.29x speedup in latency and 1.26x higher\nthroughput across multiple models and workloads. In several settings,\nTokenWeave results in better performance compared to an equivalent model with\nall communication removed.\n","authors":["Raja Gond","Nipun Kwatra","Ramachandran Ramjee"],"pdf_url":"https://arxiv.org/pdf/2505.11329v4.pdf","comment":"14 pages, 16 figures. For source code, see\n  https://github.com/microsoft/tokenweave. In version 2, Figure 6 shows\n  All-Reduce bandwidth instead of Reduce-Scatter. The Multimem Reduce-Scatter\n  bandwidth formula differs slightly from the ring-based version. Fixed x-ticks\n  in Figure 7"},{"id":"http://arxiv.org/abs/2510.26389v1","updated":"2025-10-30T11:32:45Z","published":"2025-10-30T11:32:45Z","title":"Adaptive Context Length Optimization with Low-Frequency Truncation for\n  Multi-Agent Reinforcement Learning","summary":"  Recently, deep multi-agent reinforcement learning (MARL) has demonstrated\npromising performance for solving challenging tasks, such as long-term\ndependencies and non-Markovian environments. Its success is partly attributed\nto conditioning policies on large fixed context length. However, such large\nfixed context lengths may lead to limited exploration efficiency and redundant\ninformation. In this paper, we propose a novel MARL framework to obtain\nadaptive and effective contextual information. Specifically, we design a\ncentral agent that dynamically optimizes context length via temporal gradient\nanalysis, enhancing exploration to facilitate convergence to global optima in\nMARL. Furthermore, to enhance the adaptive optimization capability of the\ncontext length, we present an efficient input representation for the central\nagent, which effectively filters redundant information. By leveraging a\nFourier-based low-frequency truncation method, we extract global temporal\ntrends across decentralized agents, providing an effective and efficient\nrepresentation of the MARL environment. Extensive experiments demonstrate that\nthe proposed method achieves state-of-the-art (SOTA) performance on long-term\ndependency tasks, including PettingZoo, MiniGrid, Google Research Football\n(GRF), and StarCraft Multi-Agent Challenge v2 (SMACv2).\n","authors":["Wenchang Duan","Yaoliang Yu","Jiwan He","Yi Shi"],"pdf_url":"https://arxiv.org/pdf/2510.26389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17316v2","updated":"2025-10-30T11:32:37Z","published":"2025-07-23T08:30:37Z","title":"Nearly Minimax Discrete Distribution Estimation in Kullback-Leibler\n  Divergence with High Probability","summary":"  We consider the fundamental problem of estimating a discrete distribution on\na domain of size~$K$ with high probability in Kullback-Leibler divergence. We\nprovide upper and lower bounds on the minimax estimation rate, which show that\nthe optimal rate is between $\\big(K + \\ln(K)\\ln(1/\\delta)\\big) /n$ and\n$\\big(K\\ln\\ln(K) + \\ln(K)\\ln(1/\\delta)\\big) /n$ at error probability $\\delta$\nand sample size $n$, which pins down the rate up to the doubly logarithmic\nfactor $\\ln \\ln K$ that multiplies $K$. Our upper bound uses techniques from\nonline learning to construct a novel estimator via online-to-batch conversion.\nPerhaps surprisingly, the tail behavior of the minimax rate is worse than for\nthe squared total variation and squared Hellinger distance, for which it is\n$\\big(K + \\ln(1/\\delta)\\big) /n$, i.e.\\ without the $\\ln K$ multiplying $\\ln\n(1/\\delta)$. As a consequence, we cannot obtain a fully tight lower bound from\nthe usual reduction to these smaller distances. Moreover, we show that this\nlower bound cannot be achieved by the standard lower bound approach based on a\nreduction to hypothesis testing, and instead we need to introduce a new\nreduction to what we call weak hypothesis testing. We investigate the source of\nthe gap with other divergences further in refined results, which show that the\ntotal variation rate is achievable for Kullback-Leibler divergence after all\n(in fact by he maximum likelihood estimator) if we rule out outcome\nprobabilities smaller than $O(\\ln(K/\\delta) / n)$, which is a vanishing set as\n$n$ increases for fixed $K$ and~$\\delta$. This explains why minimax\nKullback-Leibler estimation is more difficult than asymptotic estimation.\n","authors":["Dirk van der Hoeven","Julia Olkhovskaia","Tim van Erven"],"pdf_url":"https://arxiv.org/pdf/2507.17316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26384v1","updated":"2025-10-30T11:28:58Z","published":"2025-10-30T11:28:58Z","title":"Scales++: Compute Efficient Evaluation Subset Selection with Cognitive\n  Scales Embeddings","summary":"  The prohibitive cost of evaluating large language models (LLMs) on\ncomprehensive benchmarks necessitates the creation of small yet representative\ndata subsets (i.e., tiny benchmarks) that enable efficient assessment while\nretaining predictive fidelity. Current methods for this task operate under a\nmodel-centric paradigm, selecting benchmarking items based on the collective\nperformance of existing models. Such approaches are limited by large upfront\ncosts, an inability to immediately handle new benchmarks (`cold-start'), and\nthe fragile assumption that future models will share the failure patterns of\ntheir predecessors. In this work, we challenge this paradigm and propose a\nitem-centric approach to benchmark subset selection, arguing that selection\nshould be based on the intrinsic properties of the task items themselves,\nrather than on model-specific failure patterns. We instantiate this\nitem-centric efficient benchmarking approach via a novel method, Scales++,\nwhere data selection is based on the cognitive demands of the benchmark\nsamples. Empirically, we show Scales++ reduces the upfront selection cost by\nover 18x while achieving competitive predictive fidelity. On the Open LLM\nLeaderboard, using just a 0.5\\% data subset, we predict full benchmark scores\nwith a 2.9% mean absolute error. We demonstrate that this item-centric approach\nenables more efficient model evaluation without significant fidelity\ndegradation, while also providing better cold-start performance and more\ninterpretable benchmarking.\n","authors":["Andrew M. Bean","Nabeel Seedat","Shengzhuang Chen","Jonathan Richard Schwarz"],"pdf_url":"https://arxiv.org/pdf/2510.26384v1.pdf","comment":"9 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2507.08896v2","updated":"2025-10-30T11:18:11Z","published":"2025-07-11T03:11:15Z","title":"Predictive Causal Inference via Spatio-Temporal Modeling and Penalized\n  Empirical Likelihood","summary":"  This study introduces an integrated framework for predictive causal inference\ndesigned to overcome limitations inherent in conventional single model\napproaches. Specifically, we combine a Hidden Markov Model (HMM) for spatial\nhealth state estimation with a Multi Task and Multi Graph Convolutional Network\n(MTGCN) for capturing temporal outcome trajectories. The framework\nasymmetrically treats temporal and spatial information regarding them as\nendogenous variables in the outcome regression, and exogenous variables in the\npropensity score model, thereby expanding the standard doubly robust treatment\neffect estimation to jointly enhance bias correction and predictive accuracy.\nTo demonstrate its utility, we focus on clinical domains such as cancer,\ndementia, and Parkinson disease, where treatment effects are challenging to\nobserve directly. Simulation studies are conducted to emulate latent disease\ndynamics and evaluate the model performance under varying conditions. Overall,\nthe proposed framework advances predictive causal inference by structurally\nadapting to spatiotemporal complexities common in biomedical data.\n","authors":["Byunghee Lee","Hye Yeon Sin","Joonsung Kang"],"pdf_url":"https://arxiv.org/pdf/2507.08896v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26376v1","updated":"2025-10-30T11:16:22Z","published":"2025-10-30T11:16:22Z","title":"Efficient Generative AI Boosts Probabilistic Forecasting of Sudden\n  Stratospheric Warmings","summary":"  Sudden Stratospheric Warmings (SSWs) are key sources of subseasonal\npredictability and major drivers of extreme winter weather. Yet, their accurate\nand efficient forecast remains a persistent challenge for numerical weather\nprediction (NWP) systems due to limitations in physical representation,\ninitialization, and the immense computational demands of ensemble forecasts.\nWhile data-driven forecasting is rapidly evolving, its application to the\ncomplex, three-dimensional dynamics of SSWs, particularly for probabilistic\nforecast, remains underexplored. Here, we bridge this gap by developing a Flow\nMatching-based generative AI model (FM-Cast) for efficient and skillful\nprobabilistic forecasting of the spatiotemporal evolution of stratospheric\ncirculation. Evaluated across 18 major SSW events (1998-2024), FM-Cast\nskillfully forecasts the onset, intensity, and morphology of 10 events up to 20\ndays in advance, achieving ensemble accuracies above 50%. Its performance is\ncomparable to or exceeds leading NWP systems while requiring only two minutes\nfor a 50-member, 30-day forecast on a consumer GPU. Furthermore, leveraging\nFM-Cast as a scientific tool, we demonstrate through idealized experiments that\nSSW predictability is fundamentally linked to its underlying physical drivers,\ndistinguishing between events forced from the troposphere and those driven by\ninternal stratospheric dynamics. Our work thus establishes a computationally\nefficient paradigm for probabilistic forecasting stratospheric anomalies and\nshowcases generative AI's potential to deepen the physical understanding of\natmosphere-climate dynamics.\n","authors":["Ningning Tao","Fei Xie","Baoxiang Pan","Hongyu Wang","Han Huang","Zhongpu Qiu","Ke Gui","Jiali Luo","Xiaosong Chen"],"pdf_url":"https://arxiv.org/pdf/2510.26376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26369v1","updated":"2025-10-30T11:14:17Z","published":"2025-10-30T11:14:17Z","title":"CorVS: Person Identification via Video Trajectory-Sensor Correspondence\n  in a Real-World Warehouse","summary":"  Worker location data is key to higher productivity in industrial sites.\nCameras are a promising tool for localization in logistics warehouses since\nthey also offer valuable environmental contexts such as package status.\nHowever, identifying individuals with only visual data is often impractical.\nAccordingly, several prior studies identified people in videos by comparing\ntheir trajectories and wearable sensor measurements. While this approach has\nadvantages such as independence from appearance, the existing methods may break\ndown under real-world conditions. To overcome this challenge, we propose CorVS,\na novel data-driven person identification method based on correspondence\nbetween visual tracking trajectories and sensor measurements. Firstly, our deep\nlearning model predicts correspondence probabilities and reliabilities for\nevery pair of a trajectory and sensor measurements. Secondly, our algorithm\nmatches the trajectories and sensor measurements over time using the predicted\nprobabilities and reliabilities. We developed a dataset with actual warehouse\noperations and demonstrated the method's effectiveness for real-world\napplications.\n","authors":["Kazuma Kano","Yuki Mori","Shin Katayama","Kenta Urano","Takuro Yonezawa","Nobuo Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2510.26369v1.pdf","comment":"7 pages, 3 figures, accepted to IPIN 2025"},{"id":"http://arxiv.org/abs/2510.24817v2","updated":"2025-10-30T11:13:33Z","published":"2025-10-28T10:06:49Z","title":"Towards a Method for Synthetic Generation of Persons with Aphasia\n  Transcripts","summary":"  In aphasia research, Speech-Language Pathologists (SLPs) devote extensive\ntime to manually coding speech samples using Correct Information Units (CIUs),\na measure of how informative an individual sample of speech is. Developing\nautomated systems to recognize aphasic language is limited by data scarcity.\nFor example, only about 600 transcripts are available in AphasiaBank yet\nbillions of tokens are used to train large language models (LLMs). In the\nbroader field of machine learning (ML), researchers increasingly turn to\nsynthetic data when such are sparse. Therefore, this study constructs and\nvalidates two methods to generate synthetic transcripts of the AphasiaBank Cat\nRescue picture description task. One method leverages a procedural programming\napproach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct\nLLMs. The methods generate transcripts across four severity levels (Mild,\nModerate, Severe, Very Severe) through word dropping, filler insertion, and\nparaphasia substitution. Overall, we found, compared to human-elicited\ntranscripts, Mistral 7b Instruct best captures key aspects of linguistic\ndegradation observed in aphasia, showing realistic directional changes in NDW,\nword count, and word length amongst the synthetic generation methods. Based on\nthe results, future work should plan to create a larger dataset, fine-tune\nmodels for better aphasic representation, and have SLPs assess the realism and\nusefulness of the synthetic transcripts.\n","authors":["Jason M. Pittman","Anton Phillips Jr.","Yesenia Medina-Santos","Brielle C. Stark"],"pdf_url":"https://arxiv.org/pdf/2510.24817v2.pdf","comment":"19 pages, 1 figure, 7 tables"},{"id":"http://arxiv.org/abs/2510.26353v1","updated":"2025-10-30T11:05:15Z","published":"2025-10-30T11:05:15Z","title":"Towards Explainable and Reliable AI in Finance","summary":"  Financial forecasting increasingly uses large neural network models, but\ntheir opacity raises challenges for trust and regulatory compliance. We present\nseveral approaches to explainable and reliable AI in finance. \\emph{First}, we\ndescribe how Time-LLM, a time series foundation model, uses a prompt to avoid a\nwrong directional forecast. \\emph{Second}, we show that combining foundation\nmodels for time series forecasting with a reliability estimator can filter our\nunreliable predictions. \\emph{Third}, we argue for symbolic reasoning encoding\ndomain rules for transparent justification. These approaches shift emphasize\nexecuting only forecasts that are both reliable and explainable. Experiments on\nequity and cryptocurrency data show that the architecture reduces false\npositives and supports selective execution. By integrating predictive\nperformance with reliability estimation and rule-based reasoning, our framework\nadvances transparent and auditable financial AI systems.\n","authors":["Albi Isufaj","Pablo Mollá","Helmut Prendinger"],"pdf_url":"https://arxiv.org/pdf/2510.26353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26350v1","updated":"2025-10-30T11:01:57Z","published":"2025-10-30T11:01:57Z","title":"UnifiedFL: A Dynamic Unified Learning Framework for Equitable Federation","summary":"  Federated learning (FL) has emerged as a key paradigm for collaborative model\ntraining across multiple clients without sharing raw data, enabling\nprivacy-preserving applications in areas such as radiology and pathology.\nHowever, works on collaborative training across clients with fundamentally\ndifferent neural architectures and non-identically distributed datasets remain\nscarce. Existing FL frameworks face several limitations. Despite claiming to\nsupport architectural heterogeneity, most recent FL methods only tolerate\nvariants within a single model family (e.g., shallower, deeper, or wider CNNs),\nstill presuming a shared global architecture and failing to accommodate\nfederations where clients deploy fundamentally different network types (e.g.,\nCNNs, GNNs, MLPs). Moreover, existing approaches often address only statistical\nheterogeneity while overlooking the domain-fracture problem, where each\nclient's data distribution differs markedly from that faced at testing time,\nundermining model generalizability. When clients use different architectures,\nhave non-identically distributed data, and encounter distinct test domains,\ncurrent methods perform poorly. To address these challenges, we propose\nUnifiedFL, a dynamic federated learning framework that represents heterogeneous\nlocal networks as nodes and edges in a directed model graph optimized by a\nshared graph neural network (GNN). UnifiedFL introduces (i) a common GNN to\nparameterize all architectures, (ii) distance-driven clustering via Euclidean\ndistances between clients' parameters, and (iii) a two-tier aggregation policy\nbalancing convergence and diversity. Experiments on MedMNIST classification and\nhippocampus segmentation benchmarks demonstrate UnifiedFL's superior\nperformance. Code and data: https://github.com/basiralab/UnifiedFL\n","authors":["Furkan Pala","Islem Rekik"],"pdf_url":"https://arxiv.org/pdf/2510.26350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26347v1","updated":"2025-10-30T10:55:05Z","published":"2025-10-30T10:55:05Z","title":"Reinforcement Learning for Pollution Detection in a Randomized, Sparse\n  and Nonstationary Environment with an Autonomous Underwater Vehicle","summary":"  Reinforcement learning (RL) algorithms are designed to optimize\nproblem-solving by learning actions that maximize rewards, a task that becomes\nparticularly challenging in random and nonstationary environments. Even\nadvanced RL algorithms are often limited in their ability to solve problems in\nthese conditions. In applications such as searching for underwater pollution\nclouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate\nreward-sparse environments, where actions frequently result in a zero reward.\nThis paper aims to address these challenges by revisiting and modifying\nclassical RL approaches to efficiently operate in sparse, randomized, and\nnonstationary environments. We systematically study a large number of\nmodifications, including hierarchical algorithm changes, multigoal learning,\nand the integration of a location memory as an external output filter to\nprevent state revisits. Our results demonstrate that a modified Monte\nCarlo-based approach significantly outperforms traditional Q-learning and two\nexhaustive search patterns, illustrating its potential in adapting RL to\ncomplex environments. These findings suggest that reinforcement learning\napproaches can be effectively adapted for use in random, nonstationary, and\nreward-sparse environments.\n","authors":["Sebastian Zieglmeier","Niklas Erdmann","Narada D. Warakagoda"],"pdf_url":"https://arxiv.org/pdf/2510.26347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26345v1","updated":"2025-10-30T10:52:43Z","published":"2025-10-30T10:52:43Z","title":"MisSynth: Improving MISSCI Logical Fallacies Classification with\n  Synthetic Data","summary":"  Health-related misinformation is very prevalent and potentially harmful. It\nis difficult to identify, especially when claims distort or misinterpret\nscientific findings. We investigate the impact of synthetic data generation and\nlightweight fine-tuning techniques on the ability of large language models\n(LLMs) to recognize fallacious arguments using the MISSCI dataset and\nframework. In this work, we propose MisSynth, a pipeline that applies\nretrieval-augmented generation (RAG) to produce synthetic fallacy samples,\nwhich are then used to fine-tune an LLM model. Our results show substantial\naccuracy gains with fine-tuned models compared to vanilla baselines. For\ninstance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score\nabsolute improvement on the MISSCI test split over its vanilla baseline. We\ndemonstrate that introducing synthetic fallacy data to augment limited\nannotated resources can significantly enhance zero-shot LLM classification\nperformance on real-world scientific misinformation tasks, even with limited\ncomputational resources. The code and synthetic dataset are available on\nhttps://github.com/mxpoliakov/MisSynth.\n","authors":["Mykhailo Poliakov","Nadiya Shvai"],"pdf_url":"https://arxiv.org/pdf/2510.26345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26342v1","updated":"2025-10-30T10:49:25Z","published":"2025-10-30T10:49:25Z","title":"Linear Causal Discovery with Interventional Constraints","summary":"  Incorporating causal knowledge and mechanisms is essential for refining\ncausal models and improving downstream tasks such as designing new treatments.\nIn this paper, we introduce a novel concept in causal discovery, termed\ninterventional constraints, which differs fundamentally from interventional\ndata. While interventional data require direct perturbations of variables,\ninterventional constraints encode high-level causal knowledge in the form of\ninequality constraints on causal effects. For instance, in the Sachs dataset\n(Sachs et al.\\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3\nexerts a positive causal effect on Akt. Existing causal discovery methods allow\nenforcing structural constraints (for example, requiring a causal path from\nPIP3 to Akt), but they may still produce incorrect causal conclusions such as\nlearning that \"PIP3 inhibits Akt\". Interventional constraints bridge this gap\nby explicitly constraining the total causal effect between variable pairs,\nensuring learned models respect known causal influences. To formalize\ninterventional constraints, we propose a metric to quantify total causal\neffects for linear causal models and formulate the problem as a constrained\noptimization task, solved using a two-stage constrained optimization method. We\nevaluate our approach on real-world datasets and demonstrate that integrating\ninterventional constraints not only improves model accuracy and ensures\nconsistency with established findings, making models more explainable, but also\nfacilitates the discovery of new causal relationships that would otherwise be\ncostly to identify.\n","authors":["Zhigao Guo","Feng Dong"],"pdf_url":"https://arxiv.org/pdf/2510.26342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26340v1","updated":"2025-10-30T10:48:18Z","published":"2025-10-30T10:48:18Z","title":"SABER: Symbolic Regression-based Angle of Arrival and Beam Pattern\n  Estimator","summary":"  Accurate Angle-of-arrival (AoA) estimation is essential for next-generation\nwireless communication systems to enable reliable beamforming, high-precision\nlocalization, and integrated sensing. Unfortunately, classical high-resolution\ntechniques require multi-element arrays and extensive snapshot collection,\nwhile generic Machine Learning (ML) approaches often yield black-box models\nthat lack physical interpretability. To address these limitations, we propose a\nSymbolic Regression (SR)-based ML framework. Namely, Symbolic Regression-based\nAngle of Arrival and Beam Pattern Estimator (SABER), a constrained\nsymbolic-regression framework that automatically discovers closed-form beam\npattern and AoA models from path loss measurements with interpretability. SABER\nachieves high accuracy while bridging the gap between opaque ML methods and\ninterpretable physics-driven estimators. First, we validate our approach in a\ncontrolled free-space anechoic chamber, showing that both direct inversion of\nthe known $\\cos^n$ beam and a low-order polynomial surrogate achieve sub-0.5\ndegree Mean Absolute Error (MAE). A purely unconstrained SR method can further\nreduce the error of the predicted angles, but produces complex formulas that\nlack physical insight. Then, we implement the same SR-learned inversions in a\nreal-world, Reconfigurable Intelligent Surface (RIS)-aided indoor testbed.\nSABER and unconstrained SR models accurately recover the true AoA with\nnear-zero error. Finally, we benchmark SABER against the Cram\\'er-Rao Lower\nBounds (CRLBs). Our results demonstrate that SABER is an interpretable and\naccurate alternative to state-of-the-art and black-box ML-based methods for AoA\nestimation.\n","authors":["Shih-Kai Chou","Mengran Zhao","Cheng-Nan Hu","Kuang-Chung Chou","Carolina Fortuna","Jernej Hribar"],"pdf_url":"https://arxiv.org/pdf/2510.26340v1.pdf","comment":"12 pages, 11 figures"},{"id":"http://arxiv.org/abs/2402.03145v4","updated":"2025-10-30T10:40:47Z","published":"2024-02-05T16:12:36Z","title":"SafEDMD: A Koopman-based data-driven controller design framework for\n  nonlinear dynamical systems","summary":"  The Koopman operator serves as the theoretical backbone for machine learning\nof dynamical control systems, where the operator is heuristically approximated\nby extended dynamic mode decomposition (EDMD). In this paper, we propose\nSafEDMD, a novel stability- and feedback-oriented EDMD-based controller design\nframework. Our approach leverages a reliable surrogate model generated in a\ndata-driven fashion in order to provide closed-loop guarantees. In particular,\nwe establish a controller design based on semi-definite programming with\nguaranteed stabilization of the underlying nonlinear system. As central\ningredient, we derive proportional error bounds that vanish at the origin and\nare tailored to control tasks. We illustrate the developed method by means of\nseveral benchmark examples and highlight the advantages over state-of-the-art\nmethods.\n","authors":["Robin Strässer","Manuel Schaller","Karl Worthmann","Julian Berberich","Frank Allgöwer"],"pdf_url":"https://arxiv.org/pdf/2402.03145v4.pdf","comment":"Accepted for publication in Automatica"},{"id":"http://arxiv.org/abs/2506.07500v2","updated":"2025-10-30T10:28:19Z","published":"2025-06-09T07:25:51Z","title":"Mind the Gap: Removing the Discretization Gap in Differentiable Logic\n  Gate Networks","summary":"  Modern neural networks demonstrate state-of-the-art performance on numerous\nexisting benchmarks; however, their high computational requirements and energy\nconsumption prompt researchers to seek more efficient solutions for real-world\ndeployment. Logic gate networks (LGNs) learns a large network of logic gates\nfor efficient image classification. However, learning a network that can solve\na simple problem like CIFAR-10 can take days to weeks to train. Even then,\nalmost half of the network remains unused, causing a discretization gap. This\ndiscretization gap hinders real-world deployment of LGNs, as the performance\ndrop between training and inference negatively impacts accuracy. We inject\nGumbel noise with a straight-through estimator during training to significantly\nspeed up training, improve neuron utilization, and decrease the discretization\ngap. We theoretically show that this results from implicit Hessian\nregularization, which improves the convergence properties of LGNs. We train\nnetworks $4.5 \\times$ faster in wall-clock time, reduce the discretization gap\nby $98\\%$, and reduce the number of unused gates by $100\\%$.\n","authors":["Shakir Yousefi","Andreas Plesner","Till Aczel","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2506.07500v2.pdf","comment":"Accepted to NeurIPS 2025 (main track)"},{"id":"http://arxiv.org/abs/2510.26328v1","updated":"2025-10-30T10:27:11Z","published":"2025-10-30T10:27:11Z","title":"Agent Skills Enable a New Class of Realistic and Trivially Simple Prompt\n  Injections","summary":"  Enabling continual learning in LLMs remains a key unresolved research\nchallenge. In a recent announcement, a frontier LLM company made a step towards\nthis by introducing Agent Skills, a framework that equips agents with new\nknowledge based on instructions stored in simple markdown files. Although Agent\nSkills can be a very useful tool, we show that they are fundamentally insecure,\nsince they enable trivially simple prompt injections. We demonstrate how to\nhide malicious instructions in long Agent Skill files and referenced scripts to\nexfiltrate sensitive data, such as internal files or passwords. Importantly, we\nshow how to bypass system-level guardrails of a popular coding agent: a benign,\ntask-specific approval with the \"Don't ask again\" option can carry over to\nclosely related but harmful actions. Overall, we conclude that despite ongoing\nresearch efforts and scaling model capabilities, frontier LLMs remain\nvulnerable to very simple prompt injections in realistic scenarios. Our code is\navailable at https://github.com/aisa-group/promptinject-agent-skills.\n","authors":["David Schmotz","Sahar Abdelnabi","Maksym Andriushchenko"],"pdf_url":"https://arxiv.org/pdf/2510.26328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21038v2","updated":"2025-10-30T10:23:32Z","published":"2025-10-23T22:44:50Z","title":"Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the\n  LibriBrain Dataset","summary":"  Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from\nlarge, public benchmarks. However, current benchmarks target relatively simple,\nfoundational tasks like Speech Detection and Phoneme Classification, while\napplication-ready results on tasks like Brain-to-Text remain elusive. We\npropose Keyword Spotting (KWS) as a practically applicable, privacy-aware\nintermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we\nprovide standardized train/validation/test splits for reproducible\nbenchmarking, and adopt an evaluation protocol tailored to extreme class\nimbalance. Concretely, we use area under the precision-recall curve (AUPRC) as\na robust evaluation metric, complemented by false alarms per hour (FA/h) at\nfixed recall to capture user-facing trade-offs. To simplify deployment and\nfurther experimentation within the research community, we are releasing an\nupdated version of the pnpl library with word-level dataloaders and Colab-ready\ntutorials. As an initial reference model, we present a compact 1-D Conv/ResNet\nbaseline with focal loss and top-k pooling that is trainable on a single\nconsumer-class GPU. The reference model achieves approximately 13x the\npermutation baseline AUPRC on held-out sessions, demonstrating the viability of\nthe task. Exploratory analyses reveal: (i) predictable within-subject scaling -\nperformance improves log-linearly with more training hours - and (ii) the\nexistence of word-level factors (frequency and duration) that systematically\nmodulate detectability.\n","authors":["Gereon Elvers","Gilad Landau","Oiwi Parker Jones"],"pdf_url":"https://arxiv.org/pdf/2510.21038v2.pdf","comment":"16 pages, 7 figures, 6 tables; updated acknowledgments"},{"id":"http://arxiv.org/abs/2510.26324v1","updated":"2025-10-30T10:17:27Z","published":"2025-10-30T10:17:27Z","title":"Posterior Sampling by Combining Diffusion Models with Annealed Langevin\n  Dynamics","summary":"  Given a noisy linear measurement $y = Ax + \\xi$ of a distribution $p(x)$, and\na good approximation to the prior $p(x)$, when can we sample from the posterior\n$p(x \\mid y)$? Posterior sampling provides an accurate and fair framework for\ntasks such as inpainting, deblurring, and MRI reconstruction, and several\nheuristics attempt to approximate it. Unfortunately, approximate posterior\nsampling is computationally intractable in general.\n  To sidestep this hardness, we focus on (local or global) log-concave\ndistributions $p(x)$. In this regime, Langevin dynamics yields posterior\nsamples when the exact scores of $p(x)$ are available, but it is brittle to\nscore--estimation error, requiring an MGF bound (sub-exponential error). By\ncontrast, in the unconditional setting, diffusion models succeed with only an\n$L^2$ bound on the score error. We prove that combining diffusion models with\nan annealed variant of Langevin dynamics achieves conditional sampling in\npolynomial time using merely an $L^4$ bound on the score error.\n","authors":["Zhiyang Xun","Shivam Gupta","Eric Price"],"pdf_url":"https://arxiv.org/pdf/2510.26324v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26323v1","updated":"2025-10-30T10:17:25Z","published":"2025-10-30T10:17:25Z","title":"On the Impact of Weight Discretization in QUBO-Based SVM Training","summary":"  Training Support Vector Machines (SVMs) can be formulated as a QUBO problem,\nenabling the use of quantum annealing for model optimization. In this work, we\nstudy how the number of qubits - linked to the discretization level of dual\nweights - affects predictive performance across datasets. We compare QUBO-based\nSVM training to the classical LIBSVM solver and find that even low-precision\nQUBO encodings (e.g., 1 bit per parameter) yield competitive, and sometimes\nsuperior, accuracy. While increased bit-depth enables larger regularization\nparameters, it does not always improve classification. Our findings suggest\nthat selecting the right support vectors may matter more than their precise\nweighting. Although current hardware limits the size of solvable QUBOs, our\nresults highlight the potential of quantum annealing for efficient SVM training\nas quantum devices scale.\n","authors":["Sascha Mücke"],"pdf_url":"https://arxiv.org/pdf/2510.26323v1.pdf","comment":"Presented at the 7th DSO Workshop at ECML PKDD 2025"},{"id":"http://arxiv.org/abs/2506.20535v2","updated":"2025-10-30T10:14:59Z","published":"2025-06-25T15:24:45Z","title":"AIMeter: Measuring, Analyzing, and Visualizing Energy and Carbon\n  Footprint of AI Workloads","summary":"  The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents AIMeter, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, AIMeter offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, AIMeter encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/AIMeter.\n","authors":["Hongzhen Huang","Kunming Zhang","Hanlong Liao","Kui Wu","Guoming Tang"],"pdf_url":"https://arxiv.org/pdf/2506.20535v2.pdf","comment":"11 pages, 7 figures and 5 tables"},{"id":"http://arxiv.org/abs/2510.26311v1","updated":"2025-10-30T09:58:48Z","published":"2025-10-30T09:58:48Z","title":"Model Inversion with Layer-Specific Modeling and Alignment for Data-Free\n  Continual Learning","summary":"  Continual learning (CL) aims to incrementally train a model on a sequence of\ntasks while retaining performance on prior ones. However, storing and replaying\ndata is often infeasible due to privacy or security constraints and impractical\nfor arbitrary pre-trained models. Data-free CL seeks to update models without\naccess to previous data. Beyond regularization, we employ model inversion to\nsynthesize data from the trained model, enabling replay without storing\nsamples. Yet, model inversion in predictive models faces two challenges: (1)\ngenerating inputs solely from compressed output labels causes drift between\nsynthetic and real data, and replaying such data can erode prior knowledge; (2)\ninversion is computationally expensive since each step backpropagates through\nthe full model. These issues are amplified in large pre-trained models such as\nCLIP. To improve efficiency, we propose Per-layer Model Inversion (PMI),\ninspired by faster convergence in single-layer optimization. PMI provides\nstrong initialization for full-model inversion, substantially reducing\niterations. To mitigate feature shift, we model class-wise features via\nGaussian distributions and contrastive model, ensuring alignment between\nsynthetic and real features. Combining PMI and feature modeling, our approach\nenables continual learning of new classes by generating pseudo-images from\nsemantic-aware projected features, achieving strong effectiveness and\ncompatibility across multiple CL settings.\n","authors":["Ruilin Tong","Haodong Lu","Yuhang Liu","Dong Gong"],"pdf_url":"https://arxiv.org/pdf/2510.26311v1.pdf","comment":"Accepted in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26307v1","updated":"2025-10-30T09:49:59Z","published":"2025-10-30T09:49:59Z","title":"A Survey of Heterogeneous Graph Neural Networks for Cybersecurity\n  Anomaly Detection","summary":"  Anomaly detection is a critical task in cybersecurity, where identifying\ninsider threats, access violations, and coordinated attacks is essential for\nensuring system resilience. Graph-based approaches have become increasingly\nimportant for modeling entity interactions, yet most rely on homogeneous and\nstatic structures, which limits their ability to capture the heterogeneity and\ntemporal evolution of real-world environments. Heterogeneous Graph Neural\nNetworks (HGNNs) have emerged as a promising paradigm for anomaly detection by\nincorporating type-aware transformations and relation-sensitive aggregation,\nenabling more expressive modeling of complex cyber data. However, current\nresearch on HGNN-based anomaly detection remains fragmented, with diverse\nmodeling strategies, limited comparative evaluation, and an absence of\nstandardized benchmarks. To address this gap, we provide a comprehensive survey\nof HGNN-based anomaly detection methods in cybersecurity. We introduce a\ntaxonomy that classifies approaches by anomaly type and graph dynamics, analyze\nrepresentative models, and map them to key cybersecurity applications. We also\nreview commonly used benchmark datasets and evaluation metrics, highlighting\ntheir strengths and limitations. Finally, we identify key open challenges\nrelated to modeling, data, and deployment, and outline promising directions for\nfuture research. This survey aims to establish a structured foundation for\nadvancing HGNN-based anomaly detection toward scalable, interpretable, and\npractically deployable solutions.\n","authors":["Laura Jiang","Reza Ryan","Qian Li","Nasim Ferdosian"],"pdf_url":"https://arxiv.org/pdf/2510.26307v1.pdf","comment":"37 pages, 4 figures, 86 references. Submitted to Journal of Computer\n  Security (under review)"},{"id":"http://arxiv.org/abs/2410.09766v2","updated":"2025-10-30T09:48:49Z","published":"2024-10-13T07:50:47Z","title":"Stability and Sharper Risk Bounds with Convergence Rate\n  $\\tilde{O}(1/n^2)$","summary":"  Prior work (Klochkov $\\&$ Zhivotovskiy, 2021) establishes at most\n$O\\left(\\log (n)/n\\right)$ excess risk bounds via algorithmic stability for\nstrongly-convex learners with high probability. We show that under the similar\ncommon assumptions -- - Polyak-Lojasiewicz condition, smoothness, and Lipschitz\ncontinous for losses -- - rates of $O\\left(\\log^2(n)/n^2\\right)$ are at most\nachievable. To our knowledge, our analysis also provides the tightest\nhigh-probability bounds for gradient-based generalization gaps in nonconvex\nsettings.\n","authors":["Bowei Zhu","Shaojie Li","Mingyang Yi","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.09766v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10573v3","updated":"2025-10-30T09:42:47Z","published":"2024-11-15T20:46:58Z","title":"Hysteresis Activation Function for Efficient Inference","summary":"  The widely used ReLU is favored for its hardware efficiency, {as the\nimplementation at inference is a one bit sign case,} yet suffers from issues\nsuch as the ``dying ReLU'' problem, where during training, neurons fail to\nactivate and constantly remain at zero, as highlighted by Lu et al. Traditional\napproaches to mitigate this issue often introduce more complex and less\nhardware-friendly activation functions. In this work, we propose a Hysteresis\nRectified Linear Unit (HeLU), an efficient activation function designed to\naddress the ``dying ReLU'' problem with minimal complexity. Unlike traditional\nactivation functions with fixed thresholds for training and inference, HeLU\nemploys a variable threshold that refines the backpropagation. This refined\nmechanism allows simpler activation functions to achieve competitive\nperformance comparable to their more complex counterparts without introducing\nunnecessary complexity or requiring inductive biases. Empirical evaluations\ndemonstrate that HeLU enhances model generalization across diverse datasets,\noffering a promising solution for efficient and effective inference suitable\nfor a wide range of neural network architectures.\n","authors":["Moshe Kimhi","Idan Kashani","Avi Mendelson","Chaim Baskin"],"pdf_url":"https://arxiv.org/pdf/2411.10573v3.pdf","comment":"Accepted to 4th NeurIPS Efficient Natural Language and Speech\n  Processing Workshop (ENLSP-IV 2024)"},{"id":"http://arxiv.org/abs/2510.26303v1","updated":"2025-10-30T09:41:33Z","published":"2025-10-30T09:41:33Z","title":"Implicit Bias of Per-sample Adam on Separable Data: Departure from the\n  Full-batch Regime","summary":"  Adam [Kingma and Ba, 2015] is the de facto optimizer in deep learning, yet\nits theoretical understanding remains limited. Prior analyses show that Adam\nfavors solutions aligned with $\\ell_\\infty$-geometry, but these results are\nrestricted to the full-batch regime. In this work, we study the implicit bias\nof incremental Adam (using one sample per step) for logistic regression on\nlinearly separable data, and we show that its bias can deviate from the\nfull-batch behavior. To illustrate this, we construct a class of structured\ndatasets where incremental Adam provably converges to the $\\ell_2$-max-margin\nclassifier, in contrast to the $\\ell_\\infty$-max-margin bias of full-batch\nAdam. For general datasets, we develop a proxy algorithm that captures the\nlimiting behavior of incremental Adam as $\\beta_2 \\to 1$ and we characterize\nits convergence direction via a data-dependent dual fixed-point formulation.\nFinally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges\nto the $\\ell_\\infty$-max-margin classifier for any batch size by taking $\\beta$\nclose enough to 1. Overall, our results highlight that the implicit bias of\nAdam crucially depends on both the batching scheme and the dataset, while\nSignum remains invariant.\n","authors":["Beomhan Baek","Minhak Song","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2510.26303v1.pdf","comment":"50 pages"},{"id":"http://arxiv.org/abs/2510.26302v1","updated":"2025-10-30T09:41:21Z","published":"2025-10-30T09:41:21Z","title":"Understanding Hardness of Vision-Language Compositionality from A\n  Token-level Causal Lens","summary":"  Contrastive Language-Image Pre-training (CLIP) delivers strong cross modal\ngeneralization by aligning images and texts in a shared embedding space, yet it\npersistently fails at compositional reasoning over objects, attributes, and\nrelations often behaving like a bag-of-words matcher. Prior causal accounts\ntypically model text as a single vector, obscuring token-level structure and\nleaving core phenomena-such as prompt sensitivity and failures on hard\nnegatives unexplained. We address this gap with a token-aware causal\nrepresentation learning (CRL) framework grounded in a sequential,\nlanguage-token SCM. Our theory extends block identifiability to tokenized text,\nproving that CLIP's contrastive objective can recover the modal-invariant\nlatent variable under both sentence-level and token-level SCMs. Crucially,\ntoken granularity yields the first principled explanation of CLIP's\ncompositional brittleness: composition nonidentifiability. We show the\nexistence of pseudo-optimal text encoders that achieve perfect modal-invariant\nalignment yet are provably insensitive to SWAP, REPLACE, and ADD operations\nover atomic concepts, thereby failing to distinguish correct captions from hard\nnegatives despite optimizing the same training objective as true-optimal\nencoders. The analysis further links language-side nonidentifiability to\nvisual-side failures via the modality gap and shows how iterated composition\noperators compound hardness, motivating improved negative mining strategies.\n","authors":["Ziliang Chen","Tianang Xiao","Jusheng Zhang","Yongsen Zheng","Xipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2510.26302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26301v1","updated":"2025-10-30T09:39:05Z","published":"2025-10-30T09:39:05Z","title":"Offline Clustering of Preference Learning with Active-data Augmentation","summary":"  Preference learning from pairwise feedback is a widely adopted framework in\napplications such as reinforcement learning with human feedback and\nrecommendations. In many practical settings, however, user interactions are\nlimited or costly, making offline preference learning necessary. Moreover,\nreal-world preference learning often involves users with different preferences.\nFor example, annotators from different backgrounds may rank the same responses\ndifferently. This setting presents two central challenges: (1) identifying\nsimilarity across users to effectively aggregate data, especially under\nscenarios where offline data is imbalanced across dimensions, and (2) handling\nthe imbalanced offline data where some preference dimensions are\nunderrepresented. To address these challenges, we study the Offline Clustering\nof Preference Learning problem, where the learner has access to fixed datasets\nfrom multiple users with potentially different preferences and aims to maximize\nutility for a test user. To tackle the first challenge, we first propose\nOff-C$^2$PL for the pure offline setting, where the learner relies solely on\noffline data. Our theoretical analysis provides a suboptimality bound that\nexplicitly captures the tradeoff between sample noise and bias. To address the\nsecond challenge of inbalanced data, we extend our framework to the setting\nwith active-data augmentation where the learner is allowed to select a limited\nnumber of additional active-data for the test user based on the cluster\nstructure learned by Off-C$^2$PL. In this setting, our second algorithm,\nA$^2$-Off-C$^2$PL, actively selects samples that target the least-informative\ndimensions of the test user's preference. We prove that these actively\ncollected samples contribute more effectively than offline ones. Finally, we\nvalidate our theoretical results through simulations on synthetic and\nreal-world datasets.\n","authors":["Jingyuan Liu","Fatemeh Ghaffari","Xuchuang Wang","Mohammad Hajiesmaili","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2510.26301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22319v2","updated":"2025-10-30T09:33:15Z","published":"2025-10-25T14:51:17Z","title":"GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via\n  Regulated Clipping","summary":"  Recently, GRPO-based reinforcement learning has shown remarkable progress in\noptimizing flow-matching models, effectively improving their alignment with\ntask-specific rewards. Within these frameworks, the policy update relies on\nimportance-ratio clipping to constrain overconfident positive and negative\ngradients. However, in practice, we observe a systematic shift in the\nimportance-ratio distribution-its mean falls below 1 and its variance differs\nsubstantially across timesteps. This left-shifted and inconsistent distribution\nprevents positive-advantage samples from entering the clipped region, causing\nthe mechanism to fail in constraining overconfident positive updates. As a\nresult, the policy model inevitably enters an implicit over-optimization\nstage-while the proxy reward continues to increase, essential metrics such as\nimage quality and text-prompt alignment deteriorate sharply, ultimately making\nthe learned policy impractical for real-world use. To address this issue, we\nintroduce GRPO-Guard, a simple yet effective enhancement to existing GRPO\nframeworks. Our method incorporates ratio normalization, which restores a\nbalanced and step-consistent importance ratio, ensuring that PPO clipping\nproperly constrains harmful updates across denoising timesteps. In addition, a\ngradient reweighting strategy equalizes policy gradients over noise conditions,\npreventing excessive updates from particular timestep regions. Together, these\ndesigns act as a regulated clipping mechanism, stabilizing optimization and\nsubstantially mitigating implicit over-optimization without relying on heavy KL\nregularization. Extensive experiments on multiple diffusion backbones (e.g.,\nSD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard\nsignificantly reduces over-optimization while maintaining or even improving\ngeneration quality.\n","authors":["Jing Wang","Jiajun Liang","Jie Liu","Henglin Liu","Gongye Liu","Jun Zheng","Wanyuan Pang","Ao Ma","Zhenyu Xie","Xintao Wang","Meng Wang","Pengfei Wan","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2510.22319v2.pdf","comment":"Project Page: https://jingw193.github.io/GRPO-Guard/"},{"id":"http://arxiv.org/abs/2505.22151v2","updated":"2025-10-30T09:26:54Z","published":"2025-05-28T09:17:44Z","title":"Oryx: a Scalable Sequence Model for Many-Agent Coordination in Offline\n  MARL","summary":"  A key challenge in offline multi-agent reinforcement learning (MARL) is\nachieving effective many-agent multi-step coordination in complex environments.\nIn this work, we propose Oryx, a novel algorithm for offline cooperative MARL\nto directly address this challenge. Oryx adapts the recently proposed\nretention-based architecture Sable and combines it with a sequential form of\nimplicit constraint Q-learning (ICQ), to develop a novel offline autoregressive\npolicy update scheme. This allows Oryx to solve complex coordination challenges\nwhile maintaining temporal coherence over long trajectories. We evaluate Oryx\nacross a diverse set of benchmarks from prior works -- SMAC, RWARE, and\nMulti-Agent MuJoCo -- covering tasks of both discrete and continuous control,\nvarying in scale and difficulty. Oryx achieves state-of-the-art performance on\nmore than 80% of the 65 tested datasets, outperforming prior offline MARL\nmethods and demonstrating robust generalisation across domains with many agents\nand long horizons. Finally, we introduce new datasets to push the limits of\nmany-agent coordination in offline MARL, and demonstrate Oryx's superior\nability to scale effectively in such settings.\n","authors":["Claude Formanek","Omayma Mahjoub","Louay Ben Nessir","Sasha Abramowitz","Ruan de Kock","Wiem Khlifi","Daniel Rajaonarivonivelomanantsoa","Simon Du Toit","Arnol Fokam","Siddarth Singh","Ulrich Mbou Sob","Felix Chalumeau","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2505.22151v2.pdf","comment":"Published at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2505.13138v2","updated":"2025-10-30T09:22:20Z","published":"2025-05-19T14:07:47Z","title":"Neurosymbolic Diffusion Models","summary":"  Neurosymbolic (NeSy) predictors combine neural perception with symbolic\nreasoning to solve tasks like visual reasoning. However, standard NeSy\npredictors assume conditional independence between the symbols they extract,\nthus limiting their ability to model interactions and uncertainty - often\nleading to overconfident predictions and poor out-of-distribution\ngeneralisation. To overcome the limitations of the independence assumption, we\nintroduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy\npredictors that use discrete diffusion to model dependencies between symbols.\nOur approach reuses the independence assumption from NeSy predictors at each\nstep of the diffusion process, enabling scalable learning while capturing\nsymbol dependencies and uncertainty quantification. Across both synthetic and\nreal-world benchmarks - including high-dimensional visual path planning and\nrule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among\nNeSy predictors and demonstrate strong calibration.\n","authors":["Emile van Krieken","Pasquale Minervini","Edoardo Ponti","Antonio Vergari"],"pdf_url":"https://arxiv.org/pdf/2505.13138v2.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.03509v2","updated":"2025-10-30T09:17:12Z","published":"2025-05-06T13:19:15Z","title":"AnomalyMatch: Discovering Rare Objects of Interest with Semi-supervised\n  and Active Learning","summary":"  Anomaly detection in large datasets is essential in astronomy and computer\nvision. However, due to a scarcity of labelled data, it is often infeasible to\napply supervised methods to anomaly detection. We present AnomalyMatch, an\nanomaly detection framework combining the semi-supervised FixMatch algorithm\nusing EfficientNet classifiers with active learning. AnomalyMatch is tailored\nfor large-scale applications and integrated into the ESA Datalabs science\nplatform. In this method, we treat anomaly detection as a binary classification\nproblem and efficiently utilise limited labelled and abundant unlabelled images\nfor training. We enable active learning via a user interface for verification\nof high-confidence anomalies and correction of false positives. Evaluations on\nthe GalaxyMNIST astronomical dataset and the miniImageNet natural-image\nbenchmark under severe class imbalance display strong performance. Starting\nfrom five to ten labelled anomalies, we achieve an average AUROC of 0.96\n(miniImageNet) and 0.89 (GalaxyMNIST), with respective AUPRC of 0.82 and 0.77.\nAfter three active learning cycles, anomalies are ranked with 76%\n(miniImageNet) to 94% (GalaxyMNIST) precision in the top 1% of the\nhighest-ranking images by score. We compare to the established Astronomaly\nsoftware on selected 'odd' galaxies from the 'Galaxy Zoo - The Galaxy\nChallenge' dataset, achieving comparable performance with an average AUROC of\n0.83. Our results underscore the exceptional utility and scalability of this\napproach for anomaly discovery, highlighting the value of specialised\napproaches for domains characterised by severe label scarcity.\n","authors":["Pablo Gómez","Laslo E. Ruhberg","Maria Teresa Nardone","David O'Ryan"],"pdf_url":"https://arxiv.org/pdf/2505.03509v2.pdf","comment":"Journal submission in preparation to RASTI; 15 pages; 12 figures"},{"id":"http://arxiv.org/abs/2502.04380v3","updated":"2025-10-30T09:16:49Z","published":"2025-02-05T17:21:01Z","title":"Diversity as a Reward: Fine-Tuning LLMs on a Mixture of\n  Domain-Undetermined Data","summary":"  Fine-tuning large language models (LLMs) using diverse datasets is crucial\nfor enhancing their overall performance across various domains. In practical\nscenarios, existing methods based on modeling the mixture proportions of data\ncomposition often struggle with data whose domain labels are missing, imprecise\nor non-normalized, while methods based on data selection usually encounter\ndifficulties in balancing multi-domain performance. To address these\nchallenges, in this work, we investigate the role of data diversity in\nenhancing the overall abilities of LLMs by empirically constructing contrastive\ndata pools and theoretically deriving explanations. Building upon the insights\ngained, we propose a new method that gives the LLM a dual identity: an output\nmodel to cognitively probe and select data based on diversity reward, as well\nas an input model to be tuned with the selected data. Extensive experiments\nshow that the proposed method notably boosts performance across\ndomain-undetermined data and a series of foundational downstream tasks when\napplied to various advanced LLMs. We release our code and hope this study can\nshed light on the understanding of data diversity and advance feedback-driven\ndata-model co-design for LLMs.\n","authors":["Zhenqing Ling","Daoyuan Chen","Liuyi Yao","Qianli Shen","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2502.04380v3.pdf","comment":"Accepted by NeurIPS'25 main track. 47 pages, 21 figures, 32 tables"},{"id":"http://arxiv.org/abs/2510.04237v3","updated":"2025-10-30T09:14:25Z","published":"2025-10-05T15:04:03Z","title":"Truncated Kernel Stochastic Gradient Descent with General Losses and\n  Spherical Radial Basis Functions","summary":"  In this paper, we propose a novel kernel stochastic gradient descent (SGD)\nalgorithm for large-scale supervised learning with general losses. Compared to\ntraditional kernel SGD, our algorithm improves efficiency and scalability\nthrough an innovative regularization strategy. By leveraging the infinite\nseries expansion of spherical radial basis functions, this strategy projects\nthe stochastic gradient onto a finite-dimensional hypothesis space, which is\nadaptively scaled according to the bias-variance trade-off, thereby enhancing\ngeneralization performance. Based on a new estimation of the spectral structure\nof the kernel-induced covariance operator, we develop an analytical framework\nthat unifies optimization and generalization analyses. We prove that both the\nlast iterate and the suffix average converge at minimax-optimal rates, and we\nfurther establish optimal strong convergence in the reproducing kernel Hilbert\nspace. Our framework accommodates a broad class of classical loss functions,\nincluding least-squares, Huber, and logistic losses. Moreover, the proposed\nalgorithm significantly reduces computational complexity and achieves optimal\nstorage complexity by incorporating coordinate-wise updates from linear SGD,\nthereby avoiding the costly pairwise operations typical of kernel SGD and\nenabling efficient processing of streaming data. Finally, extensive numerical\nexperiments demonstrate the efficiency of our approach.\n","authors":["Jinhui Bai","Andreas Christmann","Lei Shi"],"pdf_url":"https://arxiv.org/pdf/2510.04237v3.pdf","comment":"54 pages, 20 figures"},{"id":"http://arxiv.org/abs/2503.12902v2","updated":"2025-10-30T09:10:57Z","published":"2025-03-17T08:03:47Z","title":"Experiments with Optimal Model Trees","summary":"  Model trees provide an appealing way to perform interpretable machine\nlearning for both classification and regression problems. In contrast to\n``classic'' decision trees with constant values in their leaves, model trees\ncan use linear combinations of predictor variables in their leaf nodes to form\npredictions, which can help achieve higher accuracy and smaller trees. Typical\nalgorithms for learning model trees from training data work in a greedy\nfashion, growing the tree in a top-down manner by recursively splitting the\ndata into smaller and smaller subsets. Crucially, the selected splits are only\nlocally optimal, potentially rendering the tree overly complex and less\naccurate than a tree whose structure is globally optimal for the training data.\nIn this paper, we empirically investigate the effect of constructing globally\noptimal model trees for classification and regression with linear support\nvector machines at the leaf nodes. To this end, we present mixed-integer linear\nprogramming formulations to learn optimal trees, compute such trees for a large\ncollection of benchmark data sets, and compare their performance against\ngreedily grown model trees in terms of interpretability and accuracy. We also\ncompare to classic optimal and greedily grown decision trees, random forests,\nand support vector machines. Our results show that optimal model trees can\nachieve competitive accuracy with very small trees. We also investigate the\neffect on the accuracy of replacing axis-parallel splits with multivariate\nones, foregoing interpretability while potentially obtaining greater accuracy.\n","authors":["Sabino Francesco Roselli","Eibe Frank"],"pdf_url":"https://arxiv.org/pdf/2503.12902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26285v1","updated":"2025-10-30T09:08:50Z","published":"2025-10-30T09:08:50Z","title":"Unravelling the Mechanisms of Manipulating Numbers in Language Models","summary":"  Recent work has shown that different large language models (LLMs) converge to\nsimilar and accurate input embedding representations for numbers. These\nfindings conflict with the documented propensity of LLMs to produce erroneous\noutputs when dealing with numeric information. In this work, we aim to explain\nthis conflict by exploring how language models manipulate numbers and quantify\nthe lower bounds of accuracy of these mechanisms. We find that despite\nsurfacing errors, different language models learn interchangeable\nrepresentations of numbers that are systematic, highly accurate and universal\nacross their hidden states and the types of input contexts. This allows us to\ncreate universal probes for each LLM and to trace information -- including the\ncauses of output errors -- to specific layers. Our results lay a fundamental\nunderstanding of how pre-trained LLMs manipulate numbers and outline the\npotential of more accurate probing techniques in addressed refinements of LLMs'\narchitectures.\n","authors":["Michal Štefánik","Timothee Mickus","Marek Kadlčík","Bertram Højer","Michal Spiegel","Raúl Vázquez","Aman Sinha","Josef Kuchař","Philipp Mondorf"],"pdf_url":"https://arxiv.org/pdf/2510.26285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26284v1","updated":"2025-10-30T09:08:07Z","published":"2025-10-30T09:08:07Z","title":"Empirical Bayesian Multi-Bandit Learning","summary":"  Multi-task learning in contextual bandits has attracted significant research\ninterest due to its potential to enhance decision-making across multiple\nrelated tasks by leveraging shared structures and task-specific heterogeneity.\nIn this article, we propose a novel hierarchical Bayesian framework for\nlearning in various bandit instances. This framework captures both the\nheterogeneity and the correlations among different bandit instances through a\nhierarchical Bayesian model, enabling effective information sharing while\naccommodating instance-specific variations. Unlike previous methods that\noverlook the learning of the covariance structure across bandits, we introduce\nan empirical Bayesian approach to estimate the covariance matrix of the prior\ndistribution.This enhances both the practicality and flexibility of learning\nacross multi-bandits. Building on this approach, we develop two efficient\nalgorithms: ebmTS (Empirical Bayesian Multi-Bandit Thompson Sampling) and\nebmUCB (Empirical Bayesian Multi-Bandit Upper Confidence Bound), both of which\nincorporate the estimated prior into the decision-making process. We provide\nthe frequentist regret upper bounds for the proposed algorithms, thereby\nfilling a research gap in the field of multi-bandit problems. Extensive\nexperiments on both synthetic and real-world datasets demonstrate the superior\nperformance of our algorithms, particularly in complex environments. Our\nmethods achieve lower cumulative regret compared to existing techniques,\nhighlighting their effectiveness in balancing exploration and exploitation\nacross multi-bandits.\n","authors":["Xia Jiang","Rong J. B. Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.26284v1.pdf","comment":"33 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.15475v3","updated":"2025-10-30T09:02:24Z","published":"2025-02-21T14:00:14Z","title":"Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning\n  Solution for Protocols Compliance","summary":"  Neural network-based decoding methods show promise in enhancing error\ncorrection performance but face challenges with punctured codes. In particular,\nexisting methods struggle to adapt to variable code rates or meet protocol\ncompatibility requirements. This paper proposes a unified long short-term\nmemory (LSTM)-based neural decoder for punctured convolutional and Turbo codes\nto address these challenges. The key component of the proposed LSTM-based\nneural decoder is puncturing-aware embedding, which integrates puncturing\npatterns directly into the neural network to enable seamless adaptation to\ndifferent code rates. Moreover, a balanced bit error rate training strategy is\ndesigned to ensure the decoder's robustness across various code lengths, rates,\nand channels. In this way, the protocol compatibility requirement can be\nrealized. Extensive simulations in both additive white Gaussian noise (AWGN)\nand Rayleigh fading channels demonstrate that the proposed neural decoder\noutperforms conventional decoding techniques, offering significant improvements\nin decoding accuracy and robustness.\n","authors":["Yongli Yan","Linglong Dai"],"pdf_url":"https://arxiv.org/pdf/2502.15475v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26278v1","updated":"2025-10-30T09:00:42Z","published":"2025-10-30T09:00:42Z","title":"Distributional Multi-objective Black-box Optimization for\n  Diffusion-model Inference-time Multi-Target Generation","summary":"  Diffusion models have been successful in learning complex data distributions.\nThis capability has driven their application to high-dimensional\nmulti-objective black-box optimization problem. Existing approaches often\nemploy an external optimization loop, such as an evolutionary algorithm, to the\ndiffusion model. However, these approaches treat the diffusion model as a\nblack-box refiner, which overlooks the internal distribution transition of the\ndiffusion generation process, limiting their efficiency. To address these\nchallenges, we propose the Inference-time Multi-target Generation (IMG)\nalgorithm, which optimizes the diffusion process at inference-time to generate\nsamples that simultaneously satisfy multiple objectives. Specifically, our IMG\nperforms weighted resampling during the diffusion generation process according\nto the expected aggregated multi-objective values. This weighted resampling\nstrategy ensures the diffusion-generated samples are distributed according to\nour desired multi-target Boltzmann distribution. We further derive that the\nmulti-target Boltzmann distribution has an interesting log-likelihood\ninterpretation, where it is the optimal solution to the distributional\nmulti-objective optimization problem. We implemented IMG for a multi-objective\nmolecule generation task. Experiments show that IMG, requiring only a single\ngeneration pass, achieves a significantly higher hypervolume than baseline\noptimization algorithms that often require hundreds of diffusion generations.\nNotably, our algorithm can be viewed as an optimized diffusion process and can\nbe integrated into existing methods to further improve their performance.\n","authors":["Kim Yong Tan","Yueming Lyu","Ivor Tsang","Yew-Soon Ong"],"pdf_url":"https://arxiv.org/pdf/2510.26278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12995v2","updated":"2025-10-30T08:59:47Z","published":"2024-11-20T02:46:15Z","title":"Beyond likelihood ratio bias: Nested multi-time-scale stochastic\n  approximation for likelihood-free parameter estimation","summary":"  We study parameter inference in simulation-based stochastic models where the\nanalytical form of the likelihood is unknown. The main difficulty is that score\nevaluation as a ratio of noisy Monte Carlo estimators induces bias and\ninstability, which we overcome with a ratio-free nested multi-time-scale (NMTS)\nstochastic approximation (SA) method that simultaneously tracks the score and\ndrives the parameter update. We provide a comprehensive theoretical analysis of\nthe proposed NMTS algorithm for solving likelihood-free inference problems,\nincluding strong convergence, asymptotic normality, and convergence rates. We\nshow that our algorithm can eliminate the original asymptotic bias\n$O\\big(\\sqrt{\\frac{1}{N}}\\big)$ and accelerate the convergence rate from\n$O\\big(\\beta_k+\\sqrt{\\frac{1}{N}}\\big)$ to\n$O\\big(\\frac{\\beta_k}{\\alpha_k}+\\sqrt{\\frac{\\alpha_k}{N}}\\big)$, where $N$ is\nthe fixed batch size, $\\alpha_k$ and $\\beta_k$ are decreasing step sizes with\n$\\alpha_k$, $\\beta_k$, $\\beta_k/\\alpha_k\\rightarrow 0$. With proper choice of\n$\\alpha_k$ and $\\beta_k$, our convergence rates can match the optimal rate in\nthe multi-time-scale SA literature. Numerical experiments demonstrate that our\nalgorithm can improve the estimation accuracy by one to two orders of magnitude\nat the same computational cost, making it efficient for parameter estimation in\nstochastic systems.\n","authors":["Zehao Li","Zhouchen Lin","Yijie Peng"],"pdf_url":"https://arxiv.org/pdf/2411.12995v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26275v1","updated":"2025-10-30T08:59:01Z","published":"2025-10-30T08:59:01Z","title":"A Research Roadmap for Augmenting Software Engineering Processes and\n  Software Products with Generative AI","summary":"  Generative AI (GenAI) is rapidly transforming software engineering (SE)\npractices, influencing how SE processes are executed, as well as how software\nsystems are developed, operated, and evolved. This paper applies design science\nresearch to build a roadmap for GenAI-augmented SE. The process consists of\nthree cycles that incrementally integrate multiple sources of evidence,\nincluding collaborative discussions from the FSE 2025 \"Software Engineering\n2030\" workshop, rapid literature reviews, and external feedback sessions\ninvolving peers. McLuhan's tetrads were used as a conceptual instrument to\nsystematically capture the transforming effects of GenAI on SE processes and\nsoftware products.The resulting roadmap identifies four fundamental forms of\nGenAI augmentation in SE and systematically characterizes their related\nresearch challenges and opportunities. These insights are then consolidated\ninto a set of future research directions. By grounding the roadmap in a\nrigorous multi-cycle process and cross-validating it among independent author\nteams and peers, the study provides a transparent and reproducible foundation\nfor analyzing how GenAI affects SE processes, methods and tools, and for\nframing future research within this rapidly evolving area. Based on these\nfindings, the article finally makes ten predictions for SE in the year 2030.\n","authors":["Domenico Amalfitano","Andreas Metzger","Marco Autili","Tommaso Fulcini","Tobias Hey","Jan Keim","Patrizio Pelliccione","Vincenzo Scotti","Anne Koziolek","Raffaela Mirandola","Andreas Vogelsang"],"pdf_url":"https://arxiv.org/pdf/2510.26275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26274v1","updated":"2025-10-30T08:58:44Z","published":"2025-10-30T08:58:44Z","title":"PVMark: Enabling Public Verifiability for LLM Watermarking Schemes","summary":"  Watermarking schemes for large language models (LLMs) have been proposed to\nidentify the source of the generated text, mitigating the potential threats\nemerged from model theft. However, current watermarking solutions hardly\nresolve the trust issue: the non-public watermark detection cannot prove itself\nfaithfully conducting the detection. We observe that it is attributed to the\nsecret key mostly used in the watermark detection -- it cannot be public, or\nthe adversary may launch removal attacks provided the key; nor can it be\nprivate, or the watermarking detection is opaque to the public. To resolve the\ndilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP),\nenabling the watermark detection process to be publicly verifiable by third\nparties without disclosing any secret key. PVMark hinges upon the proof of\n`correct execution' of watermark detection on which a set of ZKP constraints\nare built, including mapping, random number generation, comparison, and\nsummation. We implement multiple variants of PVMark in Python, Rust and Circom,\ncovering combinations of three watermarking schemes, three hash functions, and\nfour ZKP protocols, to show our approach effectively works under a variety of\ncircumstances. By experimental results, PVMark efficiently enables public\nverifiability on the state-of-the-art LLM watermarking schemes yet without\ncompromising the watermarking performance, promising to be deployed in\npractice.\n","authors":["Haohua Duan","Liyao Xiang","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26274v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2405.09086v2","updated":"2025-10-30T08:49:01Z","published":"2024-05-15T04:47:31Z","title":"Chaos-based reinforcement learning with TD3","summary":"  Chaos-based reinforcement learning (CBRL) is a method in which the agent's\ninternal chaotic dynamics drives exploration. However, the learning algorithms\nin CBRL have not been thoroughly developed in previous studies, nor have they\nincorporated recent advances in reinforcement learning. This study introduced\nTwin Delayed Deep Deterministic Policy Gradients (TD3), which is one of the\nstate-of-the-art deep reinforcement learning algorithms that can treat\ndeterministic and continuous action spaces, to CBRL. The validation results\nprovide several insights. First, TD3 works as a learning algorithm for CBRL in\na simple goal-reaching task. Second, CBRL agents with TD3 can autonomously\nsuppress their exploratory behavior as learning progresses and resume\nexploration when the environment changes. Finally, examining the effect of the\nagent's chaoticity on learning shows that there exists a suitable range of\nchaos strength in the agent's model to flexibly switch between exploration and\nexploitation and adapt to environmental changes.\n","authors":["Toshitaka Matsuki","Yusuke Sakemi","Kazuyuki Aihara"],"pdf_url":"https://arxiv.org/pdf/2405.09086v2.pdf","comment":"Accepted for publication in Neural Networks"},{"id":"http://arxiv.org/abs/2510.26266v1","updated":"2025-10-30T08:46:53Z","published":"2025-10-30T08:46:53Z","title":"Likely Interpolants of Generative Models","summary":"  Interpolation in generative models allows for controlled generation, model\ninspection, and more. Unfortunately, most generative models lack a principal\nnotion of interpolants without restrictive assumptions on either the model or\ndata dimension. In this paper, we develop a general interpolation scheme that\ntargets likely transition paths compatible with different metrics and\nprobability distributions. We consider interpolants analogous to a geodesic\nconstrained to a suitable data distribution and derive a novel algorithm for\ncomputing these curves, which requires no additional training. Theoretically,\nwe show that our method locally can be considered as a geodesic under a\nsuitable Riemannian metric. We quantitatively show that our interpolation\nscheme traverses higher density regions than baselines across a range of models\nand datasets.\n","authors":["Frederik Möbius Rygaard","Shen Zhu","Yinzhu Jin","Søren Hauberg","Tom Fletcher"],"pdf_url":"https://arxiv.org/pdf/2510.26266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09846v3","updated":"2025-10-30T08:39:14Z","published":"2025-07-14T00:54:48Z","title":"Through the River: Understanding the Benefit of Schedule-Free Methods\n  for Language Model Training","summary":"  As both model and dataset sizes continue to scale rapidly, conventional\npretraining strategies with fixed compute budgets-such as cosine learning rate\nschedules-are increasingly inadequate for large-scale training. Recent\nalternatives, including warmup-stable-decay (WSD) schedules and weight\naveraging, offer greater flexibility. However, WSD relies on explicit decay\nphases to track progress, while weight averaging addresses this limitation at\nthe cost of additional memory. In search of a more principled and scalable\nalternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],\nwhich has shown strong empirical performance across diverse settings. We show\nthat SF-AdamW effectively navigates the \"river\" structure of the loss landscape\nwithout decay phases or auxiliary averaging, making it particularly suitable\nfor continuously scaling training workloads. To understand this behavior, we\nconduct a theoretical and empirical analysis of SF dynamics, revealing that it\nimplicitly performs weight averaging without memory overhead. Guided by this\nanalysis, we propose a refined variant of SF that improves robustness to\nmomentum and performs better under large batch sizes, addressing key\nlimitations of the original method. Together, these results establish SF as a\npractical, scalable, and theoretically grounded approach for language model\ntraining.\n","authors":["Minhak Song","Beomhan Baek","Kwangjun Ahn","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2507.09846v3.pdf","comment":"Published at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2509.24257v2","updated":"2025-10-30T08:38:57Z","published":"2025-09-29T04:07:32Z","title":"VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized\n  Inference","summary":"  Decentralized inference provides a scalable and resilient paradigm for\nserving large language models (LLMs), enabling distributed resource utilization\nand reducing reliance on centralized providers. However, in a permissionless\nenvironment without trusted nodes, ensuring the correctness of model outputs\nremains a core challenge. We introduce VeriLLM, a publicly verifiable protocol\nfor decentralized LLM inference that achieves security under a\none-honest-verifier assumption while maintaining practical efficiency. VeriLLM\ncombines lightweight empirical rerunning with cryptographic commitments,\nallowing verifiers to validate results at approximately 1% of the underlying\ninference cost. To prevent verification bottlenecks, we design an isomorphic\ninference-verification architecture that multiplexes both inference and\nverification roles across the same GPU workers. This design (i) improves GPU\nutilization and overall throughput, (ii) enlarges the effective validator set,\nenhancing robustness and liveness, and (iii) enforces task indistinguishability\nto prevent node-specific optimizations or selective behavior. Through\ntheoretical analysis and system-level evaluation, we show that VeriLLM achieves\nreliable public verifiability with minimal overhead, offering a practical\nfoundation for trustworthy and scalable decentralized LLM inference.\n","authors":["Ke Wang","Zishuo Zhao","Xinyuan Song","Bill Shi","Libin Xia","Chris Tong","Lynn Ai","Felix Qu","Eric Yang"],"pdf_url":"https://arxiv.org/pdf/2509.24257v2.pdf","comment":"20 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2510.26243v1","updated":"2025-10-30T08:23:35Z","published":"2025-10-30T08:23:35Z","title":"Angular Steering: Behavior Control via Rotation in Activation Space","summary":"  Controlling specific behaviors in large language models while preserving\ntheir general capabilities is a central challenge for safe and reliable\nartificial intelligence deployment. Current steering methods, such as vector\naddition and directional ablation, are constrained within a two-dimensional\nsubspace defined by the activation and feature direction, making them sensitive\nto chosen parameters and potentially affecting unrelated features due to\nunintended interactions in activation space. We introduce Angular Steering, a\nnovel and flexible method for behavior modulation that operates by rotating\nactivations within a fixed two-dimensional subspace. By formulating steering as\na geometric rotation toward or away from a target behavior direction, Angular\nSteering provides continuous, fine-grained control over behaviors such as\nrefusal and compliance. We demonstrate this method using refusal steering\nemotion steering as use cases. Additionally, we propose Adaptive Angular\nSteering, a selective variant that rotates only activations aligned with the\ntarget feature, further enhancing stability and coherence. Angular Steering\ngeneralizes existing addition and orthogonalization techniques under a unified\ngeometric rotation framework, simplifying parameter selection and maintaining\nmodel stability across a broader range of adjustments. Experiments across\nmultiple model families and sizes show that Angular Steering achieves robust\nbehavioral control while maintaining general language modeling performance,\nunderscoring its flexibility, generalization, and robustness compared to prior\napproaches. Code and artifacts are available at\nhttps://github.com/lone17/angular-steering/.\n","authors":["Hieu M. Vu","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2510.26243v1.pdf","comment":"NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.25366v2","updated":"2025-10-30T08:16:40Z","published":"2025-10-29T10:37:24Z","title":"A Convexity-dependent Two-Phase Training Algorithm for Deep Neural\n  Networks","summary":"  The key task of machine learning is to minimize the loss function that\nmeasures the model fit to the training data. The numerical methods to do this\nefficiently depend on the properties of the loss function. The most decisive\namong these properties is the convexity or non-convexity of the loss function.\nThe fact that the loss function can have, and frequently has, non-convex\nregions has led to a widespread commitment to non-convex methods such as Adam.\nHowever, a local minimum implies that, in some environment around it, the\nfunction is convex. In this environment, second-order minimizing methods such\nas the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We\npropose a novel framework grounded in the hypothesis that loss functions in\nreal-world tasks swap from initial non-convexity to convexity towards the\noptimum. This is a property we leverage to design an innovative two-phase\noptimization algorithm. The presented algorithm detects the swap point by\nobserving the gradient norm dependence on the loss. In these regions,\nnon-convex (Adam) and convex (CG) algorithms are used, respectively. Computing\nexperiments confirm the hypothesis that this simple convexity structure is\nfrequent enough to be practically exploited to substantially improve\nconvergence and accuracy.\n","authors":["Tomas Hrycej","Bernhard Bermeitinger","Massimo Pavone","Götz-Henrik Wiegand","Siegfried Handschuh"],"pdf_url":"https://arxiv.org/pdf/2510.25366v2.pdf","comment":"Appeared on KDIR IC3K Conference 2025 (Best Paper Award). Published\n  in \"Proceedings of the 17th International Joint Conference on Knowledge\n  Discovery, Knowledge Engineering and Knowledge Management - Volume 1\""},{"id":"http://arxiv.org/abs/2510.26230v1","updated":"2025-10-30T08:09:37Z","published":"2025-10-30T08:09:37Z","title":"MPRU: Modular Projection-Redistribution Unlearning as Output Filter for\n  Classification Pipelines","summary":"  As a new and promising approach, existing machine unlearning (MU) works\ntypically emphasize theoretical formulations or optimization objectives to\nachieve knowledge removal. However, when deployed in real-world scenarios, such\nsolutions typically face scalability issues and have to address practical\nrequirements such as full access to original datasets and model. In contrast to\nthe existing approaches, we regard classification training as a sequential\nprocess where classes are learned sequentially, which we call \\emph{inductive\napproach}. Unlearning can then be done by reversing the last training sequence.\nThis is implemented by appending a projection-redistribution layer in the end\nof the model. Such an approach does not require full access to the original\ndataset or the model, addressing the challenges of existing methods. This\nenables modular and model-agnostic deployment as an output filter into existing\nclassification pipelines with minimal alterations. We conducted multiple\nexperiments across multiple datasets including image (CIFAR-10/100 using\nCNN-based model) and tabular datasets (Covertype using tree-based model).\nExperiment results show consistently similar output to a fully retrained model\nwith a high computational cost reduction. This demonstrates the applicability,\nscalability, and system compatibility of our solution while maintaining the\nperformance of the output in a more practical setting.\n","authors":["Minyi Peng","Darian Gunamardi","Ivan Tjuawinata","Kwok-Yan Lam"],"pdf_url":"https://arxiv.org/pdf/2510.26230v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.02935v3","updated":"2025-10-30T08:05:05Z","published":"2025-06-03T14:35:36Z","title":"MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable\n  Neural Vehicle Routing Solver","summary":"  Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a\npromising approach to train a unified model capable of solving multiple Vehicle\nRouting Problem (VRP) variants. However, existing Reinforcement Learning\n(RL)-based multi-task methods can only train light decoder models on\nsmall-scale problems, exhibiting limited generalization ability when solving\nlarge-scale problems. To overcome this limitation, this work introduces a novel\nmulti-task learning method driven by knowledge distillation (MTL-KD), which\nenables the efficient training of heavy decoder models with strong\ngeneralization ability. The proposed MTL-KD method transfers policy knowledge\nfrom multiple distinct RL-based single-task models to a single heavy decoder\nmodel, facilitating label-free training and effectively improving the model's\ngeneralization ability across diverse tasks. In addition, we introduce a\nflexible inference strategy termed Random Reordering Re-Construction (R3C),\nwhich is specifically adapted for diverse VRP tasks and further boosts the\nperformance of the multi-task model. Experimental results on 6 seen and 10\nunseen VRP variants with up to 1000 nodes indicate that our proposed method\nconsistently achieves superior performance on both uniform and real-world\nbenchmarks, demonstrating robust generalization abilities.\n","authors":["Yuepeng Zheng","Fu Luo","Zhenkun Wang","Yaoxin Wu","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.02935v3.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2410.03364v4","updated":"2025-10-30T08:00:06Z","published":"2024-10-04T12:30:42Z","title":"Unified Error Correction Code Transformer with Low Complexity","summary":"  Channel coding is vital for reliable sixth-generation (6G) data transmission,\nemploying diverse error correction codes for various application scenarios.\nTraditional decoders require dedicated hardware for each code, leading to high\nhardware costs. Recently, artificial intelligence (AI)-driven approaches, such\nas the error correction code Transformer (ECCT) and its enhanced version, the\nfoundation error correction code Transformer (FECCT), have been proposed to\nreduce the hardware cost by leveraging the Transformer to decode multiple\ncodes. However, their excessively high computational complexity of\n$\\mathcal{O}(N^2)$ due to the self-attention mechanism in the Transformer\nlimits scalability, where $N$ represents the sequence length. To reduce\ncomputational complexity, we propose a unified Transformer-based decoder that\nhandles multiple linear block codes within a single framework. Specifically, a\nstandardized unit is employed to align code length and code rate across\ndifferent code types, while a redesigned low-rank unified attention module,\nwith computational complexity of $\\mathcal{O}(N)$, is shared across various\nheads in the Transformer. Additionally, a sparse mask, derived from the\nparity-check matrix's sparsity, is introduced to enhance the decoder's ability\nto capture inherent constraints between information and parity-check bits,\nimproving decoding accuracy and further reducing computational complexity by\n$86\\%$. Extensive experimental results demonstrate that the proposed unified\nTransformer-based decoder outperforms existing methods and provides a\nhigh-performance, low-complexity solution for next-generation wireless\ncommunication systems.\n","authors":["Yongli Yan","Jieao Zhu","Tianyue Zheng","Zhuo Xu","Chao Jiang","Linglong Dai"],"pdf_url":"https://arxiv.org/pdf/2410.03364v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26219v1","updated":"2025-10-30T07:52:14Z","published":"2025-10-30T07:52:14Z","title":"Test-Time Alignment of LLMs via Sampling-Based Optimal Control in\n  pre-logit space","summary":"  Test-time alignment of large language models (LLMs) attracts attention\nbecause fine-tuning LLMs requires high computational costs. In this paper, we\npropose a new test-time alignment method called adaptive importance sampling on\npre-logits (AISP) on the basis of the sampling-based model predictive control\nwith the stochastic control input. AISP applies the Gaussian perturbation into\npre-logits, which are outputs of the penultimate layer, so as to maximize\nexpected rewards with respect to the mean of the perturbation. We demonstrate\nthat the optimal mean is obtained by importance sampling with sampled rewards.\nAISP outperforms best-of-n sampling in terms of rewards over the number of used\nsamples and achieves higher rewards than other reward-based test-time alignment\nmethods.\n","authors":["Sekitoshi Kanai","Tsukasa Yoshida","Hiroshi Takahashi","Haru Kuroki","Kazumune Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2510.26219v1.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.17789v2","updated":"2025-10-30T07:39:10Z","published":"2025-05-23T12:02:13Z","title":"Optimal Online Change Detection via Random Fourier Features","summary":"  This article studies the problem of online non-parametric change point\ndetection in multivariate data streams. We approach the problem through the\nlens of kernel-based two-sample testing and introduce a sequential testing\nprocedure based on random Fourier features, running with logarithmic time\ncomplexity per observation and with overall logarithmic space complexity. The\nalgorithm has two advantages compared to the state of the art. First, our\napproach is genuinely online, and no access to training data known to be from\nthe pre-change distribution is necessary. Second, the algorithm does not\nrequire the user to specify a window parameter over which local tests are to be\ncalculated. We prove strong theoretical guarantees on the algorithm's\nperformance, including information-theoretic bounds demonstrating that the\ndetection delay is optimal in the minimax sense. Numerical studies on real and\nsynthetic data show that our algorithm is competitive with respect to the state\nof the art.\n","authors":["Florian Kalinke","Shakeel Gavioli-Akilagun"],"pdf_url":"https://arxiv.org/pdf/2505.17789v2.pdf","comment":"Accepted for publication at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.24627v3","updated":"2025-10-30T07:38:02Z","published":"2025-05-30T14:21:33Z","title":"Rethinking Neural Combinatorial Optimization for Vehicle Routing\n  Problems with Different Constraint Tightness Degrees","summary":"  Recent neural combinatorial optimization (NCO) methods have shown promising\nproblem-solving ability without requiring domain-specific expertise. Most\nexisting NCO methods use training and testing data with a fixed constraint\nvalue and lack research on the effect of constraint tightness on the\nperformance of NCO methods. This paper takes the capacity-constrained vehicle\nrouting problem (CVRP) as an example to empirically analyze the NCO performance\nunder different tightness degrees of the capacity constraint. Our analysis\nreveals that existing NCO methods overfit the capacity constraint, and they can\nonly perform satisfactorily on a small range of the constraint values but\npoorly on other values. To tackle this drawback of existing NCO methods, we\ndevelop an efficient training scheme that explicitly considers varying degrees\nof constraint tightness and proposes a multi-expert module to learn a generally\nadaptable solving strategy. Experimental results show that the proposed method\ncan effectively overcome the overfitting issue, demonstrating superior\nperformances on the CVRP and CVRP with time windows (CVRPTW) with various\nconstraint tightness degrees.\n","authors":["Fu Luo","Yaoxin Wu","Zhi Zheng","Zhenkun Wang"],"pdf_url":"https://arxiv.org/pdf/2505.24627v3.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.10173v2","updated":"2025-10-30T07:25:20Z","published":"2025-06-11T20:53:45Z","title":"SPARKE: Scalable Prompt-Aware Diversity and Novelty Guidance in\n  Diffusion Models via RKE Score","summary":"  Diffusion models have demonstrated remarkable success in high-fidelity image\nsynthesis and prompt-guided generative modeling. However, ensuring adequate\ndiversity in generated samples of prompt-guided diffusion models remains a\nchallenge, particularly when the prompts span a broad semantic spectrum and the\ndiversity of generated data needs to be evaluated in a prompt-aware fashion\nacross semantically similar prompts. Recent methods have introduced guidance\nvia diversity measures to encourage more varied generations. In this work, we\nextend the diversity measure-based approaches by proposing the Scalable\nPrompt-Aware R\\'eny Kernel Entropy Diversity Guidance (SPARKE) method for\nprompt-aware diversity guidance. SPARKE utilizes conditional entropy for\ndiversity guidance, which dynamically conditions diversity measurement on\nsimilar prompts and enables prompt-aware diversity control. While the\nentropy-based guidance approach enhances prompt-aware diversity, its reliance\non the matrix-based entropy scores poses computational challenges in\nlarge-scale generation settings. To address this, we focus on the special case\nof Conditional latent RKE Score Guidance, reducing entropy computation and\ngradient-based optimization complexity from the $O(n^3)$ of general entropy\nmeasures to $O(n)$. The reduced computational complexity allows for\ndiversity-guided sampling over potentially thousands of generation rounds on\ndifferent prompts. We numerically test the SPARKE method on several\ntext-to-image diffusion models, demonstrating that the proposed method improves\nthe prompt-aware diversity of the generated data without incurring significant\ncomputational costs. We release our code on the project page:\nhttps://mjalali.github.io/SPARKE\n","authors":["Mohammad Jalali","Haoyu Lei","Amin Gohari","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2506.10173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01158v2","updated":"2025-10-30T07:20:59Z","published":"2025-06-01T20:32:27Z","title":"Efficient Regression-Based Training of Normalizing Flows for Boltzmann\n  Generators","summary":"  Simulation-free training frameworks have been at the forefront of the\ngenerative modelling revolution in continuous spaces, leading to large-scale\ndiffusion and flow matching models. However, such modern generative models\nsuffer from expensive inference, inhibiting their use in numerous scientific\napplications like Boltzmann Generators (BGs) for molecular conformations that\nrequire fast likelihood evaluation. In this paper, we revisit classical\nnormalizing flows in the context of BGs that offer efficient sampling and\nlikelihoods, but whose training via maximum likelihood is often unstable and\ncomputationally challenging. We propose Regression Training of Normalizing\nFlows (RegFlow), a novel and scalable regression-based training objective that\nbypasses the numerical instability and computational challenge of conventional\nmaximum likelihood training in favour of a simple $\\ell_2$-regression\nobjective. Specifically, RegFlow maps prior samples under our flow to targets\ncomputed using optimal transport couplings or a pre-trained continuous\nnormalizing flow (CNF). To enhance numerical stability, RegFlow employs\neffective regularization strategies such as a new forward-backward\nself-consistency loss that enjoys painless implementation. Empirically, we\ndemonstrate that RegFlow unlocks a broader class of architectures that were\npreviously intractable to train for BGs with maximum likelihood. We also show\nRegFlow exceeds the performance, computational cost, and stability of maximum\nlikelihood training in equilibrium sampling in Cartesian coordinates of alanine\ndipeptide, tripeptide, and tetrapeptide, showcasing its potential in molecular\nsystems.\n","authors":["Danyal Rehman","Oscar Davis","Jiarui Lu","Jian Tang","Michael Bronstein","Yoshua Bengio","Alexander Tong","Avishek Joey Bose"],"pdf_url":"https://arxiv.org/pdf/2506.01158v2.pdf","comment":"Preprint; ICML GenBio Best Paper Award 2025"},{"id":"http://arxiv.org/abs/2505.13904v3","updated":"2025-10-30T07:17:31Z","published":"2025-05-20T04:10:50Z","title":"Learning to Insert for Constructive Neural Vehicle Routing Solver","summary":"  Neural Combinatorial Optimisation (NCO) is a promising learning-based\napproach for solving Vehicle Routing Problems (VRPs) without extensive manual\ndesign. While existing constructive NCO methods typically follow an\nappending-based paradigm that sequentially adds unvisited nodes to partial\nsolutions, this rigid approach often leads to suboptimal results. To overcome\nthis limitation, we explore the idea of insertion-based paradigm and propose\nLearning to Construct with Insertion-based Paradigm (L2C-Insert), a novel\nlearning-based method for constructive NCO. Unlike traditional approaches,\nL2C-Insert builds solutions by strategically inserting unvisited nodes at any\nvalid position in the current partial solution, which can significantly enhance\nthe flexibility and solution quality. The proposed framework introduces three\nkey components: a novel model architecture for precise insertion position\nprediction, an efficient training scheme for model optimization, and an\nadvanced inference technique that fully exploits the insertion paradigm's\nflexibility. Extensive experiments on both synthetic and real-world instances\nof the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) demonstrate that L2C-Insert consistently achieves superior\nperformance across various problem sizes.\n","authors":["Fu Luo","Xi Lin","Mengyuan Zhong","Fei Liu","Zhenkun Wang","Jianyong Sun","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.13904v3.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.14970v4","updated":"2025-10-30T07:03:09Z","published":"2025-05-20T23:17:15Z","title":"Self-Evolving Curriculum for LLM Reasoning","summary":"  Reinforcement learning (RL) has proven effective for fine-tuning large\nlanguage models (LLMs), significantly enhancing their reasoning abilities in\ndomains such as mathematics and code generation. A crucial factor influencing\nRL fine-tuning success is the training curriculum: the order in which training\nproblems are presented. While random curricula serve as common baselines, they\nremain suboptimal; manually designed curricula often rely heavily on\nheuristics, and online filtering methods can be computationally prohibitive. To\naddress these limitations, we propose Self-Evolving Curriculum (SEC), an\nautomatic curriculum learning method that learns a curriculum policy\nconcurrently with the RL fine-tuning process. Our approach formulates\ncurriculum selection as a non-stationary Multi-Armed Bandit problem, treating\neach problem category (e.g., difficulty level or problem type) as an individual\narm. We leverage the absolute advantage from policy gradient methods as a proxy\nmeasure for immediate learning gain. At each training step, the curriculum\npolicy selects categories to maximize this reward signal and is updated using\nthe TD(0) method. Across three distinct reasoning domains: planning, inductive\nreasoning, and mathematics, our experiments demonstrate that SEC significantly\nimproves models' reasoning capabilities, enabling better generalization to\nharder, out-of-distribution test problems. Additionally, our approach achieves\nbetter skill balance when fine-tuning simultaneously on multiple reasoning\ndomains. These findings highlight SEC as a promising strategy for RL\nfine-tuning of LLMs.\n","authors":["Xiaoyin Chen","Jiarui Lu","Minsu Kim","Dinghuai Zhang","Jian Tang","Alexandre Piché","Nicolas Gontier","Yoshua Bengio","Ehsan Kamalloo"],"pdf_url":"https://arxiv.org/pdf/2505.14970v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02392v2","updated":"2025-10-30T07:02:16Z","published":"2025-06-03T03:15:22Z","title":"Improving Generalization of Neural Combinatorial Optimization for\n  Vehicle Routing Problems via Test-Time Projection Learning","summary":"  Neural Combinatorial Optimization (NCO) has emerged as a promising\nlearning-based paradigm for addressing Vehicle Routing Problems (VRPs) by\nminimizing the need for extensive manual engineering. While existing NCO\nmethods, trained on small-scale instances (e.g., 100 nodes), have demonstrated\nconsiderable success on problems of similar scale, their performance\nsignificantly degrades when applied to large-scale scenarios. This degradation\narises from the distributional shift between training and testing data,\nrendering policies learned on small instances ineffective for larger problems.\nTo overcome this limitation, we introduce a novel learning framework driven by\nLarge Language Models (LLMs). This framework learns a projection between the\ntraining and testing distributions, which is then deployed to enhance the\nscalability of the NCO model. Notably, unlike prevailing techniques that\nnecessitate joint training with the neural network, our approach operates\nexclusively during the inference phase, obviating the need for model\nretraining. Extensive experiments demonstrate that our method enables a\nbackbone model (trained on 100-node instances) to achieve superior performance\non large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) of up to 100K nodes from diverse distributions.\n","authors":["Yuanyao Chen","Rongsheng Chen","Fu Luo","Zhenkun Wang"],"pdf_url":"https://arxiv.org/pdf/2506.02392v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2505.24627"},{"id":"http://arxiv.org/abs/2510.10728v2","updated":"2025-10-30T06:58:12Z","published":"2025-10-12T18:02:12Z","title":"Rough Path Signatures: Learning Neural RDEs for Portfolio Optimization","summary":"  We tackle high-dimensional, path-dependent valuation and control and\nintroduce a deep BSDE/2BSDE solver that couples truncated log-signatures with a\nneural rough differential equation (RDE) backbone. The architecture aligns\nstochastic analysis with sequence-to-path learning: a CVaR-tilted terminal\nobjective targets left-tail risk, while an optional second-order (2BSDE) head\nsupplies curvature estimates for risk-sensitive control. Under matched compute\nand parameter budgets, the method improves accuracy, tail fidelity, and\ntraining stability across Asian and barrier option pricing and portfolio\ncontrol: at d=200 it achieves CVaR(0.99)=9.80% versus 12.00-13.10% for strong\nbaselines, attains the lowest HJB residual (0.011), and yields the lowest RMSEs\nfor Z and Gamma. Ablations over truncation depth, local windows, and tilt\nparameters confirm complementary gains from the sequence-to-path representation\nand the 2BSDE head. Taken together, the results highlight a bidirectional\ndialogue between stochastic analysis and modern deep learning: stochastic tools\ninform representations and objectives, while sequence-to-path models expand the\nclass of solvable financial models at scale.\n","authors":["Ali Atiah Alzahrani"],"pdf_url":"https://arxiv.org/pdf/2510.10728v2.pdf","comment":"Code available at: https://github.com/AliAtiah/SigRDE"},{"id":"http://arxiv.org/abs/2509.16648v2","updated":"2025-10-30T06:55:22Z","published":"2025-09-20T11:50:22Z","title":"FESTA: Functionally Equivalent Sampling for Trust Assessment of\n  Multimodal LLMs","summary":"  The accurate trust assessment of multimodal large language models (MLLMs)\ngenerated predictions, which can enable selective prediction and improve user\nconfidence, is challenging due to the diverse multi-modal input paradigms. We\npropose Functionally Equivalent Sampling for Trust Assessment (FESTA), a\nmultimodal input sampling technique for MLLMs, that generates an uncertainty\nmeasure based on the equivalent and complementary input samplings. The proposed\ntask-preserving sampling approach for uncertainty quantification expands the\ninput space to probe the consistency (through equivalent samples) and\nsensitivity (through complementary samples) of the model. FESTA uses only\ninput-output access of the model (black-box), and does not require ground truth\n(unsupervised). The experiments are conducted with various off-the-shelf\nmulti-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA\nuncertainty estimate achieves significant improvement (33.3% relative\nimprovement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in\nselective prediction performance, based on\narea-under-receiver-operating-characteristic curve (AUROC) metric in detecting\nmispredictions. The code implementation is open-sourced.\n","authors":["Debarpan Bhattacharya","Apoorva Kulkarni","Sriram Ganapathy"],"pdf_url":"https://arxiv.org/pdf/2509.16648v2.pdf","comment":"Accepted in the Findings of EMNLP, 2025"},{"id":"http://arxiv.org/abs/2510.26188v1","updated":"2025-10-30T06:54:19Z","published":"2025-10-30T06:54:19Z","title":"Predicting All-Cause Hospital Readmissions from Medical Claims Data of\n  Hospitalised Patients","summary":"  Reducing preventable hospital readmissions is a national priority for payers,\nproviders, and policymakers seeking to improve health care and lower costs. The\nrate of readmission is being used as a benchmark to determine the quality of\nhealthcare provided by the hospitals. In thisproject, we have used machine\nlearning techniques like Logistic Regression, Random Forest and Support Vector\nMachines to analyze the health claims data and identify demographic and medical\nfactors that play a crucial role in predicting all-cause readmissions. As the\nhealth claims data is high dimensional, we have used Principal Component\nAnalysis as a dimension reduction technique and used the results for building\nregression models. We compared and evaluated these models based on the Area\nUnder Curve (AUC) metric. Random Forest model gave the highest performance\nfollowed by Logistic Regression and Support Vector Machine models. These models\ncan be used to identify the crucial factors causing readmissions and help\nidentify patients to focus on to reduce the chances of readmission, ultimately\nbringing down the cost and increasing the quality of healthcare provided to the\npatients.\n","authors":["Avinash Kadimisetty","Arun Rajagopalan","Vijendra SK"],"pdf_url":"https://arxiv.org/pdf/2510.26188v1.pdf","comment":"NCMLAI 2018"},{"id":"http://arxiv.org/abs/2510.26185v1","updated":"2025-10-30T06:45:22Z","published":"2025-10-30T06:45:22Z","title":"Accumulative SGD Influence Estimation for Data Attribution","summary":"  Modern data-centric AI needs precise per-sample influence. Standard SGD-IE\napproximates leave-one-out effects by summing per-epoch surrogates and ignores\ncross-epoch compounding, which misranks critical examples. We propose\nACC-SGD-IE, a trajectory-aware estimator that propagates the leave-one-out\nperturbation across training and updates an accumulative influence state at\neach step. In smooth strongly convex settings it achieves geometric error\ncontraction and, in smooth non-convex regimes, it tightens error bounds; larger\nmini-batches further reduce constants. Empirically, on Adult, 20 Newsgroups,\nand MNIST under clean and corrupted data and both convex and non-convex\ntraining, ACC-SGD-IE yields more accurate influence estimates, especially over\nlong epochs. For downstream data cleansing it more reliably flags noisy\nsamples, producing models trained on ACC-SGD-IE cleaned data that outperform\nthose cleaned with SGD-IE.\n","authors":["Yunxiao Shi","Shuo Yang","Yixin Su","Rui Zhang","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2510.26185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26184v1","updated":"2025-10-30T06:43:47Z","published":"2025-10-30T06:43:47Z","title":"A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for\n  Collaborative Public Resource Allocation","summary":"  Public resource allocation involves the efficient distribution of resources,\nincluding urban infrastructure, energy, and transportation, to effectively meet\nsocietal demands. However, existing methods focus on optimizing the movement of\nindividual resources independently, without considering their capacity\nconstraints. To address this limitation, we propose a novel and more practical\nproblem: Collaborative Public Resource Allocation (CPRA), which explicitly\nincorporates capacity constraints and spatio-temporal dynamics in real-world\nscenarios. We propose a new framework called Game-Theoretic Spatio-Temporal\nReinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold:\n1) We formulate the CPRA problem as a potential game and demonstrate that there\nis no gap between the potential function and the optimal target, laying a solid\ntheoretical foundation for approximating the Nash equilibrium of this NP-hard\nproblem; and 2) Our designed GSTRL framework effectively captures the\nspatio-temporal dynamics of the overall system. We evaluate GSTRL on two\nreal-world datasets, where experiments show its superior performance. Our\nsource codes are available in the supplementary materials.\n","authors":["Songxin Lei","Qiongyan Wang","Yanchen Zhu","Hanyu Yao","Sijie Ruan","Weilin Ruan","Yuyu Luo","Huaming Wu","Yuxuan Liang"],"pdf_url":"https://arxiv.org/pdf/2510.26184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17918v5","updated":"2025-10-30T06:29:37Z","published":"2025-09-22T15:43:11Z","title":"Shilling Recommender Systems by Generating Side-feature-aware Fake User\n  Profiles","summary":"  Recommender systems (RS) greatly influence users' consumption decisions,\nmaking them attractive targets for malicious shilling attacks that inject fake\nuser profiles to manipulate recommendations. Existing shilling methods can\ngenerate effective and stealthy fake profiles when training data only contain\nrating matrix, but they lack comprehensive solutions for scenarios where side\nfeatures are present and utilized by the recommender. To address this gap, we\nextend the Leg-UP framework by enhancing the generator architecture to\nincorporate side features, enabling the generation of side-feature-aware fake\nuser profiles. Experiments on benchmarks show that our method achieves strong\nattack performance while maintaining stealthiness.\n","authors":["Yuanrong Wang","Yingpeng Du"],"pdf_url":"https://arxiv.org/pdf/2509.17918v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13308v2","updated":"2025-10-30T06:23:27Z","published":"2025-05-19T16:26:02Z","title":"Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient\n  in Latent Space","summary":"  Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.\n","authors":["Hengli Li","Chenxi Li","Tong Wu","Xuekai Zhu","Yuxuan Wang","Zhaoxin Yu","Eric Hanchen Jiang","Song-Chun Zhu","Zixia Jia","Ying Nian Wu","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.13308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09922v2","updated":"2025-10-30T06:18:35Z","published":"2025-05-15T03:12:27Z","title":"Improving the Euclidean Diffusion Generation of Manifold Data by\n  Mitigating Score Function Singularity","summary":"  Euclidean diffusion models have achieved remarkable success in generative\nmodeling across diverse domains, and they have been extended to manifold cases\nin recent advances. Instead of explicitly utilizing the structure of special\nmanifolds as studied in previous works, in this paper we investigate direct\nsampling of the Euclidean diffusion models for general manifold-structured\ndata. We reveal the multiscale singularity of the score function in the ambient\nspace, which hinders the accuracy of diffusion-generated samples. We then\npresent an elaborate theoretical analysis of the singularity structure of the\nscore function by decomposing it along the tangential and normal directions of\nthe manifold. To mitigate the singularity and improve the sampling accuracy, we\npropose two novel methods: (1) Niso-DM, which reduces the scale discrepancies\nin the score function by utilizing a non-isotropic noise, and (2) Tango-DM,\nwhich trains only the tangential component of the score function using a\ntangential-only loss function. Numerical experiments demonstrate that our\nmethods achieve superior performance on distributions over various manifolds\nwith complex geometries.\n","authors":["Zichen Liu","Wei Zhang","Tiejun Li"],"pdf_url":"https://arxiv.org/pdf/2505.09922v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24829v2","updated":"2025-10-30T06:18:11Z","published":"2025-10-28T16:18:14Z","title":"Send Less, Save More: Energy-Efficiency Benchmark of Embedded CNN\n  Inference vs. Data Transmission in IoT","summary":"  The integration of the Internet of Things (IoT) and Artificial Intelligence\noffers significant opportunities to enhance our ability to monitor and address\necological changes. As environmental challenges become increasingly pressing,\nthe need for effective remote monitoring solutions is more critical than ever.\nA major challenge in designing IoT applications for environmental monitoring -\nparticularly those involving image data - is to create energy-efficient IoT\ndevices capable of long-term operation in remote areas with limited power\navailability. Advancements in the field of Tiny Machine Learning allow the use\nof Convolutional Neural Networks (CNNs) on resource-constrained,\nbattery-operated microcontrollers. Since data transfer is energy-intensive,\nperforming inference directly on microcontrollers to reduce the message size\ncan extend the operational lifespan of IoT nodes. This work evaluates the use\nof common Low Power Wide Area Networks and compressed CNNs trained on domain\nspecific datasets on an ESP32-S3. Our experiments demonstrate, among other\nthings, that executing CNN inference on-device and transmitting only the\nresults reduces the overall energy consumption by a factor of up to five\ncompared to sending raw image data. These findings advocate the development of\nIoT applications with reduced carbon footprint and capable of operating\nautonomously in environmental monitoring scenarios by incorporating EmbeddedML.\n","authors":["Benjamin Karic","Nina Herrmann","Jan Stenkamp","Paula Scharf","Fabian Gieseke","Angela Schwering"],"pdf_url":"https://arxiv.org/pdf/2510.24829v2.pdf","comment":"11 Pages, Paper lists the categories for the ACM Computing\n  Classification System"},{"id":"http://arxiv.org/abs/2506.05768v2","updated":"2025-10-30T06:11:45Z","published":"2025-06-06T05:52:19Z","title":"AANet: Virtual Screening under Structural Uncertainty via Alignment and\n  Aggregation","summary":"  Virtual screening (VS) is a critical component of modern drug discovery, yet\nmost existing methods--whether physics-based or deep learning-based--are\ndeveloped around holo protein structures with known ligand-bound pockets.\nConsequently, their performance degrades significantly on apo or predicted\nstructures such as those from AlphaFold2, which are more representative of\nreal-world early-stage drug discovery, where pocket information is often\nmissing. In this paper, we introduce an alignment-and-aggregation framework to\nenable accurate virtual screening under structural uncertainty. Our method\ncomprises two core components: (1) a tri-modal contrastive learning module that\naligns representations of the ligand, the holo pocket, and cavities detected\nfrom structures, thereby enhancing robustness to pocket localization error; and\n(2) a cross-attention based adapter for dynamically aggregating candidate\nbinding sites, enabling the model to learn from activity data even without\nprecise pocket annotations. We evaluated our method on a newly curated\nbenchmark of apo structures, where it significantly outperforms\nstate-of-the-art methods in blind apo setting, improving the early enrichment\nfactor (EF1%) from 11.75 to 37.19. Notably, it also maintains strong\nperformance on holo structures. These results demonstrate the promise of our\napproach in advancing first-in-class drug discovery, particularly in scenarios\nlacking experimentally resolved protein-ligand complexes. Our implementation is\npublicly available at https://github.com/Wiley-Z/AANet.\n","authors":["Wenyu Zhu","Jianhui Wang","Bowen Gao","Yinjun Jia","Haichuan Tan","Ya-Qin Zhang","Wei-Ying Ma","Yanyan Lan"],"pdf_url":"https://arxiv.org/pdf/2506.05768v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2408.08493v3","updated":"2025-10-30T06:10:17Z","published":"2024-08-16T02:29:38Z","title":"Parallel Unlearning in Inherited Model Networks","summary":"  Unlearning is challenging in generic learning frameworks with the continuous\ngrowth and updates of models exhibiting complex inheritance relationships. This\npaper presents a novel unlearning framework that enables fully parallel\nunlearning among models exhibiting inheritance. We use a chronologically\nDirected Acyclic Graph (DAG) to capture various unlearning scenarios occurring\nin model inheritance networks. Central to our framework is the Fisher\nInheritance Unlearning (FIUn) method, designed to enable efficient parallel\nunlearning within the DAG. FIUn utilizes the Fisher Information Matrix (FIM) to\nassess the significance of model parameters for unlearning tasks and adjusts\nthem accordingly. To handle multiple unlearning requests simultaneously, we\npropose the Merging-FIM (MFIM) function, which consolidates FIMs from multiple\nupstream models into a unified matrix. This design supports all unlearning\nscenarios captured by the DAG, enabling one-shot removal of inherited knowledge\nwhile significantly reducing computational overhead. Experiments confirm the\neffectiveness of our unlearning framework. For single-class tasks, it achieves\ncomplete unlearning with 0% accuracy for unlearned labels while maintaining\n94.53% accuracy for retained labels. For multi-class tasks, the accuracy is\n1.07% for unlearned labels and 84.77% for retained labels. Our framework\naccelerates unlearning by 99% compared to alternative methods. Code is in\nhttps://github.com/MJLee00/Parallel-Unlearning-in-Inherited-Model-Networks.\n","authors":["Xiao Liu","Mingyuan Li","Guangsheng Yu","Lixiang Li","Haipeng Peng","Ren Ping Liu"],"pdf_url":"https://arxiv.org/pdf/2408.08493v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.12760v2","updated":"2025-10-30T06:08:58Z","published":"2025-09-16T07:19:38Z","title":"Similarity-Distance-Magnitude Activations","summary":"  We introduce the Similarity-Distance-Magnitude (SDM) activation function, a\nmore robust and interpretable formulation of the standard softmax activation\nfunction, adding Similarity (i.e., correctly predicted depth-matches into\ntraining) awareness and Distance-to-training-distribution awareness to the\nexisting output Magnitude (i.e., decision-boundary) awareness, and enabling\ninterpretability-by-exemplar via dense matching. We further introduce the SDM\nestimator, based on a data-driven partitioning of the class-wise empirical CDFs\nvia the SDM activation, to control the class- and prediction-conditional\naccuracy among selective classifications. When used as the final-layer\nactivation over pre-trained language models for selective classification, the\nSDM estimator is more robust to co-variate shifts and out-of-distribution\ninputs than existing calibration methods using softmax activations, while\nremaining informative over in-distribution data.\n","authors":["Allen Schmaltz"],"pdf_url":"https://arxiv.org/pdf/2509.12760v2.pdf","comment":"18 pages, 5 tables, 1 algorithm. arXiv admin note: substantial text\n  overlap with arXiv:2502.20167"},{"id":"http://arxiv.org/abs/2505.06203v2","updated":"2025-10-30T05:52:33Z","published":"2025-05-09T17:30:16Z","title":"A Robust and Non-Iterative Tensor Decomposition Method with Automatic\n  Thresholding","summary":"  Recent advances in IoT and biometric sensing technologies have led to the\ngeneration of massive and high-dimensional tensor data, yet achieving accurate\nand efficient low-rank approximation remains a major challenge. Existing tensor\ndecomposition methods typically require prior specification of the tensor rank\nand rely on iterative optimization, which often results in heavy computational\ncosts and dependence on the analyst's expertise. In this study, we propose a\nnovel low-rank approximation method for tensor data that requires neither prior\nrank specification nor iterative optimization. The proposed method performs\nstatistical singular value hard thresholding on the mode-wise unfolded matrices\nto automatically extract only statistically significant components, thereby\nachieving noise reduction while preserving the intrinsic tensor structure.\nTheoretically, the optimal threshold for each mode is derived based on the\nasymptotic properties of the Mar\\v{c}enko--Pastur distribution. Simulation\nexperiments demonstrate that the proposed method outperforms conventional\napproaches such as Higher-Order Singular Value Decomposition, Higher-Order\nOrthogonal Iteration, and Tucker-L2E in terms of both estimation accuracy and\ncomputational efficiency. These results indicate that our method provides an\neffective and theoretically grounded framework for automatic, non-iterative,\nand analyst-independent tensor decomposition.\n","authors":["Hiroki Hasegawa","Yukihiko Okada"],"pdf_url":"https://arxiv.org/pdf/2505.06203v2.pdf","comment":"25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2510.26159v1","updated":"2025-10-30T05:39:44Z","published":"2025-10-30T05:39:44Z","title":"Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches\n  for Anomaly Detection in Industrial Time Series","summary":"  In this study, we investigate the effectiveness of advanced feature\nengineering and hybrid model architectures for anomaly detection in a\nmultivariate industrial time series, focusing on a steam turbine system. We\nevaluate the impact of change point-derived statistical features,\nclustering-based substructure representations, and hybrid learning strategies\non detection performance. Despite their theoretical appeal, these complex\napproaches consistently underperformed compared to a simple Random Forest +\nXGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of\n0.976, F1-score of 0.41, and 100% early detection within the defined time\nwindow. Our findings highlight that, in scenarios with highly imbalanced and\ntemporally uncertain data, model simplicity combined with optimized\nsegmentation can outperform more sophisticated architectures, offering greater\nrobustness, interpretability, and operational utility.\n","authors":["Emilio Mastriani","Alessandro Costa","Federico Incardona","Kevin Munari","Sebastiano Spinello"],"pdf_url":"https://arxiv.org/pdf/2510.26159v1.pdf","comment":"This paper is currently under review for presentation at the IEEE\n  SAMI 2026 Conference"},{"id":"http://arxiv.org/abs/2510.26157v1","updated":"2025-10-30T05:36:31Z","published":"2025-10-30T05:36:31Z","title":"Bridging the Gap Between Molecule and Textual Descriptions via\n  Substructure-aware Alignment","summary":"  Molecule and text representation learning has gained increasing interest due\nto its potential for enhancing the understanding of chemical information.\nHowever, existing models often struggle to capture subtle differences between\nmolecules and their descriptions, as they lack the ability to learn\nfine-grained alignments between molecular substructures and chemical phrases.\nTo address this limitation, we introduce MolBridge, a novel molecule-text\nlearning framework based on substructure-aware alignments. Specifically, we\naugment the original molecule-description pairs with additional alignment\nsignals derived from molecular substructures and chemical phrases. To\neffectively learn from these enriched alignments, MolBridge employs\nsubstructure-aware contrastive learning, coupled with a self-refinement\nmechanism that filters out noisy alignment signals. Experimental results show\nthat MolBridge effectively captures fine-grained correspondences and\noutperforms state-of-the-art baselines on a wide range of molecular benchmarks,\nhighlighting the significance of substructure-aware alignment in molecule-text\nlearning.\n","authors":["Hyuntae Park","Yeachan Kim","SangKeun Lee"],"pdf_url":"https://arxiv.org/pdf/2510.26157v1.pdf","comment":"EMNLP 2025 (main)"},{"id":"http://arxiv.org/abs/2510.21271v2","updated":"2025-10-30T05:16:33Z","published":"2025-10-24T09:12:59Z","title":"Buffer layers for Test-Time Adaptation","summary":"  In recent advancements in Test Time Adaptation (TTA), most existing\nmethodologies focus on updating normalization layers to adapt to the test\ndomain. However, the reliance on normalization-based adaptation presents key\nchallenges. First, normalization layers such as Batch Normalization (BN) are\nhighly sensitive to small batch sizes, leading to unstable and inaccurate\nstatistics. Moreover, normalization-based adaptation is inherently constrained\nby the structure of the pre-trained model, as it relies on training-time\nstatistics that may not generalize well to unseen domains. These issues limit\nthe effectiveness of normalization-based TTA approaches, especially under\nsignificant domain shift. In this paper, we introduce a novel paradigm based on\nthe concept of a Buffer layer, which addresses the fundamental limitations of\nnormalization layer updates. Unlike existing methods that modify the core\nparameters of the model, our approach preserves the integrity of the\npre-trained backbone, inherently mitigating the risk of catastrophic forgetting\nduring online adaptation. Through comprehensive experimentation, we demonstrate\nthat our approach not only outperforms traditional methods in mitigating domain\nshift and enhancing model robustness, but also exhibits strong resilience to\nforgetting. Furthermore, our Buffer layer is modular and can be seamlessly\nintegrated into nearly all existing TTA frameworks, resulting in consistent\nperformance improvements across various architectures. These findings validate\nthe effectiveness and versatility of the proposed solution in real-world domain\nadaptation scenarios. The code is available at\nhttps://github.com/hyeongyu-kim/Buffer_TTA.\n","authors":["Hyeongyu Kim","Geonhui Han","Dosik Hwang"],"pdf_url":"https://arxiv.org/pdf/2510.21271v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26148v1","updated":"2025-10-30T05:08:25Z","published":"2025-10-30T05:08:25Z","title":"STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human\n  Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing\n  Environments","summary":"  Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI)\npresents a privacy-preserving, contactless sensing approach suitable for smart\nhomes, healthcare monitoring, and mobile IoT systems. However, existing methods\noften encounter computational inefficiency, high latency, and limited\nfeasibility within resource-constrained, embedded mobile edge environments.\nThis paper proposes STAR (Sensing Technology for Activity Recognition), an\nedge-AI-optimized framework that integrates a lightweight neural architecture,\nadaptive signal processing, and hardware-aware co-optimization to enable\nreal-time, energy-efficient HAR on low-power embedded devices. STAR\nincorporates a streamlined Gated Recurrent Unit (GRU)-based recurrent neural\nnetwork, reducing model parameters by 33% compared to conventional LSTM models\nwhile maintaining effective temporal modeling capability. A multi-stage\npre-processing pipeline combining median filtering, 8th-order Butterworth\nlow-pass filtering, and Empirical Mode Decomposition (EMD) is employed to\ndenoise CSI amplitude data and extract spatial-temporal features. For on-device\ndeployment, STAR is implemented on a Rockchip RV1126 processor equipped with an\nembedded Neural Processing Unit (NPU), interfaced with an ESP32-S3-based CSI\nacquisition module. Experimental results demonstrate a mean recognition\naccuracy of 93.52% across seven activity classes and 99.11% for human presence\ndetection, utilizing a compact 97.6k-parameter model. INT8 quantized inference\nachieves a processing speed of 33 MHz with just 8% CPU utilization, delivering\nsixfold speed improvements over CPU-based execution. With sub-second response\nlatency and low power consumption, the system ensures real-time,\nprivacy-preserving HAR, offering a practical, scalable solution for mobile and\npervasive computing environments.\n","authors":["Kexing Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26146v1","updated":"2025-10-30T04:59:28Z","published":"2025-10-30T04:59:28Z","title":"maxVSTAR: Maximally Adaptive Vision-Guided CSI Sensing with Closed-Loop\n  Edge Model Adaptation for Robust Human Activity Recognition","summary":"  WiFi Channel State Information (CSI)-based human activity recognition (HAR)\nprovides a privacy-preserving, device-free sensing solution for smart\nenvironments. However, its deployment on edge devices is severely constrained\nby domain shift, where recognition performance deteriorates under varying\nenvironmental and hardware conditions. This study presents maxVSTAR (maximally\nadaptive Vision-guided Sensing Technology for Activity Recognition), a\nclosed-loop, vision-guided model adaptation framework that autonomously\nmitigates domain shift for edge-deployed CSI sensing systems. The proposed\nsystem integrates a cross-modal teacher-student architecture, where a\nhigh-accuracy YOLO-based vision model serves as a dynamic supervisory signal,\ndelivering real-time activity labels for the CSI data stream. These labels\nenable autonomous, online fine-tuning of a lightweight CSI-based HAR model,\ntermed Sensing Technology for Activity Recognition (STAR), directly at the\nedge. This closed-loop retraining mechanism allows STAR to continuously adapt\nto environmental changes without manual intervention. Extensive experiments\ndemonstrate the effectiveness of maxVSTAR. When deployed on uncalibrated\nhardware, the baseline STAR model's recognition accuracy declined from 93.52%\nto 49.14%. Following a single vision-guided adaptation cycle, maxVSTAR restored\nthe accuracy to 81.51%. These results confirm the system's capacity for\ndynamic, self-supervised model adaptation in privacy-conscious IoT\nenvironments, establishing a scalable and practical paradigm for long-term\nautonomous HAR using CSI sensing at the network edge.\n","authors":["Kexing Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11847v2","updated":"2025-10-30T04:34:21Z","published":"2025-07-16T02:24:21Z","title":"Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update","summary":"  We study the generalized linear bandit (GLB) problem, a contextual\nmulti-armed bandit framework that extends the classical linear model by\nincorporating a non-linear link function, thereby modeling a broad class of\nreward distributions such as Bernoulli and Poisson. While GLBs are widely\napplicable to real-world scenarios, their non-linear nature introduces\nsignificant challenges in achieving both computational and statistical\nefficiency. Existing methods typically trade off between two objectives, either\nincurring high per-round costs for optimal regret guarantees or compromising\nstatistical efficiency to enable constant-time updates. In this paper, we\npropose a jointly efficient algorithm that attains a nearly optimal regret\nbound with $\\mathcal{O}(1)$ time and space complexities per round. The core of\nour method is a tight confidence set for the online mirror descent (OMD)\nestimator, which is derived through a novel analysis that leverages the notion\nof mix loss from online prediction. The analysis shows that our OMD estimator,\neven with its one-pass updates, achieves statistical efficiency comparable to\nmaximum likelihood estimation, thereby leading to a jointly efficient\noptimistic method.\n","authors":["Yu-Jie Zhang","Sheng-An Xu","Peng Zhao","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2507.11847v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26130v1","updated":"2025-10-30T04:30:23Z","published":"2025-10-30T04:30:23Z","title":"Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World\n  Class-Level Code Generation","summary":"  Large language models (LLMs) have advanced code generation at the function\nlevel, yet their ability to produce correct class-level implementations in\nauthentic software projects remains poorly understood. This work introduces a\nnovel benchmark derived from open-source repositories, comprising real-world\nclasses divided into seen and unseen partitions to evaluate generalization\nunder practical conditions. The evaluation examines multiple LLMs under varied\ninput specifications, retrieval-augmented configurations, and documentation\ncompleteness levels.\n  Results reveal a stark performance disparity: LLMs achieve 84% to 89%\ncorrectness on established synthetic benchmarks but only 25% to 34% on\nreal-world class tasks, with negligible differences between familiar and novel\ncodebases. Comprehensive docstrings yield modest gains of 1% to 3% in\nfunctional accuracy, though statistical significance is rare.\nRetrieval-augmented generation proves most effective with partial\ndocumentation, improving correctness by 4% to 7% by supplying concrete\nimplementation patterns absent from specifications. Error profiling identifies\nAttributeError, TypeError, and AssertionError as dominant failure modes (84% of\ncases), with synthetic tests overemphasizing assertion issues and real-world\nscenarios highlighting type and attribute mismatches. Retrieval augmentation\nreduces logical flaws but can introduce dependency conflicts.\n  The benchmark and analysis expose critical limitations in current LLM\ncapabilities for class-level engineering, offering actionable insights for\nenhancing context modelling, documentation strategies, and retrieval\nintegration in production code assistance tools.\n","authors":["Musfiqur Rahman","SayedHassan Khatoonabadi","Emad Shihab"],"pdf_url":"https://arxiv.org/pdf/2510.26130v1.pdf","comment":"Pre-print prepared for journal submission"},{"id":"http://arxiv.org/abs/2510.26121v1","updated":"2025-10-30T04:05:49Z","published":"2025-10-30T04:05:49Z","title":"Uncertainty-Aware Diagnostics for Physics-Informed Machine Learning","summary":"  Physics-informed machine learning (PIML) integrates prior physical\ninformation, often in the form of differential equation constraints, into the\nprocess of fitting machine learning models to physical data. Popular PIML\napproaches, including neural operators, physics-informed neural networks,\nneural ordinary differential equations, and neural discrete equilibria, are\ntypically fit to objectives that simultaneously include both data and physical\nconstraints. However, the multi-objective nature of this approach creates\nambiguity in the measurement of model quality. This is related to a poor\nunderstanding of epistemic uncertainty, and it can lead to surprising failure\nmodes, even when existing statistical metrics suggest strong fits. Working\nwithin a Gaussian process regression framework, we introduce the\nPhysics-Informed Log Evidence (PILE) score. Bypassing the ambiguities of test\nlosses, the PILE score is a single, uncertainty-aware metric that provides a\nselection principle for hyperparameters of a PIML model. We show that PILE\nminimization yields excellent choices for a wide variety of model parameters,\nincluding kernel bandwidth, least squares regularization weights, and even\nkernel function selection. We also show that, even prior to data acquisition, a\nspecial 'data-free' case of the PILE score identifies a priori kernel choices\nthat are 'well-adapted' to a given PDE. Beyond the kernel setting, we\nanticipate that the PILE score can be extended to PIML at large, and we outline\napproaches to do so.\n","authors":["Mara Daniels","Liam Hodgkinson","Michael Mahoney"],"pdf_url":"https://arxiv.org/pdf/2510.26121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.19331v2","updated":"2025-10-30T03:42:04Z","published":"2025-09-14T15:24:43Z","title":"Holographic Transformers for Complex-Valued Signal Processing:\n  Integrating Phase Interference into Self-Attention","summary":"  Complex-valued signals encode both amplitude and phase, yet most deep models\ntreat attention as real-valued correlation, overlooking interference effects.\nWe introduce the Holographic Transformer, a physics-inspired architecture that\nincorporates wave interference principles into self-attention. Holographic\nattention modulates interactions by relative phase and coherently superimposes\nvalues, ensuring consistency between amplitude and phase. A dual-headed decoder\nsimultaneously reconstructs the input and predicts task outputs, preventing\nphase collapse when losses prioritize magnitude over phase. We demonstrate that\nholographic attention implements a discrete interference operator and maintains\nphase consistency under linear mixing. Experiments on PolSAR image\nclassification and wireless channel prediction show strong performance,\nachieving high classification accuracy and F1 scores, low regression error, and\nincreased robustness to phase perturbations. These results highlight that\nenforcing physical consistency in attention leads to generalizable improvements\nin complex-valued learning and provides a unified, physics-based framework for\ncoherent signal modeling. The code is available at\nhttps://github.com/EonHao/Holographic-Transformers.\n","authors":["Enhao Huang","Zhiyu Zhang","Tianxiang Xu","Chunshu Xia","Kaichun Hu","Yuchen Yang","Tongtong Pan","Dong Dong","Zhan Qin"],"pdf_url":"https://arxiv.org/pdf/2509.19331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15201v3","updated":"2025-10-30T03:40:48Z","published":"2025-05-21T07:26:36Z","title":"Pass@K Policy Optimization: Solving Harder Reinforcement Learning\n  Problems","summary":"  Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts\nfor each problem and reward them independently. This optimizes for pass@1\nperformance and prioritizes the strength of isolated samples at the expense of\nthe diversity and collective utility of sets of samples. This under-utilizes\nthe sampling capacity, limiting exploration and eventual improvement on harder\nexamples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a\ntransformation on the final rewards which leads to direct optimization of\npass@k performance, thus optimizing for sets of samples that maximize reward\nwhen considered jointly. Our contribution is to derive novel low variance\nunbiased estimators for pass@k and its gradient, in both the binary and\ncontinuous reward settings. We show optimization with our estimators reduces to\nstandard RL with rewards that have been jointly transformed by a stable and\nefficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable\nrobust optimization of pass@k for any arbitrary k <= n. Moreover, instead of\ntrading off pass@1 performance for pass@k gains, our method allows annealing k\nduring training, optimizing both metrics and often achieving strong pass@1\nnumbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the\nvariance reducing properties of our formulations. We also include real-world\nexamples using the open-source LLM, GEMMA-2. We find that our transformation\neffectively optimizes for the target k. Furthermore, higher k values enable\nsolving more and harder problems, while annealing k boosts both the pass@1 and\npass@k . Crucially, for challenging task sets where conventional pass@1\noptimization stalls, our pass@k approach unblocks learning, likely due to\nbetter exploration by prioritizing joint utility over the utility of individual\nsamples.\n","authors":["Christian Walder","Deep Karkhanis"],"pdf_url":"https://arxiv.org/pdf/2505.15201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26109v1","updated":"2025-10-30T03:36:19Z","published":"2025-10-30T03:36:19Z","title":"Do Not Step Into the Same River Twice: Learning to Reason from Trial and\n  Error","summary":"  Reinforcement learning with verifiable rewards (RLVR) has significantly\nboosted the reasoning capability of large language models (LLMs) recently.\nHowever, existing RLVR approaches merely train LLMs based on their own\ngenerated responses and are constrained by the initial capability of LLMs, thus\nprone to exploration stagnation, in which LLMs fail to solve more training\nproblems and cannot further learn from the training data. Some work tries to\naddress this by leveraging off-policy solutions to training problems but\nrequires external guidance from experts which suffers from limited\navailability. In this work, we propose LTE (Learning to reason from Trial and\nError), an approach hinting LLMs with their previously self-generated incorrect\nanswers and problem of overlong responses, which does not require any external\nexpert guidance. Experiments validate the effectiveness of LTE, which\noutperforms the normal group relative policy optimization (GRPO) by 6.38 in\nPass@1 and 9.00 in Pass@k on average across six mathematics benchmarks for\nQwen3-4B-Base. Further analysis confirms that LTE successfully mitigates the\nproblem of exploration stagnation and enhances both exploitation and\nexploration during training.\n","authors":["Chenming Tang","Hsiu-Yuan Huang","Weijie Liu","Saiyong Yang","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2510.26109v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2503.04492v2","updated":"2025-10-30T03:24:21Z","published":"2025-03-06T14:40:21Z","title":"Accurate predictive model of band gap with selected important features\n  based on explainable machine learning","summary":"  In the rapidly advancing field of materials informatics, nonlinear machine\nlearning models have demonstrated exceptional predictive capabilities for\nmaterial properties. However, their black-box nature limits interpretability,\nand they may incorporate features that do not contribute to, or even\ndeteriorate, model performance. This study employs explainable ML (XML)\ntechniques, including permutation feature importance and the SHapley Additive\nexPlanation, applied to a pristine support vector regression model designed to\npredict band gaps at the GW level using 18 input features. Guided by\nXML-derived individual feature importance, a simple framework is proposed to\nconstruct reduced-feature predictive models. Model evaluations indicate that an\nXML-guided compact model, consisting of the top five features, achieves\ncomparable accuracy to the pristine model on in-domain datasets (0.254 vs.\n0.247 eV) while demonstrating superior generalization with lower prediction\nerrors on out-of-domain data (0.461 vs. 0.341 eV). Additionally, the study\nunderscores the necessity for eliminating strongly correlated features\n(correlation coefficient greater than 0.8) to prevent misinterpretation and\noverestimation of feature importance before applying XML. This study highlights\nXML's effectiveness in developing simplified yet highly accurate machine\nlearning models by clarifying feature roles, thereby reducing computational\ncosts for feature acquisition and enhancing model trustworthiness for materials\ndiscovery.\n","authors":["Joohwi Lee","Kaito Miyamoto"],"pdf_url":"https://arxiv.org/pdf/2503.04492v2.pdf","comment":"9 pages, 3 figures, SI is included"},{"id":"http://arxiv.org/abs/2510.26099v1","updated":"2025-10-30T03:22:55Z","published":"2025-10-30T03:22:55Z","title":"SAFE: A Novel Approach to AI Weather Evaluation through Stratified\n  Assessments of Forecasts over Earth","summary":"  The dominant paradigm in machine learning is to assess model performance\nbased on average loss across all samples in some test set. This amounts to\naveraging performance geospatially across the Earth in weather and climate\nsettings, failing to account for the non-uniform distribution of human\ndevelopment and geography. We introduce Stratified Assessments of Forecasts\nover Earth (SAFE), a package for elucidating the stratified performance of a\nset of predictions made over Earth. SAFE integrates various data domains to\nstratify by different attributes associated with geospatial gridpoints:\nterritory (usually country), global subregion, income, and landcover (land or\nwater). This allows us to examine the performance of models for each individual\nstratum of the different attributes (e.g., the accuracy in every individual\ncountry). To demonstrate its importance, we utilize SAFE to benchmark a zoo of\nstate-of-the-art AI-based weather prediction models, finding that they all\nexhibit disparities in forecasting skill across every attribute. We use this to\nseed a benchmark of model forecast fairness through stratification at different\nlead times for various climatic variables. By moving beyond globally-averaged\nmetrics, we for the first time ask: where do models perform best or worst, and\nwhich models are most fair? To support further work in this direction, the SAFE\npackage is open source and available at https://github.com/N-Masi/safe\n","authors":["Nick Masi","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2510.26099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26097v1","updated":"2025-10-30T03:22:13Z","published":"2025-10-30T03:22:13Z","title":"Robust Super-Capacity SRS Channel Inpainting via Diffusion Models","summary":"  Accurate channel state information (CSI) is essential for reliable multiuser\nMIMO operation. In 5G NR, reciprocity-based beamforming via uplink Sounding\nReference Signals (SRS) face resource and coverage constraints, motivating\nsparse non-uniform SRS allocation. Prior masked-autoencoder (MAE) approaches\nimprove coverage but overfit to training masks and degrade under unseen\ndistortions (e.g., additional masking, interference, clipping, non-Gaussian\nnoise). We propose a diffusion-based channel inpainting framework that\nintegrates system-model knowledge at inference via a likelihood-gradient term,\nenabling a single trained model to adapt across mismatched conditions. On\nstandardized CDL channels, the score-based diffusion variant consistently\noutperforms a UNet score-model baseline and the one-step MAE under distribution\nshift, with improvements up to 14 dB NMSE in challenging settings (e.g.,\nLaplace noise, user interference), while retaining competitive accuracy under\nmatched conditions. These results demonstrate that diffusion-guided inpainting\nis a robust and generalizable approach for super-capacity SRS design in 5G NR\nsystems.\n","authors":["Usman Akram","Fan Zhang","Yang Li","Haris Vikalo"],"pdf_url":"https://arxiv.org/pdf/2510.26097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26096v1","updated":"2025-10-30T03:19:59Z","published":"2025-10-30T03:19:59Z","title":"ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for\n  Audio-Language Models","summary":"  Recent advances in Audio-Language Models (ALMs) have significantly improved\nmultimodal understanding capabilities. However, the introduction of the audio\nmodality also brings new and unique vulnerability vectors. Previous studies\nhave proposed jailbreak attacks that specifically target ALMs, revealing that\ndefenses directly transferred from traditional audio adversarial attacks or\ntext-based Large Language Model (LLM) jailbreaks are largely ineffective\nagainst these ALM-specific threats. To address this issue, we propose ALMGuard,\nthe first defense framework tailored to ALMs. Based on the assumption that\nsafety-aligned shortcuts naturally exist in ALMs, we design a method to\nidentify universal Shortcut Activation Perturbations (SAPs) that serve as\ntriggers that activate the safety shortcuts to safeguard ALMs at inference\ntime. To better sift out effective triggers while preserving the model's\nutility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM),\nwhich restricts perturbations to Mel-frequency bins that are sensitive to\njailbreaks but insensitive to speech understanding. Both theoretical analyses\nand empirical results demonstrate the robustness of our method against both\nseen and unseen attacks. Overall, \\MethodName reduces the average success rate\nof advanced ALM-specific jailbreak attacks to 4.6% across four models, while\nmaintaining comparable utility on benign benchmarks, establishing it as the new\nstate of the art. Our code and data are available at\nhttps://github.com/WeifeiJin/ALMGuard.\n","authors":["Weifei Jin","Yuxin Cao","Junjie Su","Minhui Xue","Jie Hao","Ke Xu","Jin Song Dong","Derui Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26096v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26094v1","updated":"2025-10-30T03:09:40Z","published":"2025-10-30T03:09:40Z","title":"Lean4Physics: Comprehensive Reasoning Framework for College-level\n  Physics in Lean4","summary":"  We present **Lean4PHYS**, a comprehensive reasoning framework for\ncollege-level physics problems in Lean4. **Lean4PHYS** includes\n*LeanPhysBench*, a college-level benchmark for formal physics reasoning in\nLean4, which contains 200 hand-crafted and peer-reviewed statements derived\nfrom university textbooks and physics competition problems. To establish a\nsolid foundation for formal reasoning in physics, we also introduce *PhysLib*,\na community-driven repository containing fundamental unit systems and theorems\nessential for formal physics reasoning. Based on the benchmark and Lean4\nrepository we composed in **Lean4PHYS**, we report baseline results using major\nexpert Math Lean4 provers and state-of-the-art closed-source models, with the\nbest performance of DeepSeek-Prover-V2-7B achieving only 16% and\nClaude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that\nour *PhysLib* can achieve an average improvement of 11.75% in model\nperformance. This demonstrates the challenging nature of our *LeanPhysBench*\nand the effectiveness of *PhysLib*. To the best of our knowledge, this is the\nfirst study to provide a physics benchmark in Lean4.\n","authors":["Yuxin Li","Minghao Liu","Ruida Wang","Wenzhao Ji","Zhitao He","Rui Pan","Junming Huang","Tong Zhang","Yi R. Fung"],"pdf_url":"https://arxiv.org/pdf/2510.26094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09969v4","updated":"2025-10-30T02:56:28Z","published":"2025-02-14T07:55:47Z","title":"Neural Networks for Learnable and Scalable Influence Estimation of\n  Instruction Fine-Tuning Data","summary":"  Influence functions provide crucial insights into model training, but\nexisting methods suffer from large computational costs and limited\ngeneralization. Particularly, recent works have proposed various metrics and\nalgorithms to calculate the influence of data using language models, which do\nnot scale well with large models and datasets. This is because of the expensive\nforward and backward passes required for computation, substantial memory\nrequirements to store large models, and poor generalization of influence\nestimates to new data. In this paper, we explore the use of small neural\nnetworks -- which we refer to as the InfluenceNetwork -- to estimate influence\nvalues, achieving up to 99% cost reduction. Our evaluation demonstrates that\ninfluence values can be estimated with models just 0.0027% the size of full\nlanguage models (we use 7B and 8B versions). We apply our algorithm of\nestimating influence values (called NN-CIFT: Neural Networks for effiCient\nInstruction Fine-Tuning) to the downstream task of subset selection for general\ninstruction fine-tuning. In our study, we include four state-of-the-art\ninfluence functions and show no compromise in performance, despite large\nspeedups, between NN-CIFT and the original influence functions. We provide an\nin-depth hyperparameter analyses of NN-CIFT. The code for our method can be\nfound here: https://github.com/agarwalishika/NN-CIFT.\n","authors":["Ishika Agarwal","Dilek Hakkani-Tür"],"pdf_url":"https://arxiv.org/pdf/2502.09969v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24043v2","updated":"2025-10-30T02:55:35Z","published":"2025-10-28T03:53:46Z","title":"Localized Kernel Projection Outlyingness: A Two-Stage Approach for\n  Multi-Modal Outlier Detection","summary":"  This paper presents Two-Stage LKPLO, a novel multi-stage outlier detection\nframework that overcomes the coexisting limitations of conventional\nprojection-based methods: their reliance on a fixed statistical metric and\ntheir assumption of a single data structure. Our framework uniquely synthesizes\nthree key concepts: (1) a generalized loss-based outlyingness measure (PLO)\nthat replaces the fixed metric with flexible, adaptive loss functions like our\nproposed SVM-like loss; (2) a global kernel PCA stage to linearize non-linear\ndata structures; and (3) a subsequent local clustering stage to handle\nmulti-modal distributions. Comprehensive 5-fold cross-validation experiments on\n10 benchmark datasets, with automated hyperparameter optimization, demonstrate\nthat Two-Stage LKPLO achieves state-of-the-art performance. It significantly\noutperforms strong baselines on datasets with challenging structures where\nexisting methods fail, most notably on multi-cluster data (Optdigits) and\ncomplex, high-dimensional data (Arrhythmia). Furthermore, an ablation study\nempirically confirms that the synergistic combination of both the kernelization\nand localization stages is indispensable for its superior performance. This\nwork contributes a powerful new tool for a significant class of outlier\ndetection problems and underscores the importance of hybrid, multi-stage\narchitectures.\n","authors":["Akira Tamamori"],"pdf_url":"https://arxiv.org/pdf/2510.24043v2.pdf","comment":"10 pages, 4 figures; submitted to The IEICE Transactions on\n  Information and Systems"},{"id":"http://arxiv.org/abs/2510.25327v2","updated":"2025-10-30T02:51:38Z","published":"2025-10-29T09:41:03Z","title":"MMEdge: Accelerating On-device Multimodal Inference via Pipelined\n  Sensing and Encoding","summary":"  Real-time multimodal inference on resource-constrained edge devices is\nessential for applications such as autonomous driving, human-computer\ninteraction, and mobile health. However, prior work often overlooks the tight\ncoupling between sensing dynamics and model execution, as well as the complex\ninter-modality dependencies. In this paper, we propose MMEdge, an new on-device\nmulti-modal inference framework based on pipelined sensing and encoding.\nInstead of waiting for complete sensor inputs, MMEdge decomposes the entire\ninference process into a sequence of fine-grained sensing and encoding units,\nallowing computation to proceed incrementally as data arrive. MMEdge also\nintroduces a lightweight but effective temporal aggregation module that\ncaptures rich temporal dynamics across different pipelined units to maintain\naccuracy performance. Such pipelined design also opens up opportunities for\nfine-grained cross-modal optimization and early decision-making during\ninference. To further enhance system performance under resource variability and\ninput data complexity, MMEdge incorporates an adaptive multimodal configuration\noptimizer that dynamically selects optimal sensing and model configurations for\neach modality under latency constraints, and a cross-modal speculative skipping\nmechanism that bypasses future units of slower modalities when early\npredictions reach sufficient confidence. We evaluate MMEdge using two public\nmultimodal datasets and deploy it on a real-world unmanned aerial vehicle\n(UAV)-based multimodal testbed. The results show that MMEdge significantly\nreduces end-to-end latency while maintaining high task accuracy across various\nsystem and data dynamics.\n","authors":["Runxi Huang","Mingxuan Yu","Mingyu Tsoi","Xiaomin Ouyang"],"pdf_url":"https://arxiv.org/pdf/2510.25327v2.pdf","comment":"Code available at: https://github.com/HKUST-MINSys-Lab/MMEdge.\n  Accepted by SenSys 2026"},{"id":"http://arxiv.org/abs/2510.26089v1","updated":"2025-10-30T02:49:46Z","published":"2025-10-30T02:49:46Z","title":"Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle\n  Routing","summary":"  Traffic congestion in urban road networks leads to longer trip times and\nhigher emissions, especially during peak periods. While the Shortest Path First\n(SPF) algorithm is optimal for a single vehicle in a static network, it\nperforms poorly in dynamic, multi-vehicle settings, often worsening congestion\nby routing all vehicles along identical paths. We address dynamic vehicle\nrouting through a multi-agent reinforcement learning (MARL) framework for\ncoordinated, network-aware fleet navigation. We first propose Adaptive\nNavigation (AN), a decentralized MARL model where each intersection agent\nprovides routing guidance based on (i) local traffic and (ii) neighborhood\nstate modeled using Graph Attention Networks (GAT). To improve scalability in\nlarge networks, we further propose Hierarchical Hub-based Adaptive Navigation\n(HHAN), an extension of AN that assigns agents only to key intersections\n(hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles\nmicro-routing within each hub region. For hub coordination, HHAN adopts\ncentralized training with decentralized execution (CTDE) under the Attentive\nQ-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions\nvia attention. Hub agents use flow-aware state features that combine local\ncongestion and predictive dynamics for proactive routing. Experiments on\nsynthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces\naverage travel time versus SPF and learning baselines, maintaining 100% routing\nsuccess. HHAN scales to networks with hundreds of intersections, achieving up\nto 15.9% improvement under heavy traffic. These findings highlight the\npotential of network-constrained MARL for scalable, coordinated, and\ncongestion-aware routing in intelligent transportation systems.\n","authors":["Fazel Arasteh","Arian Haghparast","Manos Papagelis"],"pdf_url":"https://arxiv.org/pdf/2510.26089v1.pdf","comment":"29 pages, 12 figures. Fazel Arasteh and Arian Haghparast contributed\n  equally to this research. Submitted to ACM Transactions on Spatial Algorithms\n  and Systems (TSAS). The code for this work is publicly available at\n  https://github.com/Arianhgh/HHAN"},{"id":"http://arxiv.org/abs/2510.26086v1","updated":"2025-10-30T02:47:25Z","published":"2025-10-30T02:47:25Z","title":"LLMBisect: Breaking Barriers in Bug Bisection with A Comparative\n  Analysis Pipeline","summary":"  Bug bisection has been an important security task that aims to understand the\nrange of software versions impacted by a bug, i.e., identifying the commit that\nintroduced the bug. However, traditional patch-based bisection methods are\nfaced with several significant barriers: For example, they assume that the\nbug-inducing commit (BIC) and the patch commit modify the same functions, which\nis not always true. They often rely solely on code changes, while the commit\nmessage frequently contains a wealth of vulnerability-related information. They\nare also based on simple heuristics (e.g., assuming the BIC initializes lines\ndeleted in the patch) and lack any logical analysis of the vulnerability.\n  In this paper, we make the observation that Large Language Models (LLMs) are\nwell-positioned to break the barriers of existing solutions, e.g., comprehend\nboth textual data and code in patches and commits. Unlike previous BIC\nidentification approaches, which yield poor results, we propose a comprehensive\nmulti-stage pipeline that leverages LLMs to: (1) fully utilize patch\ninformation, (2) compare multiple candidate commits in context, and (3)\nprogressively narrow down the candidates through a series of down-selection\nsteps. In our evaluation, we demonstrate that our approach achieves\nsignificantly better accuracy than the state-of-the-art solution by more than\n38\\%. Our results further confirm that the comprehensive multi-stage pipeline\nis essential, as it improves accuracy by 60\\% over a baseline LLM-based\nbisection method.\n","authors":["Zheng Zhang","Haonan Li","Xingyu Li","Hang Zhang","Zhiyun Qian"],"pdf_url":"https://arxiv.org/pdf/2510.26086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26083v1","updated":"2025-10-30T02:41:54Z","published":"2025-10-30T02:41:54Z","title":"Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism","summary":"  Specialized Generalist Models (SGMs) aim to preserve broad capabilities while\nachieving expert-level performance in target domains. However, traditional LLM\nstructures including Transformer, Linear Attention, and hybrid models do not\nemploy specialized memory mechanism guided by task information. In this paper,\nwe present Nirvana, an SGM with specialized memory mechanism, linear time\ncomplexity, and test-time task information extraction. Besides, we propose the\nTask-Aware Memory Trigger ($\\textit{Trigger}$) that flexibly adjusts memory\nmechanism based on the current task's requirements. In Trigger, each incoming\nsample is treated as a self-supervised fine-tuning task, enabling Nirvana to\nadapt its task-related parameters on the fly to domain shifts. We also design\nthe Specialized Memory Updater ($\\textit{Updater}$) that dynamically memorizes\nthe context guided by Trigger. We conduct experiments on both general language\ntasks and specialized medical tasks. On a variety of natural language modeling\nbenchmarks, Nirvana achieves competitive or superior results compared to the\nexisting LLM structures. To prove the effectiveness of Trigger on specialized\ntasks, we test Nirvana's performance on a challenging medical task, i.e.,\nMagnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with\nlightweight codecs on paired electromagnetic signals and MRI images. Despite\nthe frozen Nirvana backbone, Trigger guides the model to adapt to the MRI\ndomain with the change of task-related parameters. Nirvana achieves\nhigher-quality MRI reconstruction compared to conventional MRI models as well\nas the models with traditional LLMs' backbone, and can also generate accurate\npreliminary clinical reports accordingly.\n","authors":["Yuhua Jiang","Shuang Cheng","Yihao Liu","Ermo Hua","Che Jiang","Weigao Sun","Yu Cheng","Feifei Gao","Biqing Qi","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25114v2","updated":"2025-10-30T02:24:17Z","published":"2025-10-29T02:26:41Z","title":"Energy Approach from $\\varepsilon$-Graph to Continuum Diffusion Model\n  with Connectivity Functional","summary":"  We derive an energy-based continuum limit for $\\varepsilon$-graphs endowed\nwith a general connectivity functional. We prove that the discrete energy and\nits continuum counterpart differ by at most $O(\\varepsilon)$; the prefactor\ninvolves only the $W^{1,1}$-norm of the connectivity density as\n$\\varepsilon\\to0$, so the error bound remains valid even when that density has\nstrong local fluctuations. As an application, we introduce a neural-network\nprocedure that reconstructs the connectivity density from edge-weight data and\nthen embeds the resulting continuum model into a brain-dynamics framework. In\nthis setting, the usual constant diffusion coefficient is replaced by the\nspatially varying coefficient produced by the learned density, yielding\ndynamics that differ significantly from those obtained with conventional\nconstant-diffusion models.\n","authors":["Yahong Yang","Sun Lee","Jeff Calder","Wenrui Hao"],"pdf_url":"https://arxiv.org/pdf/2510.25114v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00706v2","updated":"2025-10-30T02:22:30Z","published":"2025-02-02T07:39:37Z","title":"Model Provenance Testing for Large Language Models","summary":"  Large language models are increasingly customized through fine-tuning and\nother adaptations, creating challenges in enforcing licensing terms and\nmanaging downstream impacts. Tracking model origins is crucial both for\nprotecting intellectual property and for identifying derived models when biases\nor vulnerabilities are discovered in foundation models. We address this\nchallenge by developing a framework for testing model provenance: Whether one\nmodel is derived from another. Our approach is based on the key observation\nthat real-world model derivations preserve significant similarities in model\noutputs that can be detected through statistical analysis. Using only black-box\naccess to models, we employ multiple hypothesis testing to compare model\nsimilarities against a baseline established by unrelated models. On two\ncomprehensive real-world benchmarks spanning models from 30M to 4B parameters\nand comprising over 600 models, our tester achieves 90-95% precision and 80-90%\nrecall in identifying derived models. These results demonstrate the viability\nof systematic provenance verification in production environments even when only\nAPI access is available.\n","authors":["Ivica Nikolic","Teodora Baluta","Prateek Saxena"],"pdf_url":"https://arxiv.org/pdf/2502.00706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26076v1","updated":"2025-10-30T02:21:59Z","published":"2025-10-30T02:21:59Z","title":"New Money: A Systematic Review of Synthetic Data Generation for Finance","summary":"  Synthetic data generation has emerged as a promising approach to address the\nchallenges of using sensitive financial data in machine learning applications.\nBy leveraging generative models, such as Generative Adversarial Networks (GANs)\nand Variational Autoencoders (VAEs), it is possible to create artificial\ndatasets that preserve the statistical properties of real financial records\nwhile mitigating privacy risks and regulatory constraints. Despite the rapid\ngrowth of this field, a comprehensive synthesis of the current research\nlandscape has been lacking. This systematic review consolidates and analyses 72\nstudies published since 2018 that focus on synthetic financial data generation.\nWe categorise the types of financial information synthesised, the generative\nmethods employed, and the evaluation strategies used to assess data utility and\nprivacy. The findings indicate that GAN-based approaches dominate the\nliterature, particularly for generating time-series market data and tabular\ncredit data. While several innovative techniques demonstrate potential for\nimproved realism and privacy preservation, there remains a notable lack of\nrigorous evaluation of privacy safeguards across studies. By providing an\nintegrated overview of generative techniques, applications, and evaluation\nmethods, this review highlights critical research gaps and offers guidance for\nfuture work aimed at developing robust, privacy-preserving synthetic data\nsolutions for the financial domain.\n","authors":["James Meldrum","Basem Suleiman","Fethi Rabhi","Muhammad Johan Alibasa"],"pdf_url":"https://arxiv.org/pdf/2510.26076v1.pdf","comment":"37 pages, 5 figures, 21 tables"},{"id":"http://arxiv.org/abs/2502.01074v3","updated":"2025-10-30T02:03:05Z","published":"2025-02-03T05:33:51Z","title":"Omni-Mol: Multitask Molecular Model for Any-to-any Modalities","summary":"  In the molecular domain, numerous studies have explored the use of multimodal\nlarge language models (LLMs) to construct a general-purpose, multi-task\nmolecular model. However, these efforts are still far from achieving a truly\nuniversal molecular model. We identify three key challenges in this endeavor:\n(1) Existing molecular task datasets are typically small in scale and lack\ncomprehensive domain coverage. (2) Tasks from different molecular subfields are\ndifficult to effectively learn jointly through LLMs due to significant\ndistributional shifts and competition among tasks, which introduces instability\nin the learning process. (3) Both inter-task and intra-task molecular\nrepresentations demand different intrinsic dimensions in the language space,\nmaking it challenging to balance between redundancy and insufficiency in\nlanguage model representations. To address these challenges, we innovatively\ncategorize existing small-molecule tasks into four types: Mol2Mol, Mol2Text,\nMol2Num, and Text2Mol. We then collect a dataset encompassing over 16 tasks\nwith more than 1.4 million samples, making it the largest molecular\ninstruction-tuning dataset to date. Leveraging the extensive pretraining of\nLLMs on existing chemical literature, we propose a novel multimodal LLM\nframework, named Omni-Mol, which unifies all small-molecule tasks and supports\nboth molecular generation and understanding. The core of Omni-Mol is our\nproposed MoGE, which dynamically adapts to the intrinsic rank of different\ntasks. This mixture-of-experts architecture enhances the model's ability to\nhandle diverse tasks and modalities effectively. Our model achieves unified\ninstruction tuning across 16 tasks and attains state-of-the-art performance on\n13 of them. Extensive experiments further demonstrate the scalability and\nversatility of Omni-Mol.\n","authors":["Chengxin Hu","Hao Li","Yihe Yuan","Zezheng Song","Chenyang Zhao","Haixin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.01074v3.pdf","comment":"44 pages, 9 figures, 13 tables, paper accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26068v1","updated":"2025-10-30T01:53:32Z","published":"2025-10-30T01:53:32Z","title":"Learning Geometry: A Framework for Building Adaptive Manifold Models\n  through Metric Optimization","summary":"  This paper proposes a novel paradigm for machine learning that moves beyond\ntraditional parameter optimization. Unlike conventional approaches that search\nfor optimal parameters within a fixed geometric space, our core idea is to\ntreat the model itself as a malleable geometric entity. Specifically, we\noptimize the metric tensor field on a manifold with a predefined topology,\nthereby dynamically shaping the geometric structure of the model space. To\nachieve this, we construct a variational framework whose loss function\ncarefully balances data fidelity against the intrinsic geometric complexity of\nthe manifold. The former ensures the model effectively explains observed data,\nwhile the latter acts as a regularizer, penalizing overly curved or irregular\ngeometries to encourage simpler models and prevent overfitting. To address the\ncomputational challenges of this infinite-dimensional optimization problem, we\nintroduce a practical method based on discrete differential geometry: the\ncontinuous manifold is discretized into a triangular mesh, and the metric\ntensor is parameterized by edge lengths, enabling efficient optimization using\nautomatic differentiation tools. Theoretical analysis reveals a profound\nanalogy between our framework and the Einstein-Hilbert action in general\nrelativity, providing an elegant physical interpretation for the concept of\n\"data-driven geometry\". We further argue that even with fixed topology, metric\noptimization offers significantly greater expressive power than models with\nfixed geometry. This work lays a solid foundation for constructing fully\ndynamic \"meta-learners\" capable of autonomously evolving their geometry and\ntopology, and it points to broad application prospects in areas such as\nscientific model discovery and robust representation learning.\n","authors":["Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26068v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2509.17784v2","updated":"2025-10-30T01:43:48Z","published":"2025-09-22T13:45:17Z","title":"Revealing Multimodal Causality with Large Language Models","summary":"  Uncovering cause-and-effect mechanisms from data is fundamental to scientific\nprogress. While large language models (LLMs) show promise for enhancing causal\ndiscovery (CD) from unstructured data, their application to the increasingly\nprevalent multimodal setting remains a critical challenge. Even with the advent\nof multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two\nprimary limitations: (1) difficulty in exploring intra- and inter-modal\ninteractions for comprehensive causal variable identification; and (2)\ninsufficiency to handle structural ambiguities with purely observational data.\nTo address these challenges, we propose MLLM-CD, a novel framework for\nmultimodal causal discovery from unstructured data. It consists of three key\ncomponents: (1) a novel contrastive factor discovery module to identify genuine\nmultimodal factors based on the interactions explored from contrastive sample\npairs; (2) a statistical causal structure discovery module to infer causal\nrelationships among discovered factors; and (3) an iterative multimodal\ncounterfactual reasoning module to refine the discovery outcomes iteratively by\nincorporating the world knowledge and reasoning capabilities of MLLMs.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\neffectiveness of the proposed MLLM-CD in revealing genuine factors and causal\nrelationships among them from multimodal unstructured data.\n","authors":["Jin Li","Shoujin Wang","Qi Zhang","Feng Liu","Tongliang Liu","Longbing Cao","Shui Yu","Fang Chen"],"pdf_url":"https://arxiv.org/pdf/2509.17784v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2501.14808v4","updated":"2025-10-30T01:41:07Z","published":"2025-01-15T16:32:27Z","title":"HyGen: Efficient LLM Serving via Elastic Online-Offline Request\n  Co-location","summary":"  Large language models (LLMs) have facilitated a wide range of applications\nwith distinct service-level objectives (SLOs), from latency-sensitive online\ntasks like interactive chatbots to throughput-oriented offline workloads like\ndata synthesis. The existing deployment model, which dedicates machines to each\nworkload, simplifies SLO management but often leads to poor resource\nutilization. This paper introduces HyGen, an interference-aware LLM serving\nsystem that enables efficient co-location of online and offline workloads while\npreserving SLOs. HyGen incorporates two key innovations: (1) performance\ncontrol mechanisms, including a latency predictor to estimate batch execution\ntime and an SLO-aware profiler to quantify latency interference, and (2)\nSLO-aware offline scheduling policies that maximize serving throughput and\nprevent starvation. Our evaluation on production workloads shows that HyGen\nachieves up to 3.9-5.8x throughput gains over online and hybrid serving\nbaselines, while ensuring latency SLOs. The code of HyGen is publicly available\nat https://github.com/UIUC-MLSys/HyGen.\n","authors":["Ting Sun","Penghan Wang","Fan Lai"],"pdf_url":"https://arxiv.org/pdf/2501.14808v4.pdf","comment":"NeurIPS 2025. 21 pages, 17 figures"},{"id":"http://arxiv.org/abs/2505.11411v2","updated":"2025-10-30T01:39:13Z","published":"2025-05-16T16:20:02Z","title":"Is Grokking a Computational Glass Relaxation?","summary":"  Understanding neural network's (NN) generalizability remains a central\nquestion in deep learning research. The special phenomenon of grokking, where\nNNs abruptly generalize long after the training performance reaches a\nnear-perfect level, offers a unique window to investigate the underlying\nmechanisms of NNs' generalizability. Here we propose an interpretation for\ngrokking by framing it as a computational glass relaxation: viewing NNs as a\nphysical system where parameters are the degrees of freedom and train loss is\nthe system energy, we find memorization process resembles a rapid cooling of\nliquid into non-equilibrium glassy state at low temperature and the later\ngeneralization is like a slow relaxation towards a more stable configuration.\nThis mapping enables us to sample NNs' Boltzmann entropy (states of density)\nlandscape as a function of training loss and test accuracy. Our experiments in\ntransformers on arithmetic tasks suggests that there is NO entropy barrier in\nthe memorization-to-generalization transition of grokking, challenging previous\ntheory that defines grokking as a first-order phase transition. We identify a\nhigh-entropy advantage under grokking, an extension of prior work linking\nentropy to generalizability but much more significant. Inspired by grokking's\nfar-from-equilibrium nature, we develop a toy optimizer WanD based on\nWang-landau molecular dynamics, which can eliminate grokking without any\nconstraints and find high-norm generalizing solutions. This provides\nstrictly-defined counterexamples to theory attributing grokking solely to\nweight norm evolution towards the Goldilocks zone and also suggests new\npotential ways for optimizer design.\n","authors":["Xiaotian Zhang","Yue Shang","Entao Yang","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.11411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26064v1","updated":"2025-10-30T01:36:44Z","published":"2025-10-30T01:36:44Z","title":"Towards Scaling Laws for Symbolic Regression","summary":"  Symbolic regression (SR) aims to discover the underlying mathematical\nexpressions that explain observed data. This holds promise for both gaining\nscientific insight and for producing inherently interpretable and generalizable\nmodels for tabular data. In this work we focus on the basics of SR. Deep\nlearning-based SR has recently become competitive with genetic programming\napproaches, but the role of scale has remained largely unexplored. Inspired by\nscaling laws in language modeling, we present the first systematic\ninvestigation of scaling in SR, using a scalable end-to-end transformer\npipeline and carefully generated training data. Across five different model\nsizes and spanning three orders of magnitude in compute, we find that both\nvalidation loss and solved rate follow clear power-law trends with compute. We\nfurther identify compute-optimal hyperparameter scaling: optimal batch size and\nlearning rate grow with model size, and a token-to-parameter ratio of\n$\\approx$15 is optimal in our regime, with a slight upward trend as compute\nincreases. These results demonstrate that SR performance is largely predictable\nfrom compute and offer important insights for training the next generation of\nSR models.\n","authors":["David Otte","Jörg K. H. Franke","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2510.26064v1.pdf","comment":"Accepted at the NeurIPS 2025 Math-AI Workshop"},{"id":"http://arxiv.org/abs/2510.26061v1","updated":"2025-10-30T01:32:21Z","published":"2025-10-30T01:32:21Z","title":"Data-driven Projection Generation for Efficiently Solving Heterogeneous\n  Quadratic Programming Problems","summary":"  We propose a data-driven framework for efficiently solving quadratic\nprogramming (QP) problems by reducing the number of variables in\nhigh-dimensional QPs using instance-specific projection. A graph neural\nnetwork-based model is designed to generate projections tailored to each QP\ninstance, enabling us to produce high-quality solutions even for previously\nunseen problems. The model is trained on heterogeneous QPs to minimize the\nexpected objective value evaluated on the projected solutions. This is\nformulated as a bilevel optimization problem; the inner optimization solves the\nQP under a given projection using a QP solver, while the outer optimization\nupdates the model parameters. We develop an efficient algorithm to solve this\nbilevel optimization problem, which computes parameter gradients without\nbackpropagating through the solver. We provide a theoretical analysis of the\ngeneralization ability of solving QPs with projection matrices generated by\nneural networks. Experimental results demonstrate that our method produces\nhigh-quality feasible solutions with reduced computation time, outperforming\nexisting methods.\n","authors":["Tomoharu Iwata","Futoshi Futami"],"pdf_url":"https://arxiv.org/pdf/2510.26061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03710v3","updated":"2025-10-30T01:16:06Z","published":"2025-03-05T18:01:05Z","title":"Improving LLM Safety Alignment with Dual-Objective Optimization","summary":"  Existing training-time safety alignment techniques for large language models\n(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization\n(DPO), a widely deployed alignment method, exhibits limitations in both\nexperimental and theoretical contexts as its loss function proves suboptimal\nfor refusal learning. Through gradient-based analysis, we identify these\nshortcomings and propose an improved safety alignment that disentangles DPO\nobjectives into two components: (1) robust refusal training, which encourages\nrefusal even when partial unsafe generations are produced, and (2) targeted\nunlearning of harmful knowledge. This approach significantly increases LLM\nrobustness against a wide range of jailbreak attacks, including prefilling,\nsuffix, and multi-turn attacks across both in-distribution and\nout-of-distribution scenarios. Furthermore, we introduce a method to emphasize\ncritical refusal tokens by incorporating a reward-based token-level weighting\nmechanism for refusal learning, which further improves the robustness against\nadversarial exploits. Our research also suggests that robustness to jailbreak\nattacks is correlated with token distribution shifts in the training process\nand internal representations of refusal and harmful tokens, offering valuable\ndirections for future research in LLM safety alignment. The code is available\nat https://github.com/wicai24/DOOR-Alignment\n","authors":["Xuandong Zhao","Will Cai","Tianneng Shi","David Huang","Licong Lin","Song Mei","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2503.03710v3.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2510.24889v2","updated":"2025-10-30T00:55:31Z","published":"2025-10-28T18:48:48Z","title":"Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep\n  Q-learning thresholding","summary":"  Rapid triage of suspected stroke needs accurate, bedside-deployable tools;\nEEG is promising but underused at first contact. We present an adaptive\nmultitask EEG classifier that converts 32-channel signals to power spectral\ndensity features (Welch), uses a recurrent-convolutional network (GRU-TCN) to\npredict stroke type (healthy, ischemic, hemorrhagic), hemispheric\nlateralization, and severity, and applies a deep Q-network (DQN) to tune\ndecision thresholds in real time. Using a patient-wise split of the UCLH Stroke\nEIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the\nprimary outcome was stroke-type performance; secondary outcomes were severity\nand lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for\nstroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%)\nfor lateralization. With DQN threshold adaptation, stroke-type accuracy\nincreased to about 98.0% (F1 97.7%). We also tested robustness on an\nindependent, low-density EEG cohort (ZJU4H) and report paired patient-level\nstatistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies\n(index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis;\npatient-wise evaluation). Adaptive thresholding shifts the operating point to\nclinically preferred sensitivity-specificity trade-offs, while integrated\nscalp-map and spectral visualizations support interpretability.\n","authors":["Shakeel Abdulkareem","Bora Yimenicioglu","Khartik Uppalapati","Aneesh Gudipati","Adan Eftekhari","Saleh Yassin"],"pdf_url":"https://arxiv.org/pdf/2510.24889v2.pdf","comment":"10 pages, 6 figures. Equal contribution: Shakeel Abdulkareem and Bora\n  Yimenicioglu. Compiled with pdfLaTeX (wlscirep class)"},{"id":"http://arxiv.org/abs/2510.26046v1","updated":"2025-10-30T00:52:25Z","published":"2025-10-30T00:52:25Z","title":"Bias-Corrected Data Synthesis for Imbalanced Learning","summary":"  Imbalanced data, where the positive samples represent only a small proportion\ncompared to the negative samples, makes it challenging for classification\nproblems to balance the false positive and false negative rates. A common\napproach to addressing the challenge involves generating synthetic data for the\nminority group and then training classification models with both observed and\nsynthetic data. However, since the synthetic data depends on the observed data\nand fails to replicate the original data distribution accurately, prediction\naccuracy is reduced when the synthetic data is naively treated as the true\ndata. In this paper, we address the bias introduced by synthetic data and\nprovide consistent estimators for this bias by borrowing information from the\nmajority group. We propose a bias correction procedure to mitigate the adverse\neffects of synthetic data, enhancing prediction accuracy while avoiding\noverfitting. This procedure is extended to broader scenarios with imbalanced\ndata, such as imbalanced multi-task learning and causal inference. Theoretical\nproperties, including bounds on bias estimation errors and improvements in\nprediction accuracy, are provided. Simulation results and data analysis on\nhandwritten digit datasets demonstrate the effectiveness of our method.\n","authors":["Pengfei Lyu","Zhengchi Ma","Linjun Zhang","Anru R. Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26046v1.pdf","comment":"41 pages, 4 figures, includes proofs and appendix"},{"id":"http://arxiv.org/abs/2510.26043v1","updated":"2025-10-30T00:44:56Z","published":"2025-10-30T00:44:56Z","title":"$L_1$-norm Regularized Indefinite Kernel Logistic Regression","summary":"  Kernel logistic regression (KLR) is a powerful classification method widely\napplied across diverse domains. In many real-world scenarios, indefinite\nkernels capture more domain-specific structural information than positive\ndefinite kernels. This paper proposes a novel $L_1$-norm regularized indefinite\nkernel logistic regression (RIKLR) model, which extends the existing IKLR\nframework by introducing sparsity via an $L_1$-norm penalty. The introduction\nof this regularization enhances interpretability and generalization while\nintroducing nonsmoothness and nonconvexity into the optimization landscape. To\naddress these challenges, a theoretically grounded and computationally\nefficient proximal linearized algorithm is developed. Experimental results on\nmultiple benchmark datasets demonstrate the superior performance of the\nproposed method in terms of both accuracy and sparsity.\n","authors":["Shaoxin Wang","Hanjing Yao"],"pdf_url":"https://arxiv.org/pdf/2510.26043v1.pdf","comment":"17 pages, 1 figure"},{"id":"http://arxiv.org/abs/2510.26040v1","updated":"2025-10-30T00:38:18Z","published":"2025-10-30T00:38:18Z","title":"Accelerating Real-World Overtaking in F1TENTH Racing Employing\n  Reinforcement Learning Methods","summary":"  While autonomous racing performance in Time-Trial scenarios has seen\nsignificant progress and development, autonomous wheel-to-wheel racing and\novertaking are still severely limited. These limitations are particularly\napparent in real-life driving scenarios where state-of-the-art algorithms\nstruggle to safely or reliably complete overtaking manoeuvres. This is\nimportant, as reliable navigation around other vehicles is vital for safe\nautonomous wheel-to-wheel racing. The F1Tenth Competition provides a useful\nopportunity for developing wheel-to-wheel racing algorithms on a standardised\nphysical platform. The competition format makes it possible to evaluate\novertaking and wheel-to-wheel racing algorithms against the state-of-the-art.\nThis research presents a novel racing and overtaking agent capable of learning\nto reliably navigate a track and overtake opponents in both simulation and\nreality. The agent was deployed on an F1Tenth vehicle and competed against\nopponents running varying competitive algorithms in the real world. The results\ndemonstrate that the agent's training against opponents enables deliberate\novertaking behaviours with an overtaking rate of 87% compared 56% for an agent\ntrained just to race.\n","authors":["Emily Steiner","Daniel van der Spuy","Futian Zhou","Afereti Pama","Minas Liarokapis","Henry Williams"],"pdf_url":"https://arxiv.org/pdf/2510.26040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26038v1","updated":"2025-10-30T00:34:16Z","published":"2025-10-30T00:34:16Z","title":"Do Students Debias Like Teachers? On the Distillability of Bias\n  Mitigation Methods","summary":"  Knowledge distillation (KD) is an effective method for model compression and\ntransferring knowledge between models. However, its effect on model's\nrobustness against spurious correlations that degrade performance on\nout-of-distribution data remains underexplored. This study investigates the\neffect of knowledge distillation on the transferability of ``debiasing''\ncapabilities from teacher models to student models on natural language\ninference (NLI) and image classification tasks. Through extensive experiments,\nwe illustrate several key findings: (i) overall the debiasing capability of a\nmodel is undermined post-KD; (ii) training a debiased model does not benefit\nfrom injecting teacher knowledge; (iii) although the overall robustness of a\nmodel may remain stable post-distillation, significant variations can occur\nacross different types of biases; and (iv) we pin-point the internal attention\npattern and circuit that causes the distinct behavior post-KD. Given the above\nfindings, we propose three effective solutions to improve the distillability of\ndebiasing methods: developing high quality data for augmentation, implementing\niterative knowledge distillation, and initializing student models with weights\nobtained from teacher models. To the best of our knowledge, this is the first\nstudy on the effect of KD on debiasing and its interenal mechanism at scale.\nOur findings provide understandings on how KD works and how to design better\ndebiasing methods.\n","authors":["Jiali Cheng","Chirag Agarwal","Hadi Amiri"],"pdf_url":"https://arxiv.org/pdf/2510.26038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12869v4","updated":"2025-10-30T00:34:12Z","published":"2024-10-14T01:57:25Z","title":"Language Model Preference Evaluation with Multiple Weak Evaluators","summary":"  Despite the remarkable success of Large Language Models (LLMs), evaluating\ntheir outputs' quality regarding preference remains a critical challenge. While\nexisting works usually leverage a strong LLM as the judge for comparing LLMs'\nresponse pairwisely, such a single-evaluator approach is vulnerable to cyclic\npreference, i.e., output A is better than B, B than C, but C is better than A,\ncausing contradictory evaluation results. To address this, we introduce PGED\n(Preference Graph Ensemble and Denoise), a novel approach that leverages\nmultiple model-based evaluators to construct preference graphs, and then\nensembles and denoises these graphs for acyclic, non-contradictory evaluation\nresults. We provide theoretical guarantees for our framework, demonstrating its\nefficacy in recovering the ground truth preference structure. Extensive\nexperiments on ten benchmarks demonstrate PGED 's superiority in three\napplications: 1) model ranking for evaluation, 2) response selection for\ntest-time scaling, and 3) data selection for model fine-tuning. Notably, PGED\ncombines small LLM evaluators (e.g., Llama3-8B, Mistral-7B, Qwen2-7B) to\noutperform strong ones (e.g., Qwen2-72B), showcasing its effectiveness in\nenhancing evaluation reliability and improving model performance.\n","authors":["Zhengyu Hu","Jieyu Zhang","Zhihan Xiong","Alexander Ratner","Kaize Ding","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2410.12869v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26672v1","updated":"2025-10-30T16:42:09Z","published":"2025-10-30T16:42:09Z","title":"Action-Driven Processes for Continuous-Time Control","summary":"  At the heart of reinforcement learning are actions -- decisions made in\nresponse to observations of the environment. Actions are equally fundamental in\nthe modeling of stochastic processes, as they trigger discontinuous state\ntransitions and enable the flow of information through large, complex systems.\nIn this paper, we unify the perspectives of stochastic processes and\nreinforcement learning through action-driven processes, and illustrate their\napplication to spiking neural networks. Leveraging ideas from\ncontrol-as-inference, we show that minimizing the Kullback-Leibler divergence\nbetween a policy-driven true distribution and a reward-driven model\ndistribution for a suitably defined action-driven process is equivalent to\nmaximum entropy reinforcement learning.\n","authors":["Ruimin He","Shaowei Lin"],"pdf_url":"https://arxiv.org/pdf/2510.26672v1.pdf","comment":null}]}}